

# **The New Engineering Paradigm: A Formal Research Proposal on the Transition from Prompt Engineering to Context Engineering and V2V Methodologies**

## **Section 1: Introduction: From Linguistic Tuning to Systems Architecture in AI Interaction**

The advent of large language models (LLMs) has catalyzed a rapid evolution in the methodologies used to build intelligent applications. Initially, the primary interface for eliciting desired behavior from these models was **prompt engineering**, a practice centered on the meticulous crafting of linguistic instructions. This approach, while foundational, is increasingly being subsumed by a more mature, robust, and scalable discipline: **context engineering**. This report posits that the evolution from prompt engineering to context engineering is not a mere terminological shift but a fundamental paradigm change, representing the maturation of applied AI from a craft-based, linguistic art to a formal, systems-design discipline. It marks a transition from focusing on "what you say" to a model in a single turn to architecting "what the model knows when you say it".1  
This research plan proposes a comprehensive discovery, analysis, and synthesis of public content to map this paradigm shift. The analysis will be anchored by the influential definition of context engineering provided by researcher Andrej Karpathy: "the delicate art and science of filling the context window with just the right information for the next step".3 This framing moves the focus from the user's immediate query to the carefully curated informational environment the model operates within, ensuring it receives the right data, in the right format, at the right time.3

### **Defining the Paradigms**

To establish a clear framework, this report will define the two paradigms as distinct points on a continuum of AI system design.6  
**Prompt Engineering as "Linguistic Tuning"** will be characterized as the practice of influencing an LLM's output through the precise phrasing of instructions, the provision of illustrative examples (few-shot prompting), and the structuring of reasoning patterns (chain-of-thought).6 It is an iterative process of adjusting language, role assignments (e.g., "You are a professional translator"), and formatting constraints to guide the model's response within a single interaction.6 While powerful, these methods are often brittle, with small variations in wording leading to significant differences in output quality.6  
**Context Engineering as "Systems Thinking"** will be defined as the systematic discipline of designing, organizing, orchestrating, and optimizing the complete informational payload provided to an LLM at inference time.3 This holistic perspective transcends a single instruction to encompass the entire information ecosystem an AI system requires for reliable and consistent performance. It involves constructing automated pipelines that aggregate and filter context from diverse sources, including system prompts, user dialogue history, real-time data, retrieved documents, and external tools.6 It is a discipline focused on building stateful, multi-turn reliability.6

### **The Scope of "V2V Methodologies"**

The reference to "V2V methodologies" within the user query is interpreted as the spectrum of advanced techniques that serve as the technical underpinnings of the context engineering paradigm. This report will systematically deconstruct these methodologies, which include but are not limited to:

* **Advanced Retrieval-Augmented Generation (RAG):** The foundational technology for grounding LLMs in external knowledge.  
* **Self-Correcting and Reflective RAG Variants:** Methodologies like Self-RAG and Corrective RAG that introduce evaluation and feedback loops into the retrieval process.  
* **Structured Knowledge Retrieval:** Techniques such as GraphRAG that leverage structured data representations for more complex reasoning.  
* **Agentic Frameworks:** Systems that orchestrate memory, tools, and retrieval to enable autonomous, multi-step task execution.

The emergence of context engineering as a formal discipline is not merely a technical evolution; it is a direct economic and competitive response to a fundamental shift in the AI landscape. As powerful foundational models from providers like OpenAI, Anthropic, and Google become increasingly commoditized and accessible, the competitive advantage in building AI applications no longer derives solely from possessing a superior proprietary model.3 The technological playing field in terms of raw model capability has been leveled. Consequently, sustainable differentiation must come from another source. This new competitive moat is the ability to effectively apply a general-purpose model to an organization's unique, proprietary data and operational logic at runtime.3 An LLM, regardless of its power, cannot solve specific enterprise problems without access to internal knowledge bases, user histories, and business rules. Context engineering is the formal practice that operationalizes this differentiation, providing the architectural patterns necessary to reliably integrate this proprietary information into the model's reasoning process, thereby creating defensible, value-added AI applications.3

## **Section 2: The Brittle Limits of Prompt Engineering at Scale**

The transition toward context engineering is necessitated by the inherent and systemic limitations of prompt engineering when applied to the demands of industrial-strength AI applications. While a valuable skill for prototyping and single-turn tasks, prompt engineering's reliance on static, handcrafted instructions proves insufficient and brittle for systems that are inherently dynamic, stateful, and involve multiple interactions over time.3 This section deconstructs these limitations, arguing that they are not tactical shortcomings but fundamental architectural constraints that mandate a new approach.

### **Analysis of Limitations**

The failures of prompt engineering at scale can be categorized across several key dimensions:

* **Brittleness and Lack of Generalization:** The core practice of prompt engineering is highly sensitive to minor variations in wording, phrasing, and example placement, a characteristic frequently described as "brittle".6 A meticulously crafted prompt that performs well for a specific input can fail unexpectedly when faced with a slight semantic or structural deviation. This lack of generalization means that prompts require constant, manual adjustment and fail to create a persistent, reliable system behavior across a wide range of inputs.6 Traditional prompt engineering produces outputs that are prone to failure during integration, deployment, or when business requirements evolve, because a prompt without deep system context amounts to educated guesswork.8  
* **Failure of Scope for Stateful Applications:** The fundamental limitation of prompt engineering is one of scope.3 A static, single-turn instruction is architecturally incapable of managing the complexities of modern AI applications. A cleverly worded prompt, in isolation, cannot manage conversation history, retrieve real-time data from an API, or maintain a persistent understanding of a user's preferences across multiple sessions.3 These stateful requirements are central to creating coherent and useful user experiences, and they lie outside the purview of a single prompt.  
* **The "Failure of Context" Diagnosis:** A critical reframing of the problem is the recognition that most failures in complex AI applications are not failures of prompting but "failures of context".3 A customer service bot that forgets a user's issue mid-conversation or an AI coding assistant that is unaware of a project's overall structure has not failed because of a poorly worded instruction. It has failed because its underlying system did not provide it with the necessary contextual information—the conversation history or the repository structure—at the moment of inference.3 This diagnosis correctly shifts the focus of debugging and design from linguistic tweaking to systems architecture.  
* **Inherent Scalability Issues:** The reliance on manual prompt tweaking for every edge case is fundamentally unscalable.3 In a production environment with diverse user inputs and evolving requirements, this approach leads to an ever-expanding and unmanageable set of custom prompts, resulting in inconsistent and unpredictable system behavior.3 In contrast, context-engineered systems are designed for consistency and reuse across many users and tasks by programmatically injecting structured context that adapts to different scenarios.1  
* **The "Vibe Coding" Honeymoon:** The initial excitement around LLMs led to a culture of what has been termed "vibe coding," where developers intuitively craft prompts to achieve a desired result.10 This approach, while accessible, completely falls apart when attempting to build real, scalable software because intuition does not scale—structure does.10 This has also fueled a perception in some engineering circles of prompt engineering as a "cash grab" or a non-technical skill focused on finding "magic words," creating a cultural barrier to its integration with rigorous software development practices.11

The culture of "magic words" and arcane prompt-craft that characterized early prompt engineering created a significant barrier to collaborative and scalable development. This practice, often based on individual intuition and opaque trial-and-error, is antithetical to modern software engineering principles of clarity, maintainability, version control, and teamwork. It is difficult to document, test, or scale the "art" of a perfect prompt across a large engineering organization. The shift to context engineering represents a necessary professionalization of the field. By replacing the quest for magic words with transparent and auditable system design, it aligns LLM application development with established engineering practices. Context engineering uses the language of software architecture—"pipelines," "modules," "orchestration," and "state management"—which are standard concepts that promote collaboration, automated testing, and long-term maintainability.1 This transition effectively democratizes the development of complex AI systems, transforming it from the domain of individual "prompt whisperers" into a structured, collaborative engineering discipline.

## **Section 3: Context Engineering: A Formal Discipline for Industrial-Strength AI**

In response to the limitations of prompt engineering, context engineering has emerged as a formal, multi-faceted discipline. It moves beyond the simple metaphor of "filling the context window" to establish a comprehensive set of principles and practices for architecting the flow of information to an LLM. This section provides a formal definition of context engineering, outlines its core principles, and details its essential practices, establishing it as the foundational discipline for building reliable, industrial-strength AI applications.

### **Formal Definition and Core Principles**

Context engineering is formally defined as **the systematic discipline of designing, organizing, orchestrating, and optimizing the complete informational payload provided to a Large Language Model at inference time**.3 It is a systems-level practice that treats the LLM's entire input window not as a simple instruction field, but as a dynamic "workspace" that is programmatically populated with the precise information needed to solve a given task.5 This discipline is built upon three fundamental principles:

1. **Information Architecture:** This principle involves the deliberate organization and structuring of all potential contextual data to ensure optimal comprehension by the AI. A key practice is the establishment of a clear information hierarchy, distinguishing between **primary context** (mission-critical information for the immediate task), **secondary context** (supporting details that enhance understanding), and **tertiary context** (broader background information).13 This structured approach ensures that the most vital information is prioritized and not lost in a sea of irrelevant data.  
2. **Memory Management:** This principle addresses the strategic handling of temporal information to create stateful and coherent interactions. It involves designing systems that can manage different "memory slots," such as **short-term memory** (e.g., a conversation buffer for recent exchanges), **long-term memory** (e.g., a persistent vector store for user preferences or key facts from past sessions), and **user profile information**.6 Effective memory management is what allows an AI application to maintain continuity across multiple turns and sessions.14  
3. **Dynamic Context Adaptation:** This principle focuses on the real-time assembly and adjustment of the context based on the evolving needs of the interaction. Rather than relying on a static system prompt, a dynamically adapted system can aggregate and filter context from multiple sources on the fly, such as user dialogue history, real-time data from APIs, and retrieved documents.6 This ensures the context is always as relevant and up-to-date as possible.13

### **Core Practices and Components**

The principles of context engineering are implemented through a set of core practices and architectural components:

* **Context Retrieval:** The practice of identifying, fetching, and ranking the most relevant information from external knowledge sources like documents, databases, or knowledge base articles. This is the primary domain of Retrieval-Augmented Generation (RAG) and its advanced variants.6  
* **Context Processing and Summarization:** Techniques for condensing large volumes of text, such as long documents or conversation histories, into compact and high-utility summaries. This is crucial for managing the finite context window of LLMs, reducing noise, and improving computational efficiency.6  
* **Tool Integration:** The practice of defining and describing external functions or APIs that the model can invoke to perform actions or retrieve information beyond its internal knowledge. This includes defining the tool's purpose, parameters, and expected output format.6  
* **Structured Templates and Output Formatting:** The use of predictable and parsable formats (e.g., JSON schemas, XML tags) to organize the different elements of the context provided to the model. This is often paired with constraints on the model's output to ensure it generates data in a reliable, machine-readable form for downstream processing.6

It is crucial to understand that prompt engineering and context engineering are not competing practices; rather, **prompt engineering is a subset of context engineering**.1 A well-engineered prompt—the clear, specific instruction of *what to say*—remains a vital component. However, its ultimate effectiveness is determined by the broader context architecture that defines *what the model knows* when it receives that instruction. A brilliant prompt can be rendered useless if it is drowned in thousands of tokens of irrelevant retrieved data, a failure that context engineering is designed to prevent.1  
The establishment of this discipline imposes a new, proactive development methodology that can be described as a "context-first" pattern. This approach fundamentally inverts the traditional software development workflow for AI applications. In a traditional prompt-centric model, a developer has a task, writes code, and then attempts to craft a prompt to make an AI understand or generate that code, often reactively debugging failures.16 This frequently leads to production failures like "hallucinated API calls" or "architectural blindness" because the AI lacks a systemic understanding of the codebase it is operating on.8  
The context-first paradigm addresses this by requiring the developer to first architect the AI's understanding of the system *before* asking it to perform a task. This initial step involves creating a comprehensive context layer, which may include indexing the entire code repository, mapping dependencies, and defining existing architectural patterns.8 Only after this context has been engineered can the developer pose a high-level architectural challenge to the AI (e.g., "How should authentication be refactored to support new requirements?") rather than a simple procedural request.8 This workflow—architecting the context, posing a challenge, receiving a plan for approval, and then executing—makes the AI's knowledge base a primary development artifact, not an afterthought. This has profound implications for tooling, which must now support repository-level indexing, and for the role of the developer, who becomes a context architect first and a prompter second.

## **Section 4: Architectural Pillars of Modern Context Engineering**

A robust, context-engineered AI system is not a monolithic entity but a composition of distinct yet interconnected architectural pillars. These pillars work in concert to dynamically manage the LLM's context window, providing it with the necessary information to reason effectively and perform complex tasks. This section deconstructs the modern context engineering stack into its three core pillars: advanced Retrieval-Augmented Generation (RAG), Memory and State Management systems, and Tool Integration frameworks.

### **Pillar 1: Advanced Retrieval-Augmented Generation (RAG)**

RAG serves as the foundational pillar for grounding LLMs in external reality. Its primary function is to connect the model to up-to-date, proprietary, or domain-specific knowledge sources, thereby mitigating hallucinations and moving the model's capabilities beyond its static, pre-trained knowledge.14  
The naive RAG process consists of three main stages:

1. **Indexing:** Raw documents are loaded, cleaned, and segmented into smaller, manageable chunks. Each chunk is then passed through an embedding model to create a numerical vector representation, which is stored in a vector database.17  
2. **Retrieval:** At inference time, a user's query is also converted into a vector embedding. A semantic similarity search is performed against the vector database to find the chunks whose embeddings are closest to the query embedding.17  
3. **Generation:** The retrieved text chunks are prepended to the user's original query and fed into the LLM as part of the prompt. The LLM then generates a response that is "augmented" with this retrieved context.18

While revolutionary, this basic RAG pipeline suffers from significant challenges in production environments. Common failure modes include **bad retrieval** (low precision, where retrieved chunks are irrelevant, or low recall, where relevant chunks are missed) and **bad generation** (the model hallucinates or produces an irrelevant response despite being provided with the correct context).7 These limitations have spurred the development of the more sophisticated RAG methodologies that form the core of modern context engineering and are discussed in detail in Section 5\.

### **Pillar 2: Memory and State Management Systems**

The second pillar is dedicated to providing the AI system with continuity and personalization. Memory systems enable an application to maintain state across multiple interactions, allowing it to remember past conversations, learn user preferences, and build a coherent understanding over time.6  
Memory is typically architected into distinct types:

* **Short-Term Memory:** This functions as a conversational buffer, holding the history of recent exchanges within a single session. It is essential for maintaining the coherence of a multi-turn dialogue, allowing the model to understand references to previous statements.9 This is often implemented as a simple list of messages that grows with the conversation.  
* **Long-Term Memory:** This provides a mechanism for persistent storage of information across different sessions. It can store key facts, summaries of past conversations, or detailed user profiles. Long-term memory is typically implemented using an external storage system, such as a vector database, where information can be retrieved when needed to provide continuity and personalization.9  
* **Hierarchical Memory:** Advanced systems may employ more complex memory hierarchies that include mechanisms for compression, prioritization, and optimization. This allows the system to manage vast amounts of historical context efficiently, deciding what information is critical to retain and what can be summarized or discarded.14

### **Pillar 3: Tool Integration and Function Calling**

The third pillar extends the LLM's capabilities from a pure text processor into an active agent that can interact with the external world. By integrating tools, the model can perform actions like querying a database, calling an API, running a piece of code, or searching the web.6  
The mechanism for tool integration involves several steps:

1. **Definition:** A set of available tools is defined and described in natural language, including each tool's name, a description of what it does, and the parameters it accepts (e.g., in a JSON schema).9  
2. **Provision:** These tool definitions are provided to the LLM as part of its context.  
3. **Invocation:** When faced with a query it cannot answer from its internal knowledge or retrieved context, the LLM can generate a structured output (e.g., a JSON object) requesting a call to one of the available tools with specific parameters.  
4. **Execution:** The application code parses this request, executes the corresponding function or API call, and receives a result.  
5. **Observation:** The output from the tool execution is then fed back into the LLM's context, allowing it to use this new information to formulate its final response.9

Frameworks like LangChain and standards such as the Model Context Protocol (MCP) play a crucial role in simplifying and standardizing this process, providing abstractions that make it easier to define tools and manage the interaction loop.5  
These three pillars—Retrieval, Memory, and Tools—do not operate in isolation. They form a deeply interconnected and synergistic "cognitive architecture" for the LLM. The effectiveness of a context-engineered system lies in the orchestration of the interplay between these components. For instance, a user might ask a complex question that requires multi-step reasoning.7 The system would first consult its **Memory** to check if a similar query has been resolved before. Finding no existing answer, it might invoke a planning **Tool** to decompose the complex query into a series of simpler sub-queries.20 For each sub-query, the system would then perform **Retrieval** from a dedicated knowledge base.21 If a retrieved document proves ambiguous or outdated, the system could trigger another **Tool**, such as a web search, to gather more current information.22 Throughout this entire process, the system continuously updates its short-term **Memory** (often called a "scratchpad") with intermediate findings and the results of tool calls, building up a comprehensive context before synthesizing the final answer.9 This dynamic orchestration is the hallmark of an agentic system, where the challenge is not merely implementing each pillar, but designing the sophisticated logic that governs their interaction.

## **Section 5: The Evolution of Retrieval: A Comparative Analysis of Advanced RAG Methodologies**

The technical engine driving the context engineering paradigm is the rapid evolution of Retrieval-Augmented Generation. Moving beyond the limitations of the naive RAG pipeline, a new class of advanced methodologies has emerged. These approaches introduce sophisticated mechanisms for self-correction, reflection, and structural awareness, transforming RAG from a simple data-fetching process into an intelligent, robust, and adaptable component of the AI cognitive architecture. This section provides a detailed comparative analysis of these cutting-edge retrieval methodologies.

### **Methodology 1: Self-Correction and Reflection (Self-RAG & Corrective RAG)**

The first major advancement in RAG involves introducing a self-evaluation loop to critically assess the quality and relevance of retrieved information *before* it is used for generation. This principle of self-correction significantly enhances the system's robustness against the common failure mode of inaccurate retrieval.

* **Self-RAG (Self-Reflective Retrieval-Augmented Generation):** This framework trains a language model to generate special "reflection tokens" that actively control the retrieval and generation process.23 Instead of retrieving blindly, the model learns to make several critical decisions. First, it decides *when* retrieval is necessary by predicting a \`\` token, enabling adaptive, on-demand retrieval that can be skipped for simple queries or repeated for complex ones.24 Second, after retrieving documents, it assesses their relevance by generating an ISREL (Is Relevant) token for each passage. Finally, it critiques its own generated response to ensure it is factually supported by the evidence, using an ISSUP (Is Supported) token.25 This end-to-end training for self-reflection allows the model to balance versatility with a high degree of factual accuracy and control.  
* **Corrective RAG (CRAG):** This methodology offers a more modular, "plug-and-play" approach to improving retrieval robustness.22 It employs a lightweight, fine-tuned retrieval evaluator—separate from the main LLM—to assess retrieved documents and assign them a confidence score. Based on this score, the system triggers one of three distinct actions:  
  1. **Correct:** If confidence is high, the documents are deemed relevant and used for generation.  
  2. **Incorrect:** If confidence is low, the documents are discarded, and the system supplements its knowledge by performing a large-scale web search to find better information.  
  3. Ambiguous: If confidence is intermediate, the system uses both the retrieved documents and the web search results.22  
     CRAG's design, which includes a decompose-then-recompose algorithm to filter noise from documents, makes it an effective add-on for enhancing the reliability of existing RAG pipelines.22

### **Methodology 2: Structured Knowledge Integration (GraphRAG)**

The second major evolutionary path for RAG moves beyond processing unstructured text chunks to leveraging structured knowledge representations. GraphRAG constructs a knowledge graph from the source documents, capturing not just isolated pieces of information but also the intricate relationships between them. This enables more complex, multi-hop reasoning that is difficult to achieve with standard semantic search.28

* **Mechanism:** The GraphRAG indexing process involves using an LLM to extract key entities (nodes) and their relationships (edges) from the text, building a comprehensive knowledge graph.29 When a query is received, instead of performing a simple vector search, the system can traverse this graph. For example, it can find entities mentioned in the query and then explore their multi-hop neighbors to gather a rich, interconnected context.29 This approach is particularly effective for answering questions that require synthesizing information from multiple sources or understanding the overall structure of the knowledge base.29  
* **Variants:** Several approaches to GraphRAG exist. **KG-based GraphRAG** focuses on retrieving and traversing triplets (head, relation, tail) from the graph.29 **Community-based GraphRAG**, a method developed by Microsoft, goes a step further by applying community detection algorithms to the graph to create hierarchical summaries. This allows for both **Local Search** (exploring specific entities and their immediate connections) and **Global Search** (querying high-level community summaries to understand broad themes), providing a multi-resolution view of the data.29

### **Synthesis and Other Advanced Techniques**

The RAG landscape is rich with other innovative techniques that complement these major methodologies:

* **Query Transformation:** Before retrieval, the user's initial query can be improved. Techniques like **multi-query retrieval** use an LLM to generate several variations of the original query to cast a wider net.18 **Hypothetical Document Embedding (HyDE)** involves having the LLM generate a hypothetical ideal answer to the query, embedding that answer, and then searching for documents that are semantically similar to this ideal response.7  
* **Advanced Chunking and Re-ranking:** The quality of retrieval is highly dependent on how documents are indexed. **Semantic chunking** splits documents based on conceptual coherence rather than fixed character counts.36 The **small-to-big retrieval** technique involves retrieving small, precise chunks for high-accuracy matching but then passing larger, parent chunks to the LLM to provide more context for generation.7 After initial retrieval, a **cross-encoder re-ranker** can be used to apply a more computationally expensive but accurate model to re-order the top-k results, pushing the most relevant documents to the top.7  
* **Agentic RAG:** This represents the convergence of RAG with autonomous agents. Instead of a fixed pipeline, an AI agent orchestrates the entire retrieval process, making dynamic decisions about which retrieval strategy to use, whether to transform the query, or when to use a tool like a web search, based on the specific query and the state of the task.37

The following table provides a synthesized comparison of these advanced RAG methodologies, designed to help technical leaders map their specific business problems to the most appropriate RAG architecture.

| Methodology | Core Principle | Key Challenge Addressed | Strengths | Limitations/Trade-offs | Ideal Use Case |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Naive RAG** | Static Retrieval | Basic factual grounding from external knowledge. | Simple to implement; provides baseline grounding. | Brittle; prone to "lost in the middle" problem; sensitive to retrieval quality. | Simple Q\&A over a clean, well-structured knowledge base. |
| **Corrective RAG (CRAG)** | Retrieval → Evaluate → Act | Irrelevant or inaccurate document retrieval. | Improves robustness against bad retrieval; plug-and-play with existing systems; uses web search to augment knowledge. | Increased latency due to evaluation and potential web search steps; web results can introduce new noise. | High-stakes applications where factual accuracy is paramount (e.g., finance, legal, medical). |
| **Self-RAG** | Adaptive Retrieval & Self-Critique | The trade-off between versatility (always generating) and factuality (always retrieving). | High factual accuracy and citation precision; controllable and adaptive retrieval frequency; efficient at inference time. | Requires specialized model training or fine-tuning; more complex to implement than modular approaches. | Open-domain Q\&A; long-form generation requiring verifiable citations and high factuality. |
| **GraphRAG** | Relational Retrieval | Multi-hop reasoning and understanding contextual relationships between data points. | Captures deep relationships within data; excels at complex queries requiring synthesis; can be more token-efficient. | High upfront indexing cost and complexity; performance is dependent on the quality of the generated graph. | Analyzing interconnected data like research paper networks, codebases, or complex business intelligence reports. |

## **Section 6: The Agentic Paradigm: Orchestrating Context for Autonomous Task Execution**

The architectural pillars and advanced retrieval methodologies discussed previously converge in the **agentic paradigm**, which can be seen as the ultimate expression and application of context engineering. An AI agent is a system that leverages a continuously managed context—comprising memory, tools, and retrieved knowledge—to autonomously plan, reason, and execute complex, multi-step tasks that go far beyond a single question-and-answer exchange.5 This section will analyze the shift from linear RAG pipelines to cyclical agentic workflows and survey the frameworks that enable this new mode of AI-powered automation.

### **From RAG Pipelines to Agentic Workflows**

Traditional RAG applications, even advanced ones, typically follow a linear, sequential pipeline: a query is received, documents are retrieved, context is augmented, and a response is generated. Agentic systems fundamentally alter this flow by introducing a cyclical reasoning loop, often described as **Think → Act → Observe**.

1. **Think:** The agent analyzes the current goal and the state of its context (including the user's request, its memory, and available tools) to form a plan or decide on the next action.  
2. **Act:** The agent executes the chosen action. This could be invoking a tool (e.g., running a search query, calling an API), updating its internal memory, or generating a response to the user.  
3. **Observe:** The agent takes the result of its action (e.g., the output from a tool call, a new message from the user) and incorporates it back into its context. This updated context then serves as the input for the next "Think" step.

This loop continues until the agent determines that the overall goal has been achieved. Effective context engineering is the prerequisite for this entire process. For example, agents often use a "scratchpad" or working memory—a form of short-term, dynamically updated context—to record their intermediate thoughts, the results of tool calls, and their evolving plan.9 This scratchpad is a direct implementation of context management that allows the agent to maintain a coherent "thought process" throughout a complex task.

### **Analysis of Agent Frameworks and Design Patterns**

The rise of the agentic paradigm has been accelerated by the development of specialized frameworks that provide abstractions for building and orchestrating these complex systems. These frameworks are, in essence, toolkits for context engineering at an agentic level.

* **LangChain / LangGraph:** LangChain provides high-level abstractions for creating chains and agents that manage context through built-in memory classes and tool integrations.5 Its more recent extension, LangGraph, is explicitly designed for building cyclical, stateful agentic workflows. LangGraph represents the agent's logic as a graph where nodes are functions (e.g., call a tool, generate a response) and edges are conditional logic that directs the flow based on the current state. This makes it a powerful tool for implementing complex, multi-step reasoning and self-correction loops.5  
* **CrewAI:** This framework specializes in the orchestration of multi-agent systems. It introduces the concepts of "Crews" (teams of specialized agents) and "Flows" (workflows).5 The core idea is to break down a complex problem and assign sub-tasks to different agents, each with its own specific role, tools, and isolated context. A controller then manages the communication and collaboration between these agents.5 This approach is a powerful context engineering pattern, as it uses separation of concerns to prevent context overload in any single agent.  
* **DSPy (Declarative Self-improving Python):** DSPy takes a different, more programmatic approach. Instead of having developers write explicit prompts, it allows them to define the logic of an LLM program as a series of Python modules (e.g., dspy.ChainOfThought, dspy.Retrieve). DSPy then acts as a "compiler" that automatically optimizes these modules into highly effective, context-aware prompts for a given LLM.5 It abstracts away the raw prompt engineering, allowing developers to focus on the high-level program structure while the framework handles the low-level context management.

These frameworks enable sophisticated agentic design patterns that are direct applications of context engineering. **Multi-agent collaboration**, as seen in CrewAI and proposed in frameworks like RepoTransAgent 21, isolates context by function, allowing a "RAG Agent" to focus solely on retrieval while a "Refine Agent" focuses on code generation, improving the effectiveness of each.21 **Reflection and self-correction**, a key feature of agentic RAG, is implemented by creating cycles in the agent's logic where the output of one step is evaluated and used to decide the next, such as re-querying if initial retrieval results are poor.21  
The proliferation of these agentic frameworks signifies a new, higher layer of abstraction in AI application development. The engineering focus is rapidly shifting away from managing individual LLM prompt-completion pairs and toward designing the interaction protocols, state management systems, and collaborative workflows for teams of autonomous agents. This evolution mirrors previous shifts in the history of software engineering, such as the move from assembly language to high-level procedural languages, and more recently, the transition from monolithic applications to orchestrated microservice architectures. In this new paradigm, context engineering provides the essential infrastructure—the "network" and "state management" layers—for what can be conceptualized as "AI-native microservices." Here, autonomous agents are the services, each with a specialized role and API (its tools). The primary engineering challenge is no longer just prompt design, but the orchestration, state synchronization, and inter-agent communication required to make these services collaborate effectively to solve complex business problems.

## **Section 7: Human-in-the-Loop: Redefining Collaboration in Context-Aware Systems**

The paradigm shift from prompt engineering to context engineering does more than just alter the technical architecture of AI systems; it profoundly redefines the role of the human developer and the nature of human-AI collaboration. As AI moves from a simple instruction-following tool to a context-aware partner, the developer's role evolves from that of a "prompter" or "vibe coder" to a "context architect" and "AI orchestrator." This section explores these new models of collaboration and the practical workflows that emerge in a context-first development environment.

### **New Models of Collaboration**

The relationship between a human and a context-aware AI is more nuanced and collaborative than the simple command-response dynamic of prompt engineering.

* **Cognitive Apprenticeship with AI:** In this model, the AI acts as an expert apprentice or intelligent tutor within the development process.42 The human developer takes on the role of the master practitioner, providing the strategic direction, architectural constraints, and domain knowledge that form the AI's context. The AI, guided by this rich context, then handles the tactical implementation, such as generating code, suggesting refactoring, or identifying potential bugs.42 The AI can provide cognitive scaffolding, offering insights based on its analysis of the entire codebase, a task that would be too complex for a human to perform in real-time.42  
* **AI-Assisted Software Architecture:** With a deep understanding of the entire system's context, AI can transcend mere code generation and become a participant in high-level architectural decision-making. Instead of being given procedural requests like "Write a login function," an AI with full repository context can be posed architectural challenges: "How should the authentication service be refactored to support OAuth2 while maintaining backward compatibility with our existing JWT implementation?".8 This elevates the AI from a simple coder to a co-architect that can reason about system-wide implications, dependencies, and established patterns.16

### **"Context-First" Development Workflows**

These new collaborative models are enabled by a set of "context-first" development patterns that prioritize architecting the AI's understanding before asking it to perform tasks. These workflows stand in stark contrast to the reactive, trial-and-error nature of traditional prompt engineering.8

* **The Flipped Interaction Pattern:** In a traditional workflow, the developer provides a prompt and hopes the AI understands it, often leading to incorrect implementations due to unstated assumptions. The Flipped Interaction Pattern inverts this. The AI, positioned as an intelligent collaborator, proactively asks clarifying questions to resolve ambiguity *before* beginning implementation.8 For the authentication refactoring example, the AI might ask: "Should OAuth2 replace JWT entirely or integrate alongside it? Which OAuth2 providers need to be supported?" This dialogue prevents silent errors and significantly reduces rework.8  
* **The Agentic Plan Pattern:** For complex tasks that span multiple files or services, this pattern introduces a crucial human review step. The AI first analyzes the request and the system context to generate a detailed, multi-step implementation plan. This plan outlines which files will be modified, what new dependencies will be introduced, and how the changes will be tested. The human developer then reviews and approves this plan, ensuring it aligns with the project's architectural goals, before the AI autonomously executes it.8 This prevents the AI from making unilateral architectural decisions that could introduce "surprise dependencies" during integration.8  
* **Human-in-the-Loop (HITL) for Safety and Quality:** Beyond the development process, HITL remains a critical component for the ongoing operation of context-engineered systems, especially in high-stakes domains. Human oversight is essential for validating AI outputs, mitigating algorithmic bias that may be present in the data sources, ensuring ethical decision-making, and providing a final layer of accountability.43 Regulations like the EU AI Act mandate this level of human oversight for high-risk systems, formalizing the need for humans to be able to intervene and override AI-driven decisions.44

The adoption of a context-first approach leads to the creation of a new and critical development artifact: the **"Context Manifest"** or **"System Prompt Notebook"**.4 This is a formal, structured document or set of configuration files that explicitly defines the AI's operating environment. It contains the AI's role and persona, definitions of available tools, pointers to knowledge sources, examples of desired behavior, and high-level architectural constraints.45 This manifest is not a one-off, disposable prompt; it is a persistent, engineered resource that is as vital to the application's behavior as the source code itself.10 The logical conclusion of this trend is the formalization of **"AI Configuration as Code."** This Context Manifest will be stored in version control systems, subjected to the same rigorous code review and testing processes as the application code, and deployed as part of the CI/CD pipeline. This represents a fundamental shift in the definition of a software project, where the explicit and auditable configuration of the AI's "mind" becomes a first-class citizen of the engineering lifecycle.

## **Section 8: Strategic Implications and Future Research Directions**

The transition from prompt engineering to context engineering is more than a technical upgrade; it is a strategic inflection point for any organization seeking to build meaningful and defensible AI capabilities. Mastering this new paradigm is not just an engineering goal but a business imperative. This concluding section synthesizes the report's findings to outline the strategic importance of context engineering and identifies the key open challenges and future research frontiers that will shape the next generation of AI systems.

### **Strategic Imperatives**

* **A New Source of Competitive Advantage:** The central strategic implication is that in an era of powerful and widely accessible foundational LLMs, the primary driver of competitive advantage has shifted. It is no longer about who owns the best model, but who can most effectively connect a model to their unique, proprietary data and complex business workflows.3 The context layer—the sophisticated architecture of retrieval, memory, and tools—is the new competitive moat. Organizations that invest in building robust context engineering capabilities will be able to create AI applications that are more accurate, more personalized, and more deeply integrated into their core operations, creating a defensible advantage that cannot be easily replicated by competitors with access to the same base LLMs.  
* **A Fundamental Shift in Required Skillsets:** The value of an AI engineer is no longer measured by their ability to wordsmith the perfect "magic prompt." The most critical skill is now systems design for context.46 This requires a cross-functional expertise that blends data architecture (designing knowledge bases and retrieval strategies), software engineering (building scalable pipelines and tool integrations), and deep business domain knowledge (understanding the specific information and logic required to solve a problem). Organizations must foster these hybrid skillsets to move beyond simple AI demos and build production-critical infrastructure.8  
* **The Bridge from Demos to Production:** Context engineering is the set of principles and practices that enables AI applications to graduate from interesting but brittle prototypes to reliable, scalable, and maintainable production systems.8 By replacing manual, ad-hoc prompting with structured, repeatable, and auditable systems, context engineering provides the engineering rigor necessary for enterprise-grade deployment.

### **Challenges and Open Frontiers**

Despite its rapid advancement, the field of context engineering faces several significant challenges that represent active areas of research and development.

* **Managing Context Window Limitations:** While the context windows of LLMs are expanding, they remain a finite and expensive resource. Effectively managing this space is a critical challenge. Active research is focused on advanced strategies such as intelligent **context summarization** to distill key information, heuristic-based **context trimming** to prune less relevant data, and architectural patterns like **multi-agent systems** that **isolate context** by splitting a complex task across multiple agents, each with its own smaller, focused context window.14  
* **Evaluation and Observability:** Evaluating the performance of a complex, context-engineered system is a significant challenge. Simple output accuracy is insufficient. A new class of evaluation metrics is needed to assess the quality of the intermediate steps: Was the retrieval relevant? Was the correct tool chosen? Was the memory state updated properly? This has led to the emergence of specialized AI observability platforms (e.g., Langfuse, Trulens, Ragas) that provide deep traces into the agent's reasoning process, allowing developers to debug and optimize the entire context pipeline, not just the final output.48  
* **Context Security:** As the context window becomes the primary interface for controlling an LLM, it also becomes a new attack surface. Emerging threat vectors include **context poisoning**, where malicious or misleading information is deliberately injected into the knowledge base that an agent retrieves from, and sophisticated **prompt injection** attacks that can be delivered through retrieved documents or tool outputs, potentially causing the agent to leak data or perform unauthorized actions.39 Developing robust defenses against these context-based attacks is a critical research frontier.

### **Future Directions**

Looking forward, the continued evolution of context engineering points toward several exciting future directions:

* **Automated Context Engineering:** The next logical step is to use AI to automate the design and optimization of its own context architecture. Frameworks like AutoRAG, which can automatically test and select the best combination of chunking strategies, embedding models, and retrieval parameters for a given dataset, are early indicators of this trend.48  
* **Multi-Modal Context:** Current systems predominantly focus on textual context. A major frontier is the development of unified frameworks that can seamlessly ingest, index, and reason over multi-modal context, including images, audio, video, and structured data, to provide a more holistic understanding of the world.  
* **Cognitive Architectures:** The long-term vision of context engineering is the development of increasingly sophisticated, human-like cognitive architectures for AI. The pillars of retrieval (accessing knowledge), memory (retaining experience), and tools (acting on the world) are the foundational building blocks. Future research will focus on creating more advanced systems for reasoning, learning, and planning that are built upon these context-engineered foundations, moving us closer to more general and capable artificial intelligence.

#### **Works cited**

1. Context Engineering vs Prompt Engineering | by Mehul Gupta | Data ..., accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d)  
2. What Is Context Engineering And Why Should You Care? | In The Loop Episode 23, accessed October 15, 2025, [https://www.mindset.ai/blogs/in-the-loop-ep23-what-is-context-engineering](https://www.mindset.ai/blogs/in-the-loop-ep23-what-is-context-engineering)  
3. Context Engineering: Techniques, Tools, and Implementation \- iKala, accessed October 15, 2025, [https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/](https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/)  
4. Context Engineering : r/LocalLLaMA \- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1lnldsj/context\_engineering/](https://www.reddit.com/r/LocalLLaMA/comments/1lnldsj/context_engineering/)  
5. Context Engineering in LLMs and AI Agents | by DhanushKumar | Stackademic, accessed October 15, 2025, [https://blog.stackademic.com/context-engineering-in-llms-and-ai-agents-eb861f0d3e9b](https://blog.stackademic.com/context-engineering-in-llms-and-ai-agents-eb861f0d3e9b)  
6. Understanding Prompt Engineering and Context Engineering, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  
7. Retrieval-augmented Generation: Part 2 | by Xin Cheng \- Medium, accessed October 15, 2025, [https://billtcheng2013.medium.com/retrieval-augmented-generation-part-2-eaf2fdff45dc](https://billtcheng2013.medium.com/retrieval-augmented-generation-part-2-eaf2fdff45dc)  
8. How Context-First Prompt Engineering Patterns Actually Ship ..., accessed October 15, 2025, [https://www.augmentcode.com/guides/how-context-first-prompt-engineering-patterns-actually-ship-production-code](https://www.augmentcode.com/guides/how-context-first-prompt-engineering-patterns-actually-ship-production-code)  
9. What is Context Engineering? (And Why It's Really Just Prompt ..., accessed October 15, 2025, [https://mirascope.com/blog/context-engineering](https://mirascope.com/blog/context-engineering)  
10. Context Engineering is the New Vibe Coding (Learn this Now) \- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=Egeuql3Lrzg](https://www.youtube.com/watch?v=Egeuql3Lrzg)  
11. Beyond the Prompt: AI's Great Shift to Process Design | by Umakshi Sharma | Oct, 2025, accessed October 15, 2025, [https://ai.plainenglish.io/beyond-the-prompt-ais-great-shift-to-process-design-5fa2e1503bf0](https://ai.plainenglish.io/beyond-the-prompt-ais-great-shift-to-process-design-5fa2e1503bf0)  
12. Prompt engineering, Context Engineering, Protocol Whatever... It's all Linguistics Programming... : r/OpenAI \- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt\_engineering\_context\_engineering\_protocol/](https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt_engineering_context_engineering_protocol/)  
13. Context Engineering in AI: Principles, Methods, and Uses \- Code B, accessed October 15, 2025, [https://code-b.dev/blog/context-engineering](https://code-b.dev/blog/context-engineering)  
14. Context Engineering. What are the components that make up… \- Cobus Greyling \- Medium, accessed October 15, 2025, [https://cobusgreyling.medium.com/context-engineering-a34fd80ccc26](https://cobusgreyling.medium.com/context-engineering-a34fd80ccc26)  
15. Enhancing AI Prompts with XML Tags: Testing Anthropic's Method and o4-mini-high / Mike Levin AI SEO GEO AEO AIO in NYC, accessed October 15, 2025, [https://mikelev.in/futureproof/ai-prompts-xml-tags/](https://mikelev.in/futureproof/ai-prompts-xml-tags/)  
16. How to Use AI to Modernize Software Architecture \- DZone, accessed October 15, 2025, [https://dzone.com/articles/ai-modernize-software-architecture](https://dzone.com/articles/ai-modernize-software-architecture)  
17. Retrieval-Augmented Generation for Large Language ... \- arXiv, accessed October 15, 2025, [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997)  
18. Advanced RAG Techniques: Upgrade Your LLM App Prototype to Production-Ready\!, accessed October 15, 2025, [https://levelup.gitconnected.com/advanced-rag-techniques-upgrade-your-llm-app-prototype-to-production-ready-74839342e9c0](https://levelup.gitconnected.com/advanced-rag-techniques-upgrade-your-llm-app-prototype-to-production-ready-74839342e9c0)  
19. 13+ Popular MCP servers for developers to unlock AI actions \- DronaHQ, accessed October 15, 2025, [https://www.dronahq.com/popular-mcp-servers/](https://www.dronahq.com/popular-mcp-servers/)  
20. Context Engineering Guide, accessed October 15, 2025, [https://www.promptingguide.ai/guides/context-engineering-guide](https://www.promptingguide.ai/guides/context-engineering-guide)  
21. RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation, accessed October 15, 2025, [https://arxiv.org/html/2508.17720v1](https://arxiv.org/html/2508.17720v1)  
22. Corrective RAG (CRAG) \- Kore.ai, accessed October 15, 2025, [https://www.kore.ai/blog/corrective-rag-crag](https://www.kore.ai/blog/corrective-rag-crag)  
23. Self-Rag: Self-reflective Retrieval augmented Generation \- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2310.11511v1](https://arxiv.org/html/2310.11511v1)  
24. Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflection, accessed October 15, 2025, [https://selfrag.github.io/](https://selfrag.github.io/)  
25. Self-RAG \- Learn Prompting, accessed October 15, 2025, [https://learnprompting.org/docs/retrieval\_augmented\_generation/self-rag](https://learnprompting.org/docs/retrieval_augmented_generation/self-rag)  
26. SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI… \- Medium, accessed October 15, 2025, [https://medium.com/@sahin.samia/self-rag-self-reflective-retrieval-augmented-generation-the-game-changer-in-factual-ai-dd32e59e3ff9](https://medium.com/@sahin.samia/self-rag-self-reflective-retrieval-augmented-generation-the-game-changer-in-factual-ai-dd32e59e3ff9)  
27. Corrective Retrieval Augmented Generation (CRAG) — Paper ..., accessed October 15, 2025, [https://medium.com/@sulbha.jindal/corrective-retrieval-augmented-generation-crag-paper-review-2bf9fe0f3b31](https://medium.com/@sulbha.jindal/corrective-retrieval-augmented-generation-crag-paper-review-2bf9fe0f3b31)  
28. Advanced RAG techniques \- Literal AI, accessed October 15, 2025, [https://www.literalai.com/blog/advanced-rag-techniques](https://www.literalai.com/blog/advanced-rag-techniques)  
29. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights \- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2502.11371v1](https://arxiv.org/html/2502.11371v1)  
30. Four retrieval techniques to improve RAG you need to know | by Thoughtworks \- Medium, accessed October 15, 2025, [https://thoughtworks.medium.com/four-retrieval-techniques-to-improve-rag-you-need-to-know-f641e2b1db6c](https://thoughtworks.medium.com/four-retrieval-techniques-to-improve-rag-you-need-to-know-f641e2b1db6c)  
31. GraphRAG: The Practical Guide for Cost-Effective Document Analysis with Knowledge Graphs \- LearnOpenCV, accessed October 15, 2025, [https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/](https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/)  
32. Intro to GraphRAG \- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=f6pUqDeMiG0](https://www.youtube.com/watch?v=f6pUqDeMiG0)  
33. Getting Started \- GraphRAG \- Microsoft Open Source, accessed October 15, 2025, [https://microsoft.github.io/graphrag/get\_started/](https://microsoft.github.io/graphrag/get_started/)  
34. Advanced RAG Techniques in AI Retrieval: A Deep Dive into the ..., accessed October 15, 2025, [https://medium.com/@LakshmiNarayana\_U/advanced-rag-techniques-in-ai-retrieval-a-deep-dive-into-the-chroma-course-d8b06118cde3](https://medium.com/@LakshmiNarayana_U/advanced-rag-techniques-in-ai-retrieval-a-deep-dive-into-the-chroma-course-d8b06118cde3)  
35. Advanced RAG Techniques \- Guillaume Laforge, accessed October 15, 2025, [https://glaforge.dev/talks/2024/10/14/advanced-rag-techniques/](https://glaforge.dev/talks/2024/10/14/advanced-rag-techniques/)  
36. Four data and model quality challenges tied to generative AI \- Deloitte, accessed October 15, 2025, [https://www.deloitte.com/us/en/insights/topics/digital-transformation/data-integrity-in-ai-engineering.html](https://www.deloitte.com/us/en/insights/topics/digital-transformation/data-integrity-in-ai-engineering.html)  
37. Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG \- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.09136v1](https://arxiv.org/html/2501.09136v1)  
38. Retrieval-Augmented Generation (RAG) with LangChain and Ollama \- Medium, accessed October 15, 2025, [https://medium.com/@tschechd/retrieval-augmented-generation-rag-in-practice-implementing-a-chatbot-with-langchain-and-ollama-79d6d19642f7](https://medium.com/@tschechd/retrieval-augmented-generation-rag-in-practice-implementing-a-chatbot-with-langchain-and-ollama-79d6d19642f7)  
39. Context Engineering \- LangChain Blog, accessed October 15, 2025, [https://blog.langchain.com/context-engineering-for-agents/](https://blog.langchain.com/context-engineering-for-agents/)  
40. Context Engineering Clearly Explained \- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=jLuwLJBQkIs](https://www.youtube.com/watch?v=jLuwLJBQkIs)  
41. crewAIInc/crewAI: Framework for orchestrating role-playing ... \- GitHub, accessed October 15, 2025, [https://github.com/crewAIInc/crewAI](https://github.com/crewAIInc/crewAI)  
42. (PDF) Cowboys and Aliens in the Digital Frontier: The Emergence of ..., accessed October 15, 2025, [https://www.researchgate.net/publication/380098993\_Cowboys\_and\_Aliens\_in\_the\_Digital\_Frontier\_The\_Emergence\_of\_Techno-Social\_Learning\_in\_AI-Enhanced\_Communities\_of\_Practice](https://www.researchgate.net/publication/380098993_Cowboys_and_Aliens_in_the_Digital_Frontier_The_Emergence_of_Techno-Social_Learning_in_AI-Enhanced_Communities_of_Practice)  
43. What is Human-in-the-Loop (HITL) in AI & ML? \- Google Cloud, accessed October 15, 2025, [https://cloud.google.com/discover/human-in-the-loop](https://cloud.google.com/discover/human-in-the-loop)  
44. What Is Human In The Loop (HITL)? \- IBM, accessed October 15, 2025, [https://www.ibm.com/think/topics/human-in-the-loop](https://www.ibm.com/think/topics/human-in-the-loop)  
45. Context Engineering : r/PromptEngineering \- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1me36vh/context\_engineering/](https://www.reddit.com/r/PromptEngineering/comments/1me36vh/context_engineering/)  
46. The New Skill in AI is Not Prompting, It's Context Engineering : r/ArtificialInteligence \- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/ArtificialInteligence/comments/1m973yp/the\_new\_skill\_in\_ai\_is\_not\_prompting\_its\_context/](https://www.reddit.com/r/ArtificialInteligence/comments/1m973yp/the_new_skill_in_ai_is_not_prompting_its_context/)  
47. Manage context window size with advanced AI agents | daily.dev, accessed October 15, 2025, [https://app.daily.dev/posts/manage-context-window-size-with-advanced-ai-agents-m2mil9dwq](https://app.daily.dev/posts/manage-context-window-size-with-advanced-ai-agents-m2mil9dwq)  
48. Andrew-Jang/RAGHub: A community-driven collection of ... \- GitHub, accessed October 15, 2025, [https://github.com/Andrew-Jang/RAGHub](https://github.com/Andrew-Jang/RAGHub)