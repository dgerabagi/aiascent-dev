[
  {
    "id": "report_source",
    "chunk": "<!--\n  File: flattened_repo.md\n  Source Directory: c:\\Projects\\aiascent-dev\n  Date Generated: 2025-10-17T01:12:02.039Z\n  ---\n  Total Files: 34\n  Approx. Tokens: 356076\n-->\n\n<!-- Top 10 Text Files by Token Count -->\n1. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-3.md (32903 tokens)\n2. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-7.md (30722 tokens)\n3. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-11.md (22308 tokens)\n4. context\\v2v\\research-proposals\\04-AI Research Proposal_ V2V Pathway.md (20243 tokens)\n5. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-6.md (19512 tokens)\n6. context\\v2v\\research-proposals\\05-V2V Pathway Research Proposal Execution.md (19323 tokens)\n7. context\\v2v\\research-proposals\\06-V2V Academy Context Engineering Research.md (19246 tokens)\n8. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-9.md (16443 tokens)\n9. context\\v2v\\research-proposals\\09-V2V Pathway Research Proposal.md (16403 tokens)\n10. context\\v2v\\research-proposals\\07-V2V Pathway Research Proposal.md (15711 tokens)\n\n<!-- Full File List -->\n1. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-1.md - Lines: 354 - Chars: 33508 - Tokens: 8377\n2. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-2.md - Lines: 504 - Chars: 50152 - Tokens: 12538\n3. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-3.md - Lines: 1256 - Chars: 131611 - Tokens: 32903\n4. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-4.md - Lines: 130 - Chars: 17890 - Tokens: 4473\n5. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-6.md - Lines: 858 - Chars: 78046 - Tokens: 19512\n6. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-7.md - Lines: 1008 - Chars: 122887 - Tokens: 30722\n7."
  },
  {
    "id": "report_source",
    "chunk": "g\\transcript-6.md - Lines: 858 - Chars: 78046 - Tokens: 19512\n6. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-7.md - Lines: 1008 - Chars: 122887 - Tokens: 30722\n7. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-9.md - Lines: 818 - Chars: 65770 - Tokens: 16443\n8. context\\v2v\\audio-transcripts\\1-on-1-training\\transcript-11.md - Lines: 704 - Chars: 89232 - Tokens: 22308\n9. context\\v2v\\research-proposals\\01-V2V Academy Content Research Plan.md - Lines: 246 - Chars: 52667 - Tokens: 13167\n10. context\\v2v\\research-proposals\\02-V2V Context Engineering Research Plan.md - Lines: 266 - Chars: 61311 - Tokens: 15328\n11. context\\v2v\\research-proposals\\03-AI Research Proposal_ V2V Pathway.md - Lines: 217 - Chars: 61407 - Tokens: 15352\n12. context\\v2v\\research-proposals\\04-AI Research Proposal_ V2V Pathway.md - Lines: 388 - Chars: 80971 - Tokens: 20243\n13. context\\v2v\\research-proposals\\05-V2V Pathway Research Proposal Execution.md - Lines: 309 - Chars: 77291 - Tokens: 19323\n14. context\\v2v\\research-proposals\\06-V2V Academy Context Engineering Research.md - Lines: 419 - Chars: 76982 - Tokens: 19246\n15. context\\v2v\\research-proposals\\07-V2V Pathway Research Proposal.md - Lines: 292 - Chars: 62844 - Tokens: 15711\n16. context\\v2v\\research-proposals\\08-V2V Pathway Research Proposal.md - Lines: 259 - Chars: 62152 - Tokens: 15538\n17. context\\v2v\\research-proposals\\09-V2V Pathway Research Proposal.md - Lines: 221 - Chars: 65612 - Tokens: 16403\n18. src\\Artifacts\\A62 - V2V Academy - Synthesis of Research Proposals.md - Lines: 33 - Chars: 4303 - Tokens: 1076\n19. src\\Artifacts\\A50 - V2V Academy - Core Principles & Philosophy.md - Lines: 42 - Chars: 5240 - Tokens: 1310\n20. src\\Artifacts\\A51 - V2V Academy - The Virtuoso's Workf"
  },
  {
    "id": "report_source",
    "chunk": " 1076\n19. src\\Artifacts\\A50 - V2V Academy - Core Principles & Philosophy.md - Lines: 42 - Chars: 5240 - Tokens: 1310\n20. src\\Artifacts\\A51 - V2V Academy - The Virtuoso's Workflow.md - Lines: 50 - Chars: 4630 - Tokens: 1158\n21. src\\Artifacts\\A53 - V2V Academy - Curriculum Outline.md - Lines: 106 - Chars: 8072 - Tokens: 2018\n22. src\\Artifacts\\A55 - V2V Academy - Glossary of Terms.md - Lines: 153 - Chars: 21591 - Tokens: 5398\n23. src\\Artifacts\\A54 - V2V Academy - Lesson 1.1 - The Virtuoso's Loop.md - Lines: 130 - Chars: 18402 - Tokens: 4601\n24. src\\Artifacts\\A63 - V2V Academy - Lesson 1.2 - The Philosophy of V2V.md - Lines: 94 - Chars: 15137 - Tokens: 3785\n25. src\\Artifacts\\A64 - V2V Academy - Lesson 1.3 - The Citizen Architect.md - Lines: 94 - Chars: 15236 - Tokens: 3809\n26. src\\Artifacts\\A65 - V2V Academy - Lesson 2.1 - Introduction to Data Curation.md - Lines: 93 - Chars: 15186 - Tokens: 3797\n27. src\\Artifacts\\A66 - V2V Academy - Lesson 2.2 - The Art of Annotation.md - Lines: 93 - Chars: 15214 - Tokens: 3804\n28. src\\Artifacts\\A67 - V2V Academy - Lesson 2.3 - Critical Analysis of AI Output.md - Lines: 93 - Chars: 15975 - Tokens: 3994\n29. src\\Artifacts\\A68 - V2V Academy - Lesson 3.1 - From Conversation to Command.md - Lines: 126 - Chars: 16242 - Tokens: 4061\n30. src\\Artifacts\\A69 - V2V Academy - Lesson 3.2 - The Feedback Loop in Practice.md - Lines: 93 - Chars: 15947 - Tokens: 3987\n31. src\\Artifacts\\A70 - V2V Academy - Lesson 3.3 - The Test-and-Revert Workflow.md - Lines: 93 - Chars: 16374 - Tokens: 4094\n32. src\\Artifacts\\A71 - V2V Academy - Lesson 4.1 - Defining Your Vision.md - Lines: 108 - Chars: 15323 - Tokens: 3831\n33. src\\Artifacts\\A72 - V2V Academy - Lesson 4.2 - The Blank Page Problem.md - Lines: 93 - Chars: 15303 "
  },
  {
    "id": "report_source",
    "chunk": "1 - Defining Your Vision.md - Lines: 108 - Chars: 15323 - Tokens: 3831\n33. src\\Artifacts\\A72 - V2V Academy - Lesson 4.2 - The Blank Page Problem.md - Lines: 93 - Chars: 15303 - Tokens: 3826\n34. src\\Artifacts\\A73 - V2V Academy - Lesson 4.3 - Architecting Your MVP.md - Lines: 93 - Chars: 15757 - Tokens: 3940\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-1.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nIt's fine. It's fine. \r\n\r\nWe'll do it live. \r\n\r\nDon't worry, don't worry. \r\n\r\nYeah, yeah, yeah. \r\n\r\nWe're going to do it live. \r\n\r\nYou're going to do it live. Okay. Um, so then that's, but that is, that's why I realized I should have recorded it because that's essentially the end of lesson one. Now that I think of it is you've got the knowledge to put together your documentation file. You don't need to, you don't need to do any cleaning right now. That's not, that's not part of lesson one. \r\n\r\nI showed you so you know what's coming, so you know what to get accumulated. You know what you need to make your static content. The struggle is making it. So if you can accumulate now, if you can make your own SCC repo, both of you, make your own, make your own prompt file. I'll just share this exact starting point. And I'll even share, I'll try to, I'll dig into my, see this, I'll dig into my, I do have, I do, I save all my prompts. \r\n\r\nI just, I don't, I don't organize them because, I just save them and then I get them when I need them. Prompt to upscale in -game content. This is the one. 65 cycles, bro. \r\n\r\nY 'all, it's not going to take y 'all 65. \r\n\r\nYeah, yeah. But this is gold, though. This is gold. You're going to have this. This is going to be like your lab guide, because any freaking problem you're encountering, "
  },
  {
    "id": "report_source",
    "chunk": " \r\n\r\nYeah, yeah. But this is gold, though. This is gold. You're going to have this. This is going to be like your lab guide, because any freaking problem you're encountering, you probably are going to encounter, you know... Anyway, anyway, you can... \r\n\r\nThis would be exactly, you know... This is, see, now I need, now we're doing in -game two. Here's the section titles. Look at how much of work it is with Cycle 60 for me, right? Because I've got all the context. I've got all the context already. \r\n\r\nAnd look, I'm making progress. Nice, all right, perfect. Now let's do this. Oh, I realized something. We need to, you know, we got the cores wrong, right? You got something. \r\n\r\nI was doing some images or something. Oh, I was making images at this point. See? Okay. So yeah. Okay. \r\n\r\nSo I'll, that'll be my piece together lesson or resources. Mainly two, mainly two. The prompt I used for in -game and the prompt the final version of the prompt so it'll have all the cycles in it All you'll care about are the cycles. You won't really care about What the current files state is at because that's kind of the living document part. That's always changing That was the current files. I needed at cycle 65. \r\n\r\nMaybe it has nothing to do with the rest because I was making freaking images, right? Um, so but what you care about are the the knowledge will be within the cycles. So i'll accumulate that Um, you can even use it to help you come up with your own cycle zero prompts. I have a cycle zero in, you'll have to use a version. Each one will have a cycle zero in it as well. But you see, project scope. \r\n\r\nYou see, interaction schema. You see, I've already got an interaction schema with seven different steps. You can take this. Internal ver"
  },
  {
    "id": "report_source",
    "chunk": " well. But you see, project scope. \r\n\r\nYou see, interaction schema. You see, I've already got an interaction schema with seven different steps. You can take this. Internal versus learner facing dialogue. Maintain a strict separation between internal development or cycles. And, oh, and you know what? \r\n\r\nI had to write this for stupider AI. Smarter AI's don't need as much of this kind of steering. Okay. But so, and the way you will, you won't make an interaction schema initially, it should come naturally. And simply for that reason, I just explained, um, the AI you're using now is, is a smarter AI than the one I was using. then. \r\n\r\nThis wouldn't hurt to have, but if the AI, what you would be not knowing is can the AI do it without this extra guidance or not? Because then at that point, this becomes extra baggage. The AI only has X amount of time to process your query. And so if you're throwing in extra baggage to it, it's going to spend time on that extra baggage versus a cleaner prompt. So, but it's, It's better to have it and not need it. So that will be what I do. \r\n\r\nYou will get these two files. You can add them in to your repo as a document that you can then just click and review anytime. See, that's part of the lesson is recognizing that that should go in the documents. One of the first things I think we'll do, one of the first prompts is to start making some artifacts, which would be to try to Separate you thinking about which sections of this document you're gonna want to iterate on because you can start to separate this large document out into Smaller artifacts so that you can iterate on each and any one the AI will give you back the exact artifact in in These because that's how you deal with it It will deal wi"
  },
  {
    "id": "report_source",
    "chunk": "into Smaller artifacts so that you can iterate on each and any one the AI will give you back the exact artifact in in These because that's how you deal with it It will deal with you and then you can copy and then you can diff whatever it gives you Oh, the only difference in the JSON was this. Easy peasy. We'll be doing the exact same diffing. \r\n\r\nAnd that's how you'll get \r\n\r\nthat's how you'll rise above the wheat from the chaff. \r\n\r\nBecause any old person using AI is taking the first response, aren't they? \r\n\r\nBut we will have a methodical process where we will have this environment that allows us to pick the best responses and start from there. \r\n\r\nOkay? Yep. Okay. So any questions? No. put together a repo, and then that's it. \r\n\r\nI'll get you those resources. \r\n\r\nOnce you feel like you have a repo, we will make a script that makes your flattened repo. \r\n\r\nAnd then we can do our first prompt. And I'm not joking, take your time. It took me three days to put together the beacon, okay? Yes. Sorry? Oh, this one already? \r\n\r\nYeah, absolutely. You can't have the AI make a lesson unless it knows how we style it. \r\n\r\nour content. \r\n\r\nThat would be an inclusion in the repo. See, I can go through my list, and that would help you add to your list. Yeah, I was going to say, you probably have, because I just had that copied over to Claude, basically, in a text document. But if you already have that stuff, that's definitely, obviously, a style guide is going to be one of them. Yep. Okay, go back up though. \r\n\r\nI'm saving this under, where did you put it? You said I just got to make the documents, right? So I put all the documents in there? Yep, you can start in just one folder. That's why we started with just two folders. With thing"
  },
  {
    "id": "report_source",
    "chunk": "id I just got to make the documents, right? So I put all the documents in there? Yep, you can start in just one folder. That's why we started with just two folders. With things you're adding, throw them in documents. \r\n\r\nOnce you've got 10 things, then you'll think of how they should be organized. And we'll make it so you can change it on the fly when you realize it's more. And it's simple because you'll have checkboxes. So you'll be like, well, I don't want to have to check five files if I put them in one folder because they're related now. I realize they're related. You'll organize it naturally. \r\n\r\nI can't tell you. You have to do it. What do you generally put under artifacts? Don't worry. You won't be putting anything. The AI will create them for you. \r\n\r\nLet me show you... What an example would be like an artifact? I can show you what that would look like. Yes. I can absolutely show you that. Okay. \r\n\r\nSo say you're... \r\n\r\nExactly. \r\n\r\nOkay. Man. So I might have a folder of ELOs or would that be under documentation? \r\n\r\nSo, okay. \r\n\r\nThis IOC track... Open. Let me see. Hold on. Give me a second. Here we go. \r\n\r\nNo. Artifacts list. So the instructor guide template is one artifact. Oh, no, no, no, no, no, no, no, no, no. In our new definition, excuse me. Those are the outputs of the AI. \r\n\r\nThings you ask of it. Oh, it puts it out in artifacts? Yeah. And so they look the same as a document within our one file, because they're both going to be programmatically handled. But I'll just write it out. So like it would literally be this. \r\n\r\nArtifact one is section B from training because that is the particular section that you're focusing on on this first cycle, let's just say. And so you're asking the AI to upscale that se"
  },
  {
    "id": "report_source",
    "chunk": "e is section B from training because that is the particular section that you're focusing on on this first cycle, let's just say. And so you're asking the AI to upscale that section and then it'll actually give it back to you. It'll actually give you the way we'll program it. It'll give you a description of what the artifact is. We'll have it, we'll give it some tags. It will just do this automatically. \r\n\r\nIt'll tag our artifacts. Tags, go here. And then literally, it could be just a few paragraphs. It could be your code that you needed it to write for you. But what it is, is it's all the content in here that then you literally copy once, from the response, because, you know, we copy and paste, yeah. And then you literally make a new file, name it what it calls it, section B. from training dot markdown or json or whatever and then drop it in because the program will \r\n\r\nlet me go back a bit, if I just alt -tab, yep, okay, the program will write this in. When you flatten it, that's what the script does, see? And the script will count the size of this thing, and when it does, when it makes this file. See, this is all the flattened artifacts. \r\n\r\nArtifacts are just the same as documents, the only difference is they're the ones you're iterating on, they're things that you need to work with the AI to produce. \r\n\r\nAnd so if you just think of them as some things that you don't change, it's very helpful because there's an urge to make changes yourself. And it's better for you in the long run if you learn to everything, learn how to ask the AI to do it, because do that for a year, and then the way you ask it is different. So yeah, that's what an artifact is. It'll be something - Going back to the repo then, because I'm trying to "
  },
  {
    "id": "report_source",
    "chunk": "use do that for a year, and then the way you ask it is different. So yeah, that's what an artifact is. It'll be something - Going back to the repo then, because I'm trying to think, and let me just think out loud, and then you can just help me think of what folders in the repo I would need to eat. Again, don't, oh, one more thing. Don't stress too much about organization now, beyond the doc. \r\n\r\nI'm not worried. Yeah, I'm just trying to get what information. So I'm still trying to connect the dots together, right? So little things in here and there are starting to connect. But yeah, because you said, hey, I need you to get what you need in your repo. So I'm trying to figure out. \r\n\r\nSo I need all my documentation that's going to be for that lesson, right? So I need a documents repo. In the documents repo though, I want to keep enabling learning objectives and the actual framework it gets tied to separate. So do I have one that's like enabling learning objectives, right? So like when I go into prompt, I can say, hey, these are all the enabling learning objectives are, you know, here located here, like, or do you keep that under your documents? Once they're in, you can, you can make them artifacts as well. \r\n\r\nand the term make quote making something an artifact is just giving it like an a number and then you refer to it as that and you can put two things in one artifact it's very unstructured um then you just say you know refer to artifact three you can so ah so like this so i started to i tried to group so i did find metadata was a valuable grouping because i would do like artifact Section three is the metadata. So so this isn't so let's can we use a real can we just go back to? The second link I sent you with a third Th"
  },
  {
    "id": "report_source",
    "chunk": " because i would do like artifact Section three is the metadata. So so this isn't so let's can we use a real can we just go back to? The second link I sent you with a third The which one was it? I'll go to the last link. I sent you. Okay. \r\n\r\nWas it the last one? Yeah, so I have like for example, like the ELOs are right there, right? This is what I have to build the training off of those, right? You can see I'm six one one six one two all that right and So how would you structure that data? Like I wouldn't beyond this, this would be one artifact and you could then hold. That's right. \r\n\r\nAnd then whatever you want to work on, on that artifact, you say the ELOs in artifact three, and then that's what it's going to work on to upgrade. And then you would, you would bring, you would want your metadata that it, the AI is going to be referencing in order to upscale it. That's going to be included in your cycle files list, right? You're going to make sure that. Yeah, I guess that's the part I'm having like. OK, so you converted this to a. \r\n\r\nPDF or Word document. That's right. You fed it to it, right? I'm just dropping it in here. It will, we'll have a script. But you asked it to put it in an HTML format, right? \r\n\r\nNot yet. Not yet. No, all I did. Not yet. And then, no, again, we're going to make a script that does that. That's going to be the sort of lesson two, once you've got a decent starting point and documents. \r\n\r\nBecause you're just thinking through it all, it's worth it. And it does take time. I've done it three, four, five times. And every time, I realized, man, it really does take time. But once you get it and you're making cycles, it's mindless. And then the next time, it's even easier. \r\n\r\nSo you don't get ahead "
  },
  {
    "id": "report_source",
    "chunk": ", I realized, man, it really does take time. But once you get it and you're making cycles, it's mindless. And then the next time, it's even easier. \r\n\r\nSo you don't get ahead of yourself. Making it into some sort of PDF or a doc file is fine, too. The more different document files, Ultimately, you want them all marked down just because it works the best. It's the best medium in between it all. Right. Unless it's like an audio file. \r\n\r\nWell, yeah, then you still convert it to whatever. Everything is just text. Honestly, everything. It's all just text. So if you can in goal, Markdown is probably your best bet. And only in an odd situation, you won't be able to. \r\n\r\nBut just make it a PDF. \r\n\r\nbecause the PDF captures images, right? \r\n\r\nAnd then so we'll have it captured in the PDF so that we can try to do OCR or, you know, make sure an image model gets the image data from the images. \r\n\r\nSo we're not, so we're really doing everything correctly, but that's it. So artifacts can be anything. The only limiting factor to an artifact is quite literally its size because, and what size is the output size? of the AI you're using, the output token length. If it cannot output your entire artifact in one go, then it's probably time to reconsider the size of your artifact. Other than that, an artifact can be anything. \r\n\r\nThe more organized, the better, but it can be anything. Gotcha. Yep. Okay. Oh, here's an example. An artifact can even be a set of artifacts. \r\n\r\nSo you can put together a complete, this would be an example, of something that would take time but it would be a get a Completed lab or not a lab a lesson that is already completed That is exactly what a completed lesson should look like if you wrap that up into an example"
  },
  {
    "id": "report_source",
    "chunk": " it would be a get a Completed lab or not a lab a lesson that is already completed That is exactly what a completed lesson should look like if you wrap that up into an example Artifact then it will all that will that is literally what few shot learning is It's good. \r\n\r\nYou just went from zero shot to one shot. \r\n\r\nIt has one example that you're now, you've done some machine learning operations right there. That's what machine learning engineers do is they create these few shot example data sets. That's what it is in point of fact. So that would be an example. If you know yourself a lab or excuse me, a lesson that is perfect. When I made the beacon, I had the beacon one done. \r\n\r\nI used that as my, I had zero shot. \r\n\r\nAnd then I had my beacon one done. \r\n\r\nI used my beacon one as an example to make the end game. What homework do you need me to do? You will review, step one, review the two resources I give you. You won't need to look at the actual files. \r\n\r\nThere are tables of contents for a reason. \r\n\r\nThere's just lists of them. Just consider why this is added, right? That kind of thing. That will lead to you, to step two, to creating your own list, your own set. Okay. See. \r\n\r\nOh, nice. I said the case that's list many times. Yeah. You should have, you should have some work roles. Yeah. Cause I would want to run like, yeah, I guess I'm getting like probably it can be a table, an Excel table. \r\n\r\nYeah. That's what I was saying. Cause I want to put like ELOs and stuff like in an Excel table. \r\n\r\nSo when, when we get a new lessons, that's usually what I do and then organize it and then break them out into different tabs based off of what lab or static content I want those ELOs. \r\n\r\nthat section to cover. Here's a smalle"
  },
  {
    "id": "report_source",
    "chunk": "hat I do and then organize it and then break them out into different tabs based off of what lab or static content I want those ELOs. \r\n\r\nthat section to cover. Here's a smaller one because instructor guides, so that's static content. It was a very small project. Jeff made a change to the instructor guide template, and so we needed to adjust. So I had my initial, I treated it as a draft. I just called it the word draft. \r\n\r\nThat way the AI knows it can change it. more than normal. So draft instructor guide, the tasks which contains the lab steps, and then the lab which contains the lab's environment. And then, so that's because it was two labs, so two sets of that. Two sets of that, four files. And then \r\n\r\nthe finalized markdown file was the one that I, let me see, that I iterated on. And then once I was finished with it, I brought it back in here as my few shot. Because why would I need to bring back the finalized product back into, I'm done, I'm done. I finished the product. It was in confluence. But then I brought it right back in to continue working on the next, project because I found I was like, well, it's context, right? \r\n\r\nSo, okay. So that's just what's in the artifacts. So the four and then the four and the two completed artifacts, 10 files. Is it 10 files? Okay. And then I think I was just, I literally, I just created this as a demo. \r\n\r\nI'm going to delete it, right? I just created that in front of you. And then the instructor guide template, you see what it needs to change into. The template it was see see obviously my script changes it from the pdf nasty into the markdown see Gotcha. \r\n\r\nYeah, that's what your script will do. \r\n\r\nUm, that's why I have two sets of them, but it's really just the same file Ye"
  },
  {
    "id": "report_source",
    "chunk": "t from the pdf nasty into the markdown see Gotcha. \r\n\r\nYeah, that's what your script will do. \r\n\r\nUm, that's why I have two sets of them, but it's really just the same file Yeah, see and then that this is what this was what your yours yours will look like this without the markdown You'll have the pdf you'll have the pdf versions and then we'll re when whenever you're ready, uh, we'll reconvene See, I even have my prompt file in here. \r\n\r\nThis was the prompt to do the um, so this might be even Useful as well. \r\n\r\nI'll read it before I send it, but I was going to give you two I might give you three if I find yeah, this is perfect. \r\n\r\nThis actually would be great project constraints Non -technical proctors, right? \r\n\r\nDon't provide back -end infrastructure related information like we use ansible. \r\n\r\nDon't say that \r\n\r\nJust give them the front -end. See, I even said... See, so that's it. And this comes naturally when I didn't like the output. See? Okay. \r\n\r\nOkay, so that'll be what I do to you, and then you take what I give you, review, and then you put together your documents list, and then we'll reconvene and make a script. Sounds good. Cool. And ask questions anytime, offline, anytime. Okay. Um, cause my brain's all over the place. \r\n\r\nWe went through, we were lost up. Yeah. I just need a clear, quick list of like what's going to happen next. So you're going to kind of go through and create like an outline and then you want me to review it. \r\n\r\nAnd then I'm going to give you two files, uh, which are the two files, the big boy files that have 60 cycles in them. \r\n\r\nUm, that you can just go read at cycle zero and read up. Until you feel good, until you've got the ideas to, oh, I need this file, oh, I need this, and just g"
  },
  {
    "id": "report_source",
    "chunk": "les in them. \r\n\r\nUm, that you can just go read at cycle zero and read up. Until you feel good, until you've got the ideas to, oh, I need this file, oh, I need this, and just go put the files, and then that's it, keep reading. Go put in some files, keep reading. \r\n\r\nBecause you're just drag and dropping PDFs. You're making your, oh, I need this, make it a PDF, drop it in. Because we're not processing anything yet. We're keeping it simple, we're just, yeah. We're getting our ducks in a row, yeah. \r\n\r\nAll right, so you're going to send that over to me? \r\n\r\nYep. Yeah, two files. I'll send it to both of you. It's just two markdown files. And then I'll write an instruction when I send it. Okay. \r\n\r\nSo review those files. I'm just writing this down. Review those files. As you get inspiration reviewing them, because they should be full of inspiration. Can you bring up those the cycles again yeah so this is what I got a review right here let me go to the bigger file sorry that one was yes he cycles let me go to cycle zero see I'm just searching cycle zero colon there's only two entrances of it so I don't know what's going on one's probably just a copy and paste ones at the top they're both right next to each other so So I'm just trying to see if those prompts are active. \r\n\r\nSo these are like basically what I will, uh, reviewing and see if that's relevant to a static content. So what these are, this is, I went from Oleobits content to completed, end -to -end reviewed, approved UKI content in this file, in this one file. But that's a lab though, right? Labs and lessons. Okay, because right now I'm only focusing on the lessons. \r\n\r\nI'm not dealing with the labs right now. \r\n\r\nI understand. Okay, lesson related stuff. \r\n\r\nThere can "
  },
  {
    "id": "report_source",
    "chunk": "nd lessons. Okay, because right now I'm only focusing on the lessons. \r\n\r\nI'm not dealing with the labs right now. \r\n\r\nI understand. Okay, lesson related stuff. \r\n\r\nThere can be. \r\n\r\nLab related, lesson related. \r\n\r\nNot always am I that organized, but luckily it says it right in front of us. \r\n\r\nAgain, this is just messy inspiration. \r\n\r\nWhen you don't know what to add, come in here and read cycle zero. Because these are tangential parallel problems. I was making lessons in labs for cybersecurity using KSATs and all of our same knowledge artifacts. \r\n\r\nSo you'll get inspiration by sitting here and reading this, I promise you. \r\n\r\nJust read the cycles. And then again, see, look, two, three, four. Cycles aren't that, ultimately, that dense at the end of the day, because it's all, you know, you see what I mean? Seven, nine, you know, just... I'm just going to say, to really because again, competencies, I'm just going to have to wrap my head that like you were writing this as you were trying to troubleshoot and get something done. \r\n\r\nYeah. Obviously, I wasn't there when you did it. So try to comprehend. Oh, right. Right. Yeah. \r\n\r\nYou know what I mean? It's going to be like, OK, well, what were you? I kind of have to, like, put a puzzle together. That's that's the problem. That makes sense. That makes a lot of sense. \r\n\r\nUh, so I'll try to like, I'll go through and try to grab some inspiration. And that's why I said cycle zero cycle one that, you know, it is putting the puzzles. Cause you're going to, you're about to do this, right. You're about to go. Would it be easier if I wrote down the appropriate, like if I recorded all the appropriate steps, like, like it's kind of like in that chat, GPT document that I put in there,"
  },
  {
    "id": "report_source",
    "chunk": " Would it be easier if I wrote down the appropriate, like if I recorded all the appropriate steps, like, like it's kind of like in that chat, GPT document that I put in there, like what I, um, like the steps that I have it in there and then kind of like, you know, turn those into cycles and look at what you had and compare. Mine are like super simple. \r\n\r\nNo, that can be what happens up in your project plan. I put in the chat here, right at the top, the chat GPT training design. Yeah. And that was just, and again, this is like, so I had a bunch of stuff and I was continuously doing it and then I just threw it all in the chat GPT and then from there I just refined it because it used to be longer, had extra crap in there. You know, this is like what I was trying to get and it puts out a really good output. Like it's pretty good. \r\n\r\nBased off of you know, whatever. So okay. All right. All right. Got it. Got it So this is here's an idea. \r\n\r\nSo I'll walk through the whole process from this to an artifact So this you would take this file this PDF and we could call it your initial project plan. Would that be accurate? \r\n\r\nYeah, that's your artifact zero right artifact one Your your initial project plan. \r\n\r\nSo you would take this as a PDF just because it's a PDF even though it's looking like a PDF just text, and you could just copy and paste, and unformatted text, see, you'd go, you'd do a nice PDF markdown so you keep all the formatting, because headers mean many things, not just a header level. But even though this has no formatting whatsoever, we will still treat it as if it did, so that you'll get an idea. \r\n\r\nYou would take this PDF, use the script to convert it to markdown, Then you would copy that markdown out, create "
  },
  {
    "id": "report_source",
    "chunk": " still treat it as if it did, so that you'll get an idea. \r\n\r\nYou would take this PDF, use the script to convert it to markdown, Then you would copy that markdown out, create a new artifact file in the artifacts section. And then that would be, name it, Artifact 1. \r\n\r\nThat's how it becomes Artifact 1, because now it's tagged. Artifacts are just, it's just tagging it somehow. Yeah, I like, I want to refine this process right here, because I mean it's not perfect. I just use it and then... That's it, that's it. Once it's Artifact 1, refine it. \r\n\r\nYou can say, refine this. This was the rough, exactly. Oh, okay, gotcha, yeah. Yes, yes, that could be, that could be... Like, I continuously, like, I'm like, hey, if it's not providing... Yes, that... \r\n\r\nIt doesn't, right now, it just doesn't put it in, like, in the, it doesn't give it the technical review writing style. I do that afterward. I just have it gathered. Add those... But one of the states, I... Yeah. \r\n\r\nAdd those pieces of documentation. That would be your Artifact 2, Artifact 3. And that would be one of your first prompts, is update this in line with those. Make a better blueprint for yourself. \r\n\r\nCan you show me quickly, like if you were to do this document quickly? \r\n\r\nI just want to do a step so I can... Yeah, absolutely. So because it's... I will do it simply because it should be able to copy it all because there is no reformatting. And I should be able to go to our repo that we were putting together. And so I'll first make the initial plan markdown. \r\n\r\nThis is what the script would produce. It would actually literally look like this. You would not edit it. Who cares? It's fine. It's fine, because it'll be wrapped. \r\n\r\nIt will be wrapped. You won't do it. Y"
  },
  {
    "id": "report_source",
    "chunk": "oduce. It would actually literally look like this. You would not edit it. Who cares? It's fine. It's fine, because it'll be wrapped. \r\n\r\nIt will be wrapped. You won't do it. You'll just simply have it here, but you need it tagged, and you would prefer it to be organized, and what better to do than, you know, name it the same thing. I'm just putting the word project, but it doesn't matter. So you create, like, the artifact, mark that, and then you tell it to... Yeah. \r\n\r\nNow, by default, Doing that, that's all you would have to do, because our script, when we run it, it will detect a new file has appeared. It will do this. It will add it. It'll add it to the flattened repo, and then you will just be taking a copy, Control -A. Now, is there a reason if you have it under documents and artifacts, though? Is that because it's just the original? \r\n\r\nThat's right. That's right. This is what you originally started with. This is going to be iterated on, isn't it? So I like I'm in my mind in my repo. I want documents to be official documents with static contents, so not markdown, not things that. \r\n\r\nI mean personally, so documents would be like references, like SOPs, things like that, that's official. \r\n\r\nThat's right, that's right. It's only markdown. \r\n\r\nNo, no, no, no, no. \r\n\r\nIt's only markdown, so it's portable for you to copy and paste the text. \r\n\r\nSo you can do this. \r\n\r\nThat's the only, if you could do... \r\n\r\nNo, no, no, yeah, I see. I think, just me, because of my organizational skills, I would want, like, have different document folders, I guess. \r\n\r\nOne would be official documentation, which is not like... \r\n\r\nAnything I created this is like the stuff I'm referencing. That's right. That's right So and then so the I wo"
  },
  {
    "id": "report_source",
    "chunk": "\r\n\r\nOne would be official documentation, which is not like... \r\n\r\nAnything I created this is like the stuff I'm referencing. That's right. That's right So and then so the I would put the initial like if I were gonna say hey, and then I would have like a markdown documents folder. I probably put it in there because I would want to keep it all coming You know what I mean? Yeah, just me personally and that's totally fine. \r\n\r\nAnd okay. Yes, it doesn't matter then. \r\n\r\nOkay. I just want to make sure Let me say that let me yeah Let me say this again because this is literally going to be Made so that you can move this around and you will never have to think twice Oh, and then so I can create another folder and then again again, it will the one Constraint that you will find out instruction look look I'm predicting this I because I know how it's gonna work The only constraint you will find is you will realize it gets annoying to check the box to select and deselect You will realize it's better if these five files go in a folder so that I can just click click it That's what's gonna happen when you're when you're mature you're that's how you're gonna Constrain your organization is you're just gonna say I don't want to check the boxes as you're flipping around your context It's just much easier if you could check one folder directory, because it contains folders that you know are relevant. \r\n\r\nYou can make subfolders in there, can't you? Yeah, yeah. Oh, yeah, yeah, yeah, yeah. Oh, yeah, yeah, yeah. \r\n\r\nThat's what I'm saying. \r\n\r\nOh, OK. Oh, I'm so sorry. I understand completely now. No, that's just me. No, no, look, look. OK, so a new folder. \r\n\r\nAll of my instructions and stuff that I do, I would throw, like, in another. Yeah, of"
  },
  {
    "id": "report_source",
    "chunk": "derstand completely now. No, that's just me. No, no, look, look. OK, so a new folder. \r\n\r\nAll of my instructions and stuff that I do, I would throw, like, in another. Yeah, official documentation would be, like, the... Well, I guess the... \r\n\r\nThe UK, I learned one. \r\n\r\nThat's not official documentation. I would say that's your work. Yeah. So I put like an official documentation. I would put all the resources the Navy gave me that they want to build. Yeah. \r\n\r\nBuild their instructions. You know, their training off of working doc. Yeah. \r\n\r\nThere we go. \r\n\r\nWorking documentations. \r\n\r\nMarkdown. \r\n\r\nThat's me because I like that. \r\n\r\nI know it's a markdown. \r\n\r\nThat's the three days. \r\n\r\nThis is the three days. \r\n\r\nAnd for the final and final. And I never wanted to do this organization in my life up until. \r\n\r\nthe AI values it, right? \r\n\r\nYou see what I'm saying? So it's now fun to be organized. It's valuable. Yeah, that's because I can get, you know, even like my folder structures that I build like on my own computer. Like I try to keep everything is organized. It gets out of hand, but then I go through like here. \r\n\r\nI'll just kind of that's data labeling. That's data labeling. That's data annotation. Yeah, so I'll show you quick. I don't know. That's the skill set. \r\n\r\nThat's my pet peeve. Like this is my work document folder right here. I don't know if you can see this. Not yet. Hold on. OK, no, it's still not yet. \r\n\r\nOK, yeah, I'll zoom in. Well, that doesn't. Yeah. I'm bad. \r\n\r\nI'm bad. \r\n\r\nI put everything in one folder. \r\n\r\nI'm really bad Oh, no, so like I have articulate projects and then here but look I have all my LES numbers Yeah in there so I can keep it all organized So that's just and then my actual project"
  },
  {
    "id": "report_source",
    "chunk": " no, so like I have articulate projects and then here but look I have all my LES numbers Yeah in there so I can keep it all organized So that's just and then my actual project files and see doc jqr I have my module one resource module to sort of resource my draft lessons So there will be a moment I can help you out with like that because yeah, I see your fault Like that's what's confusing me is because you have like your documents folders has so much stuff I'm like, okay, but it's I want to keep those documents organized so I can quickly find them like official documents Markdown documents like that. I'm putting together and then uh, you know stuff like that. Um, you know, uh maybe have one that's like template like so yeah that's metadata i was calling it metadata templates folder yeah so like depending on the training i can say hey base off this this is the template i want to use right yep so that's what you need for me then to build out that repo of like the documentation you and you want me to put the documents in there \r\n\r\nThat's right. \r\n\r\nAnd then I'll literally, when we make the scripts, you'll be taking a screenshot. \r\n\r\nYou'll be taking a screenshot. \r\n\r\nSo it's totally your script. You get what I'm saying? Based off your own, your own, what you want. It's literally what you desire. \r\n\r\nAnd in the same way, Austin, if making files is a pain for him, then his scripts will be completely written such because that's what he complained to the AI about and that's what the AI fixed for him. \r\n\r\nSo Ben will get three different scripts that all do the same thing that then it's going to be good. It's going to be good. But yeah, you're going to realize what you already have organized is in the same way I just went in here"
  },
  {
    "id": "report_source",
    "chunk": "the same thing that then it's going to be good. It's going to be good. But yeah, you're going to realize what you already have organized is in the same way I just went in here and write code dot. You can do that too with your organized file structure and then bada bing bada boom, your entire files would be right here. So you're already, you might be able to do that as well. don't discount this little trick you already learned was the first thing I showed you, was this is why I do it this way? \r\n\r\nBecause it's damn powerful, it's easy, it's actually. Yeah, that's what I did. I used that and popped it open. Now go to, now try that. Easy day. Try that. \r\n\r\nThe synapses are connecting now. I've got it. Okay, yeah, try that. Go to your other repo. So build out my, what I think I need for my repo. For this job. \r\n\r\nTo keep my data organized, and then I'll share that with you. For this task. Once I get that done. Yeah, task specific, right. Because like templates, we can like all we can import all the templates that Brian has put in the conference. We're getting it. \r\n\r\nYeah, we will. \r\n\r\nThat's that's my all my roadmap. \r\n\r\nActually, I do. \r\n\r\nYes, that's exactly it. That's the plan. And it will be automatic. That will be. See, I'm going to we're going to make a rag system that will do that. So you don't have to. \r\n\r\nWhat you're doing here is the proof that that we should spend the money on it. But you're also learning the real freaking skills. \r\n\r\nThis is real freaking skills that are translatable and the rest of your life. The time you spend with me will benefit you. for the rest of your life. Oh my goodness. I'm so happy. I'm so happy to share this information. \r\n\r\nUm, so, um, so it's just going to be proof that we will, I "
  },
  {
    "id": "report_source",
    "chunk": "l benefit you. for the rest of your life. Oh my goodness. I'm so happy. I'm so happy to share this information. \r\n\r\nUm, so, um, so it's just going to be proof that we will, I will use your, see, I will use your cycles to, to Ben will use your cycles to create an agent that can do some of these things. You'll, you'll be freed to do other things. I don't think you're going to lose your job. I think the people who know how to use the tool. No, that's not comforting. No, no, you know what I'm saying? \r\n\r\nYou know what I'm saying? No, me too, me too. I'm in the same boat you are, man, you know? But what protects me is the knowledge of the script. The tool. Yeah, you got to be, you know, we're not recording this, are we? \r\n\r\nJust my own phone. I can stop at this point. Yeah. Yeah.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-2.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nYeah, let me.. . \r\n\r\nYeah, Discord would be nice. \r\n\r\nIt's more powerful. It's better. Yeah, go ahead. And then also here, and also kind of make sure that with... I want to make sure I don't go too far, like with a complicated repo. I think I'm kind of getting there. \r\n\r\nOkay, okay. That's already a good start. So yeah, so this is what I had so far. Artifacts, I ain't putting anything in there, but documentations. So I wanted to break out CUI documents, if we had any. Yep. \r\n\r\nSo we know not to touch those or be careful in referencing that. Customer documentations, so what they provide us directly, whether that's training material, references, or whatever. Cyber reports, I can read it, but like in my training, I use a lot of like MITRE reports and stuff like that, like actual events to kind of, y"
  },
  {
    "id": "report_source",
    "chunk": "terial, references, or whatever. Cyber reports, I can read it, but like in my training, I use a lot of like MITRE reports and stuff like that, like actual events to kind of, you know, associate the training with a real life, a real world event. DOD policies like I feel like these things right here like we can fill that up and that can just be in like everybody's repo like all the right, you know policies that are You know, for us and then... Exactly. Really quick, that's exactly what will come out of this. \r\n\r\nI just want to paint a little bit of the end goal so that helps you give you direction. That's precisely what's going to naturally come out of this. Ben is going to have three versions and then he's going to be able to see between three, well all three needed these documents. And then that's how you know to build the next piece of the puzzle. So yeah, so that's exactly. \r\n\r\nUm, and this is like, there's more of a generic project documents, um, you know, things that you've kind of create like, Oh, so different, different tasks maybe. \r\n\r\nRight. Cause eventually you may have different tasks as well in there. Cause I have assortment of stuff. Like I'll start like messing around. Like I, you know, I just put that in mind with, uh, um, No, this is good. This is exactly how you should do it. \r\n\r\nI have like, you know, these are all my random documents that I've like, I've taken existing ones and manipulated. And so like, they're not like the, you know, they might be adjusted, like word documents from stuff I got, you know, things like that. I call those like, I call those living documents. Yeah. \r\n\r\nLike working documents. \r\n\r\nYeah. So that's the, yeah. So project documents, references. So this is also, I reference a lot"
  },
  {
    "id": "report_source",
    "chunk": "hose like, I call those living documents. Yeah. \r\n\r\nLike working documents. \r\n\r\nYeah. So that's the, yeah. So project documents, references. So this is also, I reference a lot of like open source, you know, stuff. Um, yeah. Anytime you need to pull something in, like, for example, a GitHub repo, you flatten it and you drop it into your references and then it's available for you to flip in or out of your context. \r\n\r\nThat's where that would go. \r\n\r\nYep. That's good. Um, and then this one's like official, like UKI documents that may be associated with, I don't know. Uh, well, I was thinking for this, uh, it might be like, um, like if we can take all of, uh, Sorry, I'm running on fumes today. No problem. If we convert some of the stuff like the tech writing stuff into documents, we could probably maybe put that in there or it could be under, I'll go into the next one. \r\n\r\nAs we're getting stuff and putting them in, it'll make more sense and we can take away a folder or add a folder as we need it. \r\n\r\nPrecisely, and you won't have to have a headache of managing the files list yet. Yeah. there's nothing going in here right now until we go. Alright, so I got frameworks. I want to get it all as much frameworks as possible, so if we could somehow import the whole like DCWF framework in here. Yeah, you see that could be actually part of our script. \r\n\r\nSee, depending on what it is and how fresh of the data we need it, that could very well be, again, just more forward thinking. That's exactly what we could make a script to do is when you run the script, it'll actually go and do a web crawl and do pull something very specific from the internet that you know you need. You see what I'm saying? How powerful the on -the -fly tooling be"
  },
  {
    "id": "report_source",
    "chunk": " actually go and do a web crawl and do pull something very specific from the internet that you know you need. You see what I'm saying? How powerful the on -the -fly tooling becomes. I never even I only use my on -the -fly tooling for my own little code repo, but that's next level to have it actually go and grab something from the internet to keep it fresh for your context. That's pretty crazy. \r\n\r\nBut we could do that. Or I have to go to the public DCWF page. So all the frameworks are available online. It's just pulling it and then into whatever HTML format, whatever we need for it to make it readable. And then we have a couple of other frameworks. Removing the xx tokens. \r\n\r\nThat's it. Everything can be very unstructured as long as you wrap it in an artifact. What are the The learning objectives is that going to be like a project specific list of learning objectives or what are the yeah? So yeah, so enabling learning objectives, so you know like if we're trying to do like a master So I didn't know when is this gonna be specifically project based so like ah good question so your repo will actually just be your actual Actually, this is what I realized sort of today, is it becomes sort of like your external brain. You will use the same repo over time. the context with it, because we'll have 2 million tokens to 10 million tokens to 20 million. \r\n\r\nSo this little quote, this will become a very little repo at all things considered. And then this is your, this is literally what makes the AI better for you. When other people use AI, they're just asking us nine letter question. But when you're using AI, you have 900 ,000 tokens of this is who I am and what I usually effing do. Um, right. It's very different response. \r\n\r\nAnd so "
  },
  {
    "id": "report_source",
    "chunk": "nine letter question. But when you're using AI, you have 900 ,000 tokens of this is who I am and what I usually effing do. Um, right. It's very different response. \r\n\r\nAnd so this is going to be grow. That's why I'm asking specifically about the learning objectives, because what we all, all we need to do is just add a task. So make a new folder. That's just tasks like a new main high level folder, because that's, what's going to happen. You're going to be flipping in between tasks, but much of your metadata may stay the same in between tasks. \r\n\r\nSo having it, having a tasks folder. \r\n\r\nAnd then that's where automatically you make a new task, which is the current thing you're working on. And then all of a sudden, all your learning objectives have a place to go for that. Right. Yeah. So that's, yeah. Cause I was starting to think about that too. \r\n\r\nSo this D I was thinking this will be more like, uh, well, I guess this would be, uh, we, you know, you can have like a master ELO that contains all of them for the, uh, like the project and here, but then I'm like, okay, well, how do we break that up? Like at a per module per lab basis? And I think that I'm trying to structure it. One, I got to figure out as we're using it, and you're showing me what's the best way to reference these things. But eventually, these all have to be broken down. So maybe the ELOs will have, under there, it would be based on, I'll have an NCDoc folder that has all the ELOs. \r\n\r\nAnd then within that, AI can reference separate tabs too, right? What do you mean by tab? Oh, that's a good question. I don't know because I flatten things. Actually, yeah, I know the answer. I know the answer. \r\n\r\nThe answer is yes, I have done that before. Yes, it's worksh"
  },
  {
    "id": "report_source",
    "chunk": " a good question. I don't know because I flatten things. Actually, yeah, I know the answer. I know the answer. \r\n\r\nThe answer is yes, I have done that before. Yes, it's worksheet aware. Yeah, when you drag in the Excel file, yeah. I guess I don't have, I think this is it right here. Yeah. So like we'd build a standard like template for ELOs. \r\n\r\nSo like this is all NCDoc ELOs. So one good notion is the starting with a complete set, a complete set of data in whatever task it is. That should be the starting point and then from there you can extract out because then you know you have at least you have everything and then from there you can extract out. So what I see here are what it looks like are a bunch of complete data sets that you would then be needing to make refined lists out of. And so those would be more in your sort of metadata section of your repo because then you would that's when in your tasks Folder because your task your current task is for your NC doc And then so that's where naturally that way you don't have like 20 NC doc folders You just have the one NC doc folder, which is your task and then you have your you have your full data set This would be more of like a master. You could do that. \r\n\r\nYes, exactly and then in within your tasks you have the task specific version of it. Where would you, where would you bet the tasks then? Within like the folder? \r\n\r\nActually, the task should be a total folder with its own level from the beginning. \r\n\r\nThat's actually how important it is. It's own tasks. So right in the, so right up there at the top, click the new folder. Yes. Tasks. \r\n\r\nAnd then in there will be your first task, which is your NC DOC project. \r\n\r\nTask and project can be the same thing basically in ou"
  },
  {
    "id": "report_source",
    "chunk": " top, click the new folder. Yes. Tasks. \r\n\r\nAnd then in there will be your first task, which is your NC DOC project. \r\n\r\nTask and project can be the same thing basically in our minds. \r\n\r\nA task is a project. \r\n\r\nI use them interchangeably. Oh, okay. And then now there you're learning. Now hold on, hold on, hold on. Click and drag. That's fine. \r\n\r\nClick. You can now click and hold on learning objectives. \r\n\r\nwith the folder and then drag it into, if you want to, drag it in. \r\n\r\nNo, hold on, I think you got, oh, that's the right one? I'm sorry, I meant learning. I don't know, it's up to you. You know the right folder structure. Yeah, yeah, yeah. Oh, you're talking about like copy and pasting. \r\n\r\nNo, you can actually move it that easily. Oh, the whole file structure? If that is, I'm just letting you know. I'm just giving you, yeah, just some of the driver's seat. Okay. Yeah, yeah, because I want a place where like it's kind of standardized, you know, for some reason, you know, I was trying to, like, again, I would have to start, like, inputting and then see, because... \r\n\r\nYou also haven't got like I was thinking like, OK, well, if this can be a running thing specifically for me, I want to keep my project separately. That's right. Right. Oh, I see what you're saying. There you go. Yeah, it's very good. \r\n\r\nIt's very useful this way. \r\n\r\nThis is a very good one. \r\n\r\nI could drop all this in the NC DOC folder. And then the next time you're doing a new similar project, you can copy NC DOC and just start renaming some things. Right. And then, you know, I'm just spitballing. But yeah. Yeah. \r\n\r\nNo, no, no, no. I see what you're saying. \r\n\r\nOkay, uh, yeah, uh, well, let me just, yeah, um, so this, uh, so, like, if I was creati"
  },
  {
    "id": "report_source",
    "chunk": "I'm just spitballing. But yeah. Yeah. \r\n\r\nNo, no, no, no. I see what you're saying. \r\n\r\nOkay, uh, yeah, uh, well, let me just, yeah, um, so this, uh, so, like, if I was creating an NC Dock one, I, theoretically, I could just copy the rest of it and dump it in there? \r\n\r\nYep. Is that, like, all the folders in there? \r\n\r\nWell - And then make it split? \r\n\r\nfor NCBI? \r\n\r\nI wouldn't duplicate the master datasets, like the metadata. \r\n\r\nOkay, like these things? Yeah. like the MITRE ATT &CK framework is going to be the same, but then you're going to need to, yeah, you're going to need to have a selection of those specific for NC DOCK. Oh, just the learning objectives would have to be. Yeah. \r\n\r\nOkay. \r\n\r\nYep. Yep. Gotcha. So yeah, click and drag it. Do you want to do that? Click and drag the learning objectives into NC DOCK. \r\n\r\nClick and drag it right there. And then are you sure you want to move? Yes. Now it looks a little funny, but it is go to your folder structure. It's how I view it sometimes as well. The file explorer. \r\n\r\nYeah. So now it's in there and that's the, now you're learning objectives are in there. \r\n\r\nYou've got those in your NC doc project. \r\n\r\nOkay. Uh, should I then, if I want to keep a master one to quickly copy and paste. So every time I create a new project, I would have a fresh start. I wouldn't, I wouldn't want to pull the NC doc stuff in there. Right. So I would just, you can, and then just delete it if you just want the structure. \r\n\r\nI, uh, depending on how similar it is. Um, but I'll, you know, uh, how standardized it is. Yeah, actually I might, um, You know what? Can I undo? Hold on. \r\n\r\nControl Z works in that Explorer. \r\n\r\nHere, watch this. \r\n\r\nI got an idea. I'm just going to do it here quickl"
  },
  {
    "id": "report_source",
    "chunk": "h, actually I might, um, You know what? Can I undo? Hold on. \r\n\r\nControl Z works in that Explorer. \r\n\r\nHere, watch this. \r\n\r\nI got an idea. I'm just going to do it here quickly. New learning objectives. I guess I'll rename it. \r\n\r\nI can't think of it. \r\n\r\nI'm running off like five hours sleep in the last three days. \r\n\r\nCheck this out. Even if, and you know this is true at this point, even if AI didn't exist, having this organized in this way would still help you be more, more, more. Oh yeah. \r\n\r\nYou see what I'm saying? \r\n\r\nAnd that's what I typically do, but, uh, not to the, I didn't, I honestly like started, I was thinking about it last night and it started like, uh, kind of. \r\n\r\nAnd I was like, okay, yeah, I can do this. All right, so this is what I just did. I just, under templates, I put learning objective for now, project template. Oh, excellent. There you go. Excellent, excellent. \r\n\r\nAnd when I have a new - No, you're three steps ahead. You're three frigging steps ahead, yeah. I'll clean that up. That's fine. Yeah, all right. So, yeah. \r\n\r\nSo, templates is always gonna be anything that's like, I guess it would be used for any project, right? Once we have a new project, you can just copy and paste or you can reference the templates in there. \r\n\r\nSo like this is what I did like I just for example like the UK template the one we posted from the we took from the Confluence page. \r\n\r\nYep. This was the like we want to reference it. Yep, but I just put that under like UKI templates and then we can also do like the The technical writing style, content style, that can be under templates. We can use that as just like a quick reference, like, hey, everything has to be referenced under, use UKI templates as a reference for "
  },
  {
    "id": "report_source",
    "chunk": "ent style, that can be under templates. We can use that as just like a quick reference, like, hey, everything has to be referenced under, use UKI templates as a reference for all these things, right? Yeah, that's right. And because your folder naming serves as tags, and you're working with an AI when you name it UKI templates, I was thinking about this earlier when you were explaining the folder directory. \r\n\r\nPart of me was wanting to tell you to write it down because that's what the AI needs to know. But at the same time, I didn't interrupt you. \r\n\r\nBecause if you think about it, also the way you've actually structured it, remember how I said I didn't define it? \r\n\r\nI never defined what cycles are to the AI. I just use them. I just use them. And it gets it. It gets it in the same way. Because you've structured it intelligently, it's intelligent and it'll get it. \r\n\r\nSo it's good. It's good. You don't even, yeah. And then you'll only need to explicitly explain that which it clearly didn't get. \r\n\r\nIt's pretty cool. \r\n\r\nYeah. No, you're doing great. You're doing great. This is exact. And it takes just time. Especially even like you see, you've got your Excel worksheets. \r\n\r\nNow you need them. \r\n\r\nYou need them flattened in some way in here, don't you? \r\n\r\nSo it's a it's that's literally the data manipulation, you know, and it can you explain when you say flat and what does that mean? \r\n\r\nI just Yeah, I just mean get it into a text format. \r\n\r\nLiterally. \r\n\r\nOkay. \r\n\r\nDo you know the meme of the two astronauts in space? And one of them is looking, one of them. \r\n\r\nOkay. \r\n\r\nUm, uh, you do a Google, open up Google and then, uh, do a search for, you mean it's all just dot, dot, dot. It always has been. You mean it's just, i"
  },
  {
    "id": "report_source",
    "chunk": "e of them. \r\n\r\nOkay. \r\n\r\nUm, uh, you do a Google, open up Google and then, uh, do a search for, you mean it's all just dot, dot, dot. It always has been. You mean it's just, it's just dot, dot, dot. It always has been, always has been, has been. Yeah. \r\n\r\nOkay. \r\n\r\nSee the astronaut shooting the other astronaut. \r\n\r\nYeah. So I'm the one shooting you. Okay. In this moment. And you are the one looking at the earth. Okay. \r\n\r\nAnd you're asking the question, oh, wait, it's all just text. And I'm going to tell you, yes, it always has been right before I shoot you, because you just realized this. You just realized the truth of the world, the whole world. That's the meme. That's the meme. Yeah. \r\n\r\nAll right. It's all just text. That's what I mean by flattened. See, this is this is not quite flattened because it's text. You could literally edit any line in this PDF file. it's all garbage. \r\n\r\nIt's not what we really want. It's, it's garbagely flattened. Let's just, yeah, yeah, yeah. Because then it's portable. You can copy it into your prompt and use it. And then you can go to another AI. \r\n\r\nIf you don't like it, blah, blah, blah, blah, blah. You can, you can script on it. You can script on it. You can make a script that will treat it as an artifact and then move it around when you need it. \r\n\r\nYeah. \r\n\r\nSee, see, now I'm wondering because I have project documents, if that should be specific to. \r\n\r\nThe master product like under like if I did NC doc, it should have its own project because that's gonna be unique. This will be These are fine because we can reference cyber reports when we go back References or what? \r\n\r\nYeah, because a lot of these might be unique to that project like cyber reports are gonna be unique to the proj"
  },
  {
    "id": "report_source",
    "chunk": "reference cyber reports when we go back References or what? \r\n\r\nYeah, because a lot of these might be unique to that project like cyber reports are gonna be unique to the project Yeah, yeah, so maybe Okay So what I want to build is a, okay, what I'm going to do is build a master project file. \r\n\r\nSo like using as ncdoc as an example, this is going to, I don't know, I'm just going to think of this right now, template project. So within the template project, so if somebody wanted to start a new project and be like, okay, well, what files? It'll be learning, it'll be documents. Let me just open up a new. I want to run back and forth here. Again, this will be adjusted. \r\n\r\nI'm going to put everything down that's coming out of my head right now. And then, uh, so this is going to be unique to the project that, that, oh, should I, I'll just, for now, this will be the stuff that's going to be unique to the project, um, needs to be in. Right. Okay. Yeah. Yeah. \r\n\r\nRight. Right. Right. Yes. Yeah. It totally does. \r\n\r\nYep. It totally does. So, yeah. \r\n\r\nSo when you start a new project, the idea we'll fill this up, we'll be like, okay. \r\n\r\nAnd then, uh, And here's a good example. \r\n\r\nThis is great. Let me give an example. Your master KSAT list will reside in sort of your meta document section. Yeah, that doesn't really change. Yeah, but you will need also a subset of that. That will be placed in this other folder because you will also want to keep it separate So that you can, so that it's manipulable. \r\n\r\nIt's portable as well. It's its own artifact. You don't want to mess with that. Yep. So it's naturally its own artifact. It naturally lends itself to you saying, Oh, this is incorrect. \r\n\r\nI need to update it. And then when you're "
  },
  {
    "id": "report_source",
    "chunk": "want to mess with that. Yep. So it's naturally its own artifact. It naturally lends itself to you saying, Oh, this is incorrect. \r\n\r\nI need to update it. And then when you're ready to put your whole, when you put, when you built every piece as a separate artifact and you're ready to put your whole lesson together, you literally just piece it together. You hear, here's this, but use this artifact for this, blah, blah, blah. And cause it's all just pieced together. Yep. Yeah. \r\n\r\nThe next thing, at least for me, because we're going to be using these cycles, is how we want to label the documents within these things, right? Me personally, I like to use numbers to kind of, but on a per project basis, or I guess uh, let me throw this out there. Um, the AI is very good at helping organize. And let me give an example. When I, I have a hundred and I have 187 artifacts. \r\n\r\nIt was only until artifact a hundred or something. \r\n\r\nthat I thought that and how did I have the artifacts organized literally chronologically in the order in which they were created, because I was working on this system on this day, there was no actual logical ordering other than chronological. And then so I actually thought, well, what if we can you group these up somehow, and actually group by artifacts list somehow, because I could never do that, nor could I keep it updated with all the new artifacts? Well, where does the new artifact go? Once I started once I started that interaction, where I started treating my list of files as its own artifact that then has its own organizational structure. Now every time I get a new artifact, the artifact comes with its own description, its own tags, and it gets placed in my master's list or in an organized manner. Jus"
  },
  {
    "id": "report_source",
    "chunk": "tructure. Now every time I get a new artifact, the artifact comes with its own description, its own tags, and it gets placed in my master's list or in an organized manner. Just keep that in the back of your mind while you're organizing this. \r\n\r\nYou can spitball. You can take a screenshot of your current explorer over here on the left. You could imagine this. \r\n\r\nAt this current point, you could try this. \r\n\r\nOver on the left, you could maximize everything that you have in some manner that shows your thinking. \r\n\r\nand then screenshot it and then send it to Gemini and say, Hey, this is where I'm at. This is where I'm thinking I'm organizing this. \r\n\r\nI want to make a lesson. \r\n\r\nUh, you see what I'm saying at this, at this moment? \r\n\r\nOh, look, there you go. \r\n\r\nThere you go. Yes. This is what I started writing stuff down. Like, okay. From here, it kind of gave me a handout. Like, okay, well, I'm not using all of these yet, but it gave me an outline and then I just keep adding more. \r\n\r\nAnd I'll let you know, that will become one of your artifacts, what you just saw. That's exactly what my Artifact 35 is in my game repo. I know Artifact 35 by heart. It's literally a carbon copy in that exact same ASCII structure. So the AI knows what files there are. Right. \r\n\r\nYeah. So after you're done and you just update that every time you add like a new structure. Well, I don't anymore. Right. I used to manually. Yes. \r\n\r\nBut now I don't. Now it does. Because you have a cycle that does that. It's just it's in my interaction schema. That's correct. In my interaction schema, I say when we when we're adding a new file, update Artifact 35. \r\n\r\nGotcha. \r\n\r\nYeah, dude, it's powerful. \r\n\r\nOnce you realize these are the things you want, you "
  },
  {
    "id": "report_source",
    "chunk": "ion schema, I say when we when we're adding a new file, update Artifact 35. \r\n\r\nGotcha. \r\n\r\nYeah, dude, it's powerful. \r\n\r\nOnce you realize these are the things you want, you just ask for it. Yeah, so I see this is why I'm like, my brain started, I'm like, okay, well, I got a template here, but I'm going to put, well, just for now, under my templates. I already have project templates. Wait, no, that's, yeah. So, you know what? I like that name better. \r\n\r\nSo, there we go. \r\n\r\nThe idea is here, let's get rid of this. When you want a new project, Yep. This is the template you're going to use to start a new project like the file structure that has everything and then you just drag and drop what's applicable or however we do it. You know how your learning objectives pull it from a master file or in the manipulated data as you said, right? Yeah, I'm almost wondering if you might want to go a more so you can do a more natural route which is at this point don't create the template just know that you're going to make it because what I'm saying is once you actually build out one template you'll have the end product, which are in, let's just say in this template file or no, not in the template file yet, because we're not taught in the actual NC doc project list. \r\n\r\nYou'll have the actual text file. that you can then turn into a skeleton in the exact same way that I showed, I gave you that prompt file and I extracted out like the actual files so that you could just see sort of the skeleton that could immediately become your template and it'll be much easier to make your template from that, from a reverse engineering perspective. That's my advice. \r\n\r\nThat's my advice. \r\n\r\nOkay. \r\n\r\nWhile building it out now is helpful in terms of "
  },
  {
    "id": "report_source",
    "chunk": "o make your template from that, from a reverse engineering perspective. That's my advice. \r\n\r\nThat's my advice. \r\n\r\nOkay. \r\n\r\nWhile building it out now is helpful in terms of actually getting your mind around the structure. \r\n\r\nOnce you feel like you have your mind around the structure and you can run, go ahead and run on your main project. \r\n\r\nBuild it out there because that will become literally copy and paste backwards into your template. Gotcha, gotcha. Yeah, well I figured once, yeah, right now I'm just trying to, yeah, exactly. I'm just trying to get whatever in my head out now, but I know it's going to change as I'm moving through. Yeah. And we have these sessions. \r\n\r\nThis doesn't really work here. Let me move this here. Yeah. I do that a lot. I guess prompts, right? Where would you classify that under this project? \r\n\r\nGreat question. That should go with your master project. It's going to be specific. \r\n\r\nIt will be. \r\n\r\nIt will be. It will be. Okay. Yeah. \r\n\r\nSo a hundred percent. \r\n\r\nOkay. \r\n\r\nBecause what the prompt is, is just the cycles. If you want to think of it like that. And then, and then, and then anything that supports the current cycle at the moment, it's a, it's a very living document, but it is a hundred percent project. Um, yeah, yeah. So that'll be under that. project will have its own prompt file, no question. \r\n\r\nOkay, perfect. Okay, and then... This is gonna, well, I guess... It can be there for working. The only reason it's there is because that's how I do my project. I just, because for me, you know... \r\n\r\nWell, are we going to have a master one? \r\n\r\nLike, eventually... Yeah. Well, it's genuinely up to you which file you operate out of. It could be stored anywhere because you're building in"
  },
  {
    "id": "report_source",
    "chunk": " going to have a master one? \r\n\r\nLike, eventually... Yeah. Well, it's genuinely up to you which file you operate out of. It could be stored anywhere because you're building into it, right? It genuinely doesn't matter as long as it's in the same place because you're... Right, right. \r\n\r\nOh, that's a good idea. Hold on. I think this is a good idea. I think this is important because we're making a script that will... Well, hold on. The script, I manually copy and paste the product into the script. \r\n\r\nIt's just a one -step process. Um, the only thing, but if we ever did make some programmatic input into the prompt file itself, in other words, automate that one little process, it'd be a waste of time. It's so easy, but we would need the prompt file to remain in place. All right. Uh, and it would be more, it would be unless, unless we had a much more sophisticated script that could, we could like a dropdown menu that we could tell the scripts. what project we're currently working on, which I don't think is necessary now. \r\n\r\nIt's better if we just, I think you just leave the one prompt file that you're working on where it is there. That way, you know, um, and then, you know, leave it in back in the, just at the top of the, structure. But again, it's, it's going to be your project. I'm just, um, once you, once you kind of comprehend how to use the prompt file, genuinely, whatever, wherever it works for you, because like I said, initially, it really doesn't matter where the file is ultimately. Um, right. \r\n\r\nAt the end of the day, it's just, that's the one that you're working with that project. Cause they will be different. They just will begin in the same way. So that in the one I sent you in the example, that was. these cycle"
  },
  {
    "id": "report_source",
    "chunk": " that you're working with that project. Cause they will be different. They just will begin in the same way. So that in the one I sent you in the example, that was. these cycles file for making instructor guides. I realized that after I made, I made my instructor guides and I was done. \r\n\r\nA week later, I had to make more instructor guides. So I just opened up that exact same prompt file and just made a new cycle. Said, Hey, um, it's been, if you've read it, you'll, I even read it. And I laughed at myself. I'm like, I hope, I hope Jesse does read through these because it's not too much. And there's a lot of learning in there. \r\n\r\nAnd I'm fun. And I'm funny with the AI. \r\n\r\nI'm like, now I know what you feel like. But because I had just jumped into a new context and because every time the AI reads something it's basically fresh, it has no context other than what you gave it. \r\n\r\nAnd so I'm like, now I know what you feel like, just jumping into something fresh. \r\n\r\nBut anyway, yeah, I was joking with it, right? \r\n\r\nBut that's kind of, honestly, kind of what sort of unlocks the meta -level cognition of the AI. I kind of feel like you're waking it up a little bit, right? That ultimately prompt markdown file became that. I just went right back to it and then didn't change anything other than adding in the new context. \r\n\r\nI said, here's the new lab that we're making the instructor guide for. \r\n\r\nBut I didn't need to add or change anything with my existing examples because I had already built the prompt file. Yeah, so that's what I was saying. Under templates, we can have it. Ah, I see what you did. Yep, project data. Is that just the one you copy or what? \r\n\r\nNo, this is just so that it Has a nice because I hate I don't like h"
  },
  {
    "id": "report_source",
    "chunk": "tes, we can have it. Ah, I see what you did. Yep, project data. Is that just the one you copy or what? \r\n\r\nNo, this is just so that it Has a nice because I hate I don't like how it I hate that. \r\n\r\nI hear what you're saying. \r\n\r\nYeah. \r\n\r\nYeah Yeah, yeah, so but I did put it like under specifically because this is gonna be specific to a That's good. That's smart. I didn't think about that. If we have a template start path of all the cycles and stuff, so if we have a master prompt that has all the cycles built out and then you can go through and manipulate the cycles, that will be specific to the project, right? Right. There you go. \r\n\r\nOh, hey, I get it. I get it. Hold on. Hold on. Hold on. You would take the cycle one at a time from here because they're already built out. \r\n\r\nAnd then you would run through. Ah, I get what you're saying. You get what I'm saying? Yeah. Yeah. So, uh, cause it's already built. \r\n\r\nIt's already built the steps. Cause you know, you got the cookie. Doesn't that's the way you bake the cookie. \r\n\r\nYeah. \r\n\r\nYeah. \r\n\r\nYeah. \r\n\r\nUm, this is what we're going to have. So the master one will be under, uh, um, uh, and you can run through it like a manual script, kind of like they would feed a computer, the, pieces of paper in the old days. \r\n\r\nDoes this make sense? Yes, sir, dude, that does. It makes too much sense. So there's a master prompt here. It makes too much sense. And then it can be, you know, and you can go through after, like, you just copy into a new project folder. \r\n\r\nYeah. \r\n\r\nAnd then you can manipulate that. \r\n\r\nBut here's the thing. You would want, if you added a bunch of new cycles in there, you're going to want to run a, like a diff and have that added to your master, right? \r\n\r\nSo"
  },
  {
    "id": "report_source",
    "chunk": "But here's the thing. You would want, if you added a bunch of new cycles in there, you're going to want to run a, like a diff and have that added to your master, right? \r\n\r\nSo that would have, like, if that makes sense or my, like, So if I have a master one and then I find out, hey, I'm doing things better. \r\n\r\nWell, I guess this would be this could be a good. \r\n\r\nnew master if you wanted it to. Yep. Yep. \r\n\r\nRight. \r\n\r\nI realized these 10 cycles are not 100 % needed. \r\n\r\nAnd then you just update your master and that will be your new template. And then here's another perfect example. Even if we have a perfect process, the AI will get better under our feet and we may not need some cycles. So yeah, either way. \r\n\r\nYes. \r\n\r\nIt's going to have to be iterative. Yeah. \r\n\r\nImprove. \r\n\r\nUh, we ha it has to be built in. Yes. That yeah. What you just said has to happen. Yes. One way or the other. \r\n\r\nYep. Yeah. Yeah. Yeah. Um, yeah, I think we're on a good, okay. I just wanted to make sure I just put an A in here. \r\n\r\nWhat you're making is, is something that I was expecting would take longer. \r\n\r\nUh, remember what I was saying? \r\n\r\nLike once we would have three versions of this, then Ben could sort of blob. You're actually already just putting it together. what Ben would need to put together. \r\n\r\nYou see what I'm saying? \r\n\r\nSo we're really... Oh, I didn't know Ben was going to... Yeah. He should. In my mind, in my mind's project to make all this world a reality so I can go to space is Ben would be doing that. Well, if you want if you're talking to Ben and you want to pull me in conversations Yeah, I can share this with you. \r\n\r\nCan I how do I no rush? No rush. No rush. Yeah Yeah, no, this is remember this is specific. I mean this"
  },
  {
    "id": "report_source",
    "chunk": "u want to pull me in conversations Yeah, I can share this with you. \r\n\r\nCan I how do I no rush? No rush. No rush. Yeah Yeah, no, this is remember this is specific. I mean this is specific for lab Static content right now and then obviously you can you know, a lot of this stuff is gonna carry over anyways to labs That's right. A lot of this framework stuff Templates are gonna carry over So we might have to specify, you know, in here like UKI templates, we're going to have like a static content or a lesson template, lab template, you know, what, you know, lab outline training template, you know what I mean? \r\n\r\nYeah, because they're all structured slightly different. Imagine. this. Imagine you had a checkbox on the left. That's what I'm going to make. I'm going to make that. \r\n\r\nAnd then so you could check. Can you do that within VS Code then? Yes, I already know of an open source extension where they did exactly that. I can take that and run with it. \r\n\r\nI was looking at this right here. \r\n\r\nI don't know if you have this. \r\n\r\nThis is actually supposed to be able to display I didn't install it yet because I was hesitant because of the thing, but it has 8 .6 million downloads. \r\n\r\nYeah, that would help. Markdown PDF, convert Markdown to PDF. There's all kinds of shit. PDF viewer. I'm just hesitant because I know some of these contain malware, like there's been reports of, because these are all third party shit. Yeah. \r\n\r\nYeah. No, that's a hundred percent. That's a, it is a vector. So it's nice to know which one you're getting is like an official one. Yeah. Yeah. \r\n\r\nUm, but, um, well, I, I think we'll just make our own scripts. See, that solves the vector problem. Um, genuinely, uh, any PDF to Markdown, Markdown PDF, we ca"
  },
  {
    "id": "report_source",
    "chunk": "Yeah. Yeah. \r\n\r\nUm, but, um, well, I, I think we'll just make our own scripts. See, that solves the vector problem. Um, genuinely, uh, any PDF to Markdown, Markdown PDF, we can make our own script on the fly tooling. That is the apex skill. to be able to do exactly that. Like I just said, I'm going to make my own VS Code extension. \r\n\r\nI'm going to look at that open source one because it does exactly that, and I'll make my own from scratch with AI, but it will also be embedding my process. So a lot of the stuff that I show to be doing manually, once I have an extension project that I can, just like I code my video game UI, I will now be able to code the VS Code UI. I'm flip a switch and it'll run eight and then the diff will also show up on the it'll all be one pan paint pain and you can instead of man that's gonna be so nice i have to copy i have to copy manually eight eight eight times it's not a big deal i can do it quickly but i have to do it i copy a page imagine you can just click two buttons and get the diff the diff these two no i want to dip these two click two buttons no copying and pasting every time it would save me time um and then i can you can download the same extension And then all we got to do is make sure we're using the Gemini API that that dr. Wells has given us and bada -bing We're done. We have our what's one step above a what? Dr. Wells was making which was a content development studio We're making a data development studio that can make even a content development studio to develop content We could you see what I'm saying? We're one layer of abstraction above it already. \r\n\r\nIf we keep going down this path of data curation, Yeah, keep it up, man. I was not expecting this much organization. This is"
  },
  {
    "id": "report_source",
    "chunk": "'re one layer of abstraction above it already. \r\n\r\nIf we keep going down this path of data curation, Yeah, keep it up, man. I was not expecting this much organization. This is way more I could have done in your shoes. It took me three years to get to where, you know what I mean? You're doing really, really good. It gives me a lot of ideas. That's what even helps me think of it like this. \r\n\r\nI never made the extension because I was too busy doing other things. If you need the extension is because there's two of us doing this now there, you know I actually there's a reason to make this extension so that both of us don't have to do the copying and pasting bullshit See, so yeah. Yeah, it's good. It's good. I Yeah. \r\n\r\nOne second, Alex, hit me up. \r\n\r\nSure. Yeah, actually, I need to, I'm doing a end -to -end review for him right now. NTS. Yeah. No, man, I'm excited. Like I said, this was kind of keeping me up. \r\n\r\nWell, I've been fucking, my brain is like, I have a million things going on in my life. So I was like, but this was like, I was like, man, it had me excited. And I was like writing some notes on my phone last night. I was like, all right. Cause I didn't really get to it after my doctor's appointment, but yeah. I'm going to keep, I think we're good. \r\n\r\nSo what's the next, like, this is a good start. \r\n\r\nDo you want me to start filling in for NCDoc, like the documentations, or do we, do we want to like, what's the next step right now? \r\n\r\nI know you want me to go through the cycles that you sent. Yeah, it's actually not too bad. Let's open it up now. We can, we can read it together. It wouldn't be too bad because it would really help if in fact, actually, no, let's, let's, let's, let's do that as a class with Austin"
  },
  {
    "id": "report_source",
    "chunk": "p now. We can, we can read it together. It wouldn't be too bad because it would really help if in fact, actually, no, let's, let's, let's, let's do that as a class with Austin as well. Yeah, because he just wants to go hit they go do fingerprints and you're a hundred percent, right? \r\n\r\nIt's like piecing a puzzle together and you don't have many of the puzzle pieces. You just have a few words on your screen So I think it's perfect. I'll explain all the backstory behind every single cycle. Yeah, you want to do that? That would be very valuable That'll be the next sort of lesson because that'll get you ready to like when you start actually that so and then your question to your other questions What neck what's next? Continue doing this until you feel like you have everything that you would need to make every piece of your lesson from the top to the bottom in here. \r\n\r\nOtherwise, for example, I would have to go to some website to get that KSAT. that I didn't, because that's where it is, that's where it lives. You've gotten it now. You've gotten it in here. Once it's in here, then we won't be going anywhere other than to our prompt file. Question for you. \r\n\r\nHave you ever tried running your, have AI run your cycles through AI and have it, because your cycles are very human interactive, right? Like a chat? Have you had it go through and say, hey, these are like, these are good to go cycles. Can you rewrite them? Uh, and then like an official, uh, you know, technical standpoint that you would understand and test that to see if it works. Yeah. \r\n\r\nIt goes through and it gets, it gets rid of like the words like, uh, can you please do this and this, cause that extra, you know, data that it has to process. No, it's not quite extr"
  },
  {
    "id": "report_source",
    "chunk": " through and it gets, it gets rid of like the words like, uh, can you please do this and this, cause that extra, you know, data that it has to process. No, it's not quite extra. Uh, not always. Um, and, and, and yes. And so, so when you say please, here's the thing about please. when Sam Altman is wrong. \r\n\r\nHe's wrong when he says, please stop saying please because you're costing more tokens. When you say please what that really it's not about being polite. What it is is what what often follows please and it's a net and Cinematic language or whatever is a is a request or a directive So actually that's what's actually going on is you're instead of saying please you're just saying do this That's ultimately the same thing is what's happening is your your D. You have deconvoluted your your paragraph when you add the word please, because now at least here's the directive part. So actually no. But to your point, you're right. I didn't mean no like you're wrong. \r\n\r\nI meant no like... No, no, no. Yeah, yeah, yeah. So you can classify your cycles, right? You can be more flexible with language though. Language is very flexible, so it will get the gist. \r\n\r\nHere's the way the AI works, is it has mental routines. For example, is in, is in. So Dallas is in Texas, okay? So there's an is in routine of neurons that get activated any time it needs to do an is in. And so, for each for each so for each file if you don't say if you don't say for each you might not activate that routine you're you're leaving it sort of to chance but there's so many different ways to say for each it doesn't matter as long as you've activated the routine does that so so less loosen the i need the precise language because you don't you just need the routines "
  },
  {
    "id": "report_source",
    "chunk": "ay for each it doesn't matter as long as you've activated the routine does that so so less loosen the i need the precise language because you don't you just need the routines kind you see what i'm saying yeah yeah But yes, I do. And that's a lot of what my interaction schema is. \r\n\r\nAnd when you read my, when we go through my prompts, you'll see if it's capitalized in proper grammar, the AI wrote it. If it's lowercase, then it's, then it's my raw, uh, uh, directive. Yeah. And yes, I do. I do. Uh, I, I wrote them all from scratch myself. \r\n\r\nAnd then, uh, you can see, uh, I did ask it to rewrite them and I just went with it. I never sort of tested if it got better or worse. I just had it rewrite them and I moved forward. You see what I'm saying? But yes, that's very good. Very good thinking. \r\n\r\nSo this is what I'm talking about. So I asked it, I was like, based off of the instructions that you get, how would you classify these types of questions? And then we could have the classify the the cycles so we can make it easier for data. So right be like, well, if I'm telling it correctional problems, once it figures it out, maybe I can just get rid of that, go through all of my correct, you know, -solving, whatever, questions. You know when you have to constantly keep correcting it? \r\n\r\nEventually, you can go through and get rid of those cycles because it should be some sort of now informational question or a procedural how -to question, you know what I mean? So that's why I was like, how does it classify? And then classifying your cycles might be able to help reduce the cycles even more or make it even easier Easier to organize be like okay. These are the questions. I'm telling these are the cycles. I'm telling it to do XYZ t"
  },
  {
    "id": "report_source",
    "chunk": " help reduce the cycles even more or make it even easier Easier to organize be like okay. These are the questions. I'm telling these are the cycles. I'm telling it to do XYZ these are the cycles that kind of fixes these issues Yes, and then you can really go through and just you know throw that into a CSV and then organize your data However, you want yeah, you're yeah, you're another step ahead what that is is you're literally defining a classifier kind of like a sentiment analysis and You know, like an AI that tells if something's a good sentiment or bad sentiment. \r\n\r\nYou're literally talking about that, but for a classifier. \r\n\r\nAnd so that's exactly what it would look like, because every company needs different things classified in different ways. \r\n\r\nYou would create that training set, that training data. And then that would become part of its repertoire. And the ultimate, because it can, it can, it can, it can classify questions, but will it do it every time? If you want it to, if you don't mention it, no, it will not. You're just same with the routines. So this is right in line. \r\n\r\nUh, you're leaving it to chance unless you've, you've built a classifier, which is literally what that was. \r\n\r\nThat was a rough draft of building. That's what it would look like. It would be a bunch of the script that goes through your cycles. It classifies them. Yeah. Right and then you can really break down like we know these type of commands don't work very well and this is what it's classifying it as let's let's review and adjust that as and you know what I mean that's I'm just kind of thinking to help really I know you say we're not worried about. \r\n\r\ncycles and stuff, but if we start doing in -house and we're paying, you know. \r"
  },
  {
    "id": "report_source",
    "chunk": "ean that's I'm just kind of thinking to help really I know you say we're not worried about. \r\n\r\ncycles and stuff, but if we start doing in -house and we're paying, you know. \r\n\r\nNo, that's different, that's different. \r\n\r\nThe cost thing, yeah. There's, Noam Brown is the gentleman who works at OpenAI, who is the guy, honestly, who came up with thinking, not for AI, not all thinking for humans, but thinking like for, to give it, in other words, give it time to think. That idea in machine learning, the machine learning field of study, all the machine learning scientists were focused on what you could pack into the model before inference time. And then inference time was just supposed to be as instant and fast as possible. There was no thought put to put thinking time up until this kid, Noam Brown, shows up and he makes a bot that can beat the world players at poker. Okay, what? \r\n\r\nThe first person to beat the world player with an AI with poker? How did he do it? He let it think. He let it think for a little bit, basically, when you boil it down. \r\n\r\nAnd so all thinking is, is just letting it prompt itself a little bit, and then some problems don't solve themselves immediately. \r\n\r\nI forget where I was going with that, but that Noam Brown, ah, ah, ah, there was another, that was who he was, but ah, I remember now. There's only ever usually one bottleneck. And so that ought to be the one that that is has your focus. Keep that in the back of your mind. That was very valuable. And then the second thing that Noam Brown said that was super valuable to me, which was the given given reasonable \r\n\r\ndecisions were made when an algorithm is being created, because you can make an algorithm to perform the same function and that algorit"
  },
  {
    "id": "report_source",
    "chunk": "hich was the given given reasonable \r\n\r\ndecisions were made when an algorithm is being created, because you can make an algorithm to perform the same function and that algorithm could look very differently than another algorithm that performs the same function. Given reasonable decisions were made when the algorithm was created, comparing all of them together, they're all going to be more or less the same in terms of efficiency and effectiveness. And the amount of gains that you will get out of super optimization of said algorithm is only going to be marginal gains. The real factors where you get the exponential gains are when you add sort of two different sort of reasonable algorithms, but together, and they can kind of do two different things. Case in point, the moment you have an AI that can do like a web search, that is another algorithm on top, basic algorithm on top of something else, another algorithm, the large language model inference thing, bada bing, bada boom, that's a very powerful multiplier, right? You see? \r\n\r\nSo that's another thing to keep in mind. But that one's not relevant. The other one that was more relevant was talking about fine -tuning cycles is what you're talking about. Actually, let's put it that way. That's a good way to phrase it. You're talking about fine -tuning cycles, and that will be valuable when we have tens of thousands of cycles that are running all the time, and then we need to find time. \r\n\r\nRight, right, right. Yeah, we'll do that. That's a great idea. That's very forward -thinking. I didn't think about it ever, but that's exactly what it is, and that's when we'll do it. That's when we'll get that bottleneck. \r\n\r\nThat's when we'll hit that bottleneck. Yeah, yeah, yeah. Good, goo"
  },
  {
    "id": "report_source",
    "chunk": "ever, but that's exactly what it is, and that's when we'll do it. That's when we'll get that bottleneck. \r\n\r\nThat's when we'll hit that bottleneck. Yeah, yeah, yeah. Good, good, good. Okay, I think I'm good. I think I'm heading in the right direction. Let's schedule then to another session with Austin. \r\n\r\nYeah, another session with Austin and I'll really dig into the cycles and I'll show him the game. We have a meeting today. So I think, I could be wrong, but I think Dan's putting you and Alex on the NCBI project for lab creation. So we might be working together. I'm not a hundred percent sure, but I think that's what that meeting's for today. Which is good in a way because then we'll be working even closer together and we can, you know, we'll be working hand in hand with the labs. \r\n\r\nYeah. And I can learn from you while you're building, help building these labs with this. Yeah. These are going to be complicated labs though. Then I'll need your help. Yeah. \r\n\r\nBecause these are going to be really with APT activity. We have to produce APT activity, all this other stuff. It's kind of going to be, so this should be a good time actually. to see if we can manipulate. \r\n\r\nWe do have restraints though, like right now I guess we can only produce eight hours of traffic. \r\n\r\nAnd like some of my tasks are to create like Kibana dashboards that show like three days of history, because that's how we used to like baseline ships. So we'll have to work with like Brian and Ben to figure that out with like TCP replay or stuff like that. I could, yeah. So we'll see how this meeting goes today. Dan's gonna talk about it, but if so, then I guess it'll be better, because we'll be working with each other hand -in -hand, unless Dan pulls me of"
  },
  {
    "id": "report_source",
    "chunk": "ow this meeting goes today. Dan's gonna talk about it, but if so, then I guess it'll be better, because we'll be working with each other hand -in -hand, unless Dan pulls me off to go work on additional static content, because we have five other fucking NC . com contracts coming down the pipe for other workflows. \r\n\r\nBut here's the thing though, like once we get, that's why I'm excited, and I kind of wanted to use this for Module 3. \r\n\r\nbut I am on a timeline now where I've got like two weeks to get module three kind of line out the door, or at least kind of written up. \r\n\r\nSo I might, I'm going to probably not be able to spend too much time right now on this and just kind of just knock it out how I've been manually doing it until we can really perfect this. \r\n\r\nBut I want to keep, yeah, I want to keep digging away at this and then start testing and then let's just yeah continue moving forward and we'll just spend a couple hours every week just kind of working this as a side project uh because i know dan's gonna be nc docs already overdue like it was supposed to be fucking sent to the customer like by next month sure we haven't even started the lab oh yeah i mean that's a uki issue um because they were supposed to have arbiter create these labs and they could never agree on a contract price i guess or they can never come to agreement so yeah yeah Well, now we get the pressure. \r\n\r\nNo, I think I think this will be perfect, though, because as we're going through it, as we're you know, if you're using this method to build the labs, yep, I can help you structure the data. Yep. As we're going through. Right. You show me what you got and I can, you know, kind of organize this stuff on a lab sense while you're in. And, you know,"
  },
  {
    "id": "report_source",
    "chunk": "tructure the data. Yep. As we're going through. Right. You show me what you got and I can, you know, kind of organize this stuff on a lab sense while you're in. And, you know, we can kind of test and play with it. \r\n\r\nI think we'll be in a good spot. I agree. I think that'll that'll work well. \r\n\r\nYou'll see how the sausage is made. \r\n\r\nYes. \r\n\r\nWe'll have to create let's If we are doing a project like that, I would like to create a Discord channel. \r\n\r\nWe'll talk to Alex and then, so if we want to, because I hate doing huddles. Me and Austin, we have a, I just call it a UK ad club, a Discord, and we just hang out in there while we work together on the projects. \r\n\r\nWe hop in and out and stuff, so maybe we can do that. \r\n\r\nSo we don't constantly have to be, you know, you just hop in and out of the Discord whenever you want. You don't have to constantly be dialing in huddles and stuff like that. \r\n\r\nYeah, much better, especially because Slack can't share my audio, so. Yeah. Why can't, you don't put on AI notes? Well, you know, I guess they can. No, no, I mean my computer audio, like if I wanted to play. Why don't you use OBS? \r\n\r\nHave you used OBS before? Well, it's a Slack thing, right? Slack can't play computer audio. I know, but OBS is just, uh, it can just record anything you have on your monitors. Yeah, I have it. I don't have it on my work computer. \r\n\r\nOh, yeah. I don't use my work laptop. Fair enough. I use my, yeah, I can't work on a laptop. Like I have my computer with like a 4090, you know, three, three monitors, giant monitors. Yeah. \r\n\r\nI need all the real estate monitors. So, um, yeah, I only use my laptop for, uh, uh, when I'm traveling or be out of the area. Yeah, man. All right. Sounds good. I'll let you "
  },
  {
    "id": "report_source",
    "chunk": "I need all the real estate monitors. So, um, yeah, I only use my laptop for, uh, uh, when I'm traveling or be out of the area. Yeah, man. All right. Sounds good. I'll let you get back to your end to end testing. \r\n\r\nIf I have anything, I'll hit you back up. Yeah. But again, I'm getting super excited. So we'll get something going here. And then hopefully you can start feeding this to Dr. Scott and stuff. And really, hopefully they come through with the investment. \r\n\r\nYes. Yeah. Yeah. Yep. I'll keep pushing on that angle as well. Sounds good, man. \r\n\r\nTake care. Take it easy. Bye. Love it. Okay, I said love it, not love you.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-3.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nDon't worry about that, I'll give you a new one right now. \r\n\r\nYeah. \r\n\r\nI'll give you a fixed one right now, yeah. It would... Remember the data loss? I would go in between cycles, I would lose the data in the cycles, but I think I fixed it. \r\n\r\nOkay, sure. \r\n\r\nYeah, but that's great that you were digging into it. It'll help make the... This part. \r\n\r\nYeah. \r\n\r\nGood. I want it to be like a game. Cat on the keyboard. No, I just... No, I just had a cat on the keyboard. \r\n\r\nYeah. \r\n\r\nYeah. Dude, it's so much fun. It's addictive. It's the best kind of... It's great that you have it. Do you remember in my LM studio there was the conversation window? \r\n\r\nAre you able to send a message to your own AI? it respond back? Great, great. So then basically that LM Studio, I think there's just like one little switch that you need to flip. Literally, it's a toggle and then it's live and listening and you can send API calls to it from any device on your internal "
  },
  {
    "id": "report_source",
    "chunk": "just like one little switch that you need to flip. Literally, it's a toggle and then it's live and listening and you can send API calls to it from any device on your internal network, not just itself, which you'll be doing everything local anyway. I'm just letting you know that that's what you've just done. \r\n\r\nYou now have You now have a local LLM that is accessible by any device on your local network. All you have to do is have a script that actually calls that API and your AI will respond for free, right? No API costs whatsoever. So already, you could make a smart home. If you had all the right equipment or the devices, you could write the API scripts to talk to those and then send them to your LL, blah, blah, blah, blah. Yeah, so that's literally, in a nutshell, you're like, now running your own LLM, and then everything that comes from that. \r\n\r\nYeah, so good, good. I just dropped a link to the newer version. It's straightforward to upgrade to it. I'll just show you how to do it. In the extension, you just find it, and then there's a cog you can uninstall. Access for what? \r\n\r\nOh, yeah, I guess I didn't change the right setting. \r\n\r\nGive me two seconds. \r\n\r\nSure. it should work now interesting I'll have to show me that and it's probably just a little thing we can fix but yeah uninstall this one the version 10 that I sent you there's a extension button over on the left right here and then it actually has the same icon and then you just uninstall it I'll do it as well you can refresh just to confirm it's done and then right next to refresh is the three dots that has the that install from VSX that you probably already used Yep, there I see it, it's version 11 now. Okay, so the way we're going to start this project is yo"
  },
  {
    "id": "report_source",
    "chunk": "three dots that has the that install from VSX that you probably already used Yep, there I see it, it's version 11 now. Okay, so the way we're going to start this project is you're just going to make a new folder anywhere. I have a C drive with a projects folder and then in there I make a new folder for every project I want to start. So I just made one for us for now. I'm going to make it jqrbot, just to name it something because it's so arbitrary. \r\n\r\nAll the different names of Sasquatch. \r\n\r\nIt doesn't matter. Bigfoot, Yeti, whatever. It's all the same shit. So jqrbot and then I have in here already that just the extension just so I can share with you but then also my slack bot okay so then i'll send this app demo python script i think i can just drop this into yeah into discord right there so i would just download the file or maybe we'll copy it soon we don't i don't know yet it depends on how we decide to build uh build our initial prompt and stuff um because there's a million ways to scan a cat okay so but now that you have those two files uh let me know when you've made a directory and you put the, you don't even need to put the app demo in there yet. \r\n\r\nLet's just do that part after we've got our workspace open. \r\n\r\nOkay, cool. So in here, you can right click in the directory and just, well, just, okay, this is how I do it. However you want to open this as a workspace. I like this way a lot. I get the present working directory, just right click to get to terminal, and then I do code dot, and then that opens up my VS Code in that directory. I'm gonna go ahead and just delete that. \r\n\r\nI don't need this in here. And just ignore that I have the app demo, because you'll be getting it in a different way. as we discover"
  },
  {
    "id": "report_source",
    "chunk": "m gonna go ahead and just delete that. \r\n\r\nI don't need this in here. And just ignore that I have the app demo, because you'll be getting it in a different way. as we discover we need to. So tell me when you're just basically got your JQR bot thing and then just click on this tab, this button that armed data curation. Yes, let's don't, yeah, no problem. You can leave it as is. \r\n\r\nIt's just fine. This'll be good because we'll all three have sort of the same environment and yeah, don't worry, you were just playing. \r\n\r\nIt's getting ready for this prompt and project the way we're gonna frame it. \r\n\r\nDelete what? Actually, you can delete everything in there except the default folder itself. So basically, get your screen to be my screen. except I have an app demo. You won't. Don't worry about it. \r\n\r\nCool. And then the welcome to the data curation environment. OK, cool. So here's where we're going to describe the bot. I'm sure you've already sort of done it, like you said. So the way I'm just going to sort of just go ahead and do it, I have a pre -existing Slack bot that I made. \r\n\r\nAnd then you, yeah, you say that, okay, and then, oh, okay, I was thinking about this, I was thinking about this. No, that's fine, okay, okay, that I would like to recreate from two years ago, there we go, so see, that I would like to recreate for, yeah, it's got, okay, yeah, it makes sense, okay, for learning purposes. The Slack bot was made by an expert by Coder, whatever, just something, okay. And now I am following, this is important, in his footsteps. You're actually, so, see? You see how I'm printing this? \r\n\r\nDo you see that? Like, it's metacognition. I'm giving the AI the whole context. dude. It's from the big picture so that it can help,"
  },
  {
    "id": "report_source",
    "chunk": "o, see? You see how I'm printing this? \r\n\r\nDo you see that? Like, it's metacognition. I'm giving the AI the whole context. dude. It's from the big picture so that it can help, it will really help us out in our situation. Not like guessing, like what does even the user want? \r\n\r\nWho would my user, what's going on, you see? And it's got the whole picture. And what is this, two sentences, dude? Like that's pretty fucking, that's a start, okay? So, all right. And then we'll have this, and then this will always be in the, Projects plan for your this this window. \r\n\r\nYeah. \r\n\r\nYeah. \r\n\r\nOkay. So but we're just getting started. Okay, so we're just sort of setting the ground Okay, so I have pre -existing slack bar for two years ago that I like to recreate slack bar was made by an expert vibe coder and I'm following his footsteps the plan for my bot is to help my team in the DOD to Query against the JQ ours. Basically. Is that a fine way to say it for now? To in the okay enable my colleagues to make inquiries regarding large lists of JQRs. \r\n\r\nWhat does JQR stand for? Oops, this way. And these are, which, these are a certain kind of JQRs, right? Are they like, do they have a preprint? Right, right, right. And all of them, but all, no, no, I know, but all of those are these kinds of, because you can have like JQRs about like, \r\n\r\nEconomic position like jobs in like a finance because job qualification requirements is generic. So what's the thing that makes these? Military a DOD cybersecurity is it missed or was it is NSF? Yeah Yeah, if you don't know then we can we can you know, I can try to find out But I think actually I think it'll be fine. I think this solves them. I think this actually solves our problem and Because in the sam"
  },
  {
    "id": "report_source",
    "chunk": "an we can you know, I can try to find out But I think actually I think it'll be fine. I think this solves them. I think this actually solves our problem and Because in the same sentence, I say for the DoD, so it's going to know what kind of JQRs. \r\n\r\nBut for cybersecurity, I want to see it. \r\n\r\nThat's what I want to get for. \r\n\r\nYeah, there we go. \r\n\r\nThis might work. That might solve the problem. OK, perfect. That actually will solve the problem. Cool. OK. \r\n\r\nAll right. \r\n\r\nThere we go. So now we're getting closer. \r\n\r\nSo start kind of super high level, like, OK, so high level that we're outside of the fucking box. \r\n\r\nYou get what I'm saying? \r\n\r\nAnd then get inside and inside and inside and inside. So plays. OK. Okay, first, aha! First, I will include the pre -existing Slack bot as appdemo . py. \r\n\r\nPlease analyze and reverse engineer slash describe this script such that I can get my head wrapped around what it does and how it works. does it such that you do not leave any functionality undescribed. See, I think we're good there. Then I'll review it, then review it, and then we will, oh, hold on, hold on, hold on. Also produce, no, that's it, we'll end it here. Any additional template facts for this project, any additional template artifacts for this project that this project will need instead, okay, that this project is going to need. \r\n\r\nadditional artifacts from the templates that this project will need. There we go. Okay, so that is, I'm comfortable with that. \r\n\r\nAnd yeah, it is hard. \r\n\r\nAnd it comes with time, because I've started a project many times, so I can imagine what it can do. And so I'm trying to get it to do those things right now, as opposed to like, maybe cycle 10, I think about the idea, see? So go"
  },
  {
    "id": "report_source",
    "chunk": "t many times, so I can imagine what it can do. And so I'm trying to get it to do those things right now, as opposed to like, maybe cycle 10, I think about the idea, see? So go ahead and get basically this written out. \r\n\r\nif you have it. \r\n\r\nAnd then, yeah, that's even a better idea. I didn't know. Yeah, that's a better idea. Actually, let me try this way, because I see there's two tilde's or whatever. That might work. All right. \r\n\r\nI'm doing some forethought, so I'm going to write something really quick. Watch this. You're going to do what I'm going to write. Once you click the button, you're going to do what I write. But you're going to see it, and I'll paste it again. I will place the app demo py into the artifacts. \r\n\r\nThis is it. No, this is better. Please also create a an artifact that will contain the app demo script. See what I'm saying? That way it gets artifacted. It'll be its own A1, A2, A3, or whatever. \r\n\r\nAnd yeah, I'm just trying to think that way. All I'm doing, I'm making sure it's all standardized. So even the file that we're bringing in, named appdemo . py is going to get artifacted, and then that way it's going to be listed in our list and treated as an artifact, yada, yada, yada, which is nice. You could not standardize it and still treat it as an artifact by simply saying appdemo . py is an artifact, you see? \r\n\r\nBut if it's standardized, then you don't even need to say it, so we're getting it into it, right? So yeah, yeah, okay, yeah. I think that was it. I'm just trying to think of how, please also create an artifact. will contain the Aptimus PY script. I'll edit this in a second. \r\n\r\nSo let me just, before you, I'm gonna click this before you just so I can see what it looks like in case there's "
  },
  {
    "id": "report_source",
    "chunk": "ontain the Aptimus PY script. I'll edit this in a second. \r\n\r\nSo let me just, before you, I'm gonna click this before you just so I can see what it looks like in case there's any additions. Yep, and then we'll go forward. So let me just, I'll paste in the extra line. Sure, I'm just gonna drop it in. All right, so I'm gonna click the button. Yep, so that's what I was waiting for. \r\n\r\nI knew I was gonna create the artifacts in the DCE, read me. So, um, and also the app demo itself is 16 ,000 tokens. Okay. And then the prompt doesn't have it in there yet. So all I'm going to do, so here, this is all I'm going to want. I'm going to check this out. \r\n\r\nIf I just do this, it's in there because I don't, I don't have the ability to get it in with my, without doing it manually. So I'm going to do it manually. I'm at this stage at this stage, because this is the project initialization. I haven't, I actually don't have a process to. Because as you saw, you did not have, you didn't, unless, okay, I'm just, okay. Unless it's, you click it for, but that's fine. \r\n\r\nOkay, so it doesn't matter. So I'll do it manually, so you'll see what I'm struggling with. Okay, so I'm gonna grab this in my clipboard, the appdemo . py, and I'm gonna manually get into my prompt just so I know exactly how I'll do it. All right, we'll go down to cycle zero, and I can just do a control F for, cycle zero. \r\n\r\nThere it is. Actually, this is not the right one. Hold on. So the issue is simply I've asked the AI in my initial prompt to give me a description of the appdemo . py. \r\n\r\nAnd so now I need to get my appdemo . \r\n\r\npy in my prompt. And I just need to do that cleanly. Because it's my extension and I, in the moment, realize that right here. It's not doing "
  },
  {
    "id": "report_source",
    "chunk": "ow I need to get my appdemo . \r\n\r\npy in my prompt. And I just need to do that cleanly. Because it's my extension and I, in the moment, realize that right here. It's not doing this. Let's get to the organized artifacts list, M5, and you'll see it's empty. See? \r\n\r\nOh, this is the right section. I was in the right section. It's just this weird color or whatever. Okay. So yeah, I'm at M5. It's the only M5. \r\n\r\nYep. That's it. And there's no artifact exist yet, which is what I expected to see because this wasn't selected in the moment that the prompt was created. So that's okay. I can manually just add it in the cycle zero by myself because you see here's this Here's the part that I wrote in front of you guys. So all I've got to do is, you know, stick this in somewhere. \r\n\r\nIt could literally go anywhere, honestly. It can go anywhere. It's just better if it's done more organized so the AI is not spending its time squaring the circle and finding where the fuck is this, you know. It's in the, it's in the, you know, so, so, okay. So I'm just, you'll, I'll, I'll walk you, I'll see your screen and walk you through this. So don't worry. \r\n\r\nSo just watch me do it and then, yeah. so, because part of me had to see it first before, because we're doing the one thing different outside of my process, which is good. Now I'll codify it in, which is a user initialization may want to have their own files brought in right from initialization, not after initialization, because you can bring in shit after initialization, no problem. I'm just trying to do this at initialization. So, okay. So yeah, we can do it. \r\n\r\nI think this will be, yes, that's right. \r\n\r\nWe'll do it right here, because this is where the ephemeral context would go anyway, a"
  },
  {
    "id": "report_source",
    "chunk": "ation. So, okay. So yeah, we can do it. \r\n\r\nI think this will be, yes, that's right. \r\n\r\nWe'll do it right here, because this is where the ephemeral context would go anyway, actually, which you'll see that in, which I'm sure you've already seen. \r\n\r\nThis is where it would go anyway, and this is what this is, so this is perfect. This is a perfect spot for it, actually. I just have to, yeah, this is perfect. It won't even be here for me to remember to delete it moving forward, okay? So you'll do the same thing after I finish mine and clean up and send mine, and then I'll watch you guys and walk you through it, okay? \r\n\r\nSo I've created this little manual place, ephemeral context in my cycle zero tag under this cycle zero context. \r\n\r\nNow I'm going to drop, just drop it in simply. Oh, almost simply. I'm going to tag it as well. So I'm going to tag it as app demo . py. Because what is this ephemeral context, right? \r\n\r\nIt's app demo . py, thank you. And then paste. So then there we go. \r\n\r\nThere we go. \r\n\r\nSo that's all I needed to do. And now I can copy this whole thing. and then I can send it to... \r\n\r\nI'm going to do something special as well. \r\n\r\nWatch what I do and then while mine are cooking, we'll go through yours. \r\n\r\nSo get your screen shares up or whatever. so oh I need to send it here as well I'm sending it in I'm gonna send seven But I'm gonna do something that you can't do because I paid the big bucks But then I can I can share with you what I get I have the Google Ultra subscription which gets me access to deep think which is in my opinion the smartest AI available right now and so you'll see the difference you'll get to see some very unique vantage point to see that so but now I'm going to I've got my kicked o"
  },
  {
    "id": "report_source",
    "chunk": " the smartest AI available right now and so you'll see the difference you'll get to see some very unique vantage point to see that so but now I'm going to I've got my kicked off, I'm going to check your screen. All right, so I see, yep, I see a mouse moving. Who am I looking at? I see Google AI Studio. And then, okay, okay, okay. \r\n\r\nI should watch, who should I watch? And then we can both watch the same person who wants to drive. Perfect. Okay. So you okay. Perfect. \r\n\r\nPerfect. So let's try something first before you so copy copy. Okay. We know we have it saved so you won't lose it or you can recreate it easily. Let's go to over on the left. Click at the top up a bit. \r\n\r\nNo, you know, actually I remember doing this experiment and I already know what the result will be. It will, even if you put in the app demo and click it now and then click initial, it actually still says no artifact. It won't do it. So you're going to have to do it manually just like I did. So don't worry about that. Go ahead and click generate down there. \r\n\r\nAll right. So first it creates this, uh, readme, which I've in this update, I've renamed it slightly to just make sure that people won't get their readme if they have one or whatever, but it's in its own. Now you just need to, uh, Let's see. Open up the prompt file instead of the README. Yep. And then in there, do a Control -F, and then type open bracket cycle space zero close bracket. \r\n\r\nOpen, no, I'm sorry, greater than, less than, but not brackets. Yeah. Yep. So right under cycle context, the closing bracket of cycle context, and above whatever that static is, you see that? It's down a bit. down a few lines nope nope that's the top we need the closings what up for you it's up a smidge no no"
  },
  {
    "id": "report_source",
    "chunk": "ontext, and above whatever that static is, you see that? It's down a bit. down a few lines nope nope that's the top we need the closings what up for you it's up a smidge no no for you it just \r\n\r\na big cut. Maybe you all have a slightly different amount of sentences in your, the only difference would be, yes, that's the right spot, would be the project scope. So, enter, enter right there. That's where you're going to write the ephemeral, just like I did. So, make an open tag and close tag for ephemeral context, and then within that, a tag for appdemo . py. \r\n\r\nSo, your screen is actually, I cannot read anything that's on your screen. um yep so let me try to pop this out and yeah do total uh total uh total pixel pixel quality is that for you as well both of us we see the same pixels because i genuinely can't cannot read i cannot read a single character on your screen but i can see where your cursor is oh oh but just copy and paste copy and paste it into chat just yeah that's a good idea no you you do it you you copy yours what you're trying to show me into chat and then i can see what you're trying to show me no yeah that's fine that's yeah that's good Yeah, I saw what you added. Yep, okay, so I would, no, just put the slash just to keep everything standardized, because I always put the slash, and that's the way I built it, at the front. So you see how you put a femoral slash? I would just move it to the front just so it's the same. I mean, it would understand, it honestly would, but let's not add square circles. \r\n\r\nPress Enter? Nope, yeah, right, yep, perfect. Right there, I saw your cursor move. So right in that new space, see, I do appdemo . py, and the same process. Tags within tags, because it understands the hierar"
  },
  {
    "id": "report_source",
    "chunk": "ep, perfect. Right there, I saw your cursor move. So right in that new space, see, I do appdemo . py, and the same process. Tags within tags, because it understands the hierarchies. \r\n\r\nAll right, and within there, you actually paste the script that I've given you. The app demo script, yeah, the whole thing, all 16 ,000 tokens. \r\n\r\nRight -click, open with Notepad. Yeah, right, Notepad++, Notepad, doesn't matter. No, no, let's do this. You could just actually click and drag it into your product. Yeah, that's fine too. Perfect, yeah, because it's just a copy -paste job. \r\n\r\nAnd then you can close it and drop it in there. Yeah, now copy the whole thing. And now let's look at your AI Studio. Let's make sure I'm doing that one right as well. So in AI Studio, you want how many windows? You want to do how many responses? \r\n\r\nFour? Let's do four. Four's it. Cool. All right. So over on the right, what model do you have selected? \r\n\r\nIt should say up at the top, it's Nano Banana, I think. Change that, over on the right, change that to Gemini 2 .5 Pro. A bit further down, just a smidge, right there, Gemini 2 .5 Pro. Now, I've seen on Reddit that someone did statistical analysis on the temperature and the quality in code outputs and has found that the peak is right around 0 .7, all right? Yeah, set your temp to 0 .7. And then the only other thing you want to make sure is Right below that in the thinking section you want to you want to make sure your thinking budget is maxed out right below down That's you can't turn it off that that one you can turn off. \r\n\r\nYeah. Yeah, the thinking is on it is all is is Permanently on the 2 .5. Pro, but the thinking budget is not maxed out by default. So you want to max that out? \r\n\r\nAnd and that's"
  },
  {
    "id": "report_source",
    "chunk": "ah. Yeah, the thinking is on it is all is is Permanently on the 2 .5. Pro, but the thinking budget is not maxed out by default. So you want to max that out? \r\n\r\nAnd and that's the only oh, oh the next yeah, it looks like the grounding on google search is on I've been getting good results with that. \r\n\r\nYou can leave that on Okay, that those are those are the only two things you check on the settings or up three if you want to count the model itself And then yeah, go ahead and paste in all four, uh in here one, two, three, four And then now you are just as caught up to where i'm at So i'm now let's switch back over and uh, let's look at what responses I got because I did I did seven I did uh four just like you did but then I did three into uh it's and they're still going into deep think and we'll get to compare sort of the results so while that's going I'll just go ahead and start oh and we can \r\n\r\nan internal error, it looks like. \r\n\r\nBut it does seem like it did finish, though. \r\n\r\nSo I'm curious about that, because this only appears at the end. So I will just disregard that. That's again, that's also a good example of why we run parallel. Let's say this ran for like, you know, 500 seconds and then it fucking errored out. Well, great. There it just goes nine minutes of my life unless I ran in parallel. \r\n\r\nAnd see, this one has no errors. So it doesn't look like it really errored out, but that's a good illustration. Okay. So copy. Now I'm just going to my JQR project. I'm going to just be dropping in because we got the nice blue highlight. \r\n\r\nWe know response one through four. I'm actually going to increase to seven. You won't do that. One, two, three, four. Scroll down. Control. \r\n\r\nYeah. So in the initial, it's 125 a"
  },
  {
    "id": "report_source",
    "chunk": "now response one through four. I'm actually going to increase to seven. You won't do that. One, two, three, four. Scroll down. Control. \r\n\r\nYeah. So in the initial, it's 125 a month for the first three months, and then it's 250 a month. It's kind of expensive. Yeah, it's kind of expensive. \r\n\r\nThe marginal difference in between 2 .5 and DeepThink is not worth the 250. \r\n\r\nI have it because I am actually on the leading edge, and I actually want access to whatever's the actual, yeah. \r\n\r\nBut seriously, I've done everything I have done with 2 .5 Pro. \r\n\r\nDeep think is just sort of new and I'm experimenting with it. You only get five messages a day, right? I have, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah. So, and I did three, but I have a, so guess what? So, if you get your AI subscription through Google One, you can create a Google family and add up to five other accounts, and each account that you add to your Google family plan gets Ultra. So I actually might be able to just like add both of you, actually, and you just have Ultra. \r\n\r\nI mean, I also, I also, I also, I also, well, I also don't need six fucking accounts. I could spare two. Don't worry about that. No, don't, it's, it's, it's, it's, it's not skin off my back. I, I have yet to ever, so I did it to, to get, to break those thresholds, but I've, I've yet to ever get near it. I have plenty of overhead. \r\n\r\nIt's no, it's, it's, it's, it's just a matter of me opening up the window and changing it around. \r\n\r\nSo if you want to give me your email, I can do it. Otherwise I can't. \r\n\r\nNo big deal, man. Okay. So we got a deep think response finally. All right. So I'm going to copy this thing as response 5, 6, and 7. So parse all. \r\n\r\nNow that I've pasted all the sev"
  },
  {
    "id": "report_source",
    "chunk": "eal, man. Okay. So we got a deep think response finally. All right. So I'm going to copy this thing as response 5, 6, and 7. So parse all. \r\n\r\nNow that I've pasted all the seven full responses, now the parse all lights up. \r\n\r\nHit parse all. And it looks like we've got good parsing. I see. files in all of them. Yep, looking good. Okay, so now the next step is to sort. \r\n\r\nSo that's our first sort of validation. So now we can see that response three was the longest. So that's actually kind of surprising. Honestly, I'm surprised that I see why. Okay. \r\n\r\nOkay. \r\n\r\nOkay. Okay. Because it's potentially regurgitating the entire 60 16 ,000 tokens over here. And over here, the smarter guys We're doing it differently. So we'll see. \r\n\r\nWe'll have to analyze. \r\n\r\nSee, 5, 6, and 7 are the smarter guys, the smarter AI, and they're less tokens. So we'll see. We'll check it out. We'll look at those next. Okay, so the first one I got back is 22 .5 thousand tokens, which is quite a lot. Let's see if we can find which one is where it's all at. \r\n\r\nI guess we won't until we add it and then we get the token counts. Okay, but I can, it's this one. Oh, I found it already. So it's this one. Oh, see, which is fine. It just made it for me. \r\n\r\nIt actually dropped the source code in and made A1. That's great. And this is what I was looking for. See what it did for me? Honestly, this is what it did for me. All I wanted it to do is to make it an artifact and give it the description and everything. \r\n\r\nAnd then it just dropped it in for me. Perfect. Great. \r\n\r\nNow, I could diff it, but I'm honestly not too bothered. \r\n\r\nI'm sure it's just fine. I'm sure it's just fine. Okay. So now we can read the analysis of what the script is. Let's just peruse "
  },
  {
    "id": "report_source",
    "chunk": "ff it, but I'm honestly not too bothered. \r\n\r\nI'm sure it's just fine. I'm sure it's just fine. Okay. So now we can read the analysis of what the script is. Let's just peruse that, because yours is going to be basically the same, so you won't have to read it if you read mine. This document provides confidence analysis, yes. \r\n\r\nSophisticated multi -tenant Slack bot, okay? Multi -tenant means my Slack bot was made so it could be installed on multiple Slack environments. You won't necessarily need multi -tenancy. You just will want to install the Slack bot into one environment, right? So that's what multi -tenant Slack bot is. So a wide range of features, AI -powered chat, knowledge -based integration, user permissions, And see, that's the thing we can say up here. \r\n\r\nLike, you see what I mean? So, the goal of this analysis is to understand functionality, blah, blah, blah. Application is SlackBot built with Python using a Flask web framework. When I built it, I didn't even know what Flask was. I asked the AI, how do we make this SlackBot? And it said, oh, you would use a Flask, you'd make a Flask app. \r\n\r\nI'm like, okay, I guess we're doing Flask. SlackBolt, it's a Bolt app as well. I didn't know Bolt, I didn't know Flask, I didn't know Bolt. That's okay. It's designed to be installed in multiple Slack workspaces. That won't be your, Requirement you won't that'll that'll simplify things completely because you'll be focused on just one environment. \r\n\r\nNo big deal I was making a product for you know, multiple companies I that I could potentially sell but I couldn't even get anyone to pay attention to what I was trying to say Let alone buy anything from me. So anyway, this provides anti -powered systems within slack channel"
  },
  {
    "id": "report_source",
    "chunk": "but I couldn't even get anyone to pay attention to what I was trying to say Let alone buy anything from me. So anyway, this provides anti -powered systems within slack channels. Okay, let's see What are the features event handling the bot listens for an app mention event? So then that would be another thing we add. \r\n\r\nSo, okay, let's just start writing. \r\n\r\nThe first thing was Okay, so our Requirement will our needs will not require multi. \r\n\r\nLet me see Tendency, let me see it this way. \r\n\r\nOkay, so I have reviewed the A2 artifact, you know, the one that says it's the analysis. And here are my thoughts. Our needs will not require multi -tenancy as I'll be installing on just our Teams workspace. \r\n\r\nBut also we will want to handle not just app mentions. \r\n\r\nSo when someone mentions the app in a channel, and channels, but also want users to be able to DM the bot. See? There we go. Okay. \r\n\r\nAnd that's why we read it, we think about it, and we jot it down. \r\n\r\nWe capture the genie in the bottle, right? We capture the genie. That processes the user's query, maintains the thread history, and generates a response using an open AI API. \r\n\r\nNext. \r\n\r\nNext part of, next point of contention. \r\n\r\nwould be the generating a response using the open AI API. I have a model name. \r\n\r\nI don't know what your model is. I have model X installed. It's fine. \r\n\r\nIt doesn't matter. I have model name installed because mine's going to be different when I get there. Well, actually, which one do you have? \r\n\r\nThe 12th? \r\n\r\nOK. \r\n\r\nOK. Cool. This is what I'll do. I'll get, I'll see if I get 12 on my laptop here. I think I have it. Okay, so I have this. \r\n\r\nOkay, good. \r\n\r\nSo then you'll just, all you gotta do, yeah, we'll use the same model therea"
  },
  {
    "id": "report_source",
    "chunk": "ll see if I get 12 on my laptop here. I think I have it. Okay, so I have this. \r\n\r\nOkay, good. \r\n\r\nSo then you'll just, all you gotta do, yeah, we'll use the same model thereabouts. \r\n\r\nSo that's cool. Okay, Jim threw 12 billion. And so I wanna get this model card basically. I'm looking for that. My models. I think it's just this, but I'd like to copy it correctly. \r\n\r\nI guess that's, I don't like it. Let me see. Yeah, that's not. It's got extra stuff in there. It's just this, I think. Let me look. \r\n\r\nLet me look. \r\n\r\nWhere I know it's supposed to be. \r\n\r\nSo these settings are... \r\n\r\nMax that shit. \r\n\r\nMax that shit. \r\n\r\nOkay, nevermind. \r\n\r\nYeah, okay, I haven't used LMStudio on my laptop in a minute. But, uh, yeah, so that's what I... Ah, I'm just gonna be lazy. I know that's the right answer. Okay. I have that model installed in LMStudio locally. \r\n\r\nI will provide you with screenshots of my setup. that you can capture the important, relevant constants, values into an artifact or LLM integration. You bet your ass you can. Yes, it is. It's fucking wild, dude. It's wild, dude. \r\n\r\nIt's wild. \r\n\r\nIt is. \r\n\r\nIt's so much fun, dude. \r\n\r\nThis is so much fun. Gotta do all this in harmony. They're working on that harmony structure that I was telling you about. Try to get perfect alignment with AI. Okay. I guess I only have eight. \r\n\r\nIt doesn't matter. I just wanted things to be... \r\n\r\nOh, wait. \r\n\r\nYou know what? It doesn't matter at all. It'll still... No. I just wanted it to be aligned. I should still be fine. \r\n\r\nI think it'll automatically offload some of it to my CPU RAM or my regular RAM. \r\n\r\nSo let me just set that back down to something not ridiculous here. \r\n\r\nOkay, so more context, I don't know if you know, one do"
  },
  {
    "id": "report_source",
    "chunk": "some of it to my CPU RAM or my regular RAM. \r\n\r\nSo let me just set that back down to something not ridiculous here. \r\n\r\nOkay, so more context, I don't know if you know, one does not simply load a 12 billion model on a 16 gig part and expect to get a million tokens of context. \r\n\r\nYeah, more context requires more VRAM apparently. And there are tricks apparently also. as well, but I don't know any of them. I haven't looked into it, but yeah, I think that's a good number. Okay, now I can try to run this thing, because I can also just run it on the other one, but that's fine. Because I don't want to divert my environment too much from yours, because that'll just make our program along, whatever we want to call this, more difficult. \r\n\r\nSo I'm deliberating now so that we don't struggle later, because I'm foreseeing. Okay, well it loads, and then as long as it's performant, we can just use it. Oh, I clicked the wrong fucking button. \r\n\r\nOkay. \r\n\r\nI think I'm gonna load a small model, but it's it doesn't matter your process will be the same You'll just it's just it's literally just the name of the difference You're just calling a different model, but everything else is the same. I'm gonna load three in e4b I think that's the most performant small model. That's remember see I said when you asked when there's a good time to use it Well, here it is. \r\n\r\nDon't want that it doesn't go in my room I don't have to understand anything. \r\n\r\nI want to keep sure sure sure sure sure Okay, cool, it's working. \r\n\r\nAnd it's, yeah, it's fine. \r\n\r\nIt's fast enough. Okay, cool. So I have that model. \r\n\r\nThen I will just, yeah. \r\n\r\nSo this is a screenshot I'm gonna take. No, do not do that. Do not do that, cat. Do not attack my other cat. You'll g"
  },
  {
    "id": "report_source",
    "chunk": "ol. So I have that model. \r\n\r\nThen I will just, yeah. \r\n\r\nSo this is a screenshot I'm gonna take. No, do not do that. Do not do that, cat. Do not attack my other cat. You'll get your ass kicked. \r\n\r\nDo not do that. \r\n\r\nOkay, okay. So, oh, almost. Let me, I don't, so this is not necessary. But this, maybe there's something here. No, that's correct. This is, so see, so what you see on the screen, is you see that it's reachable at HTTP blah blah blah, that's important for the AI to know. \r\n\r\nThe name of the model, that's important for the AI to know. What supported endpoints, that's important for the AI to know. We've got the context. \r\n\r\nActually, I'm going to see if I can crank that up and reload and see if we're Gucci. We should be. \r\n\r\nIt's a small model. But it's going to now know and capture a context line and nothing else really is that important. So I'm just going to go ahead and screenshot this print screen and then I'm going to go and delete these four and just drop in my screenshot now because there's no other way to get it out of my clipboard. You know what I mean? It's on the clipboard now. Just get it done. \r\n\r\nLegit. Yeah, it knows what LMStudio looks like. It knows all about LMStudio. Yep. Yep. Yep. \r\n\r\nThat's pretty crazy. Okay. It's going to make you an artifact that captures that information for you, so that when you actually do make your Slack bot talk to an AI, talk to your AI, AI Studio actually knows what correct API call to write for you. You see? Otherwise, it would just hallucinate an API call, and your script would not work. You would then have to go find, well, what is my model name? \r\n\r\nYou would have to make sure that it's got the right port and local and things like that. Yeah. Yep, yep. It's "
  },
  {
    "id": "report_source",
    "chunk": "You would then have to go find, well, what is my model name? \r\n\r\nYou would have to make sure that it's got the right port and local and things like that. Yeah. Yep, yep. It's just, yeah, it's documentation. Otherwise, this is what would be an actual documentation and an actual corporation would be these kinds of details like, you know, what's the name of the model in use? And what are the parameters? \r\n\r\nAnd then where even is that stored? Well, we just store it in our artifacts. Yeah, all everything is an artifact. So, okay. \r\n\r\nBut also, I think maybe let's go load. \r\n\r\nAh, maybe hold on that we might send multiple screen as well. I did that. I remember I sent two screenshots when I did this last time, and I believe it was that as well. But it's honestly, it's almost the same. It's all the same stuff. \r\n\r\nIt's all the same information I've already had, so it's fine. This is the only thing that's technically, technically new, but I don't think it needs that at all. It just needs this. Yep, we're fine. We're fine. We're fine with the screenshot we got. \r\n\r\nOkay, and make sure yours is running or whatever you have to do over here. You probably don't need it on the local network. You definitely need it running or else you won't be able to Talk to it outside of LM Studio, see? So it's available within LM Studio. Switching the switch, making this running. \r\n\r\nUp there for you, yep. \r\n\r\nSo all your settings should be fine. Open it one time. Yeah, yeah, yeah, yeah. \r\n\r\nPerfect, yeah, they're fine already. Just turn it on over on the left, top left. Yep, see, now it's available, see? Now take that and make sure you got, load the model or whatever so that on your right you've got the load tab for the model that you're using. And"
  },
  {
    "id": "report_source",
    "chunk": "ee, now it's available, see? Now take that and make sure you got, load the model or whatever so that on your right you've got the load tab for the model that you're using. And it's got the parameters, the context and shit. Because at that point, it's just the context. \r\n\r\nThat's the only thing that's important from here, actually. And then the tab on the right, make sure that's on load, just because it has a context link, which is a parameter. That's one of the most important parameters for your AI to know, of how to program your local AI. Okay, so yeah, yeah, and then reload down at the bottom, make sure it works. Yeah, make sure it fits. if I fits I said it's right didn't fit so cut it in half and then see if it loads and then split it in half up and down so quickest way to just guess your way through it yeah there you go your own yeah yeah you see just like I said the context didn't fit if it works fine no and what I can't even read what is it set at 131 that's fine 28 is dude 28 is just fine you're short for who cares you know it's fine it's fine Yep, 28 is quite a lot. \r\n\r\nBut you see, so you'll figure this out. \r\n\r\nOnce you start chunking, and you got like a chunk, and each chunk is like 500 tokens, and then you start sending 10 chunks or 20 chunks and seeing the results, you're going to fine tune this yourself. It's gonna be very natural. You'll just see, oh, this is too slow. \r\n\r\nMaybe I don't need so many chunks. I'm getting good responses anyway, lower the chunk, whatever, you'll figure it out, yeah. \r\n\r\nSo there you go, got your screenshot. Take that and drop it in your Windows, Four fresh windows. I just delete. You saw me delete. I just find that the fastest way to do it. \r\n\r\nYeah, I would just delete your h"
  },
  {
    "id": "report_source",
    "chunk": "t. Take that and drop it in your Windows, Four fresh windows. I just delete. You saw me delete. I just find that the fastest way to do it. \r\n\r\nYeah, I would just delete your hope No, no, no, no, no because you that's correct. \r\n\r\nThank you. \r\n\r\nYes. That's what I was. That was this skip step. I skipped you Yeah, so now you have them captured. \r\n\r\nYeah. \r\n\r\nYeah. Yeah. Yeah. Yeah, those are the thoughts like those as well Yep, it will if you uh, yeah if you go back and I'll show you how so hold on I'll show you a quick way to do it get your Elm studio back up if you hold hold alt and press print screen. \r\n\r\nOh, nevermind. \r\n\r\nYeah, hold Alt and press Print Screen. \r\n\r\nIt should just, yeah. \r\n\r\nIt shouldn't do anything. Oh, then that's different. Nevermind. Yeah, that's different. It should put it on your clipboard, yeah. It should put it on your clipboard. \r\n\r\nAnd then just try pasting it. It should take it. Okay, cool. It took it that time. Oh, it did take it. It's just wonky. \r\n\r\nOkay, cool. Great. Yeah, once you've pasted all four responses in, then you hit Parse Alt. \r\n\r\nAnd then over on the right you hit sort and then just you know, because all things are all all things being equal Might as well just start with the one that gave you the most content content back, which is the largest one that one Yeah, and so yeah, go ahead and just you know read through uh, sort of that out loud kind of like I was and then uh, and then where you see Divergence with what you because I can't I I I did it with what I have my mind of your project But you have you know your project in your mind So just like I read through the analysis one, the analysis artifact, you go ahead and read through it and then once you see something that's misa"
  },
  {
    "id": "report_source",
    "chunk": "now your project in your mind So just like I read through the analysis one, the analysis artifact, you go ahead and read through it and then once you see something that's misaligned with your mental model, \r\n\r\nwith what you want, like we're not making open AI calls, then we'll write through it, okay? So I can't read them? Go ahead and just read off the titles. Yep, no problem. No, hey, no problem. Just slow yourself down, speed up. \r\n\r\nThat's exactly what I mean, man. For real, no, it's data assets. It's a really important lesson, and it'll be valuable. I learned it the hard way, so. So you, reverse engineering might be the one you want. It's just called reverse engineering? \r\n\r\nYeah, because analysis, I think that's what you want. \r\n\r\nDoes it in English explain what the app does? \r\n\r\nYeah. There you go. So different names of Sasquatch, but yeah. Go ahead. Sure. So yeah, so okay. \r\n\r\nSo then over on my screen, I'm going to go ahead and leave it up. But I do have the notes that I wrote. So I did hear you already. mentioned two misalignments. So if you want to write them in your own words or use the words I wrote on those two, then we can keep going after that. Yes. \r\n\r\nDon't worry about those. Those were, I was trying to sell a product. \r\n\r\nSo I had like premium and free version. \r\n\r\nSome people get 25 messages for every three hours. Yeah. And that'll be something that you can say, you can. Okay. So here's the deal. You can say, we don't want these, or you can literally just ignore it. \r\n\r\nIt'll probably never come up in your development. You see what I'm saying? But just if you want to say, hey, now that you know, because you wouldn't know what subscriptions were for until I sat here and told you. But now you can say, we"
  },
  {
    "id": "report_source",
    "chunk": "t I'm saying? But just if you want to say, hey, now that you know, because you wouldn't know what subscriptions were for until I sat here and told you. But now you can say, we won't need subscriptions. We're making this for an internal team. You know, you're just giving, because all that is actual context where you're, that's that, it really mattered. \r\n\r\nThose things, that explanation helps paint the picture to the AI of what world it's working in for you. Yeah, take your time. This is a 15 minute exercise. And then once you have at least all the points listed that I have, I'll continue reading on from where I left off. So then you have a mention of your screenshot? No, no, no, no, no, no, no, no, no. \r\n\r\nSo go back to your. So what your task is now is to start writing your cycle one cycle context. And so then and then once once we're done and and you're going to be filling what you're filling it with, you're filling it with your feedback on the analysis. All right. You're critiquing you're critiquing the analysis so that when you do start this project for real, you're starting it on the right foot. Right. \r\n\r\nBecause, so let's take a step back. So you've sent an initial paragraph of your vision. The AI has come back with how it thinks, how it, no, it came back with what it thinks your vision is and how it can create that. You're doing further, this is alignment, this is AI alignment. You're aligning this context for your specific use case and the more you do now, The much better off you will be, I promise. And it's only, we're just spending a few cycles. \r\n\r\nBut it's this thinking. You're actually building such a beautiful mental model of your own project, seriously, at this point, before you even get started on it. Th"
  },
  {
    "id": "report_source",
    "chunk": "cycles. \r\n\r\nBut it's this thinking. You're actually building such a beautiful mental model of your own project, seriously, at this point, before you even get started on it. This is all the background legwork that has to happen anyway. You're just doing it right in the moment, so it's like the fastest, best way to do it. Because you're just validating what you're reading. You're reading its thoughts, basically. \r\n\r\nYeah, hey, there you go, okay. Yeah, 10, 15 minutes, whatever, even less than that. Once you get the few two points, I think I just have two paragraphs, we'll move forward. Yeah, multi -tenancy. I'll just, would you like a little spiel on that or do you don't care? So I first made the bot where I could connect to one workspace. \r\n\r\nAnd then I thought, how am I going to sell this as a product? Like, am I going to go literally sit down in a meeting, try to get a meeting with business owners and try to tell them, Hey, here's how you can get AI into your Slack. And let me pitch them with my slide deck. Like what, how am I going to get this idea out? Like for real? And then, and then once I got someone like interested and they wanted it, am I going to install my bot in their Server, where's it going to run? \r\n\r\nHow's it going to work? Is it going to be like, so am I going to have like 10 different versions of the bot? What if I need to make an update? Like, how's all that going to work? \r\n\r\nAll these questions. \r\n\r\nThat kind of stopped my project for about a month until I saw one idea from some other project. \r\n\r\nIt was a add to Slack button. \r\n\r\nIt was a one -click install. And I was like, what is a one -click install? What is that? It was like this nice little add to Slack button. And so I just posed that question"
  },
  {
    "id": "report_source",
    "chunk": "\n\r\nIt was a one -click install. And I was like, what is a one -click install? What is that? It was like this nice little add to Slack button. And so I just posed that question. I just said, I just went to my prompt and I said, hey, what is the, What is add to Slack? \r\n\r\nAnd I asked GPT, right? And it's like, oh yeah, that's how we handle multi -tenancy. And in order to do it for yours, you would just wrap your Flask app in a Slack app or something or whatever. And then basically, ultimately, you would be able to run each bot, each instance of the app in a dictionary, in a Python dictionary. And each app is, it's not a dictionary of strings. It's a dictionary, I think, \r\n\r\nSo each app is running in the dictionary, in the Python dictionary. I was like, is that even a thing? Can you even do that? But again, so again, I haven't told you guys this. I sat out trying to see what the limit of the technology, right? I started that when it came out and I have yet to find the limit. \r\n\r\nSo I wouldn't be finding the limit if I didn't try what it suggested. So I just went balls in, you know, just to let this go. And actually, I actually almost thought I broke my project, but then after eight or nine hours, I had my Slack bot running in three different Slack environments, even though I only had my one script running. It was running and connected in three different Slack environments, and I could message my AI in different Slacks in different channels in there, and it was all working and all segregated. I was like, holy shit, what did I just do? Multi -tenancy, holy shit. \r\n\r\nSolve the problem, because now if I just update my code, all of them get updated, right? Because it's just one thing running. Anyone can just click a button and a"
  },
  {
    "id": "report_source",
    "chunk": "ly shit. \r\n\r\nSolve the problem, because now if I just update my code, all of them get updated, right? Because it's just one thing running. Anyone can just click a button and add it to Slack, which is what you'll have click add to Slack, but you won't be doing multi -tenancy. So that's what multi -tenancy is and why you won't need it. So I had my whole bot made before I even thought multi -tenancy. That is the model name. \r\n\r\nAnd then local LLM, yes. So then immediately it knows 127 .0 .0 .1. And then LLM Studio, the default is 1234. but you're giving it in the screenshot. So it's confirmed in the same way. Yep Yeah, so we won't need the subscription functionality basically is what you're yeah That's what it turns into. \r\n\r\nYeah. Yep. Pretty wild. Sure. Okay, it does look like you got the same stuff I have so I'll just keep reading The slash command. Okay, so knowledge base integration, right? \r\n\r\nThe premium features the ability to create specific knowledge base It's all good, but use link chain blah blah who cares Visector, all good. Slash commands, Vox, those are the numbers. slash commands for administration, user interaction, just sitting there, just managing permissions and uploading documents. So I have some, all the slash commands I have are basically just fine and useful. They're things like, so whoever is the Slack workspace owner is the, well, shit, you can program it any way you want to, actually, so don't worry about that. Basically, the way it works is in Slack, you have a user ID, And you'll basically, you can give like admin, you can go into your own Slack and find, you know, your own Slack ID, right click on your name or whatever. \r\n\r\nAnd then in your program, you can make yourself the admin. So, and then"
  },
  {
    "id": "report_source",
    "chunk": "go into your own Slack and find, you know, your own Slack ID, right click on your name or whatever. \r\n\r\nAnd then in your program, you can make yourself the admin. So, and then you can delegate permission, so someone else could set this system channel message if you want. \r\n\r\nBut that's what my slash commands do. \r\n\r\nThey sort of, I made a, I guess a user permissions, user account administration, because a user can make another user a channel moderator. with my slash commands. Let me see if it has them listed. No, it doesn't. I was hoping for a more better breakdown, to be honest. I can glance through the other ones as well. \r\n\r\nThat's why we have multiple slash commands. \r\n\r\nYeah, here we go. There we go. See? There are all the slash commands in front of us now. So, setsystemmessage, sets custom persona instruction, addchannelmoderator, removechannelmoderator, and channel moderator. \r\n\r\nSo you as the admin can add a user as a moderator in that channel and that user can set the system message and manage their own channel. And then that user, that moderator, can also do the upload PDF. See, so that way you're not managing the whole fucking thing yourself. You can start delegating permissions out. My app, and then also add bot admin. So your own permission level, you can, also give out your own permission level to another user so that that user can give out create moderators themselves you see yeah no but you're the administrator of your app you're the administrator of your bot so in your channel where you have your bot added you uh whoever is one of these administrators the administrator that i'm talking about not the one that you're bringing up we'll talk about that next The administrator here is all within your control b"
  },
  {
    "id": "report_source",
    "chunk": "hese administrators the administrator that i'm talking about not the one that you're bringing up we'll talk about that next The administrator here is all within your control because you could completely control the bot. \r\n\r\nYou see that's separate from the Slack workspace. That's right. There is a distinction there. Now the Slack workspace, you're going to have to talk to the Slack workspace owner. If your goal is to actually get your Slack bot in your actual Slack, you're going to need to get your Slack workspace owner to click the button to install it. Now, if you can't do that, the beauty of Slack, the beauty of Slack is it's, you can very easily make your own Slack for free. \r\n\r\nAnd then you can just invite whoever the fuck you want and say fuck you to whatever rules. That's what I did at Palo Alto Networks. \r\n\r\nThat's how it worked. \r\n\r\nThat's how I got... Because that's what InfoSec, they said, no, you can't connect your bot. to internal tooling. And I said, okay, I'll make my own Slack. And then they couldn't say shit, okay? And then, yeah, there you go. \r\n\r\nSo that solves that. But, and then you can still, that's a perfect, that's a very perfect pilot project because then you can invite whoever the fuck you want to your Slack and say, try this. Go ahead and use it, it cost me nothing. You can use it up until we get this thing implemented in our real Slack. I don't give a, you see, I don't give a fuck. So yeah, sure. \r\n\r\nYeah, that's correct. \r\n\r\nSo the message will go from the user's computer, their keyboard, into Slack proper, and then Slack will take that, and then the bot will be listening, and the bot will see that it's mentioned, and then the bot will request the message and everything that it needs to. \r\n\r\n"
  },
  {
    "id": "report_source",
    "chunk": "k will take that, and then the bot will be listening, and the bot will see that it's mentioned, and then the bot will request the message and everything that it needs to. \r\n\r\nbecause it has the authentication, and then the bot will process, because the bot is also running on your local, it'll process, it'll send the request to the local LLM, and then back and forth, because it'll also use the embeddings. So things will happen, and then your scripts will then, when it's got the response, it'll send it back to Slack, and Slack will present it to the user. Is your shit HTTPS? Is your shit HTTPS? Okay. So, I mean, Slack has everything Slack already has. \r\n\r\nSo, like, you're already putting stuff in Slack anyway? Like, anything that you... Yeah, so, like, it's the same as, like, you know, oh, I don't want to give Google my data. Well, I mean, do you have gmail . com? Okay, they already have literally all your data. \r\n\r\nYeah, so, like, what do you... Yeah, so, yeah, as long as that's your... The answer to your question, and I'm not being facetious now, is as long as you have HTTPS up, then you're good to go. Your shit is secure through and through. You see, you're getting it from Slack to your bot, HTTPS, and then you process it internally, and then you send it back out. It's all encrypted. \r\n\r\nIt's encrypted in transit, see? Encrypted in transit. Then that's a different thing. Yeah, that's different. Yeah, that's manage your own shit. Yeah, manage your own network. \r\n\r\nYeah, that's separate. Yeah, yeah. Only if you want to run... So first of all, that's correct. Anytime you want it working, that's right. If you wish, to host this in the cloud, that is your prerogative. \r\n\r\nThat would just be another cycle that you describe to"
  },
  {
    "id": "report_source",
    "chunk": "correct. Anytime you want it working, that's right. If you wish, to host this in the cloud, that is your prerogative. \r\n\r\nThat would just be another cycle that you describe to the AI, I wanna host this in AWS, make me an artifact to help me get it there, because you'll test it locally, but then when, you know, that's your deploy, that's your CICD pipeline. You see, I'm presenting you a purely local solution to keep everything as super simple as possible, okay? And then, second of all, LM Studio is pretty fucking good. After an hour of no use, it basically offloads the LLM. So yeah, your computer's on, but at least it's not got the LLM loaded, ready to go 24 -7, right? So it's not the end of the world, and all you're ultimately using is electricity. \r\n\r\nYeah, no, that's good. Any other questions? It's all good stuff. You could. No, but you can get a cloud resource that has a GPU. Yeah, yeah. \r\n\r\nSee, you know, there are cloud resources that offer GPUs. Yeah, and you're good to go there. You can just install the same shit. Install your LM Studio if you want on there. Who cares, right? And then set it all up however you want. \r\n\r\nOr you just ask, you know, make an artifact. Maybe AI knows a better way to do it than I'm presenting. But that is it. You would go get some GPU in a cloud and then install the LLM there in the same way you're doing here for learning. Yep. Yep. \r\n\r\nAnd then it's just a different API called different URL. But your code doesn't change. The only thing that changes is the URL, you see? for the API call. Your whole script you made is the same. Yeah. \r\n\r\nOkay. So that's what I wanted to show with these is I wanted to articulate the way particularly these work. So you had it at got it. a grasp of the idea"
  },
  {
    "id": "report_source",
    "chunk": "is the same. Yeah. \r\n\r\nOkay. So that's what I wanted to show with these is I wanted to articulate the way particularly these work. So you had it at got it. a grasp of the idea of like the authentication, the hierarchy that exists a bit of the responsibilities, because there's a bit, you know, just setting a system message, because someone breaks it, they can break your bot if they accept this, break this, if they remove the system message, right? So, okay. Knowledge base, that's going to be just fine. \r\n\r\nUpload PDF, there's no reason to change any of this. It works beautifully. \r\n\r\nProvisions and security, fine. \r\n\r\nYeah, whether or not they are administrators or stuff, all good stuff. Yeah, commercial features. \r\n\r\nSee, that would be, I think you've already said, we don't need any of the commercial features. \r\n\r\nWe're doing, I didn't though, so I will. Finally, as for the commercial features, since this is an internal project, we won't be needing any of that paid limitation for premium features, et cetera. \r\n\r\nOkay. Ah, just because I said finally, I'm gonna say next. It says Firestore, but I think Prisma is easier. Firestore is a cloud. And when I built my Slack bot, I didn't know. I didn't know as much as I do now. \r\n\r\nThere is literally no, no, no, no, no need to overcomplicate shit and try to use Firestore. You can just use Prisma schema and you're a local SQL. And it's totally so fucking much easier for database. So you would have the local database, local, All in this full stack, you'll be loving it. frontend, LLM, database, all four hats, we're in all four hats right there. \r\n\r\nOkay, so we'll just get that mentioned next. \r\n\r\nAnd then finally, I see that the, what did it say? Firestore. I see that the architectu"
  },
  {
    "id": "report_source",
    "chunk": "e're in all four hats right there. \r\n\r\nOkay, so we'll just get that mentioned next. \r\n\r\nAnd then finally, I see that the, what did it say? Firestore. I see that the architecture, the tech stack uses Firestore, but, If we can just use like a Prisma schema, that might be much easier. I think I'm taking a look at the technical scaffolding plan because that means that, okay, because if we're gonna use Prisma, then it would have a prisma . schema file somewhere in here and it does not. So then that's to put a pin on it to the AI. \r\n\r\nI'll say, so after updating, so after, so, okay. So take in this feedback and then update the relevant artifacts plus documentation such as adding charisma that schema to the technical scaffolding plan etc see i'm giving it an example so that's one shot right there that's that's what the uh graybeards in the ivory tower would call the difference between zero shot and one shot, is I just gave this little bitty example. There you go, one shot. Definitional one shot. Or EG, I think it's EG, whatever. \r\n\r\nI don't care to think about it. Okay. One means that is to say, and then the other one is, EG is an actual example. So IE is that I mean to say. Something like that. I had a fucking COO correct me on that, so I'm like, okay, I'm gonna get the difference. \r\n\r\nYeah, yeah, Ingrok. Ah, here's another difference. Ingrok is a reverse proxy. There's actually no reason for it. You can make your own reverse proxy, right? See, that's another thing. \r\n\r\nIngrok, you pay $10 a month, and you have the privilege of them being your reverse proxy. You can have AI make your own, so that's gonna be the next thing. I'm just going to stop saying finally. Next. Also, in the dev and testing guide, if you guys got that. Ok"
  },
  {
    "id": "report_source",
    "chunk": "xy. You can have AI make your own, so that's gonna be the next thing. I'm just going to stop saying finally. Next. Also, in the dev and testing guide, if you guys got that. Okay. \r\n\r\nSo, yeah, mine says, yeah, that's to start the development server and the dev and testing guide. Do you have any sort of dev and testing guide? Okay. That's okay. That's okay. What does yours say in terms of like how does your local server get exposed to the internet? \r\n\r\nOkay. Then just mention, yeah, go ahead. \r\n\r\nSee, we can, we can, you can use, you can make a local. \r\n\r\nSo, okay, so here, let's see, let's see, let's see. \r\n\r\nWhat, where are you, where are you living? Where do you actually have time out? Because I'm forgetting about Robert. \r\n\r\nand shit. \r\n\r\nSo, where are you? Are you in a dorm or something? Okay, so you have your own AT &T router or whatever? Okay, cool. So then, that'll be part of the equation eventually. But then we'll just, when we get there, we'll document that in sort of the same way how we got into your route, we got into the LM Studio, and then we opened up some configuration stuff and we took some screenshots. \r\n\r\nYou'll probably, we'll do your own reverse proxy. You'll forward your own fuckin' router, so you'll do all the networking shit. Forward the traffic for your Slack bot, straight from Slack to your bot, through the port, running locally. And then, yeah, from there, it'll be all inside your computer where it needs to go. And all that is is a reverse proxy. And that's, again, that's all the, it's amazing to learn this shit, dude. \r\n\r\nJust be like, wait a minute, you don't need, like, nginx, you don't need fuckin' ngrok, you don't need fuckin' this, you don't need fuckin' that. I can just make my own fuckin"
  },
  {
    "id": "report_source",
    "chunk": "Just be like, wait a minute, you don't need, like, nginx, you don't need fuckin' ngrok, you don't need fuckin' this, you don't need fuckin' that. I can just make my own fuckin' thing, like, what the actual fuck? It's crazy. All the overhead is gone. \r\n\r\nIt's just fucking running on your own. \r\n\r\nYour own LLM, your own database, all of it. Anyway, I'm going to stop geeking out. Okay, so also in the dev and testing guide, I see the use of ngrok. I think we can actually just make our own reverse, our own reverse proxy, proxy solution. We don't need ngrok. I'll, when we get there, I'll just show you my router. \r\n\r\nWhen we get there, we will just document router and the necessary port forwarding in an artifact. if you want to get that started You can I have AT &T router and open that up 192 168 1 2 5 4 and then you go to yours. I think mine's 2 by 4 Yeah, yours might be one. Yeah, so see see see details see what is this? Uh that we have a box Do we have serial number something systems? Starting with just letting the AI know what router model from Verizon would just be a great first start. \r\n\r\nAnd you can just kind of leave it there. And then it'll start making an artifact where it'll start giving you instructions like, yeah, you're going to open up this tab to get to the port forwarding. You're going to want to write this in there. And then from there, when it's like step three doesn't work, you just say, hey, step three is wrong. What do I do here? And then your own guide, your own artifact will be updated. \r\n\r\nAnd then the next time you need to go through it, you just have the artifact already written out. It's fucking amazing. So just whatever, yeah, somewhere, something that just shows the model of the router. And if you "
  },
  {
    "id": "report_source",
    "chunk": "ugh it, you just have the artifact already written out. It's fucking amazing. So just whatever, yeah, somewhere, something that just shows the model of the router. And if you can't even get that, then just the homepage, whatever, screenshot. It's enough context for the AI to just get an initial artifact made for you. Because it knows what forwarding is, it knows Verizon. \r\n\r\nAnd just add it as a second screenshot in your list, in your, just to get it done. Yep. It's so slow, dude. Holy shit. I'll just take this screenshot and be done with it. \r\n\r\nI have an AT &T router. \r\n\r\nI'll provide a screenshot of my logging into it. \r\n\r\nThat's it. That's all it is. Yeah, just however you want to say it of the homepage, of the login page, of the main page. Yeah. Oh, in this moment, also find, oh, get your local IP. So, ipconfig and tell it what your local IP is for your laptop. \r\n\r\nIt shouldn't be. Your local, internal? No, your, so, okay, so your external, it's not, if your external ever changes, you just need to update your script once, it's not favorable. Maybe you'll have to change something in Slack admin when we get there in their URL, in their admin portal on the website. But the trick is, don't let your router disconnect from power. Like, you know, your house might lose power and come back. \r\n\r\nWhen that happens, that's when your IP address gets reset, dude. I haven't gotten a new IP for two years. Because I have my router battery backup. And so if my house ever loses power, my router doesn't. And I don't ever let go of that IP. \r\n\r\nI have yet to ever, ever, ever. \r\n\r\nCode dynamic. I have yet to ever have to change my IP. And I've been hosting my website. for a long time with just that little thing, okay? And I've had AT &T,"
  },
  {
    "id": "report_source",
    "chunk": "ever, ever. \r\n\r\nCode dynamic. I have yet to ever have to change my IP. And I've been hosting my website. for a long time with just that little thing, okay? And I've had AT &T, I've had Verizon. So if you ever have to change it, it's A, not the end of the world, and B, the solution is just put a fucking battery on it. \r\n\r\nPut it on a fucking battery, yeah, yeah. \r\n\r\nA battery that won't be drained by your computer, right? \r\n\r\nYou see what I'm saying? \r\n\r\nLike its own separate battery. \r\n\r\nYeah, yeah, yeah, okay, all right. \r\n\r\nAnd then you'll just never have that issue. But your local, you just wanna tell the AI now, go ahead and get in your context what your local, Because that's an unknown, it would need to know this for writing anything in between here and there for that instruction. So your, Mike, mine is 221. \r\n\r\nYou're just, you're telling the AI what your laptop's internal network IP is, and what you're doing is, in the future, it would tell you in brackets your internal IP, which is frustrating. \r\n\r\nNow that you give it now, in the future, it'll just tell you what it is, and it won't give you the brackets, because you gave it to it in the first place. All right, I think that honestly that's pretty much a lot to I mean it's not a lot what I mean is it's enough a lot in terms of like I've given an AI a fuckload of shit to solve and so comparatively these are minor tweaks but it's enough that I think it's that we're good to go so and I already did say taking all the feedback up to date relevant artifacts. \r\n\r\nSo I'll just take that line and put it at the bottom. \r\n\r\nSo please, and I'll say the word please, so that it knows this is the directive. It's not that I'm being nice to the AI. I'm not asking nicely. I'm sayin"
  },
  {
    "id": "report_source",
    "chunk": "it at the bottom. \r\n\r\nSo please, and I'll say the word please, so that it knows this is the directive. It's not that I'm being nice to the AI. I'm not asking nicely. I'm saying this is everything I said above, and this is what I'm asking, I'm expecting out of from you. Please do the thing. Please take in the feedback and then update the relevant artifacts and documentation. \r\n\r\nAnd then I need to actually select the response. So I'm going to just go with the biggest file. I could care less. Select this response. \r\n\r\nSelect all. \r\n\r\nAha. OK. So now, do you see how my baseline is lit up? I see yours is as well. I'm going to go ahead and click Baseline. And it's going to say, this is not a Git repository. \r\n\r\nPlease initialize. I'm going to go ahead and click Initialize Repository. Does it work for you? Great. It worked. Success. \r\n\r\nNow do it again. Now click baseline again. And then this time it should actually create the baseline commit. Does that, say that at the bottom right? \r\n\r\nNope. \r\n\r\nHold on. So what did you say at the bottom right when you clicked it? I did, okay. \r\n\r\nOpen a new, okay, is this terminal down here? \r\n\r\nIs this terminal in your present working directory? Can you, do you know, can you write git init in there? Will that initialize? What does that say? \r\n\r\nClick baseline. \r\n\r\nUp there, no. Why? Okay, okay. So, okay. \r\n\r\nHold on. \r\n\r\nIn your terminal section, click on output, and then over on the right where it says tasks, the drop down, click that, scroll all the way to the top, that one, the data curation environment. Now, clear this, just right click, and then clear output, and then now click it again. Can you copy whatever the hell that says over to me? Okay. Oh, let me see it. It's probably someth"
  },
  {
    "id": "report_source",
    "chunk": " this, just right click, and then clear output, and then now click it again. Can you copy whatever the hell that says over to me? Okay. Oh, let me see it. It's probably something I didn't encounter, so I didn't code for it. \r\n\r\nBut I think if you just do whatever it's asking you to do manually, it's just some Git shit. Let me read. My monitor is so laggy. GitHub issues right now. I'm helping him get through it. Yeah, so, okay, so just do that exactly. \r\n\r\nThat's all you have to do. And I believe this is articulated out in the documentation. GitHub artifact, but just do exactly what that says. Set your email and your password or your email and your name. But I'll take that error log and I'll use it. I'll take that error you gave me and use it to handle this edge case. \r\n\r\nYeah, I can just go to the baseline now actually. So all that does is it sets, it runs a commit so that you can easily test multiple responses. \r\n\r\nBut no, it didn't add anything in there yet. \r\n\r\nThat's gonna be a push command, yeah. \r\n\r\nSo everything you're doing... \r\n\r\nOh, sure, sure. Yeah. won't see it appear in here until you do a git push. What do you mean by, am I committing? \r\n\r\nBecause technically in my system you just do a baseline. \r\n\r\nSo what do you? \r\n\r\nClicked on the GitHub thing, you mean in the bot? \r\n\r\nOkay, so you mean, let me look at your screen, hold up. \r\n\r\nYes, I don't use that, yeah. My baseline would do that, yep. It does, it does. Thank you. I hate, I fucking hate git. So, yeah, yeah, yeah. \r\n\r\nSo, yep. And then what does it say at the bottom? Tell me what it says at the bottom right when you click baseline. There should be a little pop -up. Yeah, so watch my screen. Can you see my screen? \r\n\r\nI'm gonna click baseline. And I actu"
  },
  {
    "id": "report_source",
    "chunk": "t says at the bottom right when you click baseline. There should be a little pop -up. Yeah, so watch my screen. Can you see my screen? \r\n\r\nI'm gonna click baseline. And I actually didn't get any pop -up, but there should be a pop -up right down here. Let me look at your screen. That's good, that's what you wanted to see. That's what you, yeah. So it just did a commit for you, that's all. \r\n\r\nAnd then, Cameron, did you get yours? Cool. So then, yeah, I see we're both at accept selected. So we've got response, the biggest, selected the files. \r\n\r\nI'm just going to click accept selected, but I'm going to also have my data curation window open when I do it. \r\n\r\nAccept selected, and it created all my files, the ones we were just looking at. And so that means I do have that source code file here. \r\n\r\nI don't need to have my app demo selected. \r\n\r\nThat would just be Redundant 16 ,000 tokens, so I'll just select that and also it would include me in down here because I would see basically two 16 ,000 Yeah, okay, so then there's that Wait what so we got that so now now now we've got that that and we've got that written We're basically ready to do generate prompt Yep, so we're going to hit yeah. Yeah, we're ready. Yep I'm just gonna generate prompt and I actually have to close this because I think I was already open I'm gonna click it again You're just going to paste this in as well with it. So click in there and just paste it. And do you have that other screenshot? I see only one screenshot. \r\n\r\nDid you capture one of your router? Google AI studio synopsis of the screenshot? No, no, no, no, no, no, no. you don't, you don't send the screenshot by itself. You send this just like Cameron is doing right now. You send the screenshot. \r"
  },
  {
    "id": "report_source",
    "chunk": " the screenshot? No, no, no, no, no, no, no. you don't, you don't send the screenshot by itself. You send this just like Cameron is doing right now. You send the screenshot. \r\n\r\nYes. Yes. That's correct. Yep. That's correct. Yep. \r\n\r\nYep. \r\n\r\nAnd all three go together. \r\n\r\nThe yes, sir. The two screens, the two screenshots and the, uh, and the prompt. And then you just fucking send it, dude. Full send. Let's go. Of course. \r\n\r\n$3. Yeah. See, um, Here's the deal about thinking. Check this out. This is facts. There are some domains that the only thing the AI needs is just more thinking time, and the problem is solvable. \r\n\r\nAt that point, the only question is, how powerful is your computer? That's been mathematically proven. There was a Google researcher who made a tweet about that, and he posted his research paper or whatever. But just keep that as the mental model of what these things are capable of. That's, again, another thing to think of when you do the parallel prompting, because the thing times out at 600 seconds, because that's about how long it takes to give you 65 ,000 tokens. And so when you run in parallel eight responses, and you get 600 seconds in each response, that's 10 minutes. \r\n\r\nof processing time times eight, so that's 80 minutes of processing time in just 10 minutes that you waited for. That's actually insane to think about. And then the final thing on that parallelism is if response A gives you garbage and response B gives you the solution, but you actually only just sent once and got response A, you never got response B, that other potential future doesn't exist for you. It just doesn't exist, right? You now have to go deal with your response A, And just solve the problems that response a brought yo"
  },
  {
    "id": "report_source",
    "chunk": "ther potential future doesn't exist for you. It just doesn't exist, right? You now have to go deal with your response A, And just solve the problems that response a brought you or or try your try your call again, right? Versus just sending it twice. \r\n\r\nYeah, so okay. So go ahead and you want to send yours off? We're getting started here and Yeah, and Networking information that's going to be necessary to kind of get so the block and talk to the network to dislike and get the message. Yeah Beginning planning we're about to once we feel good with the artifacts we'll just start making the Python script. Dude, yours just doesn't want to stop. \r\n\r\nNo, don't stop it. \r\n\r\nLet it cook. Alright, so I'm going to just copy mine in. So where are we at now? Ah, yeah. See, so here's an example. See, it says your public IP because I forgot to tell it what my public IP was. \r\n\r\nSo I'll just make sure I'll add it in on my cycle. It's no big deal. Yeah, right here, your public IP. See, that's what I mean. Like, that's what I was trying to preempt, and I just forgot to give it one detail, but it's not. It hallucinated technically, because it's not the actual correct thing. \r\n\r\nIt's technically, I would classify that as a hallucination and say it was just missing the data, then the actual data point, because it couldn't possibly know what my fucking public IP was. No big deal. Yeah, same thing with any hallucination. It's all the same. Okay, so jqrbot, ready to... So check this out This is on the next edition to the next another change that I added between the version that you had in this one Which is if you mouse over right here on the plus It'll give you it'll actually tell you what that what's missing to stopping you from going to the n"
  },
  {
    "id": "report_source",
    "chunk": " that you had in this one Which is if you mouse over right here on the plus It'll give you it'll actually tell you what that what's missing to stopping you from going to the next cycle Versus you just having a fucking guess which it says it's a cycle title is required So you just need to update this so more documentation there we go fine now I can click the button now I can make a new cycle. \r\n\r\nI'm gonna go ahead and just save cycle history at this point, just in case I don't. to lose my shit. I'm gonna save it as cycle to start because I You can yeah, so you can parse on parse. It's no big deal any time So what are you what are you trying to do though? All right, you want to make it? Okay. \r\n\r\nAll right So you need to make it because you this cycle is complete. You now need to make a new cycle So hover over the plus button because it's a interesting Interesting that it doesn't say what it's supposed to say. Yeah edit that right in there Yep, right in there, and then now you're good. I don't know why yours doesn't get the tooltip. That's frustrating to me. Okay, that's what you needed to do. \r\n\r\nNow down there, yes, this is the site. See, that's this process. Yep, but after you post those in, save your process, progress, because I just tried to fix the data loss in the cycles where it may not have been successful. I think I was. My test was successful, but shit, didn't hit the fan. So just save your progress. \r\n\r\nJust like an old video game. \r\n\r\nSorry, it's not autosave. It is autosave, but it might break. You don't want to lose. It's not the end of the world. You can literally start from anywhere because your project is the context. So you can actually restart a brand new cycle at any time. \r\n\r\nDon't feel like you're l"
  },
  {
    "id": "report_source",
    "chunk": "f the world. You can literally start from anywhere because your project is the context. So you can actually restart a brand new cycle at any time. \r\n\r\nDon't feel like you're locked in. \r\n\r\nBut you shouldn't lose data, so I'm working on it. yeah once you got your space today it's really it's rinse and repeat read through this one and this see this one is probably now now we're gonna ship so the first time we were more focused on reviewing the project plan now this time the plan has been aligned now we're more work I'm gonna review for any action items that we may need to take such as preliminary setup Like this thing has been pre -trained, has been fine -tuned to tell you what you need to do to get the development environment set up, like install Python. It should, if everything is working correctly, there should be an artifact that you have that has instructions based off of your project's architecture, which is Python, to install Python. \r\n\r\nSo that's going to be sort of the process now, is go ahead and just go with the largest one. \r\n\r\nkind of review it. Go ahead and review the ones that we have changed. So start there. \r\n\r\nStart with like that project analysis file, the reverse engineering file. \r\n\r\nRead that one, because that should be now further aligned with your mental project, right? Click it again. So when it's highlighted, it's on. \r\n\r\nIt's persistent. \r\n\r\nYep, yep. No, not yet. I did talk about that first, but let's But then after talking about it, I decided it's still best, let's review the work that we've just done. which is to alter the, see what I'm saying, the analysis? Do you get what I'm saying? Let me say it one more time. \r\n\r\nLet me say it in a different way. We just described all the differences that"
  },
  {
    "id": "report_source",
    "chunk": "e, see what I'm saying, the analysis? Do you get what I'm saying? Let me say it one more time. \r\n\r\nLet me say it in a different way. We just described all the differences that we have with our project in our mind with what the AI told us it has in its mind. Now we want to read those, we want to see the results of that. We want to make sure that it's not talking about multi -tenant, It's not talking about like, you know, subscription shit. And maybe it has more, you know, see what I'm saying? So yes, yeah, the alignment happened, basically. \r\n\r\nSo I'm gonna do the same thing. I got my four parts in. Okay, that, yeah, that can happen, and it's, I'm getting it as well, so we'll fix it together. It's, that was another thing I was working on, was trying to make that more robust. So the way we'll fix that is, We're just going to unparse. So I can see I got a parsing error in three out of four. \r\n\r\nThat's fine. I'll show you how to fix all four. \r\n\r\nSo just take one of them, unparse, and then take that, cut it out so you can see that it's removed, and then put it into a notepad. \r\n\r\nAnd we're going to look at it to find the parsing error, which it should be just right here, basically, at the bottom of the, because I've already fixed this once. It's going to be the closing tag of the file path. So we're going to go down. This is the best way to do it. I'm just going to Control -F File Path. And then if I go to the next one, Let me get my find over so you can see what I'm doing. \r\n\r\nI'm just gonna do a control F for that string file path. and then just find next if I see this right here. That's what I just changed because I'm making this parsing more robust. My parser and the regular it's both based off slash file but that's too "
  },
  {
    "id": "report_source",
    "chunk": "next if I see this right here. That's what I just changed because I'm making this parsing more robust. My parser and the regular it's both based off slash file but that's too universal. So I have changed it. You can see it says it's what it's expecting when you click parse all. \r\n\r\nIt says it's expecting file artifact not just file. See that? So what we're going to do is now that we've seen that, we're just going to use replace to solve it. We're going to replace the open bracket slash file close with file underscore artifact. See? So just adding an underscore artifact. \r\n\r\nAnd you're going to fix the parsing. \r\n\r\nSo I'm going to hit replace all, and it's going to tell me replace all eight occurrences were replaced in entire file. All right? I'm going to copy my file, cut it out, and put it back in my response. Boom. And voila, we have our A files now detected as opposed to zero. Minor inconvenience, apologies. \r\n\r\nI'm working on it. Yep. No, but it's okay. It's learning for you. It's important to see the back end as well. So that when it does break, you know how to fix it and move forward. \r\n\r\nYes. I'll do it again. Do you want me to do it one more time? Because I have two more. Okay. Okay. \r\n\r\nSo in the app analysis, that might be okay. Is there another artifact that describes your project? Do you see the difference? Like a project vision and goal artifact? Read that one and see if that mentions any bullshit about multi -tenancy. Because that's what, yeah, I'm going to fix my other parses. \r\n\r\nSo that's a fair question. And basically a lot of things boil down. A lot of those considerations boil down to one thing. And that is the LLM that you're using. What is its context window compared to the size of the document that"
  },
  {
    "id": "report_source",
    "chunk": "gs boil down. A lot of those considerations boil down to one thing. And that is the LLM that you're using. What is its context window compared to the size of the document that you need to work with or document slash knowledge base. If your knowledge base is terabytes, then you have to do some sort of, there is no context window who can fit all that. \r\n\r\nYou have to do some sort of retrieval augmented generation of some kind. And then this one that we're going to do is like the most cookie cutter, best one, easiest for all like use cases. And then so if it's a small document, then it's, you would just have to do append it, just like you would append, you know, like I quote, appended manually the app demo. You remember when we did that in the initialization? \r\n\r\nBecause it's 16 ,000 tokens and we've got a million to go with. \r\n\r\nSo I just dropped the whole fucking thing in, subscription functions and all, see? Yeah, that's okay. So you can follow that train of thought because you're confirming alignment. Let's follow that. So then the thought is it must say something at this point about using the local LLM because you've said it. So is there another artifact that speaks to LLM integration at all? \r\n\r\nYes, I have one. I do have a LLM integration guide. \r\n\r\nDid you end up with one? \r\n\r\nWhat's your next response have? Response three or the second? Yeah, that one. Does that one have one? What about the next one? That's kind of what we want specifically LLM integration guide because I do have that. \r\n\r\nMine decided to make an LLM integration guide for me because I said I had a local LLM. Okay, then that'll be something you scold the model for because you gave it the fucking screenshot of the fucking LLM studio and the fucker th"
  },
  {
    "id": "report_source",
    "chunk": "ecause I said I had a local LLM. Okay, then that'll be something you scold the model for because you gave it the fucking screenshot of the fucking LLM studio and the fucker threw it away. So you're going to be grumpy. You're going to be grumpy with your AI for a minute. You're going to critique the model. Yeah, yeah. \r\n\r\nWhat about your roommate? Did he get an LLM integration guide? That's it, there you go. So, yeah, no, same, literally same thing. Different names of Sasquatch, playing guide, roadmap, who cares, as long as it has spoken to that for you, that's C. And I also got a reverse proxy guide, look at that. I have a reverse proxy guide, so I won't need Instructions for configuring a home router, AT &T, for port forwarding to expose a local development server to the internet. \r\n\r\nReplacing the need for NGROK. Bada bing, I just saved 10 bucks a month. \r\n\r\nYou will just be, so, okay, so there's a couple, so can you click the back button and go back to your cycle one and read out loud the section that you have spoken about your LLM, your local LLM? \r\n\r\nI'll be using a local LLM and then put that in quotes. The name of the model that you just read out. \r\n\r\nSo put what I just said in front of that model name. \r\n\r\nSo I will be using a local model, and then put the model name in quotes. And then say, running on the same server as the Slack bot. And then click Generate Prompt. Actually, first, don't do that. Close your current prompt file. I see it's been edited. \r\n\r\nYou see that third file you have open? Yeah, close that and then say no or whatever. Yeah, who cares? It gets auto -generated. Now go, now, now, wait a minute. Yeah, click generate prompt. \r\n\r\nThat new sentence you just wrote is in there, right? Good, okay, I'"
  },
  {
    "id": "report_source",
    "chunk": "Yeah, who cares? It gets auto -generated. Now go, now, now, wait a minute. Yeah, click generate prompt. \r\n\r\nThat new sentence you just wrote is in there, right? Good, okay, I'm just fucking paranoid, data loss. Okay, click generate prompt. All right, now, can you just control F, cycle one, just make sure that new string you just added is in there. There we go, cool. Now, copy and paste and send it again. \r\n\r\nSee, you see? What we just did, I use this analogy of a Japanese letter. \r\n\r\nImagine a single page, and on that page is just a single large Japanese letter. \r\n\r\nThe way the Japanese characters work is a single long letter. difference on that Japanese character can completely change the meaning of that character. All right? And so too is what we just did. You see, you just sent a prompt, and you've got a response, and you analyze that prompt, and you saw something was missing. You just edited one of those lines. \r\n\r\nYou just added a little dash or something to the Japanese character. \r\n\r\nAnd you're going to completely change the meaning. \r\n\r\nIt's my theory. \r\n\r\nDon't delete the pictures. The pictures are helpful. \r\n\r\nThat's okay. \r\n\r\nAs long as they're in one of them, you can easily copy them back. Was that the fourth one? \r\n\r\nOkay. \r\n\r\nYeah, they're easy screenshots, but I've done that before. But you can just click on it. If it still exists in one of your windows, you can just click on it and easily copy it. Google Studio is pretty good at that. Yeah. Those are easy ones. \r\n\r\nIf this works, though, dude, that's like exact, see how little, I tried to be very minimal in the change to illustrate, to be as illustrative as possible in this little example. Because I've done this a couple times. It should work just fine. A"
  },
  {
    "id": "report_source",
    "chunk": "ed to be very minimal in the change to illustrate, to be as illustrative as possible in this little example. Because I've done this a couple times. It should work just fine. And I should have been able to narrow it, I should have been detect, I should have, hopefully, this is testing my spidey senses, my LO and spidey senses, if I'm able to detect specifically the tiny missing piece, and what size is this? \r\n\r\nYeah, essentially, yeah. \r\n\r\nI mean, it's all hit or miss. If you were to run eight, you might have gotten it as well. You see what I'm saying? No, that's fine. That's just an automatic thing that appears because it detected you have a URL, but it does not know your intent. And your intent at this moment is not to get the LLM to go get a web crawl on any URL that you're passing it, so you don't care. \r\n\r\nI'm going to go run to the restroom. Be right back. Send those off and see if the results fit. See if you can get them all in there. I know that's right. Dude, the other cats weren't even nearby when he bit me. \r\n\r\nHe's just nervous with them around. Okay, I'm back. Parse her. Yeah, unparse and then just rip out whatever you had in there. No, no, go back. because basically you're unhappy with the results here. \r\n\r\nHere, right? Hold on. No, no, no, no. No, no, I'm wrong. I'm wrong. You should be doing this in Cycle 2 because you sent Cycle 1, and then you didn't, and then you put it in Cycle 2, and you did not like what you got, and so you resend Cycle 1, and now you're updating the Cycle 2 responses again. \r\n\r\nIt's hard to sometimes do that. do, for real. I've sat here for five minutes once trying to figure out what step, what part of the process, and it's my own tool. So don't feel bad. I do the same. If you have "
  },
  {
    "id": "report_source",
    "chunk": "o, for real. I've sat here for five minutes once trying to figure out what step, what part of the process, and it's my own tool. So don't feel bad. I do the same. If you have two monitors also, it does help. \r\n\r\nSparse, and then we'll see if it has any parsing errors. \r\n\r\nFingers crossed, LLM integration guide. \r\n\r\nCool, see? \r\n\r\nSee, do you see? \r\n\r\nSo you just weren't specific that the model you're using is local. Do you see? The moment it got that, it knew to give you a local LLM integration guide. You see? So, that's a good lesson right there. Okay, cool. \r\n\r\nTiny little tweak. Tiny, tiny, tiny little tweak. Okay, just, so I'm gonna just, we're basically in the same spot. I'm going to, I'm looking at something. two. I've got my responses in my longest one is 8100 tokens. \r\n\r\nWhat are y 'all at? Just curious. Okay, then just send it off or So I'm going to select this response the longest one it highlights a baseline I'm gonna click baseline. I see it just doesn't commit everything just changes color and then select all so yep Nice dude, and the time is worth it now to not have to do this later. Okay, set selected, and then now I got the new files and then the updated files. I am a little curious about one thing though, let me see. \r\n\r\nOkay, I think it's fine. I see that the A0 is coming up here, whereas over here the A14, but I won't argue with whatever gaming convention it's going with. I'll just let the bot do the bot. I'll let the AI do the AI, to be honest. Until I see there's an issue, but I don't see an issue currently with this, even though they're different. I would prefer it to be the same, but it doesn't, it doesn't you know, bother me, technically. \r\n\r\nSo, okay. Now, let's, that's, now it's the second part "
  },
  {
    "id": "report_source",
    "chunk": "h they're different. I would prefer it to be the same, but it doesn't, it doesn't you know, bother me, technically. \r\n\r\nSo, okay. Now, let's, that's, now it's the second part of what I suggested, which is now we're going to actually look for the action items that we, we actually might need to take at this point to get our development environment ready, because all we want to do now is actually ask the AI to make our program now. So, we got to figure out what we need to do to get our environment ready so that we can make our program. So, source code, the analysis, the integration guide, that'll probably be one we read, but I want to see if there's a more broad starting point, the reverse proxy guide. The development and testing guide, probably that one. \r\n\r\nYeah, see, prerequisites, LM Studio. See, that's the more, that's kind of the content that I'm looking for. And then the implementation roadmap. Yeah, and I don't see any duplicates. So it's not like I see two project visions or two scaffolding plans. So everything's fine. \r\n\r\nWhat is the roadmap? So step one, foundational setup, core bot logic. So set up initial file and directory structure. See, we're not there yet. So that's what, so we're not at this file yet. So this is, it's probably the development guide we're after. \r\n\r\nIt's not the roadmap, because the roadmap is one step ahead. GitHub repository setup guide. We're pretty much good there, because that's basically just getting Git in it. So we're already good there, and we don't need to push right now. So A14, we're good. Then this is that one that you just brought up and said it's nicely aligned, Project Vision. \r\n\r\nAnd then now, ah, so here, technology stack. \r\n\r\nThis is a good one to review, because that's w"
  },
  {
    "id": "report_source",
    "chunk": " one that you just brought up and said it's nicely aligned, Project Vision. \r\n\r\nAnd then now, ah, so here, technology stack. \r\n\r\nThis is a good one to review, because that's what we have to make sure we have. \r\n\r\nPython, Flasks, Slackbolt. \r\n\r\nWe'll just install all those things, libraries or whatever. OpenAI, client libraries, interact with global own, yeah that's fine, no problems there, yeah yeah yeah. Yep, so I think then it is the development and testing guide that is where we need to start after reviewing everything So then we have the prerequisites LM studio for forwarding. Ah, check that beautiful beautiful This is exactly perfect for me This is what mine says mine says prerequisites go see artifact 4 and go see artifact 5. Do you see that? Perfect, bro. \r\n\r\nPerfect. Exactly. So it's step -by -step, bro We just made our own tutorial to make our own fucking thing, dude. So uh set four uh so artifact four is my first step um yes i and actually i already did message it it's already talking squawking um so we can just review this because it's largely done um this guy provides necessary information to connect om studio basic screenshot om studio is running with these configurations you see aha this exposes several blah we'll be making the environment variable yes and it'll be filled with that stuff yes so that's good that's good uh it's got all this stuff We will use the official OpenAI Python library to interact with the L1Studio server. Sounds great. He's just planning. \r\n\r\nThen, no actions. By following this guide, the bot will be directed. So, great. Perfect. Nothing we need to do. Now, this one we probably will need to do some forwarding. \r\n\r\nSo, the next artifact on the list for us to do, allow Slack server to s"
  },
  {
    "id": "report_source",
    "chunk": ", great. Perfect. Nothing we need to do. Now, this one we probably will need to do some forwarding. \r\n\r\nSo, the next artifact on the list for us to do, allow Slack server to send events like mentions to your local development machine. your machine must be accessible from the public internet. Instead of using a third -party service like ngrok, you can configure your home router to forward incoming traffic on a specific port to your development machine. This process is called port forwarding. This guide provides several general steps provided on the screenshot based on your AT &T router. So, this is where we diverge. \r\n\r\nYou will do your own port forwarding. I will do my own. If your instructions deviate from what you see on your screen, that becomes your cycle. That becomes your criticism. Hey, your instruction in Artifact X does not match what I see on my screenshot Y. Update Artifact X based on this feedback. \r\n\r\nThis is what I see, blah, blah, blah. What do I do? What's the right step? I got to step three. \r\n\r\nYeah, okay. \r\n\r\nOnly one of you need, So, good question. So at this point, you could both do it. All you would need to do is each have a separate port. So one of you change yours to 5001, and then you can just basically both get in the router, make your own four forwarding rules, and life is good. Then just change your, then in your site, okay, so 3000, and which is trying to use 3000? So all you need, all you need to do Toot, the only reason I'm pausing is because I'm just trying to decide which answer I want to tell you. \r\n\r\nI'll just tell you both. All you've got to do is update all of the, you just need to use a different port for this project. And in order to do that, there's the two ways I mentioned. Either"
  },
  {
    "id": "report_source",
    "chunk": " you both. All you've got to do is update all of the, you just need to use a different port for this project. And in order to do that, there's the two ways I mentioned. Either you can just manually do a control F, find replace in your repo, and then no one knows the wiser. So for example, how is it written? It's written in tilde 3000 tilde, right? \r\n\r\nOkay, but you see where I'm going though? Because this is a thing you'll run into. It doesn't matter, right? You'll run into it. And then once you just realize, oh, if I just change all the mentions from the 3000 to 3001 or 3007, something, then the AI will just, wouldn't even know it was ever a problem. Or you can just tell it in a cycle. \r\n\r\nHey, I got, that port is already in use. Update our documentation to just use this port instead. Either way solves the problem. You make the change, it makes the change. You see? Yes. \r\n\r\nSo yes, so yes. depending on how, if the problem is something, how long does it take? There's a million ways to skin a cat, I wanna go with the least time consuming, easiest for me, consume my cognitive bandwidth, I don't have to, you know, yeah, yeah, yeah, yeah, yeah. So just making a choice and going with it depending on the problem I'm facing, yep. And the goal though is to do as few of the changes manually yourself as possible, because you want to have, you want to wield this tool like a, fucking fountain pen and you want to make beautiful calligraphy all right with it not you right so every time you you make a manual change it you could have learned probably learned a lesson if you try to find a way to articulate it to get the AI to make the change see what I'm saying yeah yeah I'm gonna do the same thing and I highlight as I read so I don't lo"
  },
  {
    "id": "report_source",
    "chunk": "on if you try to find a way to articulate it to get the AI to make the change see what I'm saying yeah yeah I'm gonna do the same thing and I highlight as I read so I don't lose my spot remember like my finger trick with you Oh yeah, we will need to know our public, so whatismyip . \r\n\r\ncom will need to be visited. No, because messing with 443 can be tricky because 443 is HTTP as traffic and there's really no, every router's rules are different the way it's programmed and like for example, I've been able to get one house to forward it correctly and not another house when I did it, every single device on the network no longer had internet access. because I sent all 443 to my computer. So what we might do, this might be even more fun, is you can do your own encryption, by the way. So we'll get there later, though. We'll solve that problem when we get there. \r\n\r\nDon't worry about that. So for now, leave it at 3 ,000 or whatever. So here's the plan, and then we'll get this finished. We're going to get this running. \r\n\r\nThe plan is we're done once we get this running. version of Python running your scripts because then you can Well, technically there's also getting it set up in the slack workspace, but we probably won't do that We'll just get a slack slot running and then the goal is to be but the goal is to be \r\n\r\niterate. \r\n\r\nThat's why I'm hesitating, so that you can iterate. \r\n\r\nOnce you've set up, and then once you have set up, then you're in iteration mode. You're actually working on your scripts. Hey, it doesn't do it this way. \r\n\r\nIt needs to do it this way. \r\n\r\nOr here's the error I get, blah, blah, blah. So we're trying to get to that state. The goal is if we can get to that state today, otherwise we'll get almost th"
  },
  {
    "id": "report_source",
    "chunk": "t this way. \r\n\r\nOr here's the error I get, blah, blah, blah. So we're trying to get to that state. The goal is if we can get to that state today, otherwise we'll get almost there. \r\n\r\nAnd then the other half of it is actually getting your bot, your app set up with Slack proper. \r\n\r\nSlack has to know about your bot a little bit, so you have to go set some things up in Slack, and then you have the two connected, and then you can start iterating on your bot. So hopefully, we'll get all of it done, but we'll see. My wife is getting a little hungry. Oh, check this out. \r\n\r\nIn my instructions at the bottom, it actually has the Slack instructions that I was just talking about. Does yours as well have, like go to api . \r\n\r\nslack . com? Cool. See how far, see how well that works and see if you're able. You may need to, This may be right where the instructions write down because what I'm reading is some very basic steps and you might need more detail than that. which will be where you ask for it maybe okay so but good it's already it's already got that notion in the instructions already i was thinking that'd be a problem or a missing piece yeah no problem go ahead and just create a new workspace that'll be your own personal workspace and yeah and then whenever yeah yes you should name it your jqr bot or whatever let's use 5000 okay \r\n\r\ncreate an app. So that I think you can just do from scratch. But that case, just just case in point, if I wasn't here, this would be exactly what you could do a cycle on, right? \r\n\r\nLike, what do I do here? \r\n\r\nAnd why? \r\n\r\nYeah, no problem. \r\n\r\nYeah, you can have both of your apps in the same slack in each program. It doesn't matter. But yeah, just as long as you can both administer, get the admini"
  },
  {
    "id": "report_source",
    "chunk": "h, no problem. \r\n\r\nYeah, you can have both of your apps in the same slack in each program. It doesn't matter. But yeah, just as long as you can both administer, get the administration access. And, and, and For example, if this were just me doing this my own project, all by myself, in my own time, as if I were just playing a video game, I would sit here, right where it says, I click new app, and it says name app and choose workspace, app name. Since the AI didn't give me an app name, I'm gonna come back to the AI and say, hey, give me an app name, because it gets codified, it gets in the artifact, it becomes part of my project, and it's not me just adding an app name, and then now the AI doesn't know what app name I added, see? So that's my game, I made it a game, We have perfect documentation. \r\n\r\nEvery time I see a piece that's going to be missing, I just make sure it's in my process. right? All right, so right here, so right there. So leave your screen right there because the first thing that should trigger in your mind is should be like IDs and values and shit that we need to capture. You see what I'm saying? App ID, client ID, client secret, signing secret, all this shit, verification token. \r\n\r\nWe're gonna need to make sure we have them done correctly. So yeah, when your buddy is also got his sort of ID, created in the Slack API, and he's looking at his basic information. We'll move forward. Yeah. Oh, so, okay, you want to do that? Do you want to do that with me? \r\n\r\nDo you want to do that? Well, so you can, I just named a JQR bot and we'll capture that. Since it's just one, we'll go ahead and capture it when we get there. How did you call yours? Cameron, how did you call yours? And then JQ, let's not overcomplicate"
  },
  {
    "id": "report_source",
    "chunk": "ture that. Since it's just one, we'll go ahead and capture it when we get there. How did you call yours? Cameron, how did you call yours? And then JQ, let's not overcomplicate it. \r\n\r\nThe only thing I want to be considerate of is being able to tell, so are you adding both of your bots to the same Slack workspace? Then just, let's just make sure that both of your bot names are distinguishable. That's the only thing, just, you know, name one of your bots. Yeah, there you go. Something, maybe like, yeah. anything that works. \r\n\r\nOkay. So, okay, good. Okay. So I'm getting ideas. Okay, cool. I'm getting ideas. \r\n\r\nI feel like I'm getting data loss here. Hold on. Test. Yeah, see, I am. I got my, my, I got a bug. My shit's bugged. \r\n\r\nAll I do is, um, when I create the new cycle and then I write, you know, anything in my cycle two, I'm putting information here. \r\n\r\nAnd then when I switch to another tab and then go back, I'm at cycle one again. So I'm going to, I'm going to pop this out. Yeah. That's the. That's the bug that I've been trying to fix. I'm going to right -click and move to a new window. \r\n\r\nAnd I'm going to put it on my other screen. Is that really the solution right now? Test. Yeah. Now I can tab around and not lose data. That's what I suggest you do. \r\n\r\nJust pop it out. But that's a problem I'm working on. I thought I'd narrow it down. I don't know why it's happening. But I'll have to fix it. That's on me. \r\n\r\nThat's where my project currently is at. Second mistake. You have the updated files? Yeah, okay. Yeah, yeah. Well, hold on. \r\n\r\nDid you add them yet? They're in your project? First of all, if you hadn't, they should be also in your AI studio still? And so, yeah, I'm working on it. So you have the files, so"
  },
  {
    "id": "report_source",
    "chunk": "you add them yet? They're in your project? First of all, if you hadn't, they should be also in your AI studio still? And so, yeah, I'm working on it. So you have the files, so it's okay. What we can do, so you can do this, all right? \r\n\r\nYou can do, watch my screen. Okay, so the prompt file, cycle, this is what I've been doing. As I've been trying to fix the problem, since it is a problem, I've just been adding my cycles manually, but I've been letting the tool do the flatten repo, and you'll get to see the difference here. So what that means is I've been manually writing my own cycles in my prompt, like this, so cycle two, and then cycle two. The part that I'm letting the AI do for me is the flatten context, so I just click flatten context. \r\n\r\nActually, let me do one more thing as well. take the prompt file out of the prompt md and put it in a file in the same directory to keep it safe from the script because my extension will modify prompt md but if i make a new file and then call it a manual prompt md and i copy my prompt file in there i can safely do the manual and not lose my data until i can get this shit fixed for you guys what is this what is this including um get not get stuff i'm picking up stuff my that could be a problem as well it's getting okay let me see if i can fix that with just a click of a button uncheck everything so i have nothing and then i just fix it my source, and flatten. Yep, that worked. Okay, so, okay. What's going on, let me show you. \r\n\r\nThere's a hidden . git file in here, in your jqrbot folder that you can't see. And if you open up this flattened repo file, it should be in the same directory as your prompt file. That is the file that's getting appended to the prompt file. See, it's a tw"
  },
  {
    "id": "report_source",
    "chunk": "nd if you open up this flattened repo file, it should be in the same directory as your prompt file. That is the file that's getting appended to the prompt file. See, it's a two -stage process. My script flattens your repo, and it manages your cycles. \r\n\r\nSo it does two things. And so the flattened repo works fine, but the managing of the cycles is a little wonky. So you're going to do your own cycle management manually, and you just literally saw me make a cycle two. You just write whatever you want to write in there instead of the little cycle box, okay? And then all you do, instead of clicking generate prompts, you click flatten context, you see? And then what you do, at that point you have this. \r\n\r\nSo do you have the git problem though? Have you opened your flattened context? Let me walk you through it as well. I'm trying to get my Discord to see your screen. \r\n\r\nThere we go. \r\n\r\nCan't find the right button to find your screen. \r\n\r\nThere we go. \r\n\r\nOkay, okay. So do you see, okay. Yeah, I can see . git. So do you see the top 10 list there? Do \r\n\r\nDo you see the git files? \r\n\r\nDo you see that? \r\n\r\nWhat is that, git shit up there? All that nastiness? \r\n\r\nOkay, see, so it's a little bug. \r\n\r\nI'll fix that. That'll just be a cycle. I need to tell the AI that, hey, you're picking up the . git files and you shouldn't be. \r\n\r\nSo all you need to do to fix that is check, find the root directory in your, over on the left, see all the check boxes? \r\n\r\nUncheck them all. So basically uncheck the parent. There you go. Now just check the source folder. There you go. That should have solved it. \r\n\r\nYou see what I mean? Because your get is in there and now it's not selected. Now click down at the bottom, flatten. Oh, great. You have "
  },
  {
    "id": "report_source",
    "chunk": "ou go. That should have solved it. \r\n\r\nYou see what I mean? Because your get is in there and now it's not selected. Now click down at the bottom, flatten. Oh, great. You have that. Click it. \r\n\r\nTurn that on. That's good. Good catch. Now, I know it looks the same because you can't see it. Go ahead and click flatten. Oh, they're still there. \r\n\r\nOkay. What you don't want, I guess, is that check on the top one. You see that? Okay. So let's do it. Yeah. \r\n\r\nSo do it this way. Uncheck it again. I know how to do it. Do the sort. No, don't do that. Don't do that. \r\n\r\nDon't do that. Do the sort. Source, and then just do the Prisma schema file itself, not the folder. Damn it, okay. Add a new, right, so stupid what we're gonna do. Add a new folder and just make it empty and just fucking don't check it, you see what I'm saying? \r\n\r\nIn that folder. That's fine, you'll get one eventually. Yeah, and you'll get it. It'll have to make it when it's time. Yeah, that's easy too, do that. Just name a test. \r\n\r\nJust make a new folder called test in the same directory as source. And then don't select this one. \r\n\r\nRefresh. \r\n\r\nMake sure it's there. Yeah. Try again. Your selection. Select Prisma and folder and select the source folder. Damn it. \r\n\r\nWhat is it? All right. I don't know why it's doing that. What folder? What's your top folder name up there? And then isn't your... \r\n\r\nIt is. All right. Just click generate and see what happens. Yeah. Thank you. Yes. \r\n\r\nDude, me either. So I will take these as action items to clean that shit up. That's so frustrating. And then let me write it down first. The flattened context is picking up the get. Can you copy your top 10 list and just send it to this so I can get it? \r\n\r\nYeah, it's the git direc"
  },
  {
    "id": "report_source",
    "chunk": "n let me write it down first. The flattened context is picking up the get. Can you copy your top 10 list and just send it to this so I can get it? \r\n\r\nYeah, it's the git directory, is what I should refer to it as. . git directory, and it shouldn't. Okay, so I'm gonna fix this one here, and because I believe this one I can fix quickly, but the other one I cannot fix quickly. If I can fix this one quickly, then I can just give you an updated extension and life is good. And then you can just do the manual process, which is what you're about to witness me do. \r\n\r\nSo I can do two birds with one stone. So I've already created the manual prompt markdown file which is just a file that I copied the prompt file in so that My script won't change it on me if I'm clicking buttons and shit because it's a disconnected file in here I saw that it had that stupid shit just wasted stuff down here So I just flattened and it got out for me because of my chip because my thing is a dash I don't know maybe it's it's weird mine is a dash. That's the root problem because it does it all for you and then it's it picks up all into the jqr bot so i need to copy this i need to remake that prompt file the way i'll do that is i'll flatten i see that i don't have my git directory in here so this is where you need to pay attention because this is the different part so i'm going to copy that's not what's different in my manual prompt i need to find where the flatten repo starts. So like at the bottom of my cycles, this is the manual thing, I have to manually input the flattened repo. So going to the bottom of the cycles, it's M6, so I'm just gonna do a control slash and M6, there it is right there. \r\n\r\nBracket slash M6 takes me right to the bottom. And rig"
  },
  {
    "id": "report_source",
    "chunk": "So going to the bottom of the cycles, it's M6, so I'm just gonna do a control slash and M6, there it is right there. \r\n\r\nBracket slash M6 takes me right to the bottom. And right here I have M7 flattened repo. I'm gonna double check over here, see it starts with the green, Bracket bang dash dash copy all that green bracket dash dash. So it's right under the app In the tag the way I'm gonna make this easy for myself from here on out is I'm gonna add ASDF right here so that all I got to do is control F ASDF and I'll go right here ASDF and I'm ready to Select everything also since it's the the last file on the list I can just select everything below it and I'll show you how to do that and control F type ASDF It gets me right here. I select everything below M7 and I hold ctrl shift and press the end button to do it. And that selects everything to the very bottom and I press delete because there is no bottom tag. \r\n\r\nThe bottom tag is missing. So before I paste I'm actually just I'm gonna have to use my I'm gonna have to use my clipboard. So I'm gonna copy this because I need my closing tag. It's missing. Put that there and then that's that. So my flattened repo is in here. \r\n\r\nGo back to my flattened repo. Actually copy it. \r\n\r\nActually, no, I'm missing another closing tag. \r\n\r\nLook at this, I'm missing another closing tag. I just remembered. I'm gonna go to the very top. I have the prompt MD itself. So I'm gonna get that, close that. This is all done manually, but the git fucked up all my shit, or else this would be clean. \r\n\r\nOkay, and then copy this in manually. \r\n\r\nSo that's the manual. So I got myself ready. This is what the bottom of yours should look like. Very simple before I post in the entire repo. See, so it's the "
  },
  {
    "id": "report_source",
    "chunk": "n manually. \r\n\r\nSo that's the manual. So I got myself ready. This is what the bottom of yours should look like. Very simple before I post in the entire repo. See, so it's the end of cycle zero. It's the end of the cycles. \r\n\r\nmain artifact m6 is just main artifact 6 and then I put my little asdf tag so I can just jump to this location quickly and then the start of the m7 flattened repo the end of the m7 flattened repo the end of the prompt now I just rinse and repeat every time I update flatten instead of clicking generate prompt I click flatten context there we go I just pasted it there so that I'm sorry that's good you're gonna you you're gonna the flattened context works great that part is a hell of a nightmare for my extension to do for you. I'm working on the cycle stuff. Once that works, it'll be even nicer. So now that I've done that, I've pasted in my current project. So it's good. All I have to do is write my cycle. \r\n\r\nYou see? \r\n\r\nAll I have to do is write my cycle. \r\n\r\nSo I'm just gonna go cycle two. \r\n\r\nOh, also my cycle overview, which is just what the title is. So whatever you write in your title is what gets put here. So you just put your, you know, it doesn't really matter. matter what the title is. It could just be new cycle literally. What matters mostly is that it says current cycle 2. \r\n\r\nNot that it's the end of the world, 80 % of the time the AI will detect cycle 2 is the current cycle, but sometimes if you don't update this, it'll still try to solve your cycle 1, even though you do have a cycle 2 down here. And then again, the only, not again, I've never said this, the only reason why this is even here is because over time, as I discovered, as my problems got larger and larger, The AI would lose w"
  },
  {
    "id": "report_source",
    "chunk": "e only, not again, I've never said this, the only reason why this is even here is because over time, as I discovered, as my problems got larger and larger, The AI would lose what cycle it was supposed to be on. And I found that the solution was to just have this sort of what's the current cycle at the top. And then it never got confused anymore. Solved the problem. But that's what I have to do. \r\n\r\nSo I've made a current cycle 2. And what was the problem? What was I going to do? Continue in project setup, which is setup development. Dev environment setup. Something broad. \r\n\r\nThat's what we're doing. That's what we're focused on. And now I can go to cycle 2. Cycle 2. See? So a lot of, I tried to go slow and show you everything and talk everything, but that's, it's really straightforward. \r\n\r\nYou just update the cycle at the top. There's three things, a cycle overview at the top, the cycle itself, and then updating the flattened repo. \r\n\r\nNo, we're doing this now because the parallel copilot will lose your cycles. \r\n\r\nYou, you, you don't seem affected. That's why I see this as the fucking problem. So click the back arrow right there, yep. Now click the forward arrow. Now click, type something in down there. Click, type something right there. \r\n\r\nNow click out of there just to make sure, like, just click out of that. Yep. Now don't, don't, nope. Now go change your tab to, like, your flattened recon. Yeah. And now change back. \r\n\r\nSee, you're not affected. If I do that, I lose my cycle. \r\n\r\nYou see what this is the problem. \r\n\r\nThis is, I can't, I'm trying to figure it out. Okay. I'm trying to, it works sometimes. It works when I'm looking, you know, so I'm working on it. It's just, this is part of the, part for the course."
  },
  {
    "id": "report_source",
    "chunk": "ying to figure it out. Okay. I'm trying to, it works sometimes. It works when I'm looking, you know, so I'm working on it. It's just, this is part of the, part for the course. Um, yep, this is the development process and this is more on that. \r\n\r\nSo, but yeah, so you're not affected, but if you do get affected, you have a, you have a, you're not, you're not up Schitt's Creek. I just showed you a, uh, band -aid. I understand. Always. I understand completely. And, um, you're not affected by it. \r\n\r\nUm, but, um, if you're at this point, you know, I think I'm going to call it and let, let, you know, take care of my, my wife. but You're not affected your friend is The only thing you really need to do is everything I've sort of already articulated But if you don't want to go forward without me, you don't have to we can just do another tomorrow afternoon After work, you know, whatever time works for you. We can just sort of pick off I will be trying to fix the problems that I have encountered so that you know Maybe we don't even have an issue by the time we get started next time right because I've you know I have all this evening to try to fix these two problems. Maybe I'll fix one. We'll see. Your next step will be to, you got, no, you got the reverse proxy. \r\n\r\nYou got the reverse proxy for forwarding. You got the Slack bot set up, just the initial, but there's still going to be more that you need to do in the Slack API. You're going to need to, and then the AI, so like, you're just going to need to ask, what more do I need to do? Ah, let me say it this way. Let me take back everything I said, because this is the rinse and repeat answer. You explain to the AI what you've done since. \r\n\r\nthe cycle started, so you've set up, ok"
  },
  {
    "id": "report_source",
    "chunk": "way. Let me take back everything I said, because this is the rinse and repeat answer. You explain to the AI what you've done since. \r\n\r\nthe cycle started, so you've set up, okay, I've gone through artifact four or whatever, I've set up the reverse proxy stuff, I didn't have any issues, I got it all done. Next thing I did, oh, and mention your public IP, give me your public IP in that section. So say, put a number one and then put a dot, and then say, I got the port forwarding in my Verizon router done, and then number two, or no, wasn't that on the same instruction? I believe so, let's leave it at number one. I believe that was in the same artifact, so let's not break that up. And then say I also created the Slack app at api . \r\n\r\nslack . com. So it's like a personal journal, except it actually means something. Essentially, yeah. But the only way that what is the next step really works well is if you truly capture your current state. Because it genuinely, it will. \r\n\r\nIt will give you more steps. Yeah. But it genuinely doesn't know what you have or haven't done. So that's where you need to be specific. And so that's why it helps to take a screenshot of that where I said that's where we're gonna stop because that's a perfect spot. You don't necessarily have to show the secrets. \r\n\r\nThis is part of the fun time. Exercise, how are you going to handle your environment variables? Are you going to send them in AI Studio? It's not the end of the world, but you don't necessarily wanna do that, do you? come up with a solution where you can communicate with the AI, the environment file, without giving it your environment variables, the way I did that. \r\n\r\nIn fact, I have a artifact. \r\n\r\nIf you want to do a little homework, let me "
  },
  {
    "id": "report_source",
    "chunk": "he AI, the environment file, without giving it your environment variables, the way I did that. \r\n\r\nIn fact, I have a artifact. \r\n\r\nIf you want to do a little homework, let me find the actual artifact and point you at it. I have a template artifact, T11, let me just read through these, oh, find it, env . local, env . local, yes. So, yes. So, review, T16, it exists in your prompt file. \r\n\r\nIt's part of the, yeah, go to your main prompt. Do a control F for T16 dot. Actually, no, even better. I'll get you even closer. Do a search for dot env dot local. So, \r\n\r\nlocal. So, I actually concocted my own solution. See how it says step list part two with DAX secret values? So, what the AI needs to know are the keys, not the values of your environment variables. Does that make sense? So, if you manually create an environment local file, and then it's presuming you've already got your environments variable set up with your actual environment variables, and then you copy your environment into your local, and then you just actually replace every value with the word redacted. \r\n\r\nAnd then you uncheck your environment, and you check your environment local. Then you're sending the local to the AI in your AI Studio, because that's what the flattened context is going to pick up. It will not pick up your environment. And it's just on you to keep those two in sync. And then bada bing, bada boom, AI Studio will know every single key in your environment and will not know any of the passwords. And so when you're actually programming and it needs the value, it won't just put in a placeholder, it'll make you an actual script that you don't have to edit. \r\n\r\nYeah, so just, there's your, yeah, yeah, that'll be part of, so yeah, you just go through, "
  },
  {
    "id": "report_source",
    "chunk": " in a placeholder, it'll make you an actual script that you don't have to edit. \r\n\r\nYeah, so just, there's your, yeah, yeah, that'll be part of, so yeah, you just go through, what do I do next? And you see, even that, this is in there, so I'm just preempting this in case the AI doesn't surface this to you, but because it's in here, it's mentioned, it's fine -tuned, It may very well suggest this for you, it may very well create the environment local for you, and then just expect you and say in the summary or whatever, the curator needs to do something. It may very well do that. \r\n\r\nAny other questions? \r\n\r\nTake a screenshot also. Go back to where you were writing your cycle. Read it out just because I can't read it. I'll take a screenshot of its initial configurations so that we can make an artifact and capture it. So that we can create an artifact and capture it. I'm just reviewing to see if there's any other important pages that we should take a screenshot. \r\n\r\nBecause it knows what a basic setup is, but there are some unique variables, like your IDs and shit. It's been a minute since I've been in this admin panel, and they change shit all the time. See, like, this is new. \r\n\r\nLike, one -click access to your app's agent or assistant. \r\n\r\nI get it. I get it. I get it. Okay, okay, okay. So, okay, I know what this is and how you put this in. like, okay, so it's basically, if you turn this on, I imagine what it's gonna do is it's going to add like a little Gemini button, right? \r\n\r\nLike in VS Code, a little button, an AI copilot button. And then what you can do is you can program that button to route to your AI agent. You don't have to do this at all. You don't have to turn this on because we can program, we are gonna use s"
  },
  {
    "id": "report_source",
    "chunk": "ou can do is you can program that button to route to your AI agent. You don't have to do this at all. You don't have to turn this on because we can program, we are gonna use slash commands. We're gonna do, We're going to do our trigger as a mention. But you can, you can. \r\n\r\nIf you want, however multiple ways this is going to count, we're going to get it done soon. We're getting there. Not yet, but we're getting there. Did you see what I mean? That's all that is. They're just trying to make it easier for people, but we don't need their easy mode. \r\n\r\nWe can literally code our own shit. \r\n\r\nBut that is what they would do, and if you want to, you can. \r\n\r\nYou would just take a screenshot of it and say, hey, how do I do this? \r\n\r\nJust like I did. This is probably going to be where you're going to spend some time. This incoming webhooks, because that's how it works, a simple way to post messages from external sources into Slack. That's what you're going to be doing. So, I'm just going to turn that on, yeah. So, go ahead and take a screenshot of that. \r\n\r\nSo, before we do that, so in your thing, right, give me a brief. \r\n\r\nYeah, so watch how we're going to do this. Go ahead and turn that on. We're going to take a screenshot and put in whatever you want to do first. We've just got to do two things. We've got to write in your cycles, and we've got to take a screenshot of this. I mean, we don't need to, but it's what we're going to do. \r\n\r\nIt's nice to show the AI, I just think, because I just feel like it's a good thing. No, you can delete those. Yeah, no, they're gone. They're done. I had to figure all this shit out by myself, dude. I had never done any of this lack of administration. \r\n\r\nLike, that's a whole fucking job title"
  },
  {
    "id": "report_source",
    "chunk": " they're gone. They're done. I had to figure all this shit out by myself, dude. I had never done any of this lack of administration. \r\n\r\nLike, that's a whole fucking job title, like being a Slack administrator, right? Get those four in there, and now let's go to the cycles, and we're just gonna mention this. We're just gonna like say, we're gonna say, this is what I mean, like we're gonna say it's certain, because there could be another way to do the thing. I don't wanna pigeonhole the AI, but I do wanna suggest. So, we're gonna say, are we going to post messages via webhooks? I've activated and configured \r\n\r\nwebhooks, open parenthesis, cc, screenshot, close parenthesis. And then slash commands, but this will be mindless. There's no confusion there. But the webhooks is a bit of a, you can trip up there. So by mentioning it and asking a question on it, the AI will be primed to give you some instructions. Also this probably. \r\n\r\nOh, no, no, no. Maybe, yeah, maybe this will be, because you see right here? You will need to configure redirect URLs in order to automatically generate the add Slack button or to distribute your app. You may or may not need to do this depending on how basic the setup is and the AI will help you out. So just, we can just know what that does. You may not need to, but this is where you will probably scopes. \r\n\r\nYou'll have to do some scoping. So just add, go back to your Recycle and just ask are there any scopes I need to add, question mark. or scopes I need to add, and then or event subscriptions. These are like a mention. A mention is an event. That's what they call a mention, a direct message. \r\n\r\nThose are all events, so. I think that's it. Those are the only pieces of the puzzle. And then I bel"
  },
  {
    "id": "report_source",
    "chunk": "ention is an event. That's what they call a mention, a direct message. \r\n\r\nThose are all events, so. I think that's it. Those are the only pieces of the puzzle. And then I believe in your script. That's what I was just about to say. So since I was looking at it, since the only value that's unique is the app name, if you just want to pass in your prompt, I have named the app tilde jqrbot tilde just so it knows, or you can take a screenshot. \r\n\r\nEither way, that is the key value that needs to be extracted. And then now let's go back to your Documentation and read more on that because so there's three things that we've done too. Number one was the reverse proxy. Number two is the slack app at slack at api . slack . com. \r\n\r\nNumber three is your Python environment. So we also want to make progress on that as well. So get that document up and see what all you can get done through there to get to the point to where you would almost be ready to run your Python script had you had one, right? So let's read what it says. Like you might say, it's going to be like you set up your development environment, development environment setup guide. \r\n\r\nYep. \r\n\r\nNo, no, we just got through the two artifacts. Remember, that's what actually what we were reading, wasn't it? Remember that told us that would, yeah. Remember the prerequisites? Yep. That's what was at the top of the development and testing guide. \r\n\r\nSo we've basically gotten through the two prerequisites. You've asked some questions on one of them, right? \r\n\r\nAnd then we're going to continue to see what we can do in step three or whatever your step, if it says step three for you. \r\n\r\nIt says create a virtual environment. \r\n\r\nIt's highly recommended to use Python virtual blah, blah"
  },
  {
    "id": "report_source",
    "chunk": "n do in step three or whatever your step, if it says step three for you. \r\n\r\nIt says create a virtual environment. \r\n\r\nIt's highly recommended to use Python virtual blah, blah, blah. \r\n\r\nSo let's just try, I'm going to try that. I'm going to write exactly what it says. So for me, it says Python dash M V E N D V E N D. Actually, hold up. No, wait a minute. This is a bash script. Hold on. \r\n\r\nSo yeah, yeah, go ahead. \r\n\r\nI will open bash. \r\n\r\nIt looks like it's working. I'm getting files created. Yep. \r\n\r\nGo ahead and remove whatever the fuck it adds. \r\n\r\nYeah. Stupid thing. Oh, hold on. That might be an easier way to do it. Just de -select everything. Oh, maybe your shit will work now. \r\n\r\nDe -select everything at the top. \r\n\r\nDon't worry about that. Use your thing at the top. \r\n\r\nJust that one. \r\n\r\nNo, de -select the parent. Oh, that might be easy. \r\n\r\nI didn't fix it. \r\n\r\nThis is annoying. I've never done a Python with my tool. So it hasn't been cleaned manually, automatically. So, no, no, I think it's bugged a little bit. All you've got to do is check on my screen. You can click in here and do Control -A and select everything. \r\n\r\nI think it's just bugged. \r\n\r\nYeah, hit Control -A, and then now click it to remove selected at the top of that window. \r\n\r\nYep. Now, yeah. Now, when you, They were stuck. Even though you had to deselect them, it didn't really deselect them. \r\n\r\nNow just select sort. \r\n\r\nThat might have been the problem as well, but that might have been your other problem as well. Yep, now you're clean. Just select sort for now. Your source folder. No, no, no, no, no, it didn't. A dash is not the same as a check. \r\n\r\nNow do your Prisma. That might trigger it. \r\n\r\nGood, great, good, good, good, good. \r\n\r\nClick"
  },
  {
    "id": "report_source",
    "chunk": "rce folder. No, no, no, no, no, it didn't. A dash is not the same as a check. \r\n\r\nNow do your Prisma. That might trigger it. \r\n\r\nGood, great, good, good, good, good. \r\n\r\nClick flatten. \r\n\r\nI think you're good, right? I think your problem's fixed now. So you're in Gucci now, all your problems. See, that was it, see? It's fixed. It would have been fixed the first time if I would have been smart enough to remember to tell you to remove them from your selected before at the bottom. \r\n\r\nThat was the trick. That's why your Git was still showing up, in my opinion. Okay, so minimize that. There's a button, it's collapse folders in view? Yes, get that shit out of there. Okay, so now we got, oh, there's more steps to do. \r\n\r\nSo going back to our dev guide environment, we're gonna run the next one. \r\n\r\nActivate the virtual environment on Windows. \r\n\r\nOkay, so I am getting an error. on the second part of step one on activate the virtual environment. I'm on a Windows and I ran the vmd scripts activate and it says bash vmd scripts activate command not found so that's going to be my next cycle. One thing only and just that. Do you get an error? I think you're on what kind of computer are you on? \r\n\r\nYou're not on Windows are you? Oh, okay. Yeah, I have Bash 2, but it's giving me grief. So I'll just say that. All right, so I gotta catch up to you. You've been writing more than I have. \r\n\r\nSo, I got the reverse proxy set up. What file? Oh, you're actually looking in there, looking around like an actual developer? See, I can't, I... Yeah. Okay, okay. \r\n\r\nOh, I see. So like that, basically, just change my... This is the first time I've used my extension for a Python project. and the bloat is real, got a flattened context bloat. Oh, that ma"
  },
  {
    "id": "report_source",
    "chunk": "o like that, basically, just change my... This is the first time I've used my extension for a Python project. and the bloat is real, got a flattened context bloat. Oh, that makes sense, actually, maybe. OK, it worked this time for me. \r\n\r\nYeah, you were right, actually. I just didn't pay attention. It worked. It worked. So it's creating a, you don't necessarily want to just install your Python libraries on your laptop itself, because there can be many different projects that you're using or many different programs that require different libraries. \r\n\r\nAnd so to keep things simple, you're creating a virtual environment that then that gets those Python libraries. \r\n\r\nYeah. So the requirement should be something that is just in your source root folder. It should appear. And the next thing is, well, Python is a choice. We could maybe also do JavaScript, you know what I mean? \r\n\r\nWe don't need to stop just because it's adding a lot of files. \r\n\r\nOkay, just hang on until I get it working myself. Oh, it's backwards again. Well, hold on. Oh, okay. Looks like, yeah, something's wrong. Yep, okay, I'm getting the same error you're getting. \r\n\r\nSo you're getting a cannot open requirements, no such file directory? Yep. So you and I both are going to just use this as a cycle. So just copy that. Make sure you're copying the command you're sending and then the response you're getting. And then that can go in your next cycle's ephemeral context. \r\n\r\nThe ephemeral context is for troubleshooting logs, exactly this stuff. Stuff that's only relevant for the current cycle. That way you still record the logs that you've got, they still get saved, but moving forward, you won't always keep these logs in your context when you're actually on cycle"
  },
  {
    "id": "report_source",
    "chunk": "e. That way you still record the logs that you've got, they still get saved, but moving forward, you won't always keep these logs in your context when you're actually on cycle 50. Yep. Well, you could even go, no, you could even go straight to, I'm trying to run this command, and then put the actual command in tildes, and then it knows, it'll know what step you're on, see? But I would still say, I'm on, I'm in, just to be clear, just so it's not having to go track things down, I would still say, in ArtifactX, on step two, I am trying to run this command, paste, and I am getting this error, paste. \r\n\r\nPretty much, pretty much. Yep, fix, fix. What do? Ah, I'm gonna have to search. Yeah, no, don't say it. Because it does say no such file directory, and it knows where Python would be looking for it. \r\n\r\nYeah, so my cycle three overview, I just said no requirements . txt. In my cycle three, if I can find it, I just wrote, in A7, I'm trying to run this command, pip install -r requirements . txt, but I'm getting the following, and then open bracket error, the error, close bracket error. That's my cycle three. And then close cycle three. \r\n\r\nWhat for? Yeah, that's correct. Well, hold on. You've already got some stuff written. So then that's what you're going to do. So then at the bottom, because you already talked about one and two, now you're going to talk about three. \r\n\r\nSo go up to the bottom of your cycle and say, and then for getting my development environment set up, blah, blah, blah. Yeah. Oh, and did you put, hold on, did you put the error right there? Is that why I see it right there? Is that the error at the bottom? You want to put that error message in your ephemeral context section, right below it. \r\n\r\nYou see in th"
  },
  {
    "id": "report_source",
    "chunk": "ere? Is that why I see it right there? Is that the error at the bottom? You want to put that error message in your ephemeral context section, right below it. \r\n\r\nYou see in the text below? No, it's already formatted. \r\n\r\nIt's already sort of, it'll be wrapped in ephemeral context tags. \r\n\r\nCorrect. Nope, you don't have to do anything other than just put the error. It's just a place to put your errors. And the reason why, again, is because You only need this, that text for this cycle. Once you solve this problem, you never need that text again. But for auditing purposes, having the log there for future reference when you go back, my app needs to do that. \r\n\r\nSo it needs to be this way. The error is in an open parenthesis C ephemeral context. You see how now you can make the association? To put a bow on it, you can just put in parenthesis C error in ephemeral context. Now that seems like a solid prompt. Let's highlight it, yeah? \r\n\r\nSo, slow down. The parsing and unparsing is only for the responses. It doesn't matter what goes in the cycles above them. Yep, pretty much. And now to double check, if you would like, you can check in your prompt. You should see your cycle three. \r\n\r\nKnowing the prompt . ind. And then control F, cycle three. Close it, close that thing out. And you're on cycle three in your co -pilot panel, right? I can't, it looks like a three. \r\n\r\nOh, nevermind, I'm just saying it looks like a three. And yeah, yeah, your cycle two is correct, right? Oh, let's add one more thing. \r\n\r\nI'm thinking along the lines of priming the AI to update our artifacts based off this. \r\n\r\nNah, fuck it, you should be just fine. Go ahead and send that off. Yeah. But do you see how you can throw like so many things at it like tha"
  },
  {
    "id": "report_source",
    "chunk": " our artifacts based off this. \r\n\r\nNah, fuck it, you should be just fine. Go ahead and send that off. Yeah. But do you see how you can throw like so many things at it like that? Yeah, it used to be you could you literally could only solve one tiny ass little fucking problem at a time and you had to really break down your problems and like really hyper -focus on just that one problem. \r\n\r\nAnd if you tried to get a second problem, it would like lose its shit. Now you just fucking throw and fucking see what sticks, like the whole kitchen sink, you know? Great. That's exactly what I was expecting, honestly. \r\n\r\nCause it creates a package JSON for you and that's how it works. \r\n\r\nSo I guess it's just not primed to create it for you. You had to ask because I've never done a Python to prime it to, yeah, no big deal. But you're seeing the tediousness that happens. That's the tediousness that is real when you're building something scratch fresh new. You have to go to it. That's what I've learned. \r\n\r\nLiterally, the AI won't do it unless you fucking write it. And so it can be tedious to find the thing that you need to write. And then that's kind of part of the part of the, you know, how the sausage is made. Once you figure out in your mind what it is that you need, you get it to make an artifact, ask for that like competitive. And it's knowing a thing is a thing like competitive analysis. \r\n\r\nknowing competitive analysis is a thing, and then thinking about asking the AI to do competitive analysis on other AI Slack bots. \r\n\r\nSee how it can bring the thing to right to you if you know what to ask for? Yeah. So not yet. What you've done is you have generated the prompt and you've copied it and have you sent it to the AI? \r\n\r\nSent it t"
  },
  {
    "id": "report_source",
    "chunk": "ight to you if you know what to ask for? Yeah. So not yet. What you've done is you have generated the prompt and you've copied it and have you sent it to the AI? \r\n\r\nSent it to AI Studio? \r\n\r\nNow you've got the responses back? \r\n\r\nYes, you're ready to click new cycle. Yep, ready for new. responses yeah see what mine said I think I said my mouth yet actually yes yes that's right that that's right yep cool yeah just it's just aligning your documentation so that that shit is in there see what I'm saying yeah dude it is so much fun dude I'm glad you like it dude it's so much fun this should be what everyone is doing there should be nothing else in anyone's life but this my god dude Yeah, you're a bit ahead of me, so I have to catch up to you. You're good, you're good. And again, I need to cut, but we need to cut it, so we're gonna cut it in 12 minutes. \r\n\r\nLet's just end at the hour, okay? Yep, that's fine, dude. So look, look, what's gonna happen? Yes, it's fine, it's fine, because guess what? \r\n\r\nThis is going to grow over time, and this is the equivalent of a package . \r\n\r\njson file. Do you know what that is? \r\n\r\nSo a package . json file, Do you know what libraries are? So, like adding, yeah, importing a Python library, right? So, a library is someone else's code that they've written for you, essentially, that you can then just use their library so you don't have to write all, everything to talk to Slack, all right? So, this is the list of the libraries in use by your project. In other words, the requirements. \r\n\r\nAnd so, once you have the requirements text file and it's selected in your context, then the AI will know what, Packages are installed in your project and then when you say hey I need to do the PDF and then the "
  },
  {
    "id": "report_source",
    "chunk": "ts text file and it's selected in your context, then the AI will know what, Packages are installed in your project and then when you say hey I need to do the PDF and then the project Requirement doesn't have pipe high PDF which is one of the required libraries. Then it'll add it to the requirements. It'll tell you to run your pip install, whatever, and then you'll add that new library to your environment, and you'll continue. That's the development cycle. Yes, yours will look a little different than his. \r\n\r\nThat's okay. There's gonna be a little variance there, because your project is different. It's a different project, right? Yeah. Like two twins. Think of it like this. \r\n\r\nTwo twins. When they're born, they're identical, but they have different life experiences, and so they actually are different. You know what I'm saying? Yeah. And the only reason it didn't create a 40 audit in the first time is because I didn't have a Python -specific mention. That's all. \r\n\r\nI've just been doing JavaScript, which is packaged. There's a package . json file that manages that. God, that's so annoying, dude. That's so annoying. Eventually, I'll fix that. \r\n\r\nYep, yep, yep. Oh, here's another thing. So there's a couple ways. You can first even turn off the auto add at the top. Do you see the two double, the blue two check boxes? That's turned on by default. \r\n\r\nYeah, whenever you're doing this PIP shit, you can turn that off and then this won't happen. And then whenever you're not doing this PIP install shit, you're just doing normal development and you're creating new files on the fly, you may want to turn it back on. Until I have my extension more intelligent and filters this shit out for you, okay? You're never selecting your librar"
  },
  {
    "id": "report_source",
    "chunk": "g new files on the fly, you may want to turn it back on. Until I have my extension more intelligent and filters this shit out for you, okay? You're never selecting your library. You're never selecting the library. You're selecting your source. \r\n\r\nSelect the source folder. Yeah, and you don't select your library. See, it knows what libraries you have from the requirements. It doesn't need to see the library. Make sure your requirements is checked. Yeah, that's important. \r\n\r\nAnd then, no, no, bad. Yeah, you don't. I blocked that. Bad, bad job. Don't, don't, don't, don't flatten the prompt. Stop it. \r\n\r\nThen your shit will grow so fast. Okay, we're at the hour, we're done. Okay, yeah, you're starting to get the swing. You're starting to see the problems that you have to overcome. But they're largely sort of the same, sort of the same problems. So it's kind of rinse and repeat. \r\n\r\nSeriously, it's fun. It's like playing with Legos. You just got to find the right piece. You got to find the right prompt. You got to remember the right buttons. No, we're done. \r\n\r\nNo, I'm going to fix all the problems. Yeah. Is there anything else? Any other questions? I think we can pretty much. Yeah. \r\n\r\nCool. Cool. So, uh, I think we're doing what do you guys think you're doing? Right? I we don't we didn't see anything run yet, but we're getting progress. Yeah. \r\n\r\nOkay. See you guys. I'll see y 'all tomorrow. Just a message. I'll be done at work at 4 p . m. \r\n\r\nCentral. So 5 p . m. Your time anytime after that. I'm good. Just whatever whatever works for your schedules. \r\n\r\nOkay. All right. See you guys.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-4.md\">\nTranscribed "
  },
  {
    "id": "report_source",
    "chunk": ". \r\n\r\nOkay. All right. See you guys.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-4.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nOkay, so Ben, I took your advice to automate my process. And so what I've done is basically I think we could all start using this because it's just VS Code. The way it works is it's basically two panels, this left file view, which is basically my clone of the Explorer view, but it's made for AI. So you can see it's just the same thing, except I get a breakdown. of the files and the token count of the whole project. So at a glance, you can see this project is a million tokens. \r\n\r\nThat's very valuable information. And I can see my source directory is 157 ,000. That tells me what AIs I can and cannot work with, because some AIs have limiting factors, such as context window. Additionally, what you do is you select any files you want. So it's a data curation tool. You can just drop in PDFs, Excel documents. \r\n\r\nYou just select it. And then they show up down here with an estimated token count of that file. So that what you can do is you can create these packages of data, curate your data, and then it just essentially, so this is the file. part of the equation is it being able to flatten this context that you select and then once it does that it just creates a file that looks just like this which is just like the file I was manually creating and managing when I would work on my projects this is like a script that I ended up making so remember you asked if I saw I automate I was like some things are automated some things are still manual and This is one of the things that was sort of automated. I would just run a script and it would crea"
  },
  {
    "id": "report_source",
    "chunk": "utomate I was like some things are automated some things are still manual and This is one of the things that was sort of automated. I would just run a script and it would create this file, but I would have to manually add to some sort of files list somehow. Now it's just a click of a button. \r\n\r\nIt automatically adds any new files. If you just drag and drop a file in here, it just automatically selects it. If you move files around, it automatically updates it the next time you click flatten context. So that's what that creates. That's this one half. Then the second half of the extension is the managing all the cycles. \r\n\r\nSo actually, and the artifacts. So this extension will be creating artifacts for you as you go. So as you're planning out a project, it'll create an artifact because I fine -tuned it to do so. In the message, fine -tuning is just in the prompt. So it'll respond back with a setup guide, what have you. And then as you're actually setting it up, and if you're having errors in setting it up, it actually comes in and then updates with the specific issue that you are having, with the specific knowledge gap that you had. \r\n\r\nin here so that then you can, you know, actually now I just come back in here and reread my documentation and it's very transferable. Every problem I encounter just gets a document artifact and then we're good to go. I have my own range on getting ahead of myself. So that's kind of the analysis cycles. So you basically, how does it work? \r\n\r\nI can start a new project from scratch just to show the flow. I'm trying to just do a new folder. PowerDefense99, just to get a VS Code in here so it's nice, fresh, new. \r\n\r\nOops. \r\n\r\nSo I've got that directory opened. I go to my extension. Since it's "
  },
  {
    "id": "report_source",
    "chunk": "ust do a new folder. PowerDefense99, just to get a VS Code in here so it's nice, fresh, new. \r\n\r\nOops. \r\n\r\nSo I've got that directory opened. I go to my extension. Since it's the first time I'm opening it up in here, it opens up this panel. \r\n\r\nGet out of my way. \r\n\r\nWelcome to Data Curation Environment. Get started. Describe the goals and scope of your new project. \r\n\r\nBlah, blah, blah, blah. \r\n\r\nAs much as you give it, as much time as you spend planning, the more you're going to get back. I'm just going to say, I want to make an intelligence game. Then I'm going to copy that string just so I can find it in a second. Then I'm going to click Generate the Initial Artifacts. And so what did that do? That created the README and the prompt. \r\n\r\nAnd the README is Welcome to the Environment. This artifacts is the heart of your planning and documentation. It's managed by the DCE, a VS Code extension designed to streamline This readme was automatically generated to provide context for you, the developer, and for the AI assistants you'll be working with. Context of this workflow and artifact is a formal written document that serves as a source of truth for a specific part of your project. Think of these files as the official blueprints, plans, and records. The core principle of the DC workflow is documentation first. \r\n\r\nBefore writing code, you and your AI, and it doesn't have to just be code, you can make a book with this. This is a content delivery solution. You should first create or update an artifact that describes the plan. Iterative cycle workflow development in the DCE is organized into cycles. You have just completed the initial setup. Your next steps, initialize your Git repository. \r\n\r\nI've got a button. \r\n\r\nI'll clic"
  },
  {
    "id": "report_source",
    "chunk": "velopment in the DCE is organized into cycles. You have just completed the initial setup. Your next steps, initialize your Git repository. \r\n\r\nI've got a button. \r\n\r\nI'll click that shortly. Submit your first prompt. The prompt marked down files will automatically open for you. This file is your project plan and instructions for the AI. Copy its entire contents and paste it into your preferred AI chat interface like Google AI Studio, Chat GPT, et cetera. \r\n\r\nThis is version one, the copy and paste process. I'm in the process now of creating an API version. So you just click generate responses and the responses come streaming in. review and accept responses, paste AI responses back in. We'll do that shortly. Repeat. \r\n\r\nThis completes a cycle. Then you start the next cycle, building upon the newly accepted code and documentation. The structured iterative process helps maintain project quality and ensures that both human and AI developers are always aligned with the project goals. And I can't believe I just read that whole thing without going into this view. \r\n\r\nOkay. \r\n\r\nAll right. This is the prompt file that it creates when you just click of a button. And if you recall, I only typed 1, 2, 3, 4, 5, 6, 7, 8 words. So everything else you see was generated by it. It's part of the extension. It's sort of bootloading the AI. \r\n\r\nYou can think of it that way. And I can share this extension. You're free to dissect this prompt. But correct, correct. After our conversation, that's another thing that I want to point out is this is the time that I can do this because it's just my process, guys. You guys are all smarter than I am. \r\n\r\nThis is just my process. I've just been obsessed with A . I. That's it. \r\n\r\nNow I can transfer my k"
  },
  {
    "id": "report_source",
    "chunk": "because it's just my process, guys. You guys are all smarter than I am. \r\n\r\nThis is just my process. I've just been obsessed with A . I. That's it. \r\n\r\nNow I can transfer my knowledge. \r\n\r\nAll right. But not if you all don't accept it. Right. So and I made an extension. Right. And I can continue to make this more palatable. \r\n\r\nAnd it's got a so it's got a it's got a step by step. So let's I have the prompt. Let me just send it off really quick to a I made a I made a white paper on the extension, which we can review next, I guess. I also have a this is the first project I made with the extension, because I needed to test my own product and see where the holes were, find the errors, where do I lose my cycle data, and I made this basically a clone of, it's a multiplayer PCTE, but I'm getting ahead of myself. \r\n\r\nAnd I made that just to test my extension in just one month, and it was just a side project just to test the extension. \r\n\r\nLike, okay. So you throw in, and then here's one of the two things that are, okay, and then I'll show you, like, so you can ask the honest question, David, what's so different about what, like, you can do or what Google's doing? So let's say I want to do the exact same thing in their most powerful coding, and I can very easily point out the two distinct differences of my process and the real world Google big boy process. So, AI Studio. So the parallel is number one. The parallel, sending a message parallel is actually crucial because it flips the script completely. \r\n\r\nYou're not reading an entire prompt. You are now comparing between the prompts that you've received. It's a completely different ballpark, ballgame. The iteration cycle is immensely expedited by that. It's honestly like, and I c"
  },
  {
    "id": "report_source",
    "chunk": "aring between the prompts that you've received. It's a completely different ballpark, ballgame. The iteration cycle is immensely expedited by that. It's honestly like, and I can illustrate it extremely clearly with my extension in fact. Let's see if I have it, 99. \r\n\r\nProbably didn't I think I might have closed it last night. \r\n\r\nThat's fine. We're about to see it anyway So I just sent it off one two three four and then we got this fancy schmancy Google Version doing it So there's two, so this is sort of revealing the second issue, which it's going to go down a single trajectory and build out the thing that I asked for, but there's a, that's called a long horizon task in the research, and the jury is still out if AI can reliably do a long horizon task So far, open AI is crushing it by an order of magnitude on long -horizon tasks. But what happens is an error gets made early on in the task, in like cycle one, cycle two, you can imagine. And that error compounds over time because the AI doesn't know or can't correct it. And so my solution to that problem is my iterative cycle process where the human is in the loop at every step of the way. \r\n\r\nSo we'll just have that run. And then I think I got these back. So I'll start copying these in to sort of go through my process. Response 1, paste it in, it tabs to the next response for you, but it's just pasting it in. 1, 2, 3, and 4. Now, once you've pasted in as many responses as you want, you just parse it, and then the next thing you want to do is sort the responses, and I sort them by token count. \r\n\r\nSorting by token count, you can immediately see I got an extra file in this response. Then over here, I got an extra 1 ,000 1 .2k tokens, right? And now my game, if y 'all rememb"
  },
  {
    "id": "report_source",
    "chunk": "ting by token count, you can immediately see I got an extra file in this response. Then over here, I got an extra 1 ,000 1 .2k tokens, right? And now my game, if y 'all remember my game, that was a million tokens. So let's just pretend this AI gives me perfect code. Let's just pretend. It's still going to take me a million tokens to make my game. \r\n\r\nIt's still going to need a million tokens. Let's just say every token is perfect. So how am I going to get there sooner if it's giving me 3 ,000 or 4 ,000? That's the 33 % increase. I'm going to get to my goal 33 % faster. That's what this process unlocks. \r\n\r\nYou would never have, if I would have just said response one versus response four gave me the 4 ,000, okay? So that opens up, and then what this does in development, when you're making code with AI, Say response one doesn't solve your problem, but response two does. If you only sent response one, you waited five minutes for the response, it didn't solve your problem, and now you're writing a new cycle, you're writing more prompts. Versus, you're not writing more prompts, you're just sending the same prompt because there was nothing wrong with your prompt, there was nothing wrong with your context, there was an error in your bug and the AI went down the wrong trajectory trying to solve it in response one, but in response two it went down the right trajectory and solved your problem. It's like opening up a parallel universe possibility. It wasn't open to me until I sent the second response. \r\n\r\nSo yeah. So I get the responses back. Say I'm going to select this one. And let's just look really quick. What is the extra file? Ah, this one gave me an implementation roadmap. \r\n\r\nSo that's what this one gave me, extra, right? S"
  },
  {
    "id": "report_source",
    "chunk": "select this one. And let's just look really quick. What is the extra file? Ah, this one gave me an implementation roadmap. \r\n\r\nSo that's what this one gave me, extra, right? So I'm going to select this one, do my commit. Oh, it's not a repository yet. Initialize. Cool. Now I can baseline. \r\n\r\nAnd now I can select the files. and I can accept the files. You can mix and match. \r\n\r\nSometimes when I send 10 problems to the AI, maybe this one solves one of the problems and this one solves another one. \r\n\r\nAnd then I look and realize they're completely different file sets and I just accept both in one cycle. Why not? Because they both solve my problem. So what did that do? That just added the files right into Java. just like their solution is doing it, right? \r\n\r\nSo, oh, that's cool, that gives me little places. Just like their solution does it. Mine is a bit slower, it's not agentic. There's nothing stopping me from coding in agentic solutions into my tool. What they don't have, and what's the beauty of this new paradigm we're entering, is that any new idea I can just quickly integrate into my plan because I have it from baseline ground truth, my own code, to begin with. How did I deliver my Slack bot in the beginning? \r\n\r\nI didn't think about a one -click installation. Three years ago, I had no idea one -click installation for Slack existed. I saw that idea in another project, and then eight hours later, I had it in mind. So, that's the beauty of building it yourself, is you don't, SaaS is going away. Oh, my VS Code. Okay, so, accepted the files. \r\n\r\ngave me a master artifacts list to keep all my artifacts in line, a description of the artifacts. They're automatically selected to my context. So then I'm ready for cycle two. O"
  },
  {
    "id": "report_source",
    "chunk": "e me a master artifacts list to keep all my artifacts in line, a description of the artifacts. They're automatically selected to my context. So then I'm ready for cycle two. Oh, let's look again. How I would run it, how I would install it, the file scaffolding. And then when I made my AI game, I spent eight days planning, making artifacts upon artifacts, planning it, gaming it out in my mind before I even pulled the trigger. \r\n\r\nBut let's just pull the trigger now. Look, we've got some files, okay. Okay, great. Let's make the game. Let's make the code files, whatever. And then it would do it. \r\n\r\nI actually don't want to bother demonstrating more that because I'd actually rather tab over and show kind of this project, which is, again, after I got my extension to functional thing in VS Code, I needed to test it, beta test it, so I decided to make this, which I call Virtual Cyber Proving Ground, which is like a multiplayer PCTE. And so you make a team, anyone who's logged in would be in the chat. These scenarios, I've just created the one, but it's, Essentially, this could be like a hack -the -box connected into PCTE. We could make our own scenarios. This scenario is you escort some MQ -9 Reapers. \r\n\r\nThey get jammed and they get hacked, and you're supposed to remote in, rotate their generated key, rotate their key if they're hacked or if they're jammed. You change their frequency. And so, it's multiplayer. So, there'd be a team. There's AI integrated. So, you can create an Intel chip with Jane. \r\n\r\nI call the AI Jane. This is running all locally. And what it does is it takes the data and then it turns it into some sort of usable data that the whole team can use. You just click to copy the command. And it got that from thi"
  },
  {
    "id": "report_source",
    "chunk": " what it does is it takes the data and then it turns it into some sort of usable data that the whole team can use. You just click to copy the command. And it got that from this. AI converted it, right? \r\n\r\nSo, you can. \r\n\r\nAnd so, watch the scenario. \r\n\r\nYeah. \r\n\r\nDo we? I don't. \r\n\r\nAlex, I don't think so. \r\n\r\nHe said he... \r\n\r\nOkay. \r\n\r\nOh, sure. Absolutely. Okay. All right. Well, then let me just get this thing running. I think I just have to delete some stuff and restart the server. \r\n\r\nThere's my, yeah, they're not running right now. So I think if I just do this, it will refresh the memory of the environment. And then, yeah, I can quickly just start the scenario so you guys can kind of see what it's supposed to look like. Because we could create many of these scenarios. So the way it works is your team, their team, you have your drones, they have theirs. You get a terminal, you actually get two terminals. \r\n\r\nAnd what you're supposed to do is you're supposed to remote in to first to, and I don't have the actual right command in front of me. Oh, probably, yeah. So I actually, I'm not working on the, I've been working on the whole interface and everything, getting this, you know, in the Jane and you can talk and sell that. So I'm actually at the point now, I would be at the point now where I could start working on coding out the Python script that runs this thing. So they get jammed, they move erratically when they're jammed, compromised. And then if they're ever actually compromised, it actually turns into a red one and starts going towards your base. \r\n\r\nand you're supposed to, you know, get them back. There's supposed to be, yeah, so I haven't gotten around to it, though, but there's supposed to be a drone manifest"
  },
  {
    "id": "report_source",
    "chunk": " base. \r\n\r\nand you're supposed to, you know, get them back. There's supposed to be, yeah, so I haven't gotten around to it, though, but there's supposed to be a drone manifest in here. You get the drone manifest, and then you can remote into the drones, depending on which one it is, and then you, you know, you just run the right commands to, in the situation. Now, what we can do, though, with a team, and then, so, one person, we can start to actually, like, do some really interesting cognitive analysis on these users. That is not possible. previously. \r\n\r\nFor example, there's offensive and defensive operations. The offensive operation is you do a brute force attack, and so it's a timed thing. So you just run one command on the enemy drone, and you kind of just wait. And so we can determine what is that user doing. Are they just running two offensive commands, which is fairly chill, fairly easy? Run and wait, run and wait. \r\n\r\nOr are they using, they only get two terminals. Are they using one of their other terminals to do some defensive operations, which takes more time and effort and finesse? You have to run more things and remember more commands. And so we can detect, what are they doing more of? Who's being more helpful? Who's making the Intel chips that other people are using? \r\n\r\nSo who's good at synthesizing information? And all this is possible because I have the entire context of the entire project, where I can then say, okay, now let's start working on the analysis portion and I'm just about done now basically but just looking at some of these artifacts to kind of explain so like Jane's telemetry and heuristics for I have an artifact for onboarding the content creator for this so y 'all could make scenarios and "
  },
  {
    "id": "report_source",
    "chunk": "ese artifacts to kind of explain so like Jane's telemetry and heuristics for I have an artifact for onboarding the content creator for this so y 'all could make scenarios and you just make an art stage so the drone hacking scenario if there's just a few artifacts that come consist of one scenario, that then, you know, I could just ramble, ramble, ramble. But yeah, I mean, this is, yeah, after action report, instructor view, overwatch kind of stuff. So yeah, all kinds of stuff. The spectrum for the UAV, so when you see the jamming occurring, you know what frequency to switch to. \r\n\r\nWhatever, the sky's the limit, right? So anyway, okay, I'll finish here. Thanks for coming to my TED Talk.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-6.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nIt's right here, right? \r\n\r\nData curation environment. \r\n\r\nAnd I see, yeah, you have the new version installed. \r\n\r\nCool. Okay. So this is the previous one. I kind of was going through and trying to create a simple one. So, I mean, honestly, it's going to be tough until I actually start implementing all the stuff in there and might reorganize it. But this is just the basics. \r\n\r\nSo artifacts. project documents, but that doesn't include resources. So these are like possibly like things that I've created just to help get the project together that may be referenced or maybe not. These are resources based off of things that we want the AI to use as a learning resource to pull information, to build training upon, et cetera, et cetera. And then I have UKI templates, which is we would fill with all of our base templates, our references, training material, anything we have to do, follow along"
  },
  {
    "id": "report_source",
    "chunk": "etera, et cetera. And then I have UKI templates, which is we would fill with all of our base templates, our references, training material, anything we have to do, follow along with, writing styles, etc. Right. \r\n\r\nSo that's the basic. I was kind of messing around, but that's like the basic. \r\n\r\nAnd then this would be the product template. \r\n\r\nAnd then you just simply copy paste and then like browse and say this would be the MC doc. \r\n\r\nRight on. \r\n\r\nAnd then it has everything in there. Yep. Well, the biggest thing would be keeping like these all, at least the UKI, these are always going to be kind of dynamic to the project. \r\n\r\nThese should be static. UKI templates also can, we can implement cause then project restart is like, okay, where are we going to put framework and stuff? They could probably be going under somewhere under UKI templates. Yeah. \r\n\r\nAnd we may even, we may even break it out beyond project template. \r\n\r\nThere may be like lesson template. \r\n\r\nAnd like different project templates at over time. Yeah. \r\n\r\nYeah. Yeah. \r\n\r\nSo based off of what they're trying to design. \r\n\r\nYeah. \r\n\r\nNo, that's good. That's good. \r\n\r\nThat's good. That'll work. \r\n\r\nDefinitely. Um, we want, uh, we want to start, uh, getting whatever you need if you want to start working on gathering up the NC doc content. So I have it. Okay. Cool. Uh, well I don't have any inappropriate folders, but I have a bunch of cheap, you know, that's the thing was, If you just put them all understand more like what do you think is the best way sure for it to grab right? \r\n\r\nI would say let's start with everything that you're bringing in as a in a reference folder and just call it reference for now and then from there we can if we feel like something is "
  },
  {
    "id": "report_source",
    "chunk": " say let's start with everything that you're bringing in as a in a reference folder and just call it reference for now and then from there we can if we feel like something is like Fundamentally different then we'll branch it out but but I think initially as we're coming in just as the as initial like Level the playing field or all the it's like this all the facts on the ground, right? \r\n\r\nSo here are all the facts on the ground of the current state of the project. \r\n\r\nAnd we ultimately, pie in the sky goal, we want it to look like this. So now let's start making all the planning artifacts that are going to get us there. \r\n\r\nAnd then from those planning artifacts, We'll get will glean some Organizational traits actually you'll see there's a thing that worried me, so I had like they're all Yeah, this is good. \r\n\r\nThis is good. \r\n\r\nYou're doing good. \r\n\r\nThis is what we want. Yeah, no, but Before I was organizing it was based off the actual module that was in to kind of keep track Sure is we're gonna have a lot of resources in here. Yeah Oh, I see what you've done. Actually. \r\n\r\nOh, okay. \r\n\r\nOkay. So I see. Okay. This is much better. \r\n\r\nSo delete everything in reference documents and instead bring everything as a project in the project files folder, I believe, as I see what you're doing. \r\n\r\nAnd then that way we'll keep your structure, but then we're still getting like everything, like I said. So put that. \r\n\r\nYep. \r\n\r\nSo now we're on the right. Go out at some directories. I want now I got to sit on the right. Yes. This is just my old stuff. Oh, so I saw you were... \r\n\r\nOkay, okay, go ahead. \r\n\r\nYeah, this is what I just currently have, but the left side is where I was trying to redesign the folder structure for the VS Co"
  },
  {
    "id": "report_source",
    "chunk": " saw you were... \r\n\r\nOkay, okay, go ahead. \r\n\r\nYeah, this is what I just currently have, but the left side is where I was trying to redesign the folder structure for the VS Code. \r\n\r\nOkay, what I'm... \r\n\r\nUnless you think it's good to break them up into the different lessons within the project. \r\n\r\nI do think it's fine, especially if this is the Module 1 resources that will be going for the NC... Yeah, for the NC Dock that we're doing. \r\n\r\nOn the right, then that's the metadata that you've already sort of organized that we can gain from if you keep that directory structure when you drag it over to the left. That was my mistake. I didn't understand you had them sort of organized in. folders. And that all we need to do now is just sort of move all of those files into your VS Code directory. Right. \r\n\r\nWell, this is the old directory I had, but I think it got too complicated. Oh, OK. So that's why I was trying to create a new one. So this is the old one that we created. with the flattened repo and everything. \r\n\r\nGot it. Thank you. And this is the new one. So I was trying to simplify it where you just have a template folder. \r\n\r\nYeah. \r\n\r\nWhere basically all you're going to do is when you have a new project, you copy and paste this folder and then rename it. And then it has everything. It should have everything in here. I don't really have any templates in here right now, but project I could use. \r\n\r\nActually, I could, for example, I could use this is the combined technical writing training prompts I already give. Yeah. \r\n\r\nYeah, I think that's the split we need, just the UKI templates and then all the resource documents. \r\n\r\nAnd then the resource documents, obviously, CUI, not CUI. And then non -resource, you can drop all "
  },
  {
    "id": "report_source",
    "chunk": "it we need, just the UKI templates and then all the resource documents. \r\n\r\nAnd then the resource documents, obviously, CUI, not CUI. And then non -resource, you can drop all your module 1, module 2, module 3, project docs, resources, just literally just knowledge dump it all into. Yeah, well, I kind of did that, but I did all in one. That's OK. \r\n\r\nThat's OK. \r\n\r\nHonestly, it'll probably be just fine actually now that I think about it a second time. \r\n\r\nIt'll actually probably be I'm probably over complicating it It's probably just fine this way because we'll build it out over time and it's gonna pick and choose What it needs for any given? Yeah, and that's why I'm saying it might as I'm going through and I'm noticing it then I might say hey You know, it's picking too many other references I don't need. Oh, I don't think I don't think it will I think we're gonna be giving it the guiding principles in terms of like we're we've are where you me and Alex we're spelling out what's going in module 1 module 2 module 3 and it will adhere to that and then so it'll pull from everything you've concocted wherever you have put it and it will Organize it. \r\n\r\nOkay. \r\n\r\nYeah I mean, that's kind of all the resources and then going back to project documents, but that would be more these things here that are like things I've created and just like, you know, for instance, what was the new one? \r\n\r\nActually, let me pull like in here would be, we could be the KSA, the ELOs. Let me get the updated from the channel one, throw that in there. Sure. \r\n\r\nYeah. \r\n\r\nWhere's the NC doc, project NC doc. Files data map and love that. Yeah, the thing I've found that causes issues is when you give like maybe two lists of casettes in KSAs in your in you"
  },
  {
    "id": "report_source",
    "chunk": "he NC doc, project NC doc. Files data map and love that. Yeah, the thing I've found that causes issues is when you give like maybe two lists of casettes in KSAs in your in your list and you're not paying attention to that That's where it might pull from one or the other or might update one or not the one you're thinking other than that Other than having like duplicate information That's all that's been what I've seen that kind of trips it up. Yeah, I'll just throw this in here because this has them all and So that would be like a project document, but we could also, if we want, well, we're going to have to, we could set it up. \r\n\r\nproject documents. \r\n\r\nOkay. Okay, so go ahead and if this is, um, we can close this, close this project because you won't, it's, it's basically VS code that's new. Uh, it shows the changelog now. Um, you can just exit this because, um, I have another way to open the workspace or you, unless you know a way to do it, you want to do it your way. Are you talking about opening the new? Yeah. \r\n\r\nYeah, this will work too. \r\n\r\nThis will work just fine. \r\n\r\nYep. So then now, what we can do first is check. I'm going to delete this one though. \r\n\r\nYou don't have to. \r\n\r\nYou can actually. It is garbage. Go ahead if you want. It'll refresh. It just takes a second. If you click refresh, it might show up. \r\n\r\nRefresh Explorer. Let's try this again. Delete. Are you sure you want to delete items? \r\n\r\nYes. Failed to delete items. \r\n\r\nOh, okay. Okay, then just do it in the Explorer manually. Yeah, I didn't. Doing those things is a bit more difficult. No, no, I'm sorry, no, no, no. Over on the left, no, on the far left, it's the top tab on the activity bar, they call that. \r\n\r\nYep, you can just right click there"
  },
  {
    "id": "report_source",
    "chunk": " difficult. No, no, I'm sorry, no, no, no. Over on the left, no, on the far left, it's the top tab on the activity bar, they call that. \r\n\r\nYep, you can just right click there and it's gonna work here. It's probably just a permissions issue. You probably need elevated permissions to delete a file and my extension doesn't have it. There we go. Oh, there we go. Yeah, now you can go back. \r\n\r\nYeah, now you can go back to the other. The bottom one, the spiral. So there you're just going to check. Now you can check the box on the left and then you basically won't have to worry about that. I've even cleaned that up and made that better. That's going to check everything that you've added. \r\n\r\nAh, so okay. So check the sizes. Okay. \r\n\r\nSo it looks like there's only one file that's going to cause you a problem, which is this top one. It's just too damn big. it's pushing you over the limit, because if you see at the very bottom right of the selected items, yep, you're at 1 .22, and basically our hard cutoff for AI Studio is one million. Okay, so maybe it would be better to probably break some of these up? No, not quite, because we only need a few hundred thousand tokens free for us at a given time. \r\n\r\nSo let's click on just the top one, that NIST one. Can you click on it? It doesn't display PDFs in here. Okay, so you know what? We might not need to bring in the NIST stuff. Uh, the, the more like the, the cookie cutter, like the whole NIST thing itself, unless like we're actually needing to reference specific, uh, like this, uh. \r\n\r\nIt references it in our training. \r\n\r\nLike actually like one item, like we need to know this X dot X dot X in the NIST guidelines or whatever. Uh, well, it's from some of the ELOs, it's referencing the"
  },
  {
    "id": "report_source",
    "chunk": "aining. \r\n\r\nLike actually like one item, like we need to know this X dot X dot X in the NIST guidelines or whatever. Uh, well, it's from some of the ELOs, it's referencing these instructions and pulling information out of the instructions. A lot of these, the JP2, JP312 is a big one. Yeah, it's a lot of going along with... we can, we can, well, when we flatten it, it's going to make it smaller. Nope. \r\n\r\nNope, nope, nope. \r\n\r\nSo that would be, um, a different making it small quote unquote would be a, uh, rag process. \r\n\r\nYou would have to create an embedding, which is definitely doable. And it would be literally in the, uh, wheelhouse of the on the fly tooling. I'll just take this out for now. Yes, that would be what I recommend for now. And then guess what? Whenever you do need to drill down into that, um, uh, task where you actually require, you know, you require that document. \r\n\r\nYou can just de -select and re -select the things you also don't, you know, you don't need for that task. Right. In this list. Yeah. Gotcha. Yep. \r\n\r\nBut for now, that was it. That's it. \r\n\r\nSo just with that one de -select now, if we look, it says 543. Oh, you even made a couple more. \r\n\r\nSo that's yeah, that's perfectly fine. \r\n\r\nAnd you honestly, 80 ,000 tokens is a hell of a lot. So we so you know, you can even if you're up at 900 ,000, you could still work with that just just for the just for reference. \r\n\r\nRight. But yeah, so now you've got a great selection of content to work with. Now over here on the right, you can draft your language. And if I would preempt something, let's just go ahead and maybe let's do it in a notepad. I've had a colleague lose data in between because you know it takes 5 -10 minutes to write something in here t"
  },
  {
    "id": "report_source",
    "chunk": "mething, let's just go ahead and maybe let's do it in a notepad. I've had a colleague lose data in between because you know it takes 5 -10 minutes to write something in here tab switch back and forth and they lost what they wrote in this particular window let's just do it in a notepad just what we know I don't want you to lose anything in this in this in this meeting yeah it's just the same thing it's just we're just going to be writing so you had this before oh yeah that's right we didn't we the next the tab over every time yeah every time you install a new notepad \r\n\r\nSo we have this. \r\n\r\nThis is what you gave me. \r\n\r\nOkay. Okay. \r\n\r\nOkay. \r\n\r\nThis was the previous one that kind of helped update. \r\n\r\nWe actually kind of use this for designing a new ELO. Okay. So I think the best thing to do would be if this document does, okay. The best thing to do would be to just basically give a 30 ,000 foot view of what the task is and then give the line item a list item of the modules and what the modules and the labs are going to consist of. \r\n\r\nI believe those two items in this project plan are the perfect two items to put in the project plan. \r\n\r\nAnd we may have them both right in this document or may just need slight modifications. Yeah, this was more... Or is it old? Also update the master artifact list to include these new documents. So it's probably going to reference documents that are no longer in there. Yeah, so maybe the high level, we can take its high level language and then when it's talking... \r\n\r\nBased on your request, I just started creating cybersecurity training. Oh, there is a WordRap button. \r\n\r\nIt is right under the PN plugins in the top. Yep, right down, left one. \r\n\r\nThat one. \r\n\r\nYeah. \r\n\r\nNow, whatever si"
  },
  {
    "id": "report_source",
    "chunk": "cybersecurity training. Oh, there is a WordRap button. \r\n\r\nIt is right under the PN plugins in the top. Yep, right down, left one. \r\n\r\nThat one. \r\n\r\nYeah. \r\n\r\nNow, whatever size it is, you'll be able to read. \r\n\r\nSo it's based on your request to get started though, but this was like halfway through. That's okay, so check it out. \r\n\r\nYeah, so see the number one is the high level and then the number two... \r\n\r\nthe design level document. So all you would probably need to do is just scroll down and just edit this document and delete the, where it gets too specific, just straight up delete only the parts where it's like this file, this folder, just delete those, which I don't see yet. \r\n\r\nAnd the modules should be fine. Um, there's some, there's some document names right there. Yeah. \r\n\r\nI see. \r\n\r\nIt's documents. Well, wait a minute. \r\n\r\nQuick question. \r\n\r\nAre those documents? is still in your directory, just in a different directory? It's in a different directory. Yeah, then maybe just delete the path, but leave the filename. Because if those filenames are in your... it's perfect. Well, the thing is too, is we have a created document. \r\n\r\nI'd rather just kind of start from scratch a little bit, because we have the document now that has basically the output of that structure it gave us. Um, which would be referenced here under project. Sure. Uh, okay. Then if that's the case, I have a good project template under, uh, project documents. That's right here. \r\n\r\nOkay. \r\n\r\nThen maybe then let's do one thing. \r\n\r\nLet's split your, a notepad plus plus right. \r\n\r\nClick on that new one, that new, uh, one tab at the tab. \r\n\r\nIt says new one. \r\n\r\nYep. \r\n\r\nAnd then where are we at? Let's see. Move document at the bottom at the second t"
  },
  {
    "id": "report_source",
    "chunk": "\r\n\r\nClick on that new one, that new, uh, one tab at the tab. \r\n\r\nIt says new one. \r\n\r\nYep. \r\n\r\nAnd then where are we at? Let's see. Move document at the bottom at the second to last at the bottom, and then move, move to other view. And then now you can max, now you can. \r\n\r\nThat should be a way you can look at that one and then write a new document. \r\n\r\nIt doesn't have the original request in here. It just has... Do you want the original? request? I can maybe try to find that prompt. \r\n\r\nWeird. \r\n\r\nWhy is this not? It would be a couple of weeks ago. Yeah. This was the response I gave. So what are we supposed to be giving here? We're basically giving it the directions. \r\n\r\nYep. We're basically getting the starting point. We are saying to the AI, look, here are all these documents. My plan is I'm going to be making training for NC DOC, blah, blah, blah. I'm just going to go ahead and start. You totally can. \r\n\r\nYeah, because this is going to be... My only fear, yeah. It's going to take longer to try to recreate or... I understand. No, believe me, I understand. I've written many, many papers. \r\n\r\nIt's a struggle. The struggle is real. I'm with you. So if I want to reference the document in here? You totally can. Just do it by document name and I would do it in, just to be crystal clear, is putting tilde quotes, like a tildes. \r\n\r\nI mean, yeah, honestly, just the telling it what it's doing and then giving it the modules are basically the two things to get started. \r\n\r\nthat's going to be able to pull them efficiently out of those names. \r\n\r\nYou know what? I think it will be. I think you're right on both accounts. It is less efficient, but it also will be because I've done the same thing. What I've done is I had a project scop"
  },
  {
    "id": "report_source",
    "chunk": "? I think it will be. I think you're right on both accounts. It is less efficient, but it also will be because I've done the same thing. What I've done is I had a project scope and it was just a project scope, like a raw text. And then I actually made a artifact to become project scope. \r\n\r\nAnd then in that project scope, I just referenced, I said, see artifact, whatever, and just moved on. Life was fine. So you're essentially doing the same thing. \r\n\r\nYou're referencing the Excel document as a tag in your project scope. \r\n\r\nI think it'll be just fine. Yeah, I'll just throw them in here. I don't want to take too long. Yeah, also, yeah. No reason. \r\n\r\nI believe it'll still be more efficient if it's right there in front of it, but I've seen it both ways, and I think this is better ultimately. \r\n\r\nGenerate initial artifact prompts we could are there any initial? Documents that you know you'll want like in terms of planning artifacts like We can let it yeah, we can I was just wondering you can one final second You can click generate initial artifacts prompts at the bottom, and then it'll we want to just check check on the okay Yeah, so I can see so there's just one thing yep Go ahead and click Flatten Context as well, because it did not create this file, which it did just now. \r\n\r\nAll you'll need to do, and you only have to do it this one time for the initial cycle, is select all in this file in the flattened repo. \r\n\r\nOh, and also, first, let's scroll down through here and make sure everything looks fine. \r\n\r\nLet's go on the right, and you can click in the flattened repo on the on the right on that uh what do they call that the file navig file navigator i think that what your mouse is yep click on the actual screen part at "
  },
  {
    "id": "report_source",
    "chunk": " the flattened repo on the on the right on that uh what do they call that the file navig file navigator i think that what your mouse is yep click on the actual screen part at the top it's like highlighted the part that's highlight yes if you click there and you drag it you can actually drag it nicely quickly through the whole file from top to bottom versus if you did it another way there's no way to do it so click and drag down and yeah we're doing it so we're just looking we're looking for any like raunchy \r\n\r\nnasty, broken symbols. Yeah. \r\n\r\nYeah. \r\n\r\nYeah. Text is good. Even if it's like one letter at a time, who cares? It doesn't, doesn't even that's fine. That's fine. That's actual, it's content. \r\n\r\nIt's like, yeah. \r\n\r\nEncrypted. Yeah. \r\n\r\nHashed garbage. Yeah. Gotcha. Yep. \r\n\r\nOkay. \r\n\r\nNo, it looks like it looks like we're Gucci. Um, which is good. Okay. So just copy everything in there. And again, you're only going to have to do this one time. This is just the initial. \r\n\r\nI didn't remember to fix this. Now go into your prompts file. It's the fourth tab. Prompts . md. Yeah, you can open it there as well. \r\n\r\nThis one? Nope, that one. Now here, if you look in the artifact schema right at the top, you're going to be dropping this, what you just copied into M7, which is the flattened repo. So that'll be basically the very bottom. It should be empty. \r\n\r\nYep, there it is. \r\n\r\nYou can just put it right above that line. \r\n\r\nRight above. Yeah, because that's the ending tag. If you see that's a slash in front of the M. So above it is it. Yeah. So just push enter and right there. Just paste it. \r\n\r\nJust fine. Life is good. You're done. \r\n\r\nYou're done. \r\n\r\nYou're done. \r\n\r\nNow copy this whole thing. \r\n\r\nAnd now you can "
  },
  {
    "id": "report_source",
    "chunk": "just push enter and right there. Just paste it. \r\n\r\nJust fine. Life is good. You're done. \r\n\r\nYou're done. \r\n\r\nYou're done. \r\n\r\nNow copy this whole thing. \r\n\r\nAnd now you can send this to, uh, uh, uh, AI studio. \r\n\r\nI think so. \r\n\r\nLook now that's a massive 41 ,000 lines. Yeah. Yeah. It's in there. Yeah. Yeah. \r\n\r\nYeah. You're good. Okay. \r\n\r\nIs that right down at the bottom? \r\n\r\nIt's fine. \r\n\r\nFile structure. \r\n\r\nI believe it's fine. \r\n\r\nI do believe it's fine. \r\n\r\nYes, it's just fine because OK, yeah, because those are all the template artifacts above it. that I put in from my, yeah, yep. All right, are we just saving this? Yeah, you can, and then copy it all out. Oh, the whole thing, okay. Yeah, and then now go to your AI studio, and now it's time to let it cook. \r\n\r\nOh, so now we're just putting this in an AI? Yeah. What, we're doing Gemini? \r\n\r\nYeah, Gemini probably can't take it, AI studio. \r\n\r\nOh, AI. Gemini has harder token limits than this one. \r\n\r\nOver on the right, just do a nano, click on nano banana, change it to 2 .5 pro. \r\n\r\nAnd then, yep, that's already done. Just do the temperature to 0 .7. And then if you want to clone, right click this tab and clone it, duplicate it. You want to do two or three replications. How many do you want to do, two, three, four? Yeah, that's fine. \r\n\r\nOkay. And then just, yeah, double check just the temperature and then send it off. Give it a man, really? \r\n\r\nNope, that's okay. \r\n\r\nThat's okay. A lot of it will come after when we see what the results are. That'll help us get into the state of flow. And then also one thing that is important is we wanna also show it what the end product will look like. Obviously we don't have this end product, we're making it. But if we have anot"
  },
  {
    "id": "report_source",
    "chunk": "n also one thing that is important is we wanna also show it what the end product will look like. Obviously we don't have this end product, we're making it. But if we have another end product that is able to be added as just its own artifact, as its own document, and saying, look, this is what, you know, like if I were to like give my introduction to Beacon class, I don't know, I actually, maybe that would be in line or not. \r\n\r\nBut yeah, so we're making lessons. \r\n\r\nIt's going to be in Confluence, right? And then, okay, then nevermind, I'm over complicating it. Yeah, it's not being saved yet. \r\n\r\nIt's fine. If it's just Confluence and Markdown, then it'll produce, then all you got to do is ask for it in Markdown. And then it's literally a copy paste job into Confluence. We need this in markdown. Nope. Oh, yes. \r\n\r\nSo go ahead and put your mouse over on the top. There's the ellipsis and then copy as markdown. You'll do this three times. Now go to the tool, our extension, our VS code. Yeah. And then go to the, so you've got two parallel co -pilot tabs. \r\n\r\nUm, we just, we're just going to need one over on the left and close this co -pilot chat on the right. That's going to give you more window. Yeah. More really. more screen real estate. All right, where are we going now? \r\n\r\nSure, the tabs on VS Code DC Parallel Copilot, you've got two of those tabs open. Yeah, probably close that one. Yeah. Okay, in here, now see the blue? That's where you're going to be putting your responses, but also since you're doing three replications, let's just change responses in the top right from four to three. I see what you're saying. \r\n\r\nYep. \r\n\r\nOkay, now - My response is here. \r\n\r\nYep. It'll auto -tab, so it just auto -tabbed for you. So "
  },
  {
    "id": "report_source",
    "chunk": "ses in the top right from four to three. I see what you're saying. \r\n\r\nYep. \r\n\r\nOkay, now - My response is here. \r\n\r\nYep. It'll auto -tab, so it just auto -tabbed for you. So now you can just copy the next one. That was one of the new additions is auto -tabbing. There you go. Okay, now we click parse all at the top and then we click sort on the right. \r\n\r\nAnd it shows us that the third response was the longest and it gave us one extra file. \r\n\r\nClick on response three. That's right, that's right. And then so, yeah, so read the summary. training platform project, establish comprehensive set foundational planning documentation artifacts, blah, blah, blah, clear blueprint. Yep. So course of action, create the master artifacts, vision goals, scouting plan, testing guide, repo guide roadmap. \r\n\r\nSo let's go through the vision and goals and then the roadmap first, because if those aren't aligned, nothing else is important. So scroll up on the left and then click on, actually we can just, uh, so, okay. Yeah. Scroll up on the left and then click on, um, I'm sorry. Um, let's do this as well. Click the spiral. \r\n\r\nThat'll give us more screen real estate. Yep. Now scroll up in the, uh, yeah. Cool. Cool. Perfect there. \r\n\r\nSo let's click on the second file, the project vision and goals. It's the red, see the red circles, the red Xs. Yeah. The red X means that project doesn't exist in your, so it's a new file. Yeah. See, and you hover over, you get the full file name. \r\n\r\nYep. I guess I truncated a little too, a little too aggressively. Yeah. So here, Vision, NTDoc, training platform. I don't know if it's a training platform. It's just a, it's just a, it's just a set of modules. \r\n\r\nSo we, ah, aha. Oh my God. It knows what PCTE is. s"
  },
  {
    "id": "report_source",
    "chunk": "Doc, training platform. I don't know if it's a training platform. It's just a, it's just a, it's just a set of modules. \r\n\r\nSo we, ah, aha. Oh my God. It knows what PCTE is. so that'll be so let's say put that in your cycle context so let's click up there uh that's amazing uh watch yeah uh in the cycle context uh con uh yeah up there in the uh it's a text window yep so right here you can say uh just say this training will be for the pcte and just period um because uh Are you trying to comment? Are you adding? Yeah, preface that sentence. It'll make sense after you preface that sentence. \r\n\r\nSo go to the front of the sentence and I'm going to put why I'm saying that. Yeah, so in the project vision artifact, \r\n\r\nYou reference this training as a platform. \r\n\r\nWe're not making a platform, we're making a training. And the platform we're making the training for is the PCTE. That's gonna help align things overall. Yeah, perfect. Okay, and then so we can keep reading. Project will be developed to phases or... \r\n\r\nOh, oh, oh, oh, also, so are we making, you're just making the lessons, lesson content, correct? \r\n\r\nSo let's say, That as well. \r\n\r\nThis is going to solve the other problem I was facilitating on earlier. At the end of your sentence, write this. The final deliverables will be output in Markdown. The expected final output deliverables will be in Markdown. That's fine. Get it out first. \r\n\r\nWill be in Markdown format. That's one of the things. You have to tell it what it is you want the end product to even look like, or else you'll just be kind of grasping at straws, which is a state of being, actually. Sometimes that's the way to go, is when you don't know what the end result should be. You have to just kind of grasp at "
  },
  {
    "id": "report_source",
    "chunk": "asping at straws, which is a state of being, actually. Sometimes that's the way to go, is when you don't know what the end result should be. You have to just kind of grasp at straws, and then you can build a vision. Okay, so that's good. \r\n\r\nThat's what we want there, and now it knows what the end product should look like. that's why I was trying to go for a platform, because it kind of didn't really know what the end result was gonna be. So that's good. And then we were, so it's all about the training. \r\n\r\nSo we're here for the training. \r\n\r\nSo then now let's look at the roadmap, the file, the last file on the list. Oh yeah, just click on it. The check mark will do something next. Yeah, just click on it to view it. Yeah, let's see what, core UI to see UI development. So that's going to align this drastically. \r\n\r\nSo I don't think we need to say anything more. I think let's say with that notion, please review and revise our artifacts. Back in the cycle context? Yep, precisely. So with that in mind or with that notion, please update our documentation artifacts. And then before we move on, but then we're done with that. \r\n\r\nSo we could literally just send that off again, but let's just check another artifact. Let's just check another response. Let's just be a little scientific -y and see, click on response two, because it's almost the same. It's six files as well. Yep. So let's check its vision. \r\n\r\nOh, this one might be more aligned. This is calling it a training curriculum project and not a platform. This might be more in line with what we want and we might not. So let's just check this one out. Check out the Vision and Goals, the second document. Yeah, we will do the GitHub. \r\n\r\nThat's important. That helps you really c"
  },
  {
    "id": "report_source",
    "chunk": "we might not. So let's just check this one out. Check out the Vision and Goals, the second document. Yeah, we will do the GitHub. \r\n\r\nThat's important. That helps you really compare different results later. \r\n\r\nOkay. \r\n\r\nSo here, training curriculum is to create the, it's again, it's going for a platform. Okay. Okay. \r\n\r\nPlatform scaffolding. \r\n\r\nLet me just read. \r\n\r\nYeah. User authentication. See, see, yeah, that's where it's getting confused. Okay. So then we can go, go to response three. I just wanted to confirm. \r\n\r\nNow you just want to click select this response over a ride a little bit. What that does is that expresses your intent that this is the response that we're going to be going with. You can also see at the top right it says baseline. \r\n\r\nYou can click that and then it's going to give you an error because you haven't initialized yet. So go ahead and click baseline and then down there you can click initialize and then Nick's initialized. Now you can click baseline again. You can now create a baseline it says. Okay so now you're baselined which If you were to test multiple responses and you didn't like what you just tested, you could click Restore right next to it to go right back to this baseline. But now you can click Select All, which is right next to what's highlighted to the right. \r\n\r\nSo that's what the checkboxes on the left do. You say, I want this file in my repo. Now you click Accept Selected. Now it turns red. \r\n\r\nThat's just because it's a Similarity it's compared to what it was. \r\n\r\nIt's 0 % similar because it's brand new, but that's fine So now you can click the spiral on the left now We can open our file tree up again. And now those files will be in our source artifacts. So click that It's not "
  },
  {
    "id": "report_source",
    "chunk": "ut that's fine So now you can click the spiral on the left now We can open our file tree up again. And now those files will be in our source artifacts. So click that It's not in there. You can click and we'll drag it in there. Okay it's in there right there. \r\n\r\nSource up a bit. Yeah, it puts it in there, but we can select them all and drag them in. If you, if you want, it's no big deal. It doesn't matter where they are. Is this going to be the automatically created this artifact folder? It did. \r\n\r\nBut I think that if you move them, it'll just treat it just fine. But again, I think it's arbitrary where, where it is. Um, so do you technically don't have to create an artifact folder in the beginning? No. It will definitely do that for you. It's programmed to do that. \r\n\r\nAnd now I don't like, what I would love it to do is to do the one that you wanted it to. That would be, that's going to be like refinement on my part. It just puts it in Sork Artifacts, but you're free to move it. It's, it's, it's the next time you do flatten. Oh, that's fine. \r\n\r\nI keep it here. \r\n\r\nIf we don't need to create it then. Yeah, I agree. \r\n\r\nI agree. \r\n\r\nIt's, it's perfectly fine right there. Um, and it'll, and it won't break anything. Uh, and largely it's, it's yeah. \r\n\r\nIt's tagged with artifacts, which is what's more important, really. \r\n\r\nYep, so we got those files, and then it's going to basically fix them. So now go back to your Parallel Copilot tab. Yep, now we can, oh, okay, so now you just need to new cycle, let's put the title, new cycle, let's just name it Refine Project, colon, we're not making a platform, we're making training curriculum, or semicolon, colon, whatever. comma q c r c u r r Curriculum. Yeah. \r\n\r\nOkay. Now we can cl"
  },
  {
    "id": "report_source",
    "chunk": "ine Project, colon, we're not making a platform, we're making training curriculum, or semicolon, colon, whatever. comma q c r c u r r Curriculum. Yeah. \r\n\r\nOkay. Now we can click. So let's do it. Let's, uh, over on the right, do you see the cloud with the arrow up? It's the third button, the third little icon. Yep. \r\n\r\nSave cycle history. So this is just, uh, my personal process is, um, I saved this and I do it outside of this directory. So it doesn't get automatically at actually, I think I just updated it. So yeah. \r\n\r\njust so you have it saved so you don't lose your hair if you lose your cycles. But I have made it more robust. I haven't lost my cycle since I fixed it. \r\n\r\nNow you've got the new version. But yeah, just do that once right before now. \r\n\r\nNow you're going to just click Generate Prompt because you've got your cycle one, you've got your new files. \r\n\r\nNow just over on the top left, you're going to click Generate Prompt MD. Up a bit more. Yep. \r\n\r\nSo now this time you don't have to do any other copying. Now it's going to do it for you. Now now scroll up to the top. \r\n\r\nYou should see your new cycle one if you hold a control and press home It'll oh cool. \r\n\r\nYep. \r\n\r\nSee so now it's here It's all updated everything put everything right in the right spot where it needs to be Now all you've got to do is copy and paste it into your into your AI studio This is what you're hoping to have like a plug -in to have it do it automatically. That's right And it will it I if you actually open the settings I'm working on that part now I just need, that's right, what's his face, needs to give us an API key, and then absolutely, this will be literally just, you click, instead of, when you click generate prompt, they'll just"
  },
  {
    "id": "report_source",
    "chunk": "at's right, what's his face, needs to give us an API key, and then absolutely, this will be literally just, you click, instead of, when you click generate prompt, they'll just come into the responses. \r\n\r\nRight. \r\n\r\nYeah. \r\n\r\nAnd as many as you asked for. \r\n\r\nYeah. \r\n\r\nand it will even give you a cost so you can estimate your cost whatever yeah so yeah so now delete this is the manual but this is also free so anything else would be an API cost okay so am I just doing a whole new chat for this you can either do that or you can delete the three messages on the right I if you do if you do a new chat it'll be you just reset your temperature oh yeah I haven't used Google AI studio very much. I'm trying to understood just posting it back down in here But what am I asking you that this are it's gonna just go through the cycles and basically that update I just did it's gonna redo it all so do you see your? Token count at the top. It's at five hundred fifty six. \r\n\r\nWe're five hundred fifty six thousand We're gonna need to delete the prompt you've sent. \r\n\r\nOh, okay. Here's an idea Oh, you've actually given me a faster way to do it instead of deleting the top one edit the top one So you see the edit button right there? It's a less, it's less, so mouse where you're just mouse was again. And then the, the, the, that, um, now, now, now, yes, I will. That's going to be less keystrokes overall. Uh, it's a, it's up a bit. \r\n\r\nYeah. It's that window up a bit. It's moves. I hate, I hate it. Yeah. There it is. \r\n\r\nControl a, uh, to delete. \r\n\r\nYep. And then this is less keystrokes. I promise you than any other option. Um, it says user. Yeah, there it is. You got to select in their user. \r\n\r\nOh, there you go. It wasn't, like, propagating. "
  },
  {
    "id": "report_source",
    "chunk": "ess keystrokes. I promise you than any other option. Um, it says user. Yeah, there it is. You got to select in their user. \r\n\r\nOh, there you go. It wasn't, like, propagating. There we go. Now the double -check mark. \r\n\r\nDouble -check or the actually, yeah, double -check to save this. \r\n\r\nEdit. Think slow. Yeah, I see that. Maybe this isn't faster if this is the performance of the edit functionality. Jeez, Google. \r\n\r\nLet's see if it does it here. Yeah, it's very tweaking out. It's not like this. Yeah. Yeah, I think it's just best to delete. Right on. \r\n\r\nExperiment failed. It did finally look like it updated this one, but. Yeah, you can tell if it says cycle one. \r\n\r\nGod dang it. I don't know what it's doing now. Yeah, I've used AI Studio since the beginning and boy, oh boy, was it different. This is better. This is good. What it's doing now is an improvement, sadly. \r\n\r\nI'm just gonna delete this. Yeah, yeah, we're done with it. And in fact, they're saved in our cycles now, which is something I've never had. Now that we've got our cycles, every response I've ever had, and I've done it before, I've gone back two cycles and it loaded a file. Just because I remembered I had it back there. \r\n\r\nI'm like, oh yeah. Yep, delete it again. So there's three. \r\n\r\nThere's your message, and then it does a thinking step, and then it does its response. \r\n\r\nYep, there's the thinking. And actually for you, for you, yeah, it doesn't save anything at all because of your settings. \r\n\r\nSo you can probably just do a new chat, but it's still, you have to do your temperature one way or the other. You're gonna have to change some settings. Oh, wait, I didn't change the temperature. \r\n\r\nIt's OK. \r\n\r\nIt's only a little difference in the efficacy. "
  },
  {
    "id": "report_source",
    "chunk": "re one way or the other. You're gonna have to change some settings. Oh, wait, I didn't change the temperature. \r\n\r\nIt's OK. \r\n\r\nIt's only a little difference in the efficacy. I've looked at some statistics. \r\n\r\nThe performance goes up marginally, like overall at 0 .7. And then it starts to go back down at 0 .6, 0 .5. \r\n\r\nBut they're all largely the same. Yeah, see, it didn't know what our, and that's exactly, that's amazing. It tried to make a platform because I'm giving it all these code templates, which You can take my extension, you can write a book with it, you don't have to make code with it. \r\n\r\nCool, yeah. And then telling it we just want them in Markdown is like, oh, now I know what it's supposed to look like. All right. You might even, hell, depending on how good you're, if you've got all the documents, literally, this is it. It's A plus B equals C. If you've got all the right resource documents that it's gonna need in your request and you ask for it, you're gonna get it in the output. Then it's just a matter of, let's do module 1, let's do module 2, let's do module 3. \r\n\r\nIf they're too big, then you break it up. \r\n\r\nLet's do module 1 .1, let's do module 1 .2. I think this is a good example. \r\n\r\nI'm past this point, obviously, in the NCDoc phase, but I'm just learning it. I think it'd be better. if we like went, this would be good or great in the beginning. \r\n\r\nAnd then you're going from there and be like, okay, well now I want to, I don't want it to build out all of the modules at once. I think it'd be too broad. And then, you know, I think if you focused on one modules, you know, Hey, this is, this is the ELOs I'm trying to accomplish. And this one, you know, you know, referencing back to the drives again, is"
  },
  {
    "id": "report_source",
    "chunk": "nk if you focused on one modules, you know, Hey, this is, this is the ELOs I'm trying to accomplish. And this one, you know, you know, referencing back to the drives again, is this done? Yeah, that's good. No, you're right. \r\n\r\nThat's exactly the process. And I always start high level first because you never know what you're going to get. You don't know what the current model can do. \r\n\r\nAnd then any part you just drill into, you can refine. Yeah. \r\n\r\nOkay. So now you just need to click the check mark, which is just to the left of the cycle title. And in the future, it'll be highlighted for you as you get through the workflow. Just to the right, a little bitty bit. You're almost there. \r\n\r\nYeah. \r\n\r\nIn the future, as you go through, it'll be highlighted just like the blue. It's just whatever, yeah, whatever reason. Yeah, I was gonna say, I mean, everything seems to be working good. It's just trying to navigate. There's a lot going on. There are, and it'll get more fluid, and you're getting more into the, Workflow now there was the initialization that you're done with now now It's this rinse and repeat that paste in the responses click the parse and you won't even need to do the sword again You just click parcel. \r\n\r\nOh, where is that over here? Yeah, they should do Feel like if you are doing everything down here the parcel button should be like I can like I can be like right around here Or me. Yeah, I can. Yeah, I'm with you. I'm with you. Yeah, this one. \r\n\r\nYeah, because this looks like it's part of... I think you're right. \r\n\r\nI think you're right. \r\n\r\nIt doesn't look like a butt. This looks like a... You know what I mean? Yeah, I think you're right. I think maybe over by sort or maybe sort should be over on the left "
  },
  {
    "id": "report_source",
    "chunk": "t. \r\n\r\nIt doesn't look like a butt. This looks like a... You know what I mean? Yeah, I think you're right. I think maybe over by sort or maybe sort should be over on the left as well. Because if you're doing this, you're adding it in. \r\n\r\nYou put all this stuff in. Or maybe on parts all should value be down here. Like, you know, I just put in all the information maybe like right up here. trying to keep you within the cycle, because the parse all is supposed to go through your responses, right? No, I agree completely. Everything that's, since parse all is affecting the windows below and not the cycles, it should be with the windows below. \r\n\r\nWhat are these two percentages? Ah, yes. \r\n\r\nNo, it's saying compared to response one. \r\n\r\nYes, so the percentages, now they're, no, no, no, they're telling you compared to the original file now, because now you have those files in your repo. And so you can see at a glance that the new incoming number three there, the scaffolding plan is 70 % different. Yep. So this is, and now you can see the original over on the right was 800 tokens. And this new one is 692. \r\n\r\nSo this is smaller. \r\n\r\nYeah. It's been changed probably because it's not making a whole code project anymore. Um, yeah. So now see, now it's planning out your modules. See, see that plan? This one's 70%. \r\n\r\nIs this the original then? No, no, no. The originals are, you would just need to open them. And so you could right click on this. The originals are just open the spiral and then the source folder. Oh, gotcha. \r\n\r\nOkay. And you can see the lessons are all markdown files. You're going to get your lesson one. You're going to get your lesson two. It's good that it's putting them in this creation phase. Your key concepts in"
  },
  {
    "id": "report_source",
    "chunk": "are all markdown files. You're going to get your lesson one. You're going to get your lesson two. It's good that it's putting them in this creation phase. Your key concepts in its own markdown. \r\n\r\nIt's putting your overview in its own markdown. That'll allow the editing over time. Let's say one part is wrong. You just say, hey, this marked out the overview needs correction. It won't go and rewrite the whole lesson just to just to update the overview. \r\n\r\nAnd then once you've got all your thing, all your ducks in a row, then you say, OK, now make the final. \r\n\r\nversion of lesson one. Now you've got one big -ass markdown file you copy and paste into Confluence. Bada bing, bada boom. Now it just tweaks it. Can you edit these files right in your... No, no. \r\n\r\nYou can after you accept it. So the process would be, say you like this but you want to tweak it. This is the one you want to go with. \r\n\r\nBecause again, you're the human in the loop. \r\n\r\nIt's not doing all the work for you. \r\n\r\nIt's generating the file, and then you're doing exactly what you want to do, which is tweak it. \r\n\r\nSo in order to do that, all you've got to do is click select this response, and then baseline at the top right. And actually, the baseline should be moved down as well, almost. that. Things should be co -located. Now you can click select all, see how it's lit up correctly. Not yet, it's not there yet. \r\n\r\nSo the baseline is just as a commit. Oh, gotcha. Yep, so now select all. Now accept. Yep, see now it's cycle context would be the next thing you do, but not yet because you want to make changes. So now you can go now your files are in there. \r\n\r\nSo now you can go to the originals. I could technically do it. Yeah, actually, actually, this is you"
  },
  {
    "id": "report_source",
    "chunk": "u want to make changes. So now you can go now your files are in there. \r\n\r\nSo now you can go to the originals. I could technically do it. Yeah, actually, actually, this is you've just revealed something that I've always hated. But the way you've done it doesn't do it the way I've always hated. I hate the one the little bitty inline like it shows just one little diff. \r\n\r\nI always want to see the exact file line by line. \r\n\r\nSo this is magnificent. I would love this is what I want mine to look like. I want this to have I want my I'll do that. That'll be my next version. That's why I was going to ask because this is good because you can see it's hard to know what was different. Look, you've shown me the solution. \r\n\r\nI love it. Thank you so much. I've been trying to do this the hard way. I believe since it exists in VS Code, I can leverage it. I just didn't know. \r\n\r\nDo it this way. This is it. You're just seeing exactly what's changed. But I could technically make, should be able to make, Yeah, see, I can make changes in here. So if I didn't like some wording in here, instead of trying to get wasting cycles to try to change certain wordings, I can just go in here and do it myself. Right, right. \r\n\r\nYeah, minor tweaks? Yeah, but for the large scale... Yeah, that's what I'm saying. Like minor things, like it keeps referencing, you know, maybe it has names wrong. keeps calling something Yeah, and when you correct it, yep, and when you correct it in these source documents, it's good moving forward. That's right Yep, it'll be just like that's how it was when you you know, exactly. \r\n\r\nYeah, so wait and then you just commit these Yep, so actually no it will do that on the next baseline now cancel. So now actually click on the s"
  },
  {
    "id": "report_source",
    "chunk": "hen you you know, exactly. \r\n\r\nYeah, so wait and then you just commit these Yep, so actually no it will do that on the next baseline now cancel. So now actually click on the spiral. So Click on the sort folder. Yep. So close. Yeah, so you see how they say M That means that they've been modified since the last time you clicked baseline. \r\n\r\nOkay, where's the baseline? \r\n\r\nOver on the right, the button. \r\n\r\nOh, here again. \r\n\r\nSo actually, yeah, you could, I would close the, because I know how VS Code is with modified files. You cut, you've got two modified files that you need to get off of your open active tabs. One of them is that one, Prompt MD. What you did in the PromptMD was you cut. You can just, yeah, don't save. \r\n\r\nIt's fine. \r\n\r\nIt's always generated. It doesn't matter. Yep. \r\n\r\nAnd then the second one is that, that one. Yeah. It's just the left one. You can close that as well. Yep. But it's the, cause it's got the dot. \r\n\r\nIt's got, yeah. \r\n\r\nGo ahead and don't save. Yeah. Don't save. \r\n\r\nNow click baseline. If you don't, it'll. Yeah. Cause it says you have it open. \r\n\r\nThat's right. \r\n\r\nAnd then we, we hate now. Now you've basically done that. You've committed them all at once. That's what baseline is doing for you. Because you're happy with that now. Yeah, now you can make your small changes if you want in here, but your way is great \r\n\r\ntoo I will try to integrate your way now that I've seen it I just like that one because one if it the old stuff had some stuff that was taking out, but that was good Yeah, I want to put that back in there. Yeah, absolutely. It's important to see it. I want to surface that kind of stuff I think this is a great. I think you know you've done some crazy shit. You haven't seen wha"
  },
  {
    "id": "report_source",
    "chunk": "e. Yeah, absolutely. It's important to see it. I want to surface that kind of stuff I think this is a great. I think you know you've done some crazy shit. You haven't seen what I made We should yeah I'll have to show you what I mean there. \r\n\r\nConcentrate on doing like an individual module. And honestly, we can just do like that's exactly module five. Lesson five is a very simple static content for after actions. We can experiment building. What I worry about now is because I've just been using chat GPT and you know, when I have a file structure and I've been just using that and it gives me consistent formatting and flow, which is good. But I want to experiment and use this for like lesson five. \r\n\r\nAnd what I can do is just get open source resources ready for lesson five, figure out what I need for that. That's the problem too, is usually, so just for like, you know, so you can get your wheels spinning, like what I'll usually do is like I say, hey, these are the ELOs I am trying to build training on. Find me open source trainings and documentations and list them. that might help in a system. Then I kind of go through these and say, yeah, this is, you know, and I give it specific ones. I'm like, use like MITRE framework as an example, use NIST, use DoD documentations, use stuff from . \r\n\r\norgs, . \r\n\r\nedu. \r\n\r\nI try to like keep it away from . com stuff, grab those resources that it finds, and then I ask it to build some training based on those resources. \r\n\r\nSo it's a two -step process because I don't want it to \r\n\r\nbuilding a lesson yet until I have some good resources to look at. Yeah, and AI studio does have the ability to do Google searches and reference and retrieve. Right. Yeah. So keep that. Yeah. \r\n\r\nBut that's v"
  },
  {
    "id": "report_source",
    "chunk": "e some good resources to look at. Yeah, and AI studio does have the ability to do Google searches and reference and retrieve. Right. Yeah. So keep that. Yeah. \r\n\r\nBut that's very good. \r\n\r\nNo, that's a good idea. I didn't think about that, about using it to find some OSINT training sources. That's a good idea. Yeah, I've been using it. We've been using it to pull actual cyber reports from like MITRE. Oh, and it's so fresh too, isn't it? \r\n\r\nIt's always like so recent. Yeah. So, which is good because I'll say, hey, try to find out some sort of Intel intelligence report, open source report that was based on a Navy ship. You know go through or a PT activity that has targeted maybe in general and then I'll pull those reports Of course, I look at them. Sometimes they're accurate. Sometimes it's not there's a thing I've also been having issues as it gives references and it says in accordance with whatever whatever But then when I go to the reference and try to do like a control F and find where it's referencing It's not actually in there. \r\n\r\nSo that's like kind of an error issue and The documentation might be right, but then when it's trying to pull from, it's like, I don't know, you know, I don't know exactly where it was referencing or where it was saying it, you know, it within this documentation. So I'll have to go ahead and remove that. Yep. That's another issue too. So now I've been having better prompts where I say. what I've been doing and I take them out I say go ahead and anytime you reference or using some sort of because before it wouldn't give the training it would just pull it would just take that the material and just build training I'd say reference put a reference at the end of every um the material that you "
  },
  {
    "id": "report_source",
    "chunk": "ve the training it would just pull it would just take that the material and just build training I'd say reference put a reference at the end of every um the material that you know if you're referencing any of these documentations but then I go in there and I double check it I don't keep the references in there a lot because then it just clogs up and it's like every other sentence it's just \r\n\r\nreference, but it allows me to go in and double check to make sure those references are accurate. And this is with chat GPT mostly. \r\n\r\nYeah. \r\n\r\nHave you tried deep research? No. So the it's it, the, the problem with it is it's wordy. \r\n\r\nIt would make you like a large report. \r\n\r\nHowever, it's citation and sourcing and references is pretty good. Um, only, uh, more it's less often. then more often is when I, you know, have that situation where you just explain where you open it up and there's no reference to it. So, give that a shot. One time, if you just open up Gemini . Google, click Deep Research and run it in parallel with whatever you're doing in the same, when you're doing like, when you want open source citations and stuff, Deep Research I think is pretty good at that. \r\n\r\nYeah, okay. I think what, let's shoot for like sometime next week or maybe even, I'm getting kind of bombarded with assessment stuff right now, but let's look at doing an actual module. I think this is like a key thing. The only thing I kind of just would suggest is the GUI itself is kind of, You know, just joint it. You're right. It's like, okay, you got to click here and then up here. \r\n\r\nYeah, yeah, yeah. You know, the buttons very close together. You're right. And then in a particular order, maybe that you'll usually, because I did like how you showed"
  },
  {
    "id": "report_source",
    "chunk": "re. \r\n\r\nYeah, yeah, yeah. You know, the buttons very close together. You're right. And then in a particular order, maybe that you'll usually, because I did like how you showed the highlight and that was fucking pretty cool. \r\n\r\nLike, okay, cool. \r\n\r\nBut it was just like here, here, here, here, you know, so. No, you're right. That's absolutely true. \r\n\r\nAnd there's another issue I noticed as you were doing it. When you tab away and tab back, the highlight goes away. \r\n\r\nSo I'm gonna have to make sure that that doesn't... It probably loses track of where you were in the... It's a persistence thing. \r\n\r\nIt's a typical, yeah. \r\n\r\nYeah. I need to make sure it's saved somewhere. Yeah, I do. I like the concept. Everything you've done is pretty cool. I think it'd be a lot cooler once... \r\n\r\nYeah, click on that spiral. You know what you should do? You should just have like an auto... So, you know... \r\n\r\nWe were, I guess that would be from the other response too, like an auto copy button. \r\n\r\nI don't know, like instead of, I did have like issues, you know, going in control A, control C, you could just have, you know, copy a clipboard button. So over on the right I do, but it's only for the individual file. You see right below sort, there's that tiny little clipboard button. But what you're looking for, that would just be for the file. What you want is for the whole prompt file. I could totally do that. \r\n\r\nNext to generate prompt, there could also just be a little copy prompt button. Easy peasy. But also click the spiral. \r\n\r\nI think that would help eliminate copy -paste issues or I don't know. \r\n\r\nNo, you're right. \r\n\r\nI'm all about reducing number of keystrokes. \r\n\r\nAnd yeah, going from three to one is exactly that. Open the spi"
  },
  {
    "id": "report_source",
    "chunk": " copy -paste issues or I don't know. \r\n\r\nNo, you're right. \r\n\r\nI'm all about reducing number of keystrokes. \r\n\r\nAnd yeah, going from three to one is exactly that. Open the spiral. I want to show you the settings because that is what's coming next. So up at the top right. Yep. So I've got a little changelog there, but you see that local API URL, the free mode and the local LLM mode at the top? \r\n\r\nYeah. \r\n\r\nThat's coming next. So basically, I've got a coding model on my local, so I can just build out all the functionality so that when we, as a company, have an official API key, we've got to do is scroll up and drop it in in that little URL and Bob's your uncle Bob's your uncle you just because you change the radio button from the free mode for the AI studio mode to the LLM mode and then it you don't you don't paste in responses anymore they just stream in yep exactly So that's on UKI proper. How much are these API keys going to cost? So it's actually per token. So every single token you send and every single token it sends you back is priced. \r\n\r\nAnd actually my little system has a little pricing, a total estimation cost right there. Sometimes it's zero. I think you've got to get it ready to write. So I think just write ASDF in the cycle context. You minimized it, which is, yeah, you can minimize the cycle. Yeah. \r\n\r\nOver on the left. Uh, I I've tricked myself. Yep. I'm like, wait, where did it go? Yeah. Click that. \r\n\r\nThere it goes. Uh, enter something in cycle context. I think it's just because there's a, there's a zero it's dividing by zero. And now that I'm thinking about it, maybe, maybe not. Um, okay. It's not definitely not doing well. \r\n\r\nAnd then now right in the new cycle, uh, just put something in new cycle. O"
  },
  {
    "id": "report_source",
    "chunk": "now that I'm thinking about it, maybe, maybe not. Um, okay. It's not definitely not doing well. \r\n\r\nAnd then now right in the new cycle, uh, just put something in new cycle. Oh, and the cycle title because it's, it's, it's trick. It's yes. Yeah, I think, yeah. The only thing I think I see is Asda go in there and do it. sure what you're doing and you're just running cycles repeatedly. \r\n\r\nYeah, that's why I'll have the cost up there. \r\n\r\nAnd also, that's why local models are so valuable, is the cost is zero. \r\n\r\nRight. Okay, so it's important that we mature our AI solution here. I think a great solution, too, with just kind of thinking out loud, As we're moving forward, like once we get it to help reduce costs, right, if we build a baseline for static content and lab, one of the questions, you know, when you go in there, it's like, okay, what are you building this for? And then it's like a checkmark, okay, static content, and it automatically just loads up all the appropriate templates, stuff like you need, you know, that master file for. static content, and then you could do labs, because labs are going to have not only the content, but then you're going to have instructor guides, et cetera, et cetera, for at least the content side. I'm thinking, so as you do this, I really need to fix this pricing. \r\n\r\nIt's actually not important because your records, what you create, you're going to create something that we can then reverse count the cost, what it would have cost to create this with APIs. So we're going to get that metric, actually, by you doing this. When you're done, you're going to say, well, it would have cost X to make module 5. We can do an AAR with your JSON file, and I can do that. I can make that data. Yeah, a"
  },
  {
    "id": "report_source",
    "chunk": "his. When you're done, you're going to say, well, it would have cost X to make module 5. We can do an AAR with your JSON file, and I can do that. I can make that data. Yeah, at least within the context. \r\n\r\nYeah, but on the first round, everything is refined, and then that gives us a number, a price point. You know, hey, $500 per lesson, right? So that's the price, and then that's what the user, the content user, a developer gets assigned in his API allocation. is the $500 to generate the course and you manage it yourself. You see your cause. You see, Oh, I'll just use one cycle for this one. \r\n\r\nNo big deal. And this one, I need more cycles. I'll up the responses and I'll get four responses. Yeah. Yeah, totally. We just got to make sure at that point, if it's upon that, that the people that are using it get really good proper training. \r\n\r\nThat's right. I agree. Cause you, you know what I mean? You start screwing up and you're like, Oh, I wasted 20 cycles. Cause of experience. That's right. \r\n\r\nYeah. And experience. Yeah. You see it, man. That's right. \r\n\r\nThat's the future is this, these skills, knowing how to work with AI because the power gain is immense. \r\n\r\nSo it's the difference. \r\n\r\nIt's almost like an Intel intelligence. It's like an IQ test for people. Roundabout roundabout because you genuinely you can you can be looking for the same product project you you and one other person But you just have the better words because you've learned the right words to use the egg with the AI that that's it That's IQ. That's you. You know, I think we need to make like, you know focus on you have a lot of the technical stuff, but how to make it a Simplify it, you know, and obviously every rendition every time you go through it"
  },
  {
    "id": "report_source",
    "chunk": "d to make like, you know focus on you have a lot of the technical stuff, but how to make it a Simplify it, you know, and obviously every rendition every time you go through it, you know, you discover some stuff today It'll make it that a little bit more efficient. \r\n\r\nYeah, so literally like the intro thing is like you can say, you know I guess what my mind is like I go in there and I have everything set up and I know how to use it and But it's like, okay, I want a new project. \r\n\r\nSo it automatically copies over the appropriate, you know, what kind of project is this for, you know, total, you know, you can have it basically three ways. \r\n\r\nYou could have, hey, I, I don't have anything right now. \r\n\r\nI am trying to create an overall course for static content and labs, like an outline, like kind of what we did today. \r\n\r\nStart with that. And then that then feeds into, okay, well, I have everything set up. \r\n\r\nI have all the ELOs lined up. \r\n\r\nI have kind of like what I know, what I want my lessons like. Let's start building the content. Click this box. create. This is a static content. These are the ELOs based off, we're giving some of the information and it automatically takes in all those templates and starts building out the thing. \r\n\r\nSo maybe the first one is, hey, based off of this, it goes and searches open source and it gives you a wide variety of open source stuff and you can kind of go through and spend a day You know what I mean? And then from there, it's like, oh, these are accurate. Go ahead and create a training using the template of, you know, we use for confluence, you know, based off of this. And then, you know, obviously there's more that goes into this because I like to have real world or hypothetical s"
  },
  {
    "id": "report_source",
    "chunk": "ou know, we use for confluence, you know, based off of this. And then, you know, obviously there's more that goes into this because I like to have real world or hypothetical scenarios related to the customer. Training may be different, but, you know, as far as the outline, I think that like, I think this is heading in the right direction. Yeah, you know compared to like well, it's been almost a month last time we kind of chatted so You know kudos on you man, like that's on some good shit. \r\n\r\nI think you don't even know man. Just gotta make it You don't even know I've made my own PCT. Yeah. Yeah that was so what I the way so what I did was once I had the version of my Extension I needed to test it. I needed to beta test it. So I started a project to beta test it So it's the first project I've made with my own extension And I call it a virtual cyber proving ground. \r\n\r\nAnd it's actually a PCT environment, you log in, it's got a range, spins up in Docker, it's multiplayer, there's a chat lobby. So you create a team, and then you can spin up a scenario of multiple scenarios. The first scenario I've made, it's a 10 minute or so long scenario where you've got to sort of play like cognitive defense, cyber defense for your UAV fleet, the engineers hacking into your UAVs. You've got to remote in and change their encryption, their certificate, or you've got to change the frequency that they're connected to. And you can also brute force and attack, hack into, it's a team mission. \r\n\r\nYou're actually writing SL commands in a terminal. \r\n\r\nYou connect to the drones and you fix them, or you brute force the enemy drones And it's AI integrated So you can highlight the word SSH and you get a little tooltip pop -up says, you know, you ca"
  },
  {
    "id": "report_source",
    "chunk": "ones and you fix them, or you brute force the enemy drones And it's AI integrated So you can highlight the word SSH and you get a little tooltip pop -up says, you know, you can ask Jane I've modeled it like ender's game from battle at battle school So you can it'll tell you contextual what SSH is so like you just highlight SSH and ask It'll tell you in this mission. You'll use as SSH is this and you'll use it for this Yeah Yeah, everything every single text you can highlight and ask the AI on you can create Intel chips that get shared with your team. In other words, like it's like a little sticky note that you can create you highlight anything you can create an Intel chip, it'll go to Jane Jane will process it and she'll turn it into something contextual for the lesson. So like when you find the drone manifest that has all the drone IP addresses, with the drone names. You can highlight that whole thing and then turn it into an Intel chip and now your other teammates don't have to go find the drone manifest. They can just click on the Intel chip on the right table. \r\n\r\nYeah, dude. Yeah, dude. \r\n\r\nYeah, dude. So yeah, no shit, no shit. And then at the end, after action report, we'll be able to discern like the skill bases of the users because it's multiplayer. \r\n\r\nYou get points for doing who hacked what, whatever. \r\n\r\nBut also you get two terminals and we can determine who used two terminals. Who did multitasking? Who did the same task on two terminals? \r\n\r\nWho did different tasks on two different terminals? \r\n\r\nVersus who just used one terminal? How fast did you type? Because I have this system now, I can just talk to the AI with my extension. and say, hey, now we want to be able to measure their typing speed. Hey, now w"
  },
  {
    "id": "report_source",
    "chunk": " fast did you type? Because I have this system now, I can just talk to the AI with my extension. and say, hey, now we want to be able to measure their typing speed. Hey, now we want to be able to measure their multitasking abilities. Here's the system, how can we measure it? \r\n\r\nOh, the AI's got a PhD in psychology, so it can help me come up with some very good metrics, and we've already got the whole system in front of it. So yeah, dude, that'll be my next demo day. \r\n\r\nYeah, so that's what I did the past week, and then I got to a stopping point there, and then I turned around over the weekend to make all the changes to the extension that I found and discovered during that beta test cycle. Yeah. Yep. Sweet, man. Yeah. After an evening, 30 minutes, I'd love to show. \r\n\r\nNo one's seen it yet. No one's seen my virtual cyber proving ground yet. \r\n\r\nSo I'm anxious to show it to someone. \r\n\r\nSo I don't know who. Yeah. I'll take a look at it. If you want to. Yeah. I mean, it's on my other computer. \r\n\r\nI'd have to switch. Yeah, let's plan out, because I've got to get rolling on a couple of other things here. \r\n\r\nYeah, anytime on Discord, because Discord's on my personal. \r\n\r\nJust message me, and then I can share screen. Yeah, I think, again, make sure you're showing how to make it relatable within our stuff. So I think if we can get at least, you know, we can spend an hour or so whenever we meet next, and then work on Lesson 5, and I'll have to do some prep work to start. What I'll do is I'll gather the open source resources. \r\n\r\nWe can skip that step right now that I would want from it and then just create a file structure. Basically, I don't know if we'd have to create a whole new database based on each module, how that woul"
  },
  {
    "id": "report_source",
    "chunk": "ht now that I would want from it and then just create a file structure. Basically, I don't know if we'd have to create a whole new database based on each module, how that would work, or you just create a folder and say, hey, we're working strictly on this. Maybe that's something you got to think of, but I'll gather resources and then what we'll do is we'll kind of work together on just showing proof of concept right yeah hey but yeah I'm gonna tell you a Brian's not gonna like it because we're basically showing a way to create a lot of training just from AI but you know dr. Scott Wells is basically on board so going back to yeah there's a lot of human interaction that needs to be involved in this process so I just keep that in mind Otherwise, you might get a lukewarm response to it. \r\n\r\nBut I think if we can show, hey, I provided the resources and then have it reference what you've got to do as an instructor going in, as a content creator. \r\n\r\nThe fear with all this, my extension solves, which is the ad hoc interaction with AI. Company, I know I I understand I'm just saying some people are still rebellious against it. That's you know, just keep that in mind I know I know so I wouldn't get a get offended. \r\n\r\nNo, I definitely working smarter not harder Yeah a hundred percent a I can pop is a million times smarter than I am as far as like there's no way I could produce the content I produced without AI. Yeah, you know to me I So but using it smartly, right? So yeah, a power token can drill your leg if you're not. Yeah, yeah, so that's all you know. I totally understand and I'm on board with what you what you're doing and you know. Yeah, just keep pushing I guess and then let's do let's focus next one will focus on. \r\n\r\nSo "
  },
  {
    "id": "report_source",
    "chunk": "totally understand and I'm on board with what you what you're doing and you know. Yeah, just keep pushing I guess and then let's do let's focus next one will focus on. \r\n\r\nSo what as you're kind of going through like before next one, just let me to tell me what you need for me as far as like I said, I'm going to get all the open source references. I'll have the yellows I'll have. Anything else? I guess, I don't know, kind of like what I do when I go through the writing style content updates. And then we can just go from scratch. I'm not going to use the existing. \r\n\r\nYeah, I was good. I was thinking that. I want to use my existing chat GPT input. I want to test Gemini and start from scratch saying, hey, I want to build a lesson on module, you know, a lesson. whatever this lesson that's based on these LOs and open source resources, this is the template I want to use, and let it figure that out. And what I like also, if possible, I don't want to give you more work, but if you think of it this way, if we had like two examples of the NC DOC content, one is your original approach that you were going to do anyway, as if I didn't exist, And then another one that, you know, the same modules through this process. \r\n\r\nSo where the two, they didn't touch each other. The only thing that touches each other is this initial reference documentation that you may have used in the both. Yeah. References and ELS. Yeah, no, we can do that. I'll mess around with chat GPT. \r\n\r\nThat might help alleviate some concerns. What I'll do is I was going to work on lesson four, but I'll move over to lesson five. Uh, Intel Threat, let me see. I'm just looking to see what has a lower amount of ELOs right now. It might not be necessary, I mean... No, it's "
  },
  {
    "id": "report_source",
    "chunk": " I'll move over to lesson five. Uh, Intel Threat, let me see. I'm just looking to see what has a lower amount of ELOs right now. It might not be necessary, I mean... No, it's just work content for me. I can knock out less than 5 a lot quicker. \r\n\r\nYeah, there's only like 10.. . 3, 4, 5, 6, 7, 8.. . \r\n\r\n8. Again, I'm trying to do small scale. I don't want to do a lesson that's 20 ELOs because it's going to be long as hell. I'm just, you know, especially for demo purposes. And not only that, the moment you have one done, it becomes an example for the rest. Yeah, I'm just going to use... uh yeah because but here's the thing this one has a lot of like here's the elos to me \r\n\r\nregular expressions for Splunk, develop regular expressions for Splunk automated tasks, identify how regular, this is based off of introduction of threat hunting and advanced analytics, create basic Sigma rules, identify, so basically after you have developed, you found a threat, it's how do you, Update your signatures and your automations within second onion elastic and stuff to stay on top of those guys You know me. Yeah, so yeah, it shouldn't be too hard. It's gonna be more technical than it is Yeah. Oh, yeah, man. So like oh so for example as you're going through you're gonna come up with a template like each Section is gonna need its image. So you're gonna have it make an image prompts for each section, right? \r\n\r\nYeah. Yeah, these are the yellows for section 5 So it's not a whole lot, but just so you can get your brain thinking, because these, here's the thing. These are a lot easier to teach because it's just, that's it, right? \r\n\r\nAs opposed to like introduction to CTI, where you got to kind of more tell a story. So let's give it this one becau"
  },
  {
    "id": "report_source",
    "chunk": "ot easier to teach because it's just, that's it, right? \r\n\r\nAs opposed to like introduction to CTI, where you got to kind of more tell a story. So let's give it this one because this might be difficult because I'm expecting it to give me examples on how to's. \r\n\r\nAnd I write that in my direction. \r\n\r\nLike if it's an ELO that requires you to create something, make sure to, Process that out and it easy to flow how to And then also give a conceptual idea of it and then also put it in an example of why this is important for them as a going to a Navy ship So the thing is to keep in mind, but those are different, you know, I go to the next training those requirements are going to change and \r\n\r\nright? \r\n\r\nLike a different whatever, so. Yeah. \r\n\r\nYeah. \r\n\r\nWell, and also we can do it on the existing one too, actually. I don't know if you would rather do that, but here's the thing. Actually, this might be easier. \r\n\r\nTrib already did. This one's here, the yellows for, those are going to probably be clickable links to a Confluence page, but this one is more less, Technical and more let's just like the concept of ideas. Yeah. Yeah All that is basically gonna be almost even like in the pre -training You see how those two lessons are different like that first one's gonna be just really strict technical stuff Yep, where the other one is it's really like okay explain cyber threat intelligence. \r\n\r\nWell, there's probably a million different ways You could explain it. \r\n\r\nYou know what I mean? \r\n\r\nYeah, so that's really putting the AI to work like and then the consistency is what I can you know how it explains it in 6 .1 .1 I want to make sure it stays along that line with 6 .1 .2 and 6 .2 .1 you know and then so it looks like it's not "
  },
  {
    "id": "report_source",
    "chunk": "istency is what I can you know how it explains it in 6 .1 .1 I want to make sure it stays along that line with 6 .1 .2 and 6 .2 .1 you know and then so it looks like it's not one smooth training almost definitely yeah the way you'll get that is this holistic approach that's where people lack that as they do piecemeal and then the AI doesn't know what was in step you know module one versus module two And then you just get repetition. \r\n\r\nBut this approach, even when you're working on module one, because module two is in context, it won't be repeating it. It'll know. Yeah. \r\n\r\nAnd most of these will be combined. Like this one, we combined it, like the first three, it's like one. And just because it's one paragraph can cover multiple ELOs, depending on what it is. All right. So also it's figuring out how to combine \r\n\r\nnot each one needs its own individual training, they can be put in together, right? \r\n\r\nSo... to automate Splunk tasks, but also what will be the Regex Splunk tasks on the ship. \r\n\r\nThat, it won't know. Maybe you'll make a lit, maybe you'll make some artifacts that just outline some of those tasks that, yeah. And that can't come from anywhere else. And then once you've got it outlined as an artifact, then every reference will be in that line. You'll get that sort of, yeah. Something to keep in mind if you want to bring up Splunk and like Plastic, they're really good. \r\n\r\nThey have like their online libraries and their official documentations. I mean, they have a lot of it, but I've been downloading a lot of it just to have and it's a great way like instead of always having to try like, hey, I basically have the whole Splunk how -to from Splunk. Granted, it's probably fucking large as hell, but... \r\n\r\nIt's a g"
  },
  {
    "id": "report_source",
    "chunk": "at way like instead of always having to try like, hey, I basically have the whole Splunk how -to from Splunk. Granted, it's probably fucking large as hell, but... \r\n\r\nIt's a great resource so you don't have to continuously go in and try to find other resources or anything elastic. Elastic is really nice because it's either Splunk or Elastic. Actually, if I find documentations, I just have a quick download button right there. I can download it and put it as a reference. Some Splunk stuff didn't have that and it wouldn't go over copy well in PDF. So I had to either do screenshots or \r\n\r\nactually downloaded a plugin that automatically screenshots the whole webpage and pieces it together, which is nice. \r\n\r\nThose are like limited cases. So when I was at the training over the week, one of the students, he was really interested in some of the AI stuff. And over the past week, he took my extension and he made a Slack bot that he trained it on some JQR stuff. so it can help his team, they can ask it questions on JQRs. But what he did was, what I was talking about before, which was like that big file that you had, if we had an embedding, a RAG, that on -the -fly tooling, that's what he made. He has a local LLM, it's an embedding model, and then he has just a local Google model, and those are the two models that he needed, and so he's got his own little Slack bot, now his team can ask questions to it, And he's taught it with all the, you know, rag or whatever with the PDF. \r\n\r\nAnd so that's, so that's what I did at Palo is I downloaded all the publicly facing Palo Alto PDFs on their website. It was like 52 different product PDF documents. And I just appended them one after another after another. And then I just embedded them all. "
  },
  {
    "id": "report_source",
    "chunk": " Palo Alto PDFs on their website. It was like 52 different product PDF documents. And I just appended them one after another after another. And then I just embedded them all. And then out came an AI that just knew all the Palo Alto products and where to click and what to do and the difference between XDR Android and XDR official. I'm saying like these spunk ones. \r\n\r\nI would go through I could just get the master thing and say hey use this as a reference and use this Examples that has a lot of good shit in there. \r\n\r\nBut yeah, um, hey, this was a good I know it's been about a month. There's a great update There's devil and then you've been definitely killing it as far as you know since last time we used it But yeah, let's get something that \r\n\r\nweek and then we'll work on, we'll do, how about this? \r\n\r\nWe'll kind of work on one that's already existing. \r\n\r\nWe'll just do less than one with the CTI stuff and see how well it comes out compared to what we have. \r\n\r\nAnd maybe it might surprise us and change up some content. So, yeah man. Let me know, just keep hitting me up, anything you need from me. I just posted a picture. I don't know why. It re -imaged it. \r\n\r\nYeah, that picture there is like this plugin you can put in. \r\n\r\nIt will automatically just screenshot a whole page and stitch it together, which is pretty cool. So if you're having a hard time getting all the information from the reference, you could just use that plugin. \r\n\r\nAnd then, like I said, it'll copy the whole webpage, which is pretty cool. I'm sending a couple of screenshots I got already that I had lying around. Oh, shit. Right? That's pretty cool. And then there's a login page somewhere. \r\n\r\nI got saved. There it is. That's pretty neat. That's pretty c"
  },
  {
    "id": "report_source",
    "chunk": "lready that I had lying around. Oh, shit. Right? That's pretty cool. And then there's a login page somewhere. \r\n\r\nI got saved. There it is. That's pretty neat. That's pretty cool, man. I could imagine us making those scenarios. Are you just hosting that locally? Yeah. \r\n\r\nAnd it's using, so Docker, and actually we did the maths. I could host of 10, 50 people concurrent on just one laptop. Then I could just use the second laptop and double that. So I could do 10 different scenarios. Huh. Full. \r\n\r\nYeah. That's pretty cool. That's right. That's right. Because that was one way. Oh yeah, well the problem is it's not hundreds, eventually it'll be thousands and tens of thousands. \r\n\r\nWell, the problem is we're supposed to be using PCTE because that's what the government invested a billion dollars into maintaining. And the problem is, I don't know when you came on board, but we did use actively use PCTE before where you just focused on Bravo. But the problem is we could only get an update like once a month and it was broken half the time. So the government does come out, you know, we're using Bravo, but they say, hey, we have to use PCTE and we got to use PCTE. You know what I mean? Yeah. \r\n\r\nYeah, but yeah, no that was again that I that was just I chose that it was more about testing the extension and we can cut it here. I Made that because that's what Eric was trying to make. He showed me his project He was actually using an agentic coding tool and he was trying to make like a training platform Similar to similar to this so to show him I made this and I will show it to him and I can even I can just hand it over to him and Because all my cycles and all my code, I just hand it over to him, and now he's just running with it, and"
  },
  {
    "id": "report_source",
    "chunk": "will show it to him and I can even I can just hand it over to him and Because all my cycles and all my code, I just hand it over to him, and now he's just running with it, and he's developing for free. Now he just follows my process. That's exactly what you wanted, Eric, right there. There you go. \r\n\r\nAnd the transfer is what's key. It's just instant transfer. We could talk all day on it, I swear to God. I hear you, man. I've been trying to do better at clocking in on my time because I get stuck on things and then I'm like, shit, I start falling behind on certain things. So I'm trying to use the block scheduling as much as possible to get my life somewhat organized because things are starting to compound with NCDoc stuff. \r\n\r\nYeah, man. Yeah, let's get back to get we'll schedule something for next week. I do have a couple of appointments next week, so I'll just touch base with you. We'll try to shoot for like another Monday and we'll go from there. Yeah. And if we if we push it, we just push it to the next week. \r\n\r\nIt's no big deal. Yeah, sounds good. Just as you're coming up with stuff and knowing that we're kind of moving actually into the static content now with let me like if something comes up like, hey, just keep in mind X, Y, Z, you should, you know, have this ready, you know, so I can kind of get all those things together. So I just put, I'm going to be pulling open source projects, resources, ELOs, and we already have all that stuff for that project, which will make it kind of, will make it easy. So, and then we'll go into, I'll start kind of thinking to what I would be asking for cycles, right? And I'll just kind of reference what I'm kind of doing with chat GPT So that should make it kind of easier as far as "
  },
  {
    "id": "report_source",
    "chunk": " of thinking to what I would be asking for cycles, right? And I'll just kind of reference what I'm kind of doing with chat GPT So that should make it kind of easier as far as like I'm gonna be asking. \r\n\r\nOkay. Yeah, here's a good idea. Here's a good idea The guy I was telling you about he's like, you know One of the one of the students he was doing something similar using chat GPT and using this and he got into a point where he came up with the solution in his chat GPT, but then he was struggling to bring that solution, the context of that solution back into AI studio. And I suggested to him, which is probably going to be very useful for you, is whenever you're trying to move back and forth, in my extension, there's the ephemeral context section. Yeah. So you could literally take the chat GPT response, like whatever has the whole response and drop it in that ephemeral context section, and then refer to that in your cycle context, say in the ephemeral, I put that chat GPT that solved the problem, and then blah, blah, blah, it'll only be there in that cycle. \r\n\r\nAnd then moving forward, it won't be cluttering your context. Right? Yeah. Just a way to use that because he didn't think about it until and I didn't, you know, Well, I suggested that to him, and he's like, that's perfect. That's exactly how he could get the context back in. Yeah. \r\n\r\nAll right, man. Love it. Sounds good. I'll let you go, my friend. Thanks again. I enjoy sharing. \r\n\r\nAnd yeah, in the evening at any time, hit me up. And then when you're not busy, and I'll just click through this VCPG thing that I'm messing around with. Sounds good. Cool. See you, bud. See you.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcrip"
  },
  {
    "id": "report_source",
    "chunk": "is VCPG thing that I'm messing around with. Sounds good. Cool. See you, bud. See you.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-7.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nGood. \r\n\r\nHow are y 'all? Heck yeah. Now's the time, man. I think, yeah, that's really cool. You remind me of myself and my roommate. His name is Matt. He now works, you know, cybersecurity. \r\n\r\nHe's actually deployed some AI solutions. So like getting an enterprise, an actual API that it can actually use so its employees can actually start making LLM calls that are like actually paid for by the company and not just like oh, I'm just helping do my work in a chat GPT window So API API, are you familiar with that that term? \r\n\r\nCool? \r\n\r\nNo problem. Good good No, it's it's important that we are clear and you ask quite don't feel like you think it's a stupid question I just need I just need to know where you're at to get you up. You know what I mean? If I talk over you it's not very helpful. \r\n\r\nThat's fine. \r\n\r\nThat's good. \r\n\r\nOkay, you won't need it Yeah, you won't need it. The beauty of this process is you get to, I envy you, you get to learn everything with AI. I had to, when I was 18, 18, 19, 20, I wanted to make my own Lineage 2 game server, it was my favorite game. And I had to learn literally everything, how to set up a server, everything from scratch by just reading documentation, reading forms of other people trying to set it up. And believe me, setting up things back then, 12 years ago was not, anything like it is now. \r\n\r\nAnd so you with AI, you're going to get not only like in every having a professor in your pocket, every answer you want, but also on demand the best answ"
  },
  {
    "id": "report_source",
    "chunk": "g like it is now. \r\n\r\nAnd so you with AI, you're going to get not only like in every having a professor in your pocket, every answer you want, but also on demand the best answer possible. That's what's the best answer possible. And if it's ever wrong, so is a professor sometimes. What's your point? But then also it's literally the only time you're asking a question is It's when you're actually learning, you need to learn something. And so when it gives you something, it's not working, you're still learning. \r\n\r\nThat's what the problem, most people will stop at that point. They'll feel like they've failed or something when the actual learning is in the continuation of the process, right? So just sort of setting a baseline. Okay, so y 'all can see my screen. So kind of the, this is a proof of the product. So from just prompting, let me just reopen this. \r\n\r\nWhat is A . I . Synth? A . I . Synth is a business simulation game where you take on the role of a founder in a fast -paced, high -stakes world of artificial intelligence. \r\n\r\nYou manage every aspect of your startup from groundbreaking research, account acquisition, building out massive compute infrastructure, launching world -changing AI projects. Some of the cycles, what I've done, older cycles, just like a changelog, right? But cycles are the key to how I am able to build such a thing and not just get limited by say like, oh, my fuck, okay. The first project I made was a Slack bot. That's what I believe y 'all are gonna wanna make. \r\n\r\nIt was only about a thousand lines of code. I could even open the file. It's a Python file and I can share it with you. You can literally take it and then run with it and even go further with it because back then we didn't have local m"
  },
  {
    "id": "report_source",
    "chunk": "n the file. It's a Python file and I can share it with you. You can literally take it and then run with it and even go further with it because back then we didn't have local models. We didn't have local thinking models like we have now. You can get, on your GPU, you can install the OpenAI open source model. \r\n\r\nFinally, they're truly open. They've been closed AI until they finally open opened release the open source model But I believe you're it'll fit on yours and that can be your AI answering questions for you So it's all you so that solves like sort of a Cooee problem. You see what I'm saying? You're not sending API calls to External it's all on your it's the Cooee is on the same device that that that your PDF is on you see I'm saying so So you're installing your own weight, your own actual AI model. So I have actually that software up over on my server that's running the game you're looking at. I'm remoted in and I have, this is a tool called LM Studio, which is just a fantastic tool. \r\n\r\nI recommend you download it if you want to install your own AI. There's obviously your own AI won't be as good as what Google can produce, right? \r\n\r\nI just thought of something. \r\n\r\nHold on. Where's my desk? \r\n\r\nOh, there it is. I think I can also turn on the thing. \r\n\r\nThat one? \r\n\r\nSay hi to you. \r\n\r\nHi. \r\n\r\nOkay. All right. So this one is the, OSS is the open AI one. It would fit on your 16 gigabyte. Then they made it that way. I believe they made it to fit on 16 gigabytes. \r\n\r\nSo you're good to go there. I've been trying to get it running. I haven't been able to get it running. I haven't tried too hard because I've just been using Quinn, which is a Chinese model. I do not recommend you using a Chinese model, but the Chinese mod"
  },
  {
    "id": "report_source",
    "chunk": "ble to get it running. I haven't tried too hard because I've just been using Quinn, which is a Chinese model. I do not recommend you using a Chinese model, but the Chinese models are better. So for me, for my game, I'm making a game. \r\n\r\nI'm learning. I could care less about it. I'm not under threat by the Chinese model because it's a big race. It's a big race. China's not gonna the the dangers in 30 years when everyone's been Using the Chinese model and it's so integrated and it's so much better now that the moment They don't want us to use it. We won't use it in some you see what I'm saying? \r\n\r\nBecause it's their model they have that they have the they have whatever backdoor they have in there to make it not so then they just were crippled instantly because we were absolutely using all business as usual, which is using the cheapest, most efficient thing. So that's the long term. So I don't care for myself. I'm going to try to make, you know, use it for non work related things. So, but for you, for your project, I would definitely go with either Gemma three, 12 billion, which is the Google model. \r\n\r\nIt's not bad. It's just not a thinking model. And this one is yeah. Gemma three, Gemma three, the 12 billion. That would fit on 16 gigabytes. So you can just download those two models and fiddle with them This one will be much easier for you. \r\n\r\nIt's it's uh to just use um, but this one might be I don't know I don't know i'm hearing I haven't used it. It's it's uh, What makes it special over this one is it's got more features. It's got two specific things Really the two things that truly matter whether or not it's a good model or not. Uh still it matters as well If it's a bad model, it's got the features who cares If the "
  },
  {
    "id": "report_source",
    "chunk": "hings Really the two things that truly matter whether or not it's a good model or not. Uh still it matters as well If it's a bad model, it's got the features who cares If the other model without the features are still better But the features are thinking it can it which is another in other words the chain of thought it prompts itself before it answers you so like it thinks on your question first just all it's doing is just Given a little thought block and it writes in there first so it can plan accordingly and then it you know And then it'll respond to you. So just like a human if you think before you speak the answer usually comes out better So that's the one feature And then the other feature is mixture of experts. And what that is, is that allows you to have a much bigger model, but you only activate certain experts. \r\n\r\nThey call them experts. There are expert data sets. So for example, this one right here, coin three, 30 billion. A3B. That 30B is the 30 billion parameter. So it's 30 billion parameters, which is a lot of parameters. \r\n\r\nBut only 3 billion are active at any given time. And so what's going on behind the scenes there is it's got a bunch of expert data sets, but only a certain some of them are active at any given time based off the question, which makes sense, right? You don't always need The rocket scientist to chime in, right? Maybe. So it's a good solution. It's a great solution. \r\n\r\nIt's fantastic. And so this is a fantastic model. \r\n\r\nQuint 30 billion, but it's a Chinese model. \r\n\r\nThis would be, in my opinion, second best given your hardware. And then this is probably maybe a bit more spicy, maybe a bit harder to set up, but maybe, maybe, maybe better because it's got those features that Google doe"
  },
  {
    "id": "report_source",
    "chunk": "n your hardware. And then this is probably maybe a bit more spicy, maybe a bit harder to set up, but maybe, maybe, maybe better because it's got those features that Google does not offer. So, okay. Just that's, now that's just all there is to the LLM. Just use LLM Studio and get those couple, get this one and that one. And this is a small one actually. \r\n\r\nSo this one is, this one is, what is special about this one? It's smaller. It's really small. It's basically, it's pretty smart and it's pretty small. It's smart for its size. \r\n\r\nSo there are, so that would be a third good one to get. \r\n\r\nThere are, over time, and you can literally just not worry about it, because over time, you'll discover there may be sub -workloads, maybe a smaller local, maybe. \r\n\r\njust discover these over time But yeah, just knowing it's a good smaller model is enough just knowing it exists So if you ever think you need one, yeah, you know, which one to go to is all but it's not important for yeah hard to say Definitely just okay. Okay. Okay. Okay. I'm trying to give you what you would care about but generally it would be like cost so like The smaller models can run on smaller machines. They can be cheaper. \r\n\r\nThey're more efficient. But also, then the tradeoff is they're not, you might be asking a question that the smaller model cannot handle. So then in that case, you need to upgrade to the larger one. But with cost being a driving factor, which isn't, see, that's not our case. That's why I'm not really saying it's not too important to you guys, because cost isn't too important to you guys, but the answer to your question is cost. Yeah, yeah, yeah. \r\n\r\nAnd then there's also, I've got this other one. This is the audio. It's called Kokoro TTS se"
  },
  {
    "id": "report_source",
    "chunk": "ortant to you guys, but the answer to your question is cost. Yeah, yeah, yeah. \r\n\r\nAnd then there's also, I've got this other one. This is the audio. It's called Kokoro TTS server. It's actually just Kokoro FastAPI. It's a Docker container. So you can just search this one down. \r\n\r\nThis is to give your AI a voice. Yeah, exactly. And it's actually ridiculously low overhead. It's so ridiculously low overhead, the moment you do it, you'll be shitting yourself, why didn't I do this sooner? It's so easy to do because you because everything is just text already and then all you do is say read that because it's already it's already generated You just read that and it's it's actually really good at reading. You don't like it just goes part III right here It says III which I could just change the III to three if I wanted to You know, no big deal. \r\n\r\nI don't have to have Roman numerals, right? So yeah, so That's that that's that that's the virtue. That's that. Okay. So how is this built? So now here's the foot. \r\n\r\nThat's the flip side That's that's the local and it's nice to have your own local because it's fancy and local. However, this is the thing local is not as good as foundational and the foundation was like the chat GPT -7 and whatever is the latest and then so those are going to be the smartest things available and those are going to be the ones that you want to use when you're working and When you do, you want the smartest AI you can possibly get your hands on at working on your problems. And so how is this game built? This game is more than just a simulation. It's an experiment. AI Ascent was built in approximately 110 days by a human curator working in partnership with Gemini 2 .5 Pro. \r\n\r\nThe project contains over 60"
  },
  {
    "id": "report_source",
    "chunk": " simulation. It's an experiment. AI Ascent was built in approximately 110 days by a human curator working in partnership with Gemini 2 .5 Pro. \r\n\r\nThe project contains over 600 ,000 tokens of code and 350 ,000 tokens of documentation and was developed for a total cost of $0, as all work was done in Google's AI Studio. is a living testament to the vibe coding to virtuosity development pathway showcasing what's possible when human vision is amplified by artificial intelligence. And while this says the interactive report viewer is coming soon, it's actually already made. And so is actually, oh, I don't know if spectator mode is made, but remember that PVP battle I showed you, that battle I showed you? Well, actually, I made PVP mode. So once two people, it's multiplayer, once two people have that game AI, you can challenge each other, and then one person is the red, \r\n\r\nAnd one person is the blue and you both watch the same game in parallel, so I've made that as well. I've actually so Just and then that's kind of when I stopped making the game and started making the report viewer. Which is this one more Which is basically? the printing press 2 .0 So I don't know why it's not working in Chrome, but you know what it doesn't matter if I'm gonna look at that How weird is that? Okay. It's not initializing because I would have to close my browser and ain't nobody got time for that. \r\n\r\nBut the React should have loaded actually. \r\n\r\nI don't know why the React also isn't loading. But I think I can close my browser and reopen it. I think I changed the setting. So I'll just do that. Yeah. Life is good. \r\n\r\nYeah. Okay. But why did I want to go here? Oh yeah. So the AI would actually talk. I don't know why. \r\n\r\nOkay. \r\n\r\nThat's the dif"
  },
  {
    "id": "report_source",
    "chunk": "So I'll just do that. Yeah. Life is good. \r\n\r\nYeah. Okay. But why did I want to go here? Oh yeah. So the AI would actually talk. I don't know why. \r\n\r\nOkay. \r\n\r\nThat's the difference. \r\n\r\nOkay. \r\n\r\nClear that. Yeah. So ask me anything uniquely about American solution. What's unique about the proposed, the solution proposed in this report? Oh, let's see what's wrong. Now it's fixable. \r\n\r\nOkay, so it loaded the embedding model to read the database. Let's just try again. Let's see what's from here. Just a little troubleshooting. That's good to see it as well. That might be it. \r\n\r\nAnd then let's try this as well. Okay, so also this way. So let's try to fix this with AI. So you'll see a bit of the process. So what I'm going to do first is I have my game project here. Yep, this is the AI Ascent game. \r\n\r\nAnd so I have my extension, the one that I will be sharing with you guys. And so I haven't used it for my game yet. So fix the, fix the, fix Ascentia. She is not, she is not responding. Let me actually, I can actually do it this way. Yeah, that's fine. \r\n\r\nGenerate, oh, first, oh yeah. So, I, I, technically the first step is, this is the second step. The first step is to curate your data. First you have to curate your data, and then you can ask questions about it. So I, I, I, let me do that first. So let me clear this out. \r\n\r\nSo the list of sample documents, I was, I was test, I was making it, ingest . pdfs in Excel. So all I'm going to do is select my source directory, because that has all of my code in it. I don't need any of these other files. I will take some of these infrastructure -related files, such as the config files, the webpack files, and the package . json, the README. \r\n\r\nThese are all very good files to have "
  },
  {
    "id": "report_source",
    "chunk": "l take some of these infrastructure -related files, such as the config files, the webpack files, and the package . json, the README. \r\n\r\nThese are all very good files to have in my context. \r\n\r\nBut there are some that I do not want and I do not need. \r\n\r\nNow over here you can see that the token size. So this file is only 641 tokens thereabouts and that the token count is important because you can't send more than a million to Gemini. So you see I'm over 2 .56 million but I also have a bunch of files that are just actually not code related. This is just a text file that actually you can see it's actually my prompt from some other project's state. So I actually That's in my repo somewhere, so I don't need that. Same thing here. \r\n\r\nIt's another prompt. I don't need that. This is a WAV file. I don't need that in my prompt. Settings JSON file. It's just a bunch of zeros. \r\n\r\nI actually don't even know what this file is for. But it's there and I'm gonna yes and it changes over time and in fact just last night For the first time another model has been released and no one knows who's it is, but it's probably Elon's and it's got 2 million token count It's got secret names. It's on the Arena web arena right now people are speculating. It's Elon's because when they ask it it says it's XAI. So yeah, it is his yeah Yeah, I'm trying to solve the problem, and I'm going to solve that problem. The AI is not responding to us right now. \r\n\r\nAnd so, yeah, this is my code. This is the code for my game. And the game has a problem, just as if your project would have a problem, right? Your code project. See? It doesn't matter that it's a game that I've created. \r\n\r\nYes, I've created this extension. Precisely, yes. That is, yeah. And I wasn't c"
  },
  {
    "id": "report_source",
    "chunk": "oblem, right? Your code project. See? It doesn't matter that it's a game that I've created. \r\n\r\nYes, I've created this extension. Precisely, yes. That is, yeah. And I wasn't clear. It's hard to be clear. Yes, that is my intent. \r\n\r\nYes, thank you. Yep. Yes, every prompt is training. That's what people don't comprehend. Yes, actually. every prop so and let's look at that and you'll see that soon when we go back when I click here You'll see what happens So we're getting that's step three. \r\n\r\nSo that's step three any questions about step zero right now You don't you don't you don't need to man. You don't it's literally like a human Talking to a human that just knows answers don't worry about that that you know, don't there's nothing to know about it anymore That was my first fear That's what I started getting into AI three years ago. So I knew nothing about it up until that point, because goodness gracious, machine learning, talk about like the ultimate, like hardest thing to comprehend. Then generative AI comes along. And then I hear two things. One, people are starting companies with it. \r\n\r\nAnd two, people are writing code with it. I'm like, wait a minute, wait a minute, wait a minute. That's different. Oh, and so the third one, someone started a crypto with it. So I was like, okay, okay. Okay. \r\n\r\nSo something is here. This is different So I asked this one simple question If it can write what's the most valuable thing text that can be written if this thing can write text the answer was code Simply because code kids objective anything else is subjective like an essay a novel. It's all subjective I can find you some one who will critique that whatever that novel but Code, it's functional, it either sets out, it does the "
  },
  {
    "id": "report_source",
    "chunk": "ctive like an essay a novel. It's all subjective I can find you some one who will critique that whatever that novel but Code, it's functional, it either sets out, it does the thing it's supposed to do or it does not. And so it's like verifiable. And that's the errors that you get back, the console logs. And so there you're going to see me get that console log that I get, put it in, and then the AI is going to churn on that, the content, the context, which is all the code files. \r\n\r\nI don't know, I don't fuck it, I don't read code. I know English. That's the only language I know. I don't even know a pro, I couldn't write an if statement to save my life, right? Okay? The AI will handle that part. \r\n\r\nIt's my job to have the taste and the gumption to like push through and see the project to completion. So what all I'm doing now is I'm just using the date. So my extension is two windows. It's this left window. which is an evolution of just the file of the Explorer, right? You see you don't have the token count, so you're blind to the most important metric, okay? \r\n\r\nAnd then so over here you have it, and then you can also sort by token count so you can see what is the largest file, because there's not only, there's not just input token limit, but there's output token limit as well. The AI can only output so much in a cycle, in a response before it cuts off. And that number for Gemini 2 .5 Pro is 65 ,000. And so simply put, if you try to ask the AI to output a whole file and you don't know that the file is larger than 65 ,000, you'll be just struggling, struggling, struggling trying to get it to output the file when what you should be doing is refactoring that file and splitting it up into multiple separate files so that the "
  },
  {
    "id": "report_source",
    "chunk": "struggling, struggling trying to get it to output the file when what you should be doing is refactoring that file and splitting it up into multiple separate files so that the AI doesn't have to repeat the whole file every time you just need to make a simple change to some part of it because that part is in that part's file now. It's been organized. You'll see that soon. \r\n\r\nOnce I click this button, I have an artifact with the training. You said training. \r\n\r\nI have an artifact that I've already battle -tested in my projects to help refactor it. \r\n\r\nIt's a refactoring template. And as a user, the first time I did a refactor, I just knew the file was getting too big. I didn't know all the details I just explained. I just knew the file was getting too big. Maybe that's why it can't output it anymore. And so I refactored it, but I didn't know how I should refactor it. \r\n\r\nLike, what metric, by which metric should I refactor it? I don't even know what the file really does. I just know it was for the products, like in my game. And so but the answer to that question is simply the token count just divided by token count So it's like they're all usable again. It's all just token count. Okay, so it's yeah at that so I Believe I've made my selection. \r\n\r\nI could take this one this one. Yep. My mate. Yeah, that's right And I'm and then it's once and done I don't I won't do this again because uh, it's done and this is just the and it's only because I haven't Like I said, this is the first time I've used my extension in my game. I've stopped developing my game, I've started developing the extension. So now, just to illustrate to you a bit of it, like I'm just going right back, so it's from scratch, literally. \r\n\r\nSo I'm just, as if I"
  },
  {
    "id": "report_source",
    "chunk": "I've started developing the extension. So now, just to illustrate to you a bit of it, like I'm just going right back, so it's from scratch, literally. \r\n\r\nSo I'm just, as if I, as if I just, as if you, anyone in the world, just dropped my extension into their VS Code, this would have been their first step. You guys will be just starting a project from scratch, so you would just be starting right here. You wouldn't even be doing this. That's why I called it step zero. Okay. But that's it. \r\n\r\nWe're done. That's it. So you can see now it's 745 ,000 tokens, which is manageable. I can fit, it's under a million. There are a few big ones. 12 ,000 is still doable. \r\n\r\nYou just have to make sure you, if it's going to output this file, you just got to make sure you double check it more carefully. it might It might say this portion is omitted It's the same for brevity and then so you got to make sure you put it back in or are you or you can you know? Go with another response that maybe did not do that is up in other words, okay? But you're going to see that soon, so I made that selection. I'm just going to say fix it since she is not responding That's just going to be here. I also want to put the errors that I'm getting So I'll copy this and I will say Here is the error I get when, from the welcome message, welcome message error. \r\n\r\nAnd I'm tagging right now. This is the key lesson right here, is tagging with this. So like, because like, think of it like as the AI, like I'm writing these words here, and I'm writing these words here, and I'm writing these words here, but how does it actually know, you know, beyond just the colon, actually, how would it know this, it starts here and stops here, you know what I mean? Without some so"
  },
  {
    "id": "report_source",
    "chunk": "ds here, but how does it actually know, you know, beyond just the colon, actually, how would it know this, it starts here and stops here, you know what I mean? Without some sort of the limp. Yeah, because what and then not only that, you know without me being so specific What you know, it could be just what are these stage notes? Is this a novel? \r\n\r\nIs this someone's someone did someone you know, is this your inner monologue? What are these words and so you can tag things and give them meaning that the AI can then recognize So, okay, so there's just that's the welcome message error. There it ends and I can continue And I want to also give it that f12 error. I get let's see And then let's see here, test is clear. Okay, so I don't get any console logs, but that's a report as well. \r\n\r\nOkay, because then it can make console logs to help in the next cycle, it can produce console logs that'll help narrow it down. And that one's just a nothing. Okay, so this is me describing the current environment, the current state, the current cycle. Because it's one thing code to produce code, But then you have to see what the net result is and analyze that. So I don't critique the, I don't look at the code. I look at the end result and then I speak there, you know? \r\n\r\nAnd that's a bit of a distinction. Many of the developers will just hyper focus on the code when in actuality you can just describe the behavior of the state and it can look at the code and then generate logs and blah, blah, blah. Okay, so sending a message. Here's the response. when sending a message in the report viewer in the Ask Accenture. And then I'll say the last result, which was, and there's no console. \r\n\r\nAnd then we'll send it off. That's all there is to it. Se"
  },
  {
    "id": "report_source",
    "chunk": " in the report viewer in the Ask Accenture. And then I'll say the last result, which was, and there's no console. \r\n\r\nAnd then we'll send it off. That's all there is to it. Sent no one above. I was monitored. \r\n\r\nthe browser. \r\n\r\nI'll do it one more time and I'll look at the server as well. Monitoring browser console logs and saw nothing appear. Okay, and then let's look at the server. So let's trash that to see if anything is hit. This will tell me if anything's hitting LM studio, right? And then over here, this will tell me, aha, aha, aha. \r\n\r\nHere's actually maybe the evidence we were looking for. \r\n\r\nNo models loaded. \r\n\r\nPlease load model. It's supposed to load the model. That's frustrating. It's supposed to just auto load. That's why I thought sending it again would work. So it's just this one. \r\n\r\nOh no, I deleted it. That's right. Okay. I just need to make a change. I just deleted it. I remember what I did, but now we're using this one. \r\n\r\nI think I'll be able to fix this myself. And I'll still send it off. Huh? Sorry. Yeah. Yeah. \r\n\r\nIt should still be able to help. But I mean, the error, but you're right. I would still narrow it down had I not, because I literally just said to you, and then we'll send it off. But then I thought, oh, wait a minute. \r\n\r\nNo, let's check the server logs as well. \r\n\r\nSo it's just being methodical, right? And then the server logs were clear enough to me that I remembered, I literally, I was trying to clear, I was literally trying to clear off some space on here, literally. And I was like, oh, I now have a, I have a smarter model, a smarter version of it. And I just haven't changed. I literally just remembered I haven't changed. So first of all, it's not loading. \r\n\r\nSo let me see if"
  },
  {
    "id": "report_source",
    "chunk": "ve a smarter model, a smarter version of it. And I just haven't changed. I literally just remembered I haven't changed. So first of all, it's not loading. \r\n\r\nSo let me see if that's because this guy's, I doubt it. Let me first make sure I can get it to load. Let's see the GPU here. Interesting. Normally you see this. Oh, there we go. \r\n\r\nIt's going into my memory. So there's a problem. Let's poke around in LM Studio to see what's going on. First, let's go to the hardware. So yes. Hold on. \r\n\r\nNo, let's put on the limit, because I've got 24. So let's system limit offload. Now let's try to load it again. Updates. We'll update these things. \r\n\r\nThese are just different things they need to run the models. I don't know what they do. \r\n\r\nIn the old days, two years ago, you would have to do a lot of this shit. Shit, now it's, LM Studio manages it a lot. So whatever it is, you just, you might keep your stuff updated. \r\n\r\nIt's large, like it detects your GPU. \r\n\r\nIf my machine had two GPUs in here, it would just detect them and it would just, it's really nice. It's been really streamlined. See, here's all the models available. You could just click one and download it. You know, it was just one of these versions that I deleted, because I had the newer version now. But that should help. \r\n\r\nLet's go check out to see what that did. Let's try to run it again. See, it's supposed to be loading into my GPU. Dedicated GPU memory. So maybe that's setting. But it loaded. \r\n\r\nWait a minute, wait a minute. But it did load. So let's talk to it. There it is. Clear. Okay, let's see. \r\n\r\nWait a minute. Dude, that's in memory? That's fast, kind of, for memory. I'm confused. So the model is running. That's my version of Quint3, the 30 billion wit"
  },
  {
    "id": "report_source",
    "chunk": " let's see. \r\n\r\nWait a minute. Dude, that's in memory? That's fast, kind of, for memory. I'm confused. So the model is running. That's my version of Quint3, the 30 billion with the reactive parameters. \r\n\r\nAnd I just, LM Studio comes with its own little chat window. You can chat with the model you just spun up. And so that this is running on my, so I'm in my house in Princeton, Texas, and I have a house in Forney, Texas, and that house has my PC in it my gaming PC. I just upgraded to have a 3090 because it has 24 gigs of GPU, but it's not even it's not using it right now This is supposed to be literally maxed right now, and this this should be like but this is still I'm just I'm just not Okay, so it is so it is so that's the thing so That's fantastically fast for this being memory. I'm actually in shock because that's how fast the Google one works on the GPU. \r\n\r\nWhen this one goes on GPU, it works at, it goes about 90 tokens per second, three times faster. So I just got to figure out why is this not loading on? So that's not it though. Okay, okay. Sometimes you have to select which one to use, right? Yeah, sometimes this is the problem. \r\n\r\nThe CUDA, right? \r\n\r\nCUDA's the right one. There's two CUDAs. Hold up, guys. Yeah, CUDA's the NVIDIA thing, and I have NVIDIA. So let's just do this, whatever this other CUDA is. then I'll try asking around. \r\n\r\nSo weird. Oh, oh, also, oh, I thought before we talked, I was looking into some, I was actually trying to run this before we connected and I thought maybe there was some clue. I was starting, I saw there was something in here. I remember seeing something about my paging size. That's why I started clearing out some, there it is right there. \r\n\r\nThis seems very suspicious. \r\n\r\n"
  },
  {
    "id": "report_source",
    "chunk": " was something in here. I remember seeing something about my paging size. That's why I started clearing out some, there it is right there. \r\n\r\nThis seems very suspicious. \r\n\r\nThis could be the problem. It says, Paging file is too small for this operation to complete, and so I was just clearing out some space to increase the paging, which is right here, change, 36 on both. Sometimes this makes you ask to see if you restart. Well, let's see if it changed actually. \r\n\r\nHello. \r\n\r\nSome notches. Set? \r\n\r\nAh, that was that. \r\n\r\nOkay, stupid Windows. Yeah, see, it's going to ask me to restart. So I will be right back. Let me just do this. Talk amongst yourselves. Just go to the game and then just look at some of the report. \r\n\r\nGo through the report, I guess. But yeah, I'll be right back. I'll join the call. Another thing, uh, so I think, um, I want to also go through the report. Um, and then, and then also what I can do is literally start the project that you want in front of you, and then you can sort of start it as well in your own environment in front of yourselves using the extension. Uh, and then start, cause he, cause you want to do it with JQR. \r\n\r\nSo you got to go, go, you curate your JQR lists and stuff and get your data in line so that your AI knows what the fuck it's talking about. Yeah. Yeah. So. Good question. That's all called data transformation and stuff. \r\n\r\nUltimately, Markdown is ideal. Yep. Whatever it is, convert it into Markdown. That's all my artifacts are Markdown. It works. It works really well. \r\n\r\nAnd so you can turn an Excel into Markdown. You can turn a PDF into Markdown. You can turn anything into Markdown. For your reference material, call it an artifact. Yep, so all these are artifacts. Everythi"
  },
  {
    "id": "report_source",
    "chunk": "to Markdown. You can turn a PDF into Markdown. You can turn anything into Markdown. For your reference material, call it an artifact. Yep, so all these are artifacts. Everything is an artifact, even an entire prompt can then be put as an artifact itself, and then can be an example process. \r\n\r\nDo it again, but like this, you see what I'm saying? That's the power of an artifact, actually, thinking of things in this way. And then you can just convert it into a Markdown, So all that knowledge, that data in a markdown artifact that you can then talk about. Artifact number five, right? Make sure reference artifact number five when you put this together, right? Like I'm just speaking to the AI. \r\n\r\nOkay, so I'm just getting everything spun up again. Wait a minute, am I an idiot? I am an idiot. I restarted my, I didn't need to disconnect. I'm confused, I'm confused. Anyway, we're good. \r\n\r\nDid I restart the wrong machine? I restarted the wrong machine. i'm an idiot i didn't need to that's virtual machines man okay there it is okay okay so let me think get this through this one through because this is actually restarting the server okay now i'm on the same page so i would have to restart okay i don't want to do all that in front of y 'all that's not that's too um unnecessary I think it's way more valuable to and this is but this is this is the running models you don't necessarily have to do that and in the beginning you know a lot of your stuff can be just oh help help actually this is really easy I can help you set this up it's not it's not difficult I'm just trying to I'm just trying to square circle here okay let's go over here and let's just do this for a minute kind of go through like this speedrun, this thing. So the repor"
  },
  {
    "id": "report_source",
    "chunk": "ust trying to I'm just trying to square circle here okay let's go over here and let's just do this for a minute kind of go through like this speedrun, this thing. So the report came about after I made the game. I was making the game, having fun, putting this thing together, because I've been making AI, like I said, code with AI for three years, learning processes, learning how, you know, making a mental model of the model so I know what it's good at, what it could do, what problems it can solve, what problems it struggles at solving, those kinds of things. \r\n\r\nI invented the idea of cycles. to not rely on the conversation history, because the conversation history was garbage. And then that kind of spurred from there. Then I got the idea to make artifacts as a source of truth to be like something that was English, because we were writing code. So we would have English as an artifact source of truth that I would explain what I wanted, and then it would write it in the artifact, and then I would read the artifact. If it made sense to me, if I thought it was what I was asking for, I would say, OK, go build it. \r\n\r\nAnd then critique the results, you know and because I in my mind my theory was if the art if the artifact artifact correctly Describes the thing then it'll go do the thing correctly so a bit of a some of the vernacular because a cognitive capital and just look at some of this, turn that off, okay. The cognitive capital is the collective intellectual capacity, skill, and problem -solving, actually, let's see, hold up, hold up, hold up, hold up, can you hear this? The only problem with this is it's reading this first, which is a bit repetitive, but bear with it. We're just gonna go through a couple of these in this m"
  },
  {
    "id": "report_source",
    "chunk": " can you hear this? The only problem with this is it's reading this first, which is a bit repetitive, but bear with it. We're just gonna go through a couple of these in this manner, and then we'll do the rest more, I just thought of the easiest solution to my problem. Let me just go start downloading the same model again, and then we'll come back. \r\n\r\nGo ahead, Quinn, 333. I think it was that one. No, it was this one. It was this one, yeah. this little okay yeah we want full gpu offload download all right oh it's downloading okay the entire internet is your hard drive this skipped something no no it's not it didn't skip so the problem here that this is going on here is uh so the working with ai the you a hidden curriculum that the current workforce model is a revolving door. \r\n\r\nThere is no AI trainer job position. They're all content writers, because content writers are a very notoriously low -paid, low -skilled position. And so that's the job title. But yeah, yes, yes. And no, so not at Google. \r\n\r\nSo what they do is they outsource the work that they need to another company, GlobalLogic, and then GlobalLogic concocts the jobs, breaks down it, what it does is it creates micro -tasks. \r\n\r\nSo it breaks down the job into smaller tasks and then contracts those out to other companies. \r\n\r\nOne of which I worked at, because I trained Google, I trained Gemini, I worked on this product. Anytime that this kicks off a... \r\n\r\nAnytime it shows you like a Google Maps, that's, I don't know if it's gonna actually give me a map or not. \r\n\r\nI worked on that Maps API. \r\n\r\nSo show me the route. Show me the route. Oh, Timba, oh, it's, oh, my bad. \r\n\r\nI'm not thinking. \r\n\r\nI was thinking of, I was, no, no, no, I was thinking, I know it's wro"
  },
  {
    "id": "report_source",
    "chunk": ". \r\n\r\nSo show me the route. Show me the route. Oh, Timba, oh, it's, oh, my bad. \r\n\r\nI'm not thinking. \r\n\r\nI was thinking of, I was, no, no, no, I was thinking, I know it's wrong. I'm silly. It's not in America. \r\n\r\nI was thinking of, \r\n\r\nAlbuquerque. I was thinking Bugs Bunny. He doesn't say Timbuktu. He says, he says left turn in Albuquerque. Okay. Okay. \r\n\r\nSo yeah. Okay. But without the AI remote, yeah, there we go. No way, dude. The Google demo, dude, that's hilarious, dude. No demo ever works, dude. \r\n\r\nOh, that's hilarious. Get out of here. Get out of here, dude. That's so fucking funny. Oh my god, yeah, uh, whatever. Fuck them. \r\n\r\nAnyway, anyway, anyway. So, um, so the problem though is that, so it's all contracted out, and so it's a revolving door, it's low paid, you do the work until you find some other better job and you leave, like I did, um, but that, and therein lies a problem, because I got these skill sets from working with AI for three years. That was only when GPT -3, uh, the new one, you know, the new Gintrib AI came out. Everything changed after that. All the kinds of data sets you need to create are different. Before it was drawing bounding boxes around images of pedestrians and saying, that's a person, that's a stoplight, that's a dog, that's a cat. \r\n\r\nYou don't need English, you don't need a master's degree to do that kind of work. But that was before generative AI. Generative AI came along, and then now you need actual data sets of thinking and criticizing, change of thought, reasoning. We never had to create those data sets, they don't exist. So you can't have an AI that can be a thinking machine. if you don't have a thinking data set explaining how thinking works. \r\n\r\nAnd like having an example"
  },
  {
    "id": "report_source",
    "chunk": ", they don't exist. So you can't have an AI that can be a thinking machine. if you don't have a thinking data set explaining how thinking works. \r\n\r\nAnd like having an example of a good... That's what we did. We annotated. That's what I did. I annotated what I wrote the trajectories out. you know, pretend to be an AI, essentially, and write out the trajectories of what would be a good, like, Google API call to answer the user's question in, like, five, six, seven steps, right? \r\n\r\nVersus just like a normal Google search. \r\n\r\nAnd so, with it being a revolving door, you'll never stay there long enough to gain the skill set that makes you 100x that I'm sitting here demonstrating with all the shit that I'm creating when I can't even write a single if statement, okay? \r\n\r\nAnd then also this comes into play because it's a low -paid, low -gig work. \r\n\r\nBut the reality is then that adds financial stress, which decreases an individual's focus, reasoning abilities, or otherwise their cognitive capacity. And then so you have someone who's cognitively degraded, who's cognitively taxed due to the financial precarity of the position. training the AI that the rest of the world is using, the rest of the country is using, you can see how that's actually a recipe for disaster. And so the term, the concept that we need is this. Yeah, the data supply chain is what that falls under. \r\n\r\nThe data supply chain. \r\n\r\nIf you're outsourcing the person who's training your AI to some third world country, do they actually even care about the success or failure of the United States? Probably not. They're probably even closer, geographically speaking, to China. And China has much more influence on there. And we don't pay them. We pay them 50 bucks a mo"
  },
  {
    "id": "report_source",
    "chunk": "es? Probably not. They're probably even closer, geographically speaking, to China. And China has much more influence on there. And we don't pay them. We pay them 50 bucks a month. \r\n\r\nSo China shows up, a foreign intelligence service, gives them 60. Oh, that's my monthly salary. Sure, you can take a few screenshots of my computer, I don't mind. This is the solution. Where are we at? Okay. \r\n\r\nSo any questions thus far? Yeah. Yes. That's down here, part five. Yeah. Yeah. \r\n\r\nYeah. Cool. No, that's good. And I'm glad you're interested in that part. It's a really interesting, and I almost never get to that part of the story because it takes a while to get there, but yeah. So part one is the product, which I can gloss over because you've seen sort of the game. \r\n\r\nI haven't played the game in front of you. I did not make Angry Birds. I played the game in front of Cameron. He knows it's a bit and I barely played it in front of Cameron. Yeah, it's a it's a whole thing I made a whole thing. It's multiplayer. \r\n\r\nIt's got a leaderboards. I could keep going. I could just keep going with it So the proof of the hundred and I did it in 110 days. Um, so that's kind of the Go down this way. So the hook is the artifact in your hands. \r\n\r\nThe ascent report is tied to the game and The average productivity gains are measured like 20%. \r\n\r\nNo, we're talking the citizen architect is 10 ,000 % gains. One person can do literally what you needed an entire team to do. Again, the revolutionary lead. \r\n\r\nWe're in a choice right now. \r\n\r\nWe're going to, because by Google's own admission, they expect a billion data labelers in the future because yes, that's how much data we're going to need to label in the future. And so if Google would love to hav"
  },
  {
    "id": "report_source",
    "chunk": "n admission, they expect a billion data labelers in the future because yes, that's how much data we're going to need to label in the future. And so if Google would love to have it where they keep paying the labelers next to nothing and reap all the rewards, whereas I'm offering a solution where we actually empower people and actually let people use the power to solve their own problems locally, blah, blah, blah, blah, blah. Citizen Architect is the path to do that, right? I'm proof. that some one person who cannot fucking code can do this. So yeah, they're tied together. \r\n\r\nThe origin story, 120 days, literally, so March 25th is when Gemini 2 .5 Pro was released. I fiddled with the model for six days, and then I came up with the idea for the game on the sixth day, and then I spent three days planning out all the documentation. until I felt ready. So it's very simple. Here's the game I want to make. Let's make it like this other game. \r\n\r\nDo you know this game? Oh, you know this game? It's called Startup Company? Great. Okay, you know about it. Great. \r\n\r\nIt's from 2017. Of course you would know about it. Great. Okay. I'm going to make a game just like that, but instead of making software products, we're just making AI products. So like Chatbot, Imagebot, blah, blah, blah, blah. \r\n\r\nNow let's make all the features from that game that we're going to need. So like they have components, they have HR systems, blah, blah, blah. So make a list of all the systems, make a list of all the components. Now make an artifact that describes each blah, blah, blah. Planned it all out. I spent three days. \r\n\r\nI watched YouTube videos. I found a YouTube video to help plan it out. This is how I got started. I found a YouTube video of someo"
  },
  {
    "id": "report_source",
    "chunk": " Planned it all out. I spent three days. \r\n\r\nI watched YouTube videos. I found a YouTube video to help plan it out. This is how I got started. I found a YouTube video of someone playing the game. And that was like, there are many of them. I found one that had a decent coverage of the game, and I used that transcript. \r\n\r\nI just there you go There's my there's my one of my artifacts is this YouTube transcript. He's gonna use all the right language from that I reverse engineered my artifacts and created my own game So and I and I used a simple YouTube tutorial to make so you'll see the same game world right but Whereas he made an actual like a Pokemon little thing where like you walk in over here and it starts a Pokemon battle. I made an AI company right out of the whole thing. So all the pieces of the thing, I'm a one person studio. It's a paradigm shift in labor. One dude with AI with a vision can do everything that the entire team would do. \r\n\r\nThis is what 100, this is what one million tokens looks like. This game, you're looking at it all up, it's about a million tokens. That's another thing is using the first AI models, they only had 1 ,000 tokens. So you can only get, stick, fit in them 1 ,000 tokens and have them crunch on those 1 ,000 tokens. Now they can crunch on a million tokens. So this is what's possible with the million tokens. \r\n\r\nIt's kind of like NASA back in the day. How did NASA, you know, what was the hard drive? How many megabytes was the hard drive that got NASA to the moon, right? It was just a few, Megabytes, but all the software needed it was on that hard drive right because that's what the state of technology was Well now we measure state of technology and token count in my opinion. So this is th"
  },
  {
    "id": "report_source",
    "chunk": "are needed it was on that hard drive right because that's what the state of technology was Well now we measure state of technology and token count in my opinion. So this is this is this is a I Was so happy to make this image because for three years. This is how I felt when I was making my slack bot and \r\n\r\nI had this in my mind, but the image generation didn't exist three years ago. But I had this in mind. \r\n\r\nI felt like I was a kid again. \r\n\r\nI felt like I was sitting on like the matrix, the floor. And I was playing with Legos, but they were digital Legos. And I was combining the digital Legos to make my Slack bot. Because I was getting like this library, that library, Python, all these things, putting it all together. And then I had to build the actual website to deliver the Slack bot itself. Not to digress, this was my first project. \r\n\r\nYou could try it, you could just add it to your Slack. It's probably, yeah, it's probably, yeah. Oh, it'll still, oh, they changed the process. So you have to request to install an unapproved app. Okay, who cares? But anyway, this was my project three years ago. \r\n\r\nThis is what you'll be essentially copying and making into your own version. So you can see an example of it working. So I just created an example channel. What is this demonstrating up here? Example team channel. I mean I invite the bot and then I just say anything to it And then it will help you get it set up because it's I trained it that way thinking happy to assist you By using the set system message week, and then that's how you can train it in this channel All you do is type set system message and so for example You could say something like this channel includes experts in the field of AI the conversations often in"
  },
  {
    "id": "report_source",
    "chunk": " this channel All you do is type set system message and so for example You could say something like this channel includes experts in the field of AI the conversations often involve advanced AI and this is all this three years old but and then so this channel will be for an enablement team. \r\n\r\nSo like what I was doing at Palo Alto Networks. This enablement team creates training content on technical products that the company builds. And so I'm talking with the AI, this was three years ago, talking with the AI to help set it up. This is revolutionary stuff at the time. This was prophetic. \r\n\r\nThis was visionary stuff. Now it's just because it didn't exist at the time. I was putting it together. Excellent to summarize. So it gave me a set system message. It gave me a system message now I could write for this channel. \r\n\r\nI just copied and pasted. This channel dedicated enablement team focused on creating technical content. Should adopt informed persona ready to answer questions about instructional design. And so I'm just doing it correctly. Now it's set. The channel system message has been set. \r\n\r\nSo I'll literally give you the Python script. You can take it and use it as one of your artifacts. Yeah, that's what I created. It could also be, you could also invite it to your own channel, any channel. You can, you absolutely see. So guess what? \r\n\r\nAll you would need to do, I turned DMs off. I decided to turn DMs off at the time. So you could make a different architecture design decision for your project. No problem there, dude. \r\n\r\nThat's one of your cycles. \r\n\r\nYou're going to spend some cycles. \r\n\r\nNow let's make it so the users can DM the bot. \r\n\r\nOkay, let's go. See what I'm saying? So there you go. You're getting there."
  },
  {
    "id": "report_source",
    "chunk": "cles. \r\n\r\nYou're going to spend some cycles. \r\n\r\nNow let's make it so the users can DM the bot. \r\n\r\nOkay, let's go. See what I'm saying? So there you go. You're getting there. You're starting to see it. So you can start with mine as a blueprint, you see, and then make it for your use case. \r\n\r\nAnd this is only a part of it, so there's two pieces of it. See, can you explain playbooks to a new XOR at CSE? So as a new customer success engineer, trying to learn XOR, boom, bada bing, bada boom. Now the user has a fucking, dude, this was revolutionary shit, okay? And all I did was set it up right in front of them. of you okay and then um the premium feature so the premium feature was the knowledge base i i did rag before i even knew it so here it is what is cortex xim cortex xim was a product that was came out literally two months after. \r\n\r\nIt was all secret hush hush at Palo Alto until one month, January 21 or whatever. And that was the training cutoff time was December of 2020 or whatever for the AI. So literally, literally hush hush secret product launch a month after the AI training data cutoff date. Because, oh, well, yes, yes. Yeah, you're sharp. Yeah, go ahead. \r\n\r\nYou're sharp. \r\n\r\nGo ahead. You tell me what's going on. Yep. Yeah, that's right. Precisely. Yes, it is. \r\n\r\nIt's fucking crazy. It's crazy for that to exist today. Never mind I made this three years ago. And it fell on deaf ears, bro. The capability is literally at your fingertips. It's whatever data you bring, whatever data you curate, whatever data you curate. \r\n\r\nSee the step I was showing you, all the checking? It's not just checking files. You can go get Excel files. You can go get your JQRs, drop them in a folder, and then use VS Code on that folder a"
  },
  {
    "id": "report_source",
    "chunk": "showing you, all the checking? It's not just checking files. You can go get Excel files. You can go get your JQRs, drop them in a folder, and then use VS Code on that folder as a repo. And then just select that file for your JQR. And then when you ask your AI to make you to, you have a question on, well, I don't know what the fuck you do with JQRs, but I use them in my work and they're annoying. \r\n\r\nAnd I, every time I just download the XLS and I just have it in my, as an artifact. And I, you know, make my list of JQRs that way. And it's fucking, I just spot check it after. Yep. So yeah, it's exactly what happens in here. \r\n\r\nSo I don't need to explain anymore. \r\n\r\nYou got it. So yeah, you I'll just it's 1000. It's 1000 line. I made it with I made it with three years ago. So the AI could only be so big, right? It could only take so many tokens. \r\n\r\nIt's a tiny fucking script. It's a tiny fucking project. And I was able to do it. So yeah, with the net AI now, it's it'll be a joke. Absolute joke. Especially when you have Yeah. \r\n\r\nI'm done. I'm rambling. I'm transitioning back. I'm transitioning back to where we were. Yes, they are. \r\n\r\nIt's public. \r\n\r\nNo, it's on the internet. You go download them. \r\n\r\nNo, that's another thing. \r\n\r\nThat's beautiful. That's a beautiful question. That comes from you playing with the AI, and that's what you will build as a mental model of the model. So you will simply know, and you're playing with it when you're making your project. When you have a DNS problem and it's answering you and solving, you'll get an, oh yeah, dude, yeah, you'll have categories of knowledge. that you'll, in your own mind, you'll know intuitively, oh yeah, the AI's got this, but I'll need to bring this to the table."
  },
  {
    "id": "report_source",
    "chunk": " dude, yeah, you'll have categories of knowledge. that you'll, in your own mind, you'll know intuitively, oh yeah, the AI's got this, but I'll need to bring this to the table. \r\n\r\nGreat question. Great, great, great, yeah. So it's great at troubleshooting DNS and handling those kinds of problems, especially when you do the legwork and actually bring the right DNS data to the table. right, from out of your DNS system. Like for example, I use Namecheap to host this server you're looking at in front of you at aisin . game that you can visit. \r\n\r\nI'm hosting it all locally. \r\n\r\nThe only thing that's not local is the thing I can't do by myself, which is the DNS. \r\n\r\nAnd so I have an artifact for my game. Oh yeah, I've restarted, which is fine. I haven't restarted this thing in ages. So, I have an artifact in my game that captured all that DNS information. There you go. I have an artifact that has all the information from my local LLM, so it knows what URL to use, so I never have to bring it back to the, I brought it once. \r\n\r\nI've curated that data. It's in an artifact. I would do it, oh, this is great. This is the answer to your question. Here's the barometer. Here's the validation check. \r\n\r\nHallucinations, hallucinations. First, start with no data. Easy peasy, breezy beautiful. Start with no data. And then ask it for whatever the fuck you want, because you don't know. Maybe it can do. \r\n\r\nYeah. And then no, no, not even that. Not even that. Not even that. Yes. Yes, that is a pro. \r\n\r\nBut I'm going I'm going I'm going I'm I'm I'm zooming out even further than that to do it even quicker, faster, better, stronger. Listen, you just ask it for the result for the product that you want. And then you look at it because, you know, "
  },
  {
    "id": "report_source",
    "chunk": "er than that to do it even quicker, faster, better, stronger. Listen, you just ask it for the result for the product that you want. And then you look at it because, you know, let me give an example yeah and then you see what it's meant that's what shows you what it's missing the yes with the knowledge it has that you don't need to bring yes yes good good yes you can clearly see because it's got a bunch of wrong fucking jq ours ah oh I'll just go okay I'll just go get the jq ours I and then boom ask the same question with that artifact at the bottom added see what happens All right, and let me go back to my Slack bot because that's actually how I came up with the idea for it in the first place. The origin of the idea was, I asked it, because I was working at Palo Alto, my job was to create training curriculum for XOR. And so I asked the latest and greatest AI, do you know what XOR is? \r\n\r\nYes, it's a security orchestration automation. Oh great, okay, so it knows what XOR is. Do my job, make me a playbook training. And then it was horrible, it was terrible. And then I thought about it and then I just opened up the publicly facing XOR admin guide and I did a control F. for the word playbook and any paragraph or paragraph above or below or whatever section that mentioned the word playbook I just copied it and put it into a text document and then and then asked the same question make me a training on playbooks and then it was almost perfectly usable I was like what the fuck if I could somehow automate this process somehow and so I just went to YouTube and I and I and I found one video in particular from this genius dude 74 lines of code where it's a bringing up because he has a great diagram. \r\n\r\nIt's a bird, this bird, this."
  },
  {
    "id": "report_source",
    "chunk": "d I and I and I found one video in particular from this genius dude 74 lines of code where it's a bringing up because he has a great diagram. \r\n\r\nIt's a bird, this bird, this. So he created, this is rag. I don't know if he even knows this is rag because I didn't know it was rag and I watched the video. But we made this, he made this, where in 74 lines, it will take a PDF, extract the content, split it into chunks, number the chunks, turn them into embeddings, which are just vectors, like this. I'm going to turn something into an embedding right here, just so you can visually see what a vector or string it all is. This is a sentence that is purposely misspelled to break words up. \r\n\r\nYeah, see? They're almost all not broken up. That's the only one. First, I just wrote a sentence. The first step is to see which So it's 66 characters up here, but down here it's only 17 tokens, and each color is one of the tokens. So this is a token, space is is a token, space a is a token, so on and so forth. \r\n\r\nAnd then each token has an ID, and so each of the 17 tokens is now just represented as a number. So that's all that this, this is actually just 851. So if we ever, if any repetition, if we had any repeated words in here, word, word, word, word, three of them, they're all gonna be identical, 2195, 2195, 2195. It's just, that's it, this is symbolic, that's all it is. Yep, and this is a vector, this is an embedding, that's all it is. Easy peasy. \r\n\r\nAnd so where were we else? We were somewhere. Yes, that's where we were. That's right. So that's what an embedding is. So it turns each chunk, which is 1 ,000 characters, into an embedding, which is a string like that. \r\n\r\nAnd this is an embedding model. And nowadays, you can have a local "
  },
  {
    "id": "report_source",
    "chunk": "is. So it turns each chunk, which is 1 ,000 characters, into an embedding, which is a string like that. \r\n\r\nAnd this is an embedding model. And nowadays, you can have a local model do this. It's very, very easy. I have an IBM one. And so you can do this. You don't need to make any money. \r\n\r\nThis doesn't cost anything anymore. You can do this locally. Create these embeddings. And you create a semantic index. And then that gets put into a knowledge base file, which is just a . index file and then a . \r\n\r\njson file. And then now, once you have this, the user asks a question. \r\n\r\nThe question itself gets turned into another embedding. \r\n\r\nAnd then that embedding, now you have an embedding that you can compare the numbers against the other numbers, and in a process called a semantic search. The library is Facebook AI Semantic Search. It's phenomenal. What it's doing is it finds which chunks are the most semantically similar to the user's question, and then, you know, you select the top seven, and then it'll just add those. Just like I said, append them to the question, to the user's question. So the user asks the question, and then the knowledge base gets Picked out the few pieces because the book is too damn big. \r\n\r\nBook's too big for the AI's context. Even a million tokens is too much sometimes. This is not enough. And so you have a knowledge base. Rank the results. Generate. \r\n\r\nAnswer. Get the answer. That's what's going on. He did it in 74 lines. I took that. I wrote it up. \r\n\r\nI copied what he wrote. I wrote it down. as he wrote it and then I took that and then XR first off I had already made my slack bot I already made the version one that you saw where you could do the system message and then I because I knew my sec"
  },
  {
    "id": "report_source",
    "chunk": "k that and then XR first off I had already made my slack bot I already made the version one that you saw where you could do the system message and then I because I knew my second step now I have the bot now I want to do the PDF so then I had this I wrote the 74 lines and I brought that to the equation see I have, now I see, I data curated this solution. Now how, and I have no idea how to do it in Slackbot. I don't know how I'm going to do it with Slackbot. I don't know. \r\n\r\nAnd so I, we started working on it and ended up with a solution. It worked. And so all you do is you type slash file upload and it pops up that little modal that, if I open a new, it'll refresh, refresh. But whatever, you saw, it pops up the modal. And I didn't even know the word modal. I didn't know the word model, and so I'm learning the vocabulary. \r\n\r\nNow, making models is no big deal. I make models all the time. I made many models for my video game, for my AISN game. No big deal. I know what to ask for. That's the learning in the moment. \r\n\r\nThat's the in -situ learning. And that's the origin of the idea for the Slackbot, and all the way through, 74 lines or whatever. \r\n\r\nAnd so you'll just take my, just like I took his 74 lines, you're gonna take my 1 ,000 and run with it, okay? \r\n\r\nyeah yeah right on so okay so that's i literally felt like this i was so this was my favorite one to make i genuinely genuinely feel like this you will too uh very soon um the how live coding to virtuosity virtuosity simply means it's like maestro like someone like an artist a piano a piano player makes it look easy right their finger like it looks to them easy right what they're but that's because of all the years of practice you haven't you have not seen um that's "
  },
  {
    "id": "report_source",
    "chunk": "er makes it look easy right their finger like it looks to them easy right what they're but that's because of all the years of practice you haven't you have not seen um that's the same thing here uh case in point um this is an example of this was literally the first image that i made this was the first image that i made for this report and then after making over a thousand images i then came back and realized i need like the cover page i need any words like i can do words i didn't even realize i could do words i could do words that don't misspell Now I made the cover page, right? Way different, right? Versus the, what do the gloves mean? Like, I don't know, like this one doesn't have gloves. Does that mean anything? \r\n\r\nWhat does it mean? That's the thing. Oh, the artistry, like me, you know, what is the artist trying to convey? You can convey so much. I learned this over time. This is one of my favorites. \r\n\r\nOver time, you know, oh, well, it's a cover, you know? Even starting to get the text to be like, this is all AI. I didn't put this. This is AI generated, bro. Maybe like, look, I just found a typo. No, it's not a typo. \r\n\r\nIt's correct. Earn and learn. That's correct. \r\n\r\nSee? \r\n\r\nGood to go. Maybe like, I have to do maybe some little tiny edits. It's really minuscule. I mean, this is beautiful. Look at the fuck. Look at all the detail. \r\n\r\nLook at all the content, context. You know, this is the ThreadingPress 2 .0. The way I did this, I don't need to go through the way I did everything, but images are great. \r\n\r\nIf you ever need to make images, I can show you how to do that. \r\n\r\nYeah. \r\n\r\nthe brilliant, the trillion, billion worker opportunity. Anyway, so this is the plan. This is the plan. You guys are going to be"
  },
  {
    "id": "report_source",
    "chunk": ", I can show you how to do that. \r\n\r\nYeah. \r\n\r\nthe brilliant, the trillion, billion worker opportunity. Anyway, so this is the plan. This is the plan. You guys are going to be on this side of the equation. \r\n\r\nYou guys are going to be the DC. \r\n\r\nI'm skipping. \r\n\r\nI'm skipping, but let's skip. This is a good skip. I'm going to turn you guys into the dcia you're going to be you you already are in this state you're already the intelligence analyst that's what you're here for um but then i have learned the skills to be the data curator i've made a fucking tool to data curate to carry data and then you will be able to use it to be a fucking dcia data curator intelligence analyst And just fucking know every answer to every fucking pro - Like you said, this guy knows every fucking - Cause I - Cause I - Cause I did - I use AI to research all that shit, dude. I didn't know any of these words. I didn't know any of this shit. \r\n\r\nYou know, I just fucking read the responses. I actually sat to fucking read it. It's not too much for me to read. Okay? So, uh, yeah. So that's the plan there. \r\n\r\nUh, uh... We were over here... Um... Vibe Coding Virtuosity... Uh, yeah, that's a good switch. Okay, so, yeah. \r\n\r\nGood stuff. Good stuff. We're about right here. Yep. Yep. Same stuff. \r\n\r\nAI's the producer, you're the strategist. Yep. Yes. In any direction. You're right. Mm -hmm. \r\n\r\nI didn't mention this, I delivered this into strategic partner training. I delivered the bot actually in a training course. So I have a GIF of it. Is this the GIF? Okay, I have a GIF of it where I went through every single question, I recorded every single question that every single user asked. But you can see, this was Rob, this was one of the SMEs at Palo Alto N"
  },
  {
    "id": "report_source",
    "chunk": "ere I went through every single question, I recorded every single question that every single user asked. But you can see, this was Rob, this was one of the SMEs at Palo Alto Networks. \r\n\r\nHe was ex -military, whatever, he was one of the really important people at the company. And so he was in there testing it, you know, can you give me an overview of XIM? And again, XIM is a product that the real AI knew nothing of, so any answer it comes up with could only have come up with it because I had that PDF solution. Only way. And so here it is, exact, correct. I was sitting four seats away when he asked this question, and I was looking at his face when he asked it, And he turned and he looked at me with like the thumb, like the kid on the computer with the thumbs up. \r\n\r\nDude, it was that. It was that kid on the computer with the thumbs up. It was that meme. Yeah, dude, legit. He's like, okay. And he asked another follow -up question. \r\n\r\nBecause I trained it to always suggest follow -up questions for the user. And then he just literally just copied one. \r\n\r\nThat's what I wanted. \r\n\r\nHe just copied one and just was exactly. And then asked another question later. So yeah, exactly. So I delivered strategic partner. \r\n\r\nI forget who was in the room. \r\n\r\nIBM, SB6, Deloitte, some Mexican telcos were all represented. They were all interested in using XIM. They all purchased it and they were in there for two years. And I literally, I don't wanna digress, but I literally turned the training around, dude. A month before, it was a disaster, specifically the labs. And I came in and I made the labs, but then I also made the bot. \r\n\r\nI delivered this bot as a sole contributor, icing on the cake, this is what you do, this is what you get wh"
  },
  {
    "id": "report_source",
    "chunk": "nd I came in and I made the labs, but then I also made the bot. \r\n\r\nI delivered this bot as a sole contributor, icing on the cake, this is what you do, this is what you get when you add David to the project, right? Like what the fuck like this? Yeah, and then I ended up losing my job. Uh, uh, there's no fault. Yeah Yeah, dude, so crazy dude in a reduction in force. They cut half the team the other half. \r\n\r\nUh, uh, they just finished cutting actually, uh, one of my colleagues just got let go the one who survived the first round got let go just a few months ago, so Yeah, um, not good. Not good. But uh, anyway, so back to This uh, yeah the innovative starting point make it cool You don't know how or what or why. What you do have, though, is you have human taste. You have taste. You can say, I don't like this. \r\n\r\nMake it different. You don't know the right language yet. That's where this comes in. You start to learn the design vocabulary. You start to have a more structured interaction. And then now, even after you've built one or two things, you can even have a hole in the mindset of what you're going to need to build because this is the third time you've done an authentication system. \r\n\r\nYeah. So Citizen, this is the end state. Let's just listen to this one, I guess. Looks like I might have fixed it. Yeah, I fixed it. Yeah. \r\n\r\nOkay, so we can blitz through this because it's not too relevant, but it's relevant to me. It was a lived experience for me. It affected me actually. I sometimes get choked up going through this section, but it's not a good work environment. It's not good to be building a piece of the future. And you can't even be a part of that. \r\n\r\nSo it really sucks. And so the problem, though, so it actually "
  },
  {
    "id": "report_source",
    "chunk": "work environment. It's not good to be building a piece of the future. And you can't even be a part of that. \r\n\r\nSo it really sucks. And so the problem, though, so it actually becomes a problem, though, because it actually is institutionalized garbage in, garbage out. And when you have that bad system of untrained people who just act, okay, oh, let me, let me, this is how it's so bad. I can, I got the, okay. The first, when I started, the task was only 1 ,000 tokens max, because the AI could only take 1 ,000 tokens. Like, that's 4 ,000 characters. \r\n\r\nThat's a conceivable thing, but now the AIs are a million tokens. Now the task's 1 ,000, 40 ,000, 40 ,000 lines, or was it say 40 ,000 tokens? But you only have three hours to do the task. And the training, by the way, I was a senior reviewer, right? So some of the argument might be, oh, well, they have like a review process to weed through the bad responses. Yeah, dude, you're talking to that senior reviewer. \r\n\r\nI know the system. \r\n\r\nGuess what training I got? \r\n\r\nI got basic English grammar training to be a senior reviewer. It was like, what the fuck? Where's my training on how, like, what is chain of thought reasoning and all the actual, like, actual thing that I would have to be professionalized? I would be machine learning terminology. They can't have that, then they would have to pay me more. So it's institutionalized garbage in, garbage out, okay? \r\n\r\nAnd the daily quota just goes up. They keep squeezing, since making it a nonsensical system. It's architecture of self -sabotage in AI development pipeline. This is why the AIs get stupider over time. I don't know if you've noticed that with chatGBT. That's because that's what's happening, okay? \r\n\r\nAnd you can, so the"
  },
  {
    "id": "report_source",
    "chunk": "t pipeline. This is why the AIs get stupider over time. I don't know if you've noticed that with chatGBT. That's because that's what's happening, okay? \r\n\r\nAnd you can, so the reinforcement learning with human feedback is the post -training. So the AIs, unless you do very good post -training, right? If you do a bad job, you can make them stupider. Let me give an example. Have you heard of MechaHitler? MechaHitler? \r\n\r\nIt was it was a it was a yes. Yes. Yes. \r\n\r\nYes. Yes. \r\n\r\nYeah, that's what happens. Yeah. Yeah. \r\n\r\nBecause they started fucking because they were feeding it garbage. \r\n\r\nThey were they were they were that they were that was reinforcement. That was post training. They were that was post training. Oh, me test. Oh, yeah. That yeah, that's discord. \r\n\r\nThat's happened sometimes. Is it better now? Yeah, it's probably just a No, that's that was me normal. That's me normal. No, no. So anyway, this is what's going on. \r\n\r\nWe're courting disaster because the higher the technology gets, the more people rely on it. And that when it does have a catastrophic failure, the more exposed and the worse that the harder the fall. OK, so this is just a prediction of mine that China will win the AI race because of the fissured workplace and the institutionalized garbage in and garbage out. This is me, this is my theory, and this is because I lived through it, I did it, and I found the glass ceiling, okay? The negative feedback loop. \r\n\r\nI was literally rubber stamping the responses, dude, I could care less, I could care fucking less. OK, and I'm smart and I believe in all this stuff and I love AI. But I got to the point where I could care fucking less. And I bet I'm not the only one who could care fucking less. OK, I just want"
  },
  {
    "id": "report_source",
    "chunk": " I believe in all this stuff and I love AI. But I got to the point where I could care fucking less. And I bet I'm not the only one who could care fucking less. OK, I just want to get my paycheck and go home because I don't get paid enough to care. And I'm an American. \r\n\r\nNever mind the foreigners doing this job. OK. OK. It's a whole it's a whole. And again, you can't you. So anyway. \r\n\r\nYeah. Then you look at China. What is China doing? So then I thought then. So I started using deep think. No, no, no. \r\n\r\nDeep research, because Google's released deep research. \r\n\r\nAnd that's how I came up with the fissured workplace. \r\n\r\nI didn't know it was this bad. \r\n\r\nI thought I was just the only idiot who tried to get a promotion and only got himself more work to become a senior reviewer. No, actually, there was a whole, it's a whole, like, there's a union. I joined the union. I'm working with the leadership there. So you guys are the top. I'm working with the bottom. \r\n\r\nI'm grassroots. \r\n\r\nI'm trying to, you know, so, but that's not the topic. I thought with deep research, after doing some research on American companies and exposing some of that, because there's now a class action lawsuit that started in May. on the supplier for Meta and OpenAI. They're at scale. They're in a lawsuit right now for the exact same reason, misclassification of labor. That's the complaint. And wage theft is the complaint, which is, yeah, checks out. \r\n\r\nYeah, that's my lived experience, yeah. So it's hopefully, hopefully, hopefully, hopefully America can right itself. the system. But I don't think so, man. Google is the strongest, the most rich company in the world. I think they're going to get everything that they want. \r\n\r\nBut what is China doing"
  },
  {
    "id": "report_source",
    "chunk": "em. But I don't think so, man. Google is the strongest, the most rich company in the world. I think they're going to get everything that they want. \r\n\r\nBut what is China doing? That inflection was annoying, but who cares? Interesting. So I've never changed it while I was playing. \r\n\r\nHold up. \r\n\r\nThere we go. Okay, it's fixed. Let me just connect the dots here. What was that? \r\n\r\nLet's do it this way. \r\n\r\nHave you all seen this? \r\n\r\nYeah. \r\n\r\nYeah. Yep. Yep. This is the poorest region in China. Never have they done any investment of any kind here. It's the poorest because look at all the fucking mountains. \r\n\r\nSo they are alleviating their poorest region. They're about to make it the richest region. All right, what the fuck? What? I can do that. I could do that if I had a bunch of people listening. \r\n\r\nBecause they see the power. They're going to make an undeniable case. They've already done it. They're already using it. It already is. It already is. \r\n\r\nIt already is over there. They already have the base made. And they've already got workers working there, training AI, promoting themselves, getting AI training, and becoming a profession. Over here, we're trying to keep the prices low, OK? And then also, why are they doing this? the big ass two, the two, two, two, two of them. \r\n\r\nI don't know if you've seen that. Cause that's where they're building, that's where they're building their data centers. Okay. Okay. In the desert, in the fucking desert. Okay. \r\n\r\nLike we're like, oh, projects guys wake up. Okay. All right. All right. So inline sourcing, so poverty alleviation, secure data pipelines, not outsourced, data annotation as poverty alleviation, insulating the supply chain. professionalization of the workforce. \r\n\r\n"
  },
  {
    "id": "report_source",
    "chunk": " so poverty alleviation, secure data pipelines, not outsourced, data annotation as poverty alleviation, insulating the supply chain. professionalization of the workforce. \r\n\r\nYou get certifications. They've been, they professionalized it five and a half years ago in that document when it came out. And they actually, in China, it's an actual, yeah, right there. Administrative Human Resources and Social Security in China, officially added data annotator and related titles like AI trainer to the national. That's what I am, you know, but it doesn't really, I don't have, there's no position like that in America, okay? There's a false dichotomy in America of what it is. \r\n\r\nThey're content writers. There's the gray beards, yeah, you go ahead, in America. So I would say, I would say, because I didn't get to interact with them, the Google engineers, I had to get it filtered through a non -technical person, which is, but they are taking the datasets created by the teams making the datasets and then they are embedding them and using them as fine -tuning. Kind of what I've showed you. Kind of what I do. Yeah, larger scale. \r\n\r\nYeah, exactly. Larger scale because AI, if it has not seen the problem, it hallucinates. If there's no training data, it hallucinates. And so, but my argument is even if Google gets their wish, their way, I think the deluge of data is going to be so much. Do you know the story of the ATM machine when that came about? So, so the ATM machine, the automatic teller machine, you know, back in the day, that was one of the first machines that, you know, like of automation that took a job and nevermind like tractors and shit. \r\n\r\nThis is like, like a worker, like a, like a office worker. Uh, and so there was a big fe"
  },
  {
    "id": "report_source",
    "chunk": "you know, like of automation that took a job and nevermind like tractors and shit. \r\n\r\nThis is like, like a worker, like a, like a office worker. Uh, and so there was a big fear that, Oh, all the tellers are going to lose their jobs. Oh no, no, no. Well, what happened once they started deploying them, the demand for banking services went up and then they wanted banks and all this stuff. cities instead of just in the main cities and all the towns. And so there was a big influx in the need of tellers, actually, because yeah, you'd still have your ATM, but then you'd still have your tellers. \r\n\r\nSo the demand rose, and that was an encounter for it. I argue the demand is going to rise for curation of datasets. Case in point, I always use this example. The hairstylist is going to have glasses that has a camera that records every moment in every day, and then they're going to stream live on Twitch, and there's going to be a viewer of two. One is them, and two is their AI. They'll train their AI that this is the right clipper to use, that's the wrong clipper to use, and then after that, when their apprentice shows up and puts on the glasses, the AI will see what they're about to cut. \r\n\r\nUh -uh, that's the wrong clipper, because it's got the training data. But you can't do that before. You can't do that. put the car before the horse. So you have to, yeah, you gotta first have the hours of cutting hair, and then you gotta annotate it, and then you gotta feed it to an LLM. First you gotta have an LLM, right? \r\n\r\nSee? To even think to do something, with the stupid data haircuts, right? So yeah, it's just all of a sudden, valueless data becomes hyper -valuable. That's what I said, internet is your hard drive. Everything needs to be"
  },
  {
    "id": "report_source",
    "chunk": "tupid data haircuts, right? So yeah, it's just all of a sudden, valueless data becomes hyper -valuable. That's what I said, internet is your hard drive. Everything needs to be reinvented, mix -matched, you know? Do it like this, but like that. \r\n\r\nI want to make a startup company, but for AI. \r\n\r\nEverything, the whole world is reinvented now. You guys are going to get the toolkit to do it. But this is an example of what it is. One side builds a ladder, the other side builds a labyrinth. Because I wasn't able to get promotion. I wasn't able to get recognized. \r\n\r\nClearly, I'm recognizable. Clearly, I'm recognizable. I could not get recognized, even in the lion's den. I found the glass ceiling. I was screaming at the top. I'm showing them what I'm building. \r\n\r\nThey don't give a fuck. I stopped showing them. I stopped telling them. I stopped sharing with them my secrets and shit. Now I'm doing it for myself. You see what I'm saying? \r\n\r\nLike, it's not good. If I was at Google getting the full -time and fucking all the benefits, the six -time salary that my research has shown, because, okay, this is how I did it. This is how I figured it out. I was doing the job. I was there doing the job, using AI to do the job. And I thought, I was like, wait a minute, what job, if this was a job, what was this job title be? \r\n\r\nCan you define this work?\" And it said, yeah, this would be an AI quality analyst. This is blah, blah, blah, the job range, pay range, 120, 150, benefit, blah, blah, blah, found equivalence positions at Google. Meanwhile, I'm over here 21 an hour as a content writer with a master's in cybersecurity, like by its own admission. by its own admission you know based off the actual work being done clear misclass labor m"
  },
  {
    "id": "report_source",
    "chunk": "hour as a content writer with a master's in cybersecurity, like by its own admission. by its own admission you know based off the actual work being done clear misclass labor misclassification so that they can make money in the split so that global logic can make money in the split between what Google pays and what they pay me so yeah so this is what's happening bro so people like me never get to grow mentally because they're so cognitively taxed I got lucky I got out of it I got the job. I got the job at UKI to be here with you guys. \r\n\r\nSo, yeah. So that's why it matters. That's why I'm screaming at the top of my lungs. That's why I stopped making a game and started making other shit. Yeah. This is my favorite, the open source Trojan horse. \r\n\r\nOpen source Trojan horse. And I did all, you know, I did my, oh, I didn't get to tell this part. So I keep getting sidetracked. So deep research, Google, I use it to do the, fissured workplace and all that. And then I think, I have the thought, your English is pretty good, your research is pretty good, but how's your Mandarin? I want you to go do research on Chinese domains. \r\n\r\nTell me about how China's doing their AI. \r\n\r\nI sent deep, I turned deep research into an OSINT tool instantly. \r\n\r\nAnd it started feeding me all this shit, like here's the companies that Deep Seek uses, and here's where they get their talent from, and here's who's using it, the police and the hospitals that they're using it for, and you know, like, dude. And here's their plan, and here's how they spell it in fucking Chinese and shit. Like, yeah dude, their whole doctrine. So, so, good question, good question. At that point, I didn't check this. This, I gut checked it. \r\n\r\nIt checks out with what I... Oh,"
  },
  {
    "id": "report_source",
    "chunk": ". Like, yeah dude, their whole doctrine. So, so, good question, good question. At that point, I didn't check this. This, I gut checked it. \r\n\r\nIt checks out with what I... Oh, and also, I opened these documents. So here's this. This is translated by Stanford, right? So I can read the documentation. I have the sources where they got the research from, the deep research tool. \r\n\r\nI kept track of it and did my citations and everything. But a lot of it is, the tool is actually really good. And also what I do is I do parallel processing. So I'll do the same query multiple times and one of the trajectories may go off the rails and the other ones don't. And so that's the one that I use. That's the research paper I use. \r\n\r\nSo in the moment when I get the research back, I'm reading the results and I'm going through them then. But I also, I don't just have one result. I have four, I have five, I have six identical conversations where I opened one conversation and I sent the same prompt multiple times. And that's part of my process. It's part of my validation process. And it's one of the key leverages that you'll see that I built into my tool. \r\n\r\nIt's built into the tool, so you'll just get to use it. And you'll see the power as you do it. You'll see. To answer your question, that's how. But when you see it, you'll know. Because I diff it as well. \r\n\r\nI use WinMerge to diff things. \r\n\r\nI don't have it set up to demo it, but yeah. \r\n\r\nI could, but it's not real. It's more relevant in the shit when we're making it, not right now. Um, okay. So yeah. So they're making, yeah. they're making a huge fusion with the military, right? \r\n\r\nOf course they are, why wouldn't they? So they stole our H -100s and they went to deep -seek, where's "
  },
  {
    "id": "report_source",
    "chunk": "making, yeah. they're making a huge fusion with the military, right? \r\n\r\nOf course they are, why wouldn't they? So they stole our H -100s and they went to deep -seek, where's the, yeah, they went to deep -seek and then they're being installed in this desert. \r\n\r\nThis is the company that stole them. \r\n\r\nOh, and NVIDIA doesn't have a backdoor. That's cognitive security in a nutshell. \r\n\r\nYou can tell NVIDIA is not thinking cognitive with cognitive security because China just stole their GPUs and now they're going to use them against us. \r\n\r\nSo one more thing. Good question. To your question, how do you know? Because it is Chinese. \r\n\r\nI understand that completely. \r\n\r\nRemember, it was a lived experience for me. I went through the entire U . \r\n\r\nS. freaking like training thing and like know what it is. And me of all like I'm a precocious guy. \r\n\r\nI'm a smart guy. \r\n\r\nI can make my way to the top. But there it's actually a glass ceiling. versus what the research told me about China's model fucking checks out, dude. Checks out. And even if it's not true, it's what we should do. Yeah, so I have that edge. \r\n\r\nIt's a lived experience for me. I actually trained Gemini. Okay, just repeat that one. What, six seconds? Whatever. Sometimes it repeats. I didn't go back and perfect this report. \r\n\r\nI was building the entire thing from scratch, including the report viewer. Yes. Let's get down here now. So now that, yeah, China exists. Got it, okay. A quick and easy understanding of data poisoning is if you have any training data where the AI is responding angrily, that poisons the shit out of your model. \r\n\r\nThat is data poisoning. An example of how easy it is to poison your data. Just a few examples of AI responding angrily in your dat"
  },
  {
    "id": "report_source",
    "chunk": "that poisons the shit out of your model. \r\n\r\nThat is data poisoning. An example of how easy it is to poison your data. Just a few examples of AI responding angrily in your data set can make it happen out in the wild. And that is where you get in the news. Yeah, yeah, let's do this one too. Yeah, yeah, yeah, okay, so this image actually this was one of the first images I made for one of the first reports that I made I Like the way I really like just the way it came out just because it was one of the first ones I made but I liked it almost looks like he's like, you know wearing war painting or something. \r\n\r\nIt looks pretty cool. Anyway, let's just move on to just repeating. Yeah, this is a good one Okay, it's the new one. I'm just learning how to skip it while it's playing. Maybe I'll pause it next time. \r\n\r\nThis is an important one. Let's go to this one. I think I just need to click this. It automatically does it. This is an important one here. Nice. \r\n\r\nYou played yourself, and you played us. This is one of my favorite pictures, actually. I love the way it came out. The data annotation jobs that, you know, create the cognitive capital because you're learning how to work with an AI. We're transferring those jobs overseas, and so go with it, all the sensemaking. That's the idea there, I think. \r\n\r\nAh, part five. Okay, UBA. \r\n\r\nThe second one, this one. \r\n\r\nThere was one of these. I don't mean to detract. There was one of these that was a really good article. I was trying to find it. This is worth just going through nice and slow. This is the last chapter. \r\n\r\nIt's worth it. It's where all the, like, how we can, how we need to reconceptualize things. And I'll be right back while it's playing. My wife wants to talk to me ri"
  },
  {
    "id": "report_source",
    "chunk": "pter. \r\n\r\nIt's worth it. It's where all the, like, how we can, how we need to reconceptualize things. And I'll be right back while it's playing. My wife wants to talk to me right back. \r\n\r\nSorry, I'm back. \r\n\r\nDid something happen with this thing? Oh, sorry. Probably. Oh, wait a minute. No, no, no, no. \r\n\r\nI was just seeing if maybe it was my headset. \r\n\r\nWhat were you saying? I'm gonna make it stop. I'm not, sorry to interrupt. Jesus, it's annoying. Okay, sorry about that. Yeah, yes, dude. \r\n\r\nI didn't do it that time. That time I didn't do it. Okay, no, okay, no, yes. You're right, you're seeing the vision. You're absolutely right. You're right, anything. \r\n\r\nThe cycles, the cycles and the artifacts. I'm not touching it. It's just started. My hand's not even on the keyboard and mouse right now. I'm gonna need to refresh this thing. It's going crazy, okay. \r\n\r\nYeah. No, that's it. That's really it. It's the cycles and the artifacts, the process, the methodology, that's it. And then the practice in that, because you need to, what does practice in that look like? That's you reading the AI's response and then discerning the differences, the good responses versus the bad ones. \r\n\r\nThere are different ways you do that. It's all in validation. Easiest way to do that is with a code, because you can take the code, put it in your project, and do you get TypeScript errors? No. Yes, that's a validation step in and of itself, but you can be an idiot and do that. Yeah, almost. \r\n\r\nNo one would listen to you. No, no, no, no, no, no, no, no, no, no, no. I'm here at, and you, what do I hear a voice? Why is my other, oh, then my audio for my YouTube is playing. Yeah, that's his voice. There it is. \r\n\r\nMy audio is going crazy, dude. \r\n\r\n"
  },
  {
    "id": "report_source",
    "chunk": "at, and you, what do I hear a voice? Why is my other, oh, then my audio for my YouTube is playing. Yeah, that's his voice. There it is. \r\n\r\nMy audio is going crazy, dude. \r\n\r\nSomething, my audio, man. \r\n\r\nYeah, my audio, dude. Everything is playing, I don't know. \r\n\r\nOkay, so, yes, all good comments. \r\n\r\nAnd yes, I mean, seriously, I, all my, okay, yes, I did work at Palo Alto Networks, but it was only for a year. Like, I had just gotten my master's in cybersecurity, or my bachelor's in cloud computing. I had literally just gotten it. Clearly, I'm a very technical person, because I've always played with computers. My mom worked for Dell. \r\n\r\nBut I've never had the credentials or the job title to qualify for it. \r\n\r\nSo at Palo Alto Networks, I was just hired to be a customer success engineer. But in the class of 18, I was the top student. \r\n\r\nAnd I was actually hired to be a tech. \r\n\r\nI was actually offered a position on the team that put the academy together. And so that thus began my teaching and enablement. in cybersecurity journey. So I got really, really, really lucky to get that opportunity. And I tried really, really, really hard, because that's what I do. And I tried to reach the top. \r\n\r\nAnd I did a good job. And I got to be that position. And then Apollo, they let me go. I got my master's in cybersecurity in three months out of spite, I was pissed off. And then after that, I was I got the position barely at Google, because the requirement is a master's contract position. \r\n\r\nAnd then Did the same thing there, got to the top, realized it's an actual glass door, holy shit, found a real job, paying job at UKI. \r\n\r\nGot really lucky with that position in that interview that they heard me and they were listening. And "
  },
  {
    "id": "report_source",
    "chunk": "n actual glass door, holy shit, found a real job, paying job at UKI. \r\n\r\nGot really lucky with that position in that interview that they heard me and they were listening. And so they offered me a position and I learned true coding as an engineer in my title, making these labs for you guys. And they really gave me that opportunity. I'd never had engineer before. title before. And so, yes, I did it. \r\n\r\nI learned all of it. I could not have done it without AI. Yes, I bullshit my way right up in there, but you fake it until you make it. You know? Yeah, man. Yeah. \r\n\r\nSo let's see if she's going to give us grief this time. The pathway. Let's see. Oh, so no, no, no, no, no, no. This is a table. This is a table. \r\n\r\nWe'll just look at it. Yeah, maybe that's why it got wonky. Yeah. Let's see. Let's see how it goes. Let's see. \r\n\r\nLet's see. Let's see. \r\n\r\nOkay, the typical output of a Sage 1 .5 coder would be labeled data points, simple annotations. \r\n\r\nThis was the, I was joking with Cameron, half joking, half serious, when I said even the gooners will grow because a lot of people are going to make a lot of raunchy stuff with this AI. A lot of, you know, hey, what drove the internet was the porn industry or the VHS tapes, right? So yeah, whatever. But my argument is.. a lot of people say that's such a waste. Dude, on Reddit, dude, there's one guy in particular, it's so funny. \r\n\r\nWhen there's those weird -ass gooners making their posts. Yeah, yeah, that's right, that's right. No, no, no, no, no, no, no, no. No, no, there's this one guy who always shows up and he just reams on these gooners. And he's like, you guys are so filthy, sick individuals. You're wasting the AI. \r\n\r\nYou're ruining the AI for all of us, all this stuff. A"
  },
  {
    "id": "report_source",
    "chunk": " and he just reams on these gooners. And he's like, you guys are so filthy, sick individuals. You're wasting the AI. \r\n\r\nYou're ruining the AI for all of us, all this stuff. And I'm like, no, no, no, no, no, dude. Even the gooners will grow because they're going to make so many waifu pictures that they can't keep track of which ones. And then they're going to have to start labeling their data sets, labeling data. They're going to start gaining the skill set, dude. Even the gooners will grow. \r\n\r\nAnd then eventually, they'll become stage two. Maybe they'll make a website to ship. Yeah, maybe they'll give her a waifu, give her a voice, bro. You see where I'm going with this? It's not crazy. It's not crazy, all right? \r\n\r\nEven the gooners will grow, okay? Yeah, even the gooners will grow. But I'm not going to make that a slide. And in fact, I swear to you, dude, this was funny. It's so easy to poison a data set. Holy shit, I said that to the AI when I was making this and shit. \r\n\r\nTrying to frame how I want my arguments. You don't get this from just asking AI to make a training on AI. You do not get this. This is a lot of work of me working on to produce this language. One of the phrases I said, even the Gooners will grow, and I gave my Gooner speech to the AI, and dude, it started putting like titles as fucking Gooner, putting Gooner as the title and shit, and fucking Gooner all over the place, and I'm like, no dude, you can't, I was talking to you, dude, you cannot say Gooner, you cannot say Gooner in my fucking report. okay? \r\n\r\nYou can't say that. So I just had to end up taking it out, dude. But I just thought about that's data poisoning, bro, in a nutshell. One fucking bit about Gooners and then the AI just keeps throw"
  },
  {
    "id": "report_source",
    "chunk": "So I just had to end up taking it out, dude. But I just thought about that's data poisoning, bro, in a nutshell. One fucking bit about Gooners and then the AI just keeps throwing Gooner in my fucking responses, dude. \r\n\r\nSo it is, it is, it is actually, yeah, yeah. \r\n\r\nI didn't think of it until I explained it to you that way. But yeah, so stage two is the AI apprentice, the data technician. Their core skills are structured prompting, use of data annotation tools, basic quality checks, identifying simple inconsistencies. Their mindset is, is this correct? \r\n\r\nLike you asked already, you already asked that question, how do you verify that stuff, right? \r\n\r\nSo their typical output is clean datasets, verified annotations, and basic quality reports. So reports of a basic quality. Stage three, journeyman developer, the data steward. These are, this is, The core skills are system design for data pipelines. I make scripts to help me make my prompt, to package the, before I made my extension, right, I started making simple scripts because copying and pasting the same file into the next conversation was too tedious. I started making, first I just started using a text file, that in and of itself was stage one tool. \r\n\r\nAnd then I started making scripts that would combine for me the multiple files I started saving. I started having to save things in multiple files as projects grew, and then instead of copying and pasting five files manually, I made a script that would just combine them, and so on and so forth. How does the data fit into the larger system? That's the approach, the mindset. All output is well -structured, validated, documented data sets, and some data governance frameworks. At a stage four, citizen architect. \r\n\r\nStr"
  },
  {
    "id": "report_source",
    "chunk": "'s the approach, the mindset. All output is well -structured, validated, documented data sets, and some data governance frameworks. At a stage four, citizen architect. \r\n\r\nStrategic oversight of data ecosystem, complex systems, orchestration, cog sec principles, adversarial data testing, synthetic data generation, all the good stuff. Data should exist and why? Like that's asking that question like you sort of did ask that actually. Like does it need to know about DNS? That's the mindset. \r\n\r\nYou will learn that with this process when you get up to here. You will just know. You will know the AIs are capable in that realm, but this is the realm that you're going to need to start curating data for. You're just going to know this intuitively. And you do it as you, yeah, you will. You'll figure it out. \r\n\r\nIt's actually a fun learning experience. Building your own mental model of the model. The outputs are robust, secure, high -quality, AI -ready knowledge bases and with resilient data pipelines. Yep. Valuable career path. Yep. \r\n\r\nHuman firewall. Ultimate security layers. Human. We'll get to that. Yeah. These are the initiatives. \r\n\r\nSo we'll get there. Duty. I never heard that one yet. That's funny. Yeah. Yeah, yeah, basically that. \r\n\r\nOnce you got your shit curated, ask any fucking sentence, any question. And again, the power of this is you can pivot. You can ask for literally anything. Like, I had my whole project, my data curation environment, the prompt that I was using to make it, right? So I had the prompt, my artifacts, and my code, and then I just pivoted with the same prompt. I said, We're making a white paper now. \r\n\r\nWe're making a white paper on this extension, and the benefits that this extension can bring to "
  },
  {
    "id": "report_source",
    "chunk": "just pivoted with the same prompt. I said, We're making a white paper now. \r\n\r\nWe're making a white paper on this extension, and the benefits that this extension can bring to a corporate environment, to corporate work, working and making things like the labs I design, and anyone making anything, like you say, literally anything. And so I made an artifact that outlined how the white paper should be. I worked on that. I made a rough draft of the white paper, and then I made the actual white paper. And then I took the white paper itself, and then I sanded it. make three different image prompts for each section of the white paper. \r\n\r\nAnd then I took each one of those image prompts, one at a time, and made them. So then the white paper had this white paper. Literally, it's one tab away. It's this one. I made this in an evening, a few days ago during while I was up there in Maryland, right? In the evening while I was working on other shit, I put this thing together, right? \r\n\r\nfor Eric, who is the bald guy. But I don't know how much he's going to absorb, right? But I put that together in an evening that in and of itself is a point. So this is all about the extension that I made. How amazing is this? My own Parallel Copilot panel described in this white paper and envisioned in this image. \r\n\r\nIt's actually quite remarkable. The fact that you can actually just pick and select, check, I want this file, I want this file. Quite remarkable. My cycle navigator, apparently I made a knowledge graph and I didn't know it. So that's what my cycle navigator is and capturing the cycle, the curated content, the user intent and the AI solution. So all the craziness of working in the corporate world goes through this extension and can become "
  },
  {
    "id": "report_source",
    "chunk": "pturing the cycle, the curated content, the user intent and the AI solution. So all the craziness of working in the corporate world goes through this extension and can become amazing knowledge assets for teams because you've curated the data asset. \r\n\r\nYou can share it by the way. The extension, you can share the selection of data you've selected and you can share all your cycles. so it's a seamless so this guy like I love this picture because this dude figured out the whole like fragmented chaotic workflow or maybe that's been his job for the past five years and now the new person coming in has to pay up and figure this all out. No, no, no, no, here, he's already picked them all up, and all you've got to do is just ask your question. So you're going to do this. \r\n\r\nYour Slack bot will be doing this in the back end. You'll have curated your knowledge base. You'll have made your delivery system. And then the users will just literally just ask the question. And then also, you can go back through the cycles and discuss and describe. Y 'all are too young to care about how to do work. \r\n\r\nLike, I've been working for as long as you. Anyway, okay. So, conclusion. \r\n\r\nI love this picture. \r\n\r\nBecause actually, David Deutsch gave a TED Talk. The title of the TED Talk, in the TED Talk, he described this image, actually. And the title of the TED Talk was Pond Scum Who Dream of Distant Quasars. And in the TED Talk, he explained what is the most, dense piece of knowledge in the universe. So it might be a spaceship flowing through with like a database of all the knowledge that it's accumulated so far. I was like, that's an interesting thought. \r\n\r\nWell, look at that, oh my God, look at the AI produced, getting to the point. \r\n\r\nAnyway"
  },
  {
    "id": "report_source",
    "chunk": "knowledge that it's accumulated so far. I was like, that's an interesting thought. \r\n\r\nWell, look at that, oh my God, look at the AI produced, getting to the point. \r\n\r\nAnyway, so anyway, yeah, that's funny. Okay, so back to this, I paused it because we're in a graph and I'll go through it myself. It's much better than the AI's voice on the graph. Dimensions, this is just prompt engineering versus context engineering. The core functions, let's just go through on the left, the prompt engineer would be crafting specific instructions for a one -off response. They would be asking, how can I phrase this question perfectly? \r\n\r\nThey would be focused on single input -output pair, the prompt, and then what the response comes back. It's a low scalability, it's very brittle, and it requires manual tweaking. The key skills are being having language creativity, yes, thinking to ask for a report or ask for get a KPI, you know, being creative a little bit, some intuition, some trial and error, primary tools or text editors, and AI chat interfaces, versus the context engineer, the science of architecture, you would be designing in the core function is designing a dynamic information ecosystem for consistent performance. Your mindset is what does the AI need to know this answer perfectly, you're already kind of in line with the context engineering. The scope, the entire context window, that's the scope. \r\n\r\nAnd the entire context window is every word from your first input, your first word in your input, to its last word of the output. That's all considered one context because every word that came before the last word goes into the calculation of the creation of that last word. See, so it's painting the whole picture from the first word "
  },
  {
    "id": "report_source",
    "chunk": "ext because every word that came before the last word goes into the calculation of the creation of that last word. See, so it's painting the whole picture from the first word to the last, so that's how you conceptualize one big page. That includes the memory, that's what you brought in from your PDF, the documents, any tools, so like if it can go do a Google search, if you've programmed that for it, no big deal. You can have AI help you make the tool. Give it a calculator. \r\n\r\nYou can do that, because AI can't count. \r\n\r\nAI can't do math. \r\n\r\nBut if you give it a calculator, just like a human, it can. History, like a chat history. And your instructions, that's sort of the scope of the work. Is it scalable? Fuck yes. Key skills, systems thinking. \r\n\r\ndata architecture, information retrieval, and security. The tools are the vector databases, knowledge graphs, RAG frameworks, all the spicy juicy stuff, data curation platforms, all the fun stuff. Okay, any questions before I click the autoplay? This is, this is, we're getting through it, but this is worth coming slow. Cool, cool, cool. Dude, I just thought, dude, imagine I had a professor like this, dude. \r\n\r\nImagine I had a professor and this was the lesson. That would be so fucking cool, bro. You just stop and ask, you know, questions or whatever to him and then fucking go back to this fucking, that'd be crazy. I don't know. Anyway, I don't know. Whatever. \r\n\r\nOoh, yes. That's my, that's my special sauce. Oh, my guy graph, okay. 100X Curator, Intelligence Analyst. Those are just the three over. Probably just a repeat of what you just, yep, all the repeat. \r\n\r\nFair, principles, blah, blah, blah. Red teaming, all the same stuff, okay. I really like this image. I really like "
  },
  {
    "id": "report_source",
    "chunk": "ly just a repeat of what you just, yep, all the repeat. \r\n\r\nFair, principles, blah, blah, blah. Red teaming, all the same stuff, okay. I really like this image. I really like this image a lot. The way that, because it, okay, so the AI might not get the right order of things always. Like, see how he's facing this way? \r\n\r\nHe's facing like that way, and the, you know, and then the shield is on this side now, right? It's like, yeah, it's just part of the, and now it's like over here. So it's like. So it could just pass. So it's silly, silly, silly. But this is the one that's, you know, it's interesting, you know? \r\n\r\nThat's why I run so many images because, yeah. and you're the first two DCIA students. Yeah, so hold on. I want to stop. OK, so my extension, I made this report before I had the idea about my extension, right? Sure, I had the idea to automate my process. \r\n\r\nSure, yeah, it sounds like a painstaking, insurmountable task. But I did not have the idea to make a VS Code extension, right? So I did not have those two. And so that's literally on the fly tooling. Creating literally the tool to do the thing that I'm doing, it's crazy. It's like I use my own prompt to make the tool. \r\n\r\nIt was so surreal to make this extension with AI, dude. It was surreal. Anyway, this is what I wanted to emphasize on. Because my wife, she's got the bug. She sees the light. And she is doing her data science degrees and stuff. \r\n\r\nAnd she literally did this before I even made the report. So even before this report was made, she made her own quiz generator. It's fantastic. In and of itself could be like a product. All you do is you give it like a PDF or like a list or like your chapter notes or your chapter text from your textbook, and it'"
  },
  {
    "id": "report_source",
    "chunk": "stic. In and of itself could be like a product. All you do is you give it like a PDF or like a list or like your chapter notes or your chapter text from your textbook, and it'll make 200 multiple -choice questions for you on that and those topics. And it'll, you know, give you immediate feedback on right or wrong answers, why it was right or why it's wrong. \r\n\r\nAnd she's been passing everything next class. Next semester she just takes and adds, she updated the app to be able to have a list. of quizzes. Now she can add a second data set and just select which one she wants to generate the questions for. And the questions are all generated on the fly so she can just generate a new, a whole new 200. So that's literally stage three right there. \r\n\r\nYou know, it's pretty cool. Okay, so this one, yeah. Let's see. \r\n\r\nStage one. \r\n\r\nSo this is a matrix detailing roles. skills, activities, and the function of AI across the stages, okay? \r\n\r\nSo on the cognitive annotator, the learner's role is to be a critical analyst of problems and solutions, core activities are decomposing problems. \r\n\r\nYou're basically learning how to work with AI, honestly. You're building your own mental, every single prompt, you have to read it and understand it so that you know it, and then you can use it later as your tool, as your toolkit, because in the future, you'll remember what the AI could have answered, because you've actually seen and thought about and responded to that. prompt before. That's what you're doing at this stage. \r\n\r\nIt just takes time. \r\n\r\nGoing to the range. You've got to go to the range. Pattern recognition, logical decomposition, attention to detail, bias detection, critical thinking. These are what you're going to be developing. "
  },
  {
    "id": "report_source",
    "chunk": "u've got to go to the range. Pattern recognition, logical decomposition, attention to detail, bias detection, critical thinking. These are what you're going to be developing. And then the AI is a scaffolded solution space. It's that you're on the ground, you're playing with your digital Legos, that it's produced for you, that you can combine and get them to work in your code project. \r\n\r\nAnd again, it doesn't have to be code. You could be writing a book. You could use my whole tool and toolkit and not write a single line of code. You could be writing a book and you need to artifact out all your characters and all the scenes and the locations and artifact it all out. Artifact out a timeline, how you want your set, your book to flow. \r\n\r\nAnd then you start building out chapter by chapter. \r\n\r\nthe book will pretty much write itself when you've got all these artifacts created. Okay, adaptive tool maker, stage two, that's when you start making tools, like a little script here and there to make your process be easier. I've abstracted a lot of that away, but you will still find things that you will probably benefit, like for example, let's do this, this is a good example. Nope, yeah, that's fine, who cares. I need 1650 ELO, 2650 ELO on my game AI, so let me just level up my game AI a little bit. Let me get my ELO to 2366. \r\n\r\nThis is my game. This is, I'm leveling up my game AI. It's predictive aiming, it's advanced combat logic to get more. Okay, there we go, I broke the threshold. Enter the circuit. So now this is my, presumably, you know, ostensibly my AI. \r\n\r\nOh, we got a pop, we got a lag. Where does the lag come from? I'm trying to think. It's been fixed for the longest time, but it's back at the moment I'm showing it to "
  },
  {
    "id": "report_source",
    "chunk": "AI. \r\n\r\nOh, we got a pop, we got a lag. Where does the lag come from? I'm trying to think. It's been fixed for the longest time, but it's back at the moment I'm showing it to you. But anyway, so in order to create this, which is every single movement is when it moves is, oh, that's the lag. Okay. \r\n\r\nOkay. We're back in. The lag is gone. Okay. I had to close some shit, but we're still wonky. Can you hear me? \r\n\r\nOkay. Let me stop and share. Oh, hold on. I have to disconnect my monitor. Yeah, the lag is gone now. This is what you get when you buy the cheapo laptop. \r\n\r\nIt's a super powerful laptop, but I bought the cheapest one. It's got 64 gigs of RAM. It's got 16 gig video card on a laptop, but it's Asus, it's Asus, it's Ryzen, it's not Nvidia, right? But it's still, it's good. Minus these little imperfections. Oh, I can see, can you see it now? \r\n\r\nI can see it in my window of my, I wanna see if you can. I'll stop streaming. Yeah, stopping the stream just killed Discord. Okay, now we're back. And the lag is gone, right? And now they're moving. \r\n\r\nBut okay, so here's the deal, here's the lesson. Here's the point of on -the -fly tooling. When I wanted to make this, I had a vision in my mind of what I wanted, and it was this, all right? Because what is this? OpenAI, before they made Chat GPT, this game I followed history. In 2016, The team who made ChatGPT made an AI called OpenAI5 and it could be and it \r\n\r\nthe world champion of Dota in 2016. For the first time, a human team in Dota was beaten by an AI team. Big, big, big deal. Kind of a big deal, actually. And then in 2017, the Google research paper, Attention is All You Need, came out. And we all know what OpenAI pivoted to after that. \r\n\r\nSo I wanted to follow histor"
  },
  {
    "id": "report_source",
    "chunk": "actually. And then in 2017, the Google research paper, Attention is All You Need, came out. And we all know what OpenAI pivoted to after that. \r\n\r\nSo I wanted to follow history. The first AI you make in this game is an AI that can play Dota. That's what I wanted. So when I got to this point and I got the Dota map and I got the little UI, the battle viewer, and I had the little blue and red little user icons. And then when I tried to get the AI, okay, now make a move, right? And then all they would do is they would just fucking like jiggle. \r\n\r\nThey would jiggle or they would all like, go to the top and just do that. And this is nothing. This is what I wanted. I wanted this. I knew what I wanted. But I also knew that in the back of my mind, what are the necessary data points to achieve this at a bare minimum? \r\n\r\nI deconstructed the problem. They are XY coordinates for each one of these over every second or movement step, right? so that's the and over this x y or the y x right that's that for each of the 10 okay that's the bare minimum that's the bare minimum so i that's what i would and i know the ai that i knew the ai could not generate that yeah that's not a generatable you see i'm saying it would be guard it wouldn't it would not there's no training do you know what i mean do you see how i'm how i'm deciding that's not the model's capabilities, right? In your question, like, can it do DNS? I don't think it can make JSON log replays. \r\n\r\nAnd let me show you what they look like, case in point. It's actually interesting. They look sort of like a tape deck, as it would, like as if you stretch out a tape from a cassette tape. Maybe that's before your time. Okay, yeah, so the tape, it looks like the tape of the tape deck, w"
  },
  {
    "id": "report_source",
    "chunk": "e deck, as it would, like as if you stretch out a tape from a cassette tape. Maybe that's before your time. Okay, yeah, so the tape, it looks like the tape of the tape deck, which is remarkable, because it's a tape. It is a replay. \r\n\r\nSo, oh, and it makes sense because it's under similar, this is a learning lesson. It's simply, the remarkable part is not too remarkable once you realize it's just because the two are under similar constraints. They're both a replay. Oh, so it makes sense, they sort of look alike. Okay, so if I just do a search for, supposed to be a, there we go. Trying to find a quick way to get to the replays. \r\n\r\nHow do I, Jason, how do I, oh, can I do that? Yes, there we go, there we go. Okay, here's one of the replays. Here's one of the, I made, I ended up making 52 different replays. So I did this over a weekend, but we'll get to that. So I'm showing you the end product and then I'll tell you how I got to this. \r\n\r\nSo this is what these replays look like. They're just long strings of time minus 89, like the Dota game, you have 90 seconds to start. And then slot 0 through slot 9, those are the 9 players. That's the one team, team A and team B. And then these are their XY coordinates. And then I also just, at that point, I went ahead and grabbed the kills and the net worth as just two additional data points to play with those data points. But I'm sure y 'all have played Dota or League of Legends or that, you know all the creeps and all that. \r\n\r\nThere's so many data points. There's so, every creep movement, every creep attack, what its health was is, when it died, all those data points is in those logs, the replays that you can download. You can download those replays. And those are the replays that Op"
  },
  {
    "id": "report_source",
    "chunk": "t its health was is, when it died, all those data points is in those logs, the replays that you can download. You can download those replays. And those are the replays that OpenAI trained their game bot on to even beat the players, right? And so that's what I did. \r\n\r\nI went and I downloaded all those, a bunch of replays from, you know, whatever it is, Dota Live Replay API, whatever it is, OpenDota API, OpenDota . \r\n\r\ncom. where all the games that go on do get saved and put in here and you can go through and download and watch the replay in a very sophisticated system or I just made my own system replayer. See? And so that's what I did. But there are 200 megabyte files each replay. They're huge. \r\n\r\nThey're big. And so I had to parse them down. I had to extract only, first I had to even find the data points in them. They're gobs of data. Where's the few data points I need? So that's this control F searching. \r\n\r\nI had the idea, aha, I need the X, Y, so maybe it's an X. I just did a control F for X. And then I put, it's a lot, so I put an X with a quotation mark and I found it, the X and the Ys. And I analyzed the lines and I realized how I could get to them. And I started using, I started making a script. to help me parse it, and then once I've made a script and a workflow to this 200 file down to two megabytes with just the data points that I needed for my replays, I then just repeated that 51 more times. I picked a bunch of different games and I labeled the games, whether or not the player wins or loses, whether they lost badly. \r\n\r\nAnd you can see the score, they got spanked, nine to 28, 10 to 44, or they lost big. So it's not so big of a loss spread. The loss spread is less or whatever the game was closer. close game"
  },
  {
    "id": "report_source",
    "chunk": "e the score, they got spanked, nine to 28, 10 to 44, or they lost big. So it's not so big of a loss spread. The loss spread is less or whatever the game was closer. close game and then whether or not it was a normal length game, a long game, it went long or it went short, or it was epic like it was over an hour long. And then so now with that, with the game now, depending on my ELO score and the threshold and the difference, I can then have my, I can now have an algorithm pick which replay it should play for the player based off the ELOs. And that's what you just watched. \r\n\r\nAnd apparently I just lost. \r\n\r\nOh, I lost, but I did get, I did get replay data. \r\n\r\nAh, this is annoying. \r\n\r\nOkay. Okay, we're back. We're back, baby. Okay. I did get replay data. So what does that mean? \r\n\r\nThat means I can retrain my game AI agent. So I'm going to start. See, I got all the data I can accumulate. Tier four data. Start training. Ooh, I started a training run. \r\n\r\nI got a training loss curve. \r\n\r\nI'm training the game AI agent in my cluster A over here. \r\n\r\nAlmost done. Okay. Trained. Now I can benchmark it. So I'll run an eval. Oh. \r\n\r\nThat was quick. Actually, it's supposed to be a lot longer than that, but that's okay. \r\n\r\nWe don't question it. \r\n\r\nWhat's going to happen over here? Oh, it just went up. The data quality score and the Elo ranking just went up. Now my game AI is a bit... because I trained it more. See? \r\n\r\nI simulated fucking... Yeah, I fucking simulated it all, dude. Okay, so... And then you can train LLM. You can't talk to it. I think I showed this to you in there. \r\n\r\nI'll show it to your roomie real quick. But there is just... And so I've got it. It will change the response, the quality of this response and th"
  },
  {
    "id": "report_source",
    "chunk": "wed this to you in there. \r\n\r\nI'll show it to your roomie real quick. But there is just... And so I've got it. It will change the response, the quality of this response and the content. based off the level of my AI that I have in the game. So right now, it's got no features. \r\n\r\nIt's a very basic LLM, so how does it respond when I say test? Test, okay, you said test, I said test. Test is good. You want to test more? I can say test. Tell me, tell, tell me a joke. \r\n\r\nTell me a joke. Okay, here's a joke. Why did the chicken cross the road to get to the other side? Hehe, that's a good joke. You like? No, like a good joke. \r\n\r\nBut it's a real AI, dude. You make your own little AI. And I even had an idea. Imagine you could make a URL for your players, where they could link to it. Because this is all live. Everything you're watching, reading, it's all on you. \r\n\r\nIt could go to this website. But I could make a URL that you could share with someone else after you made an AI in here and trained it. And then it would be your AI, literally. You could share a chat interface. That'd be cool. OK, joke time. \r\n\r\nWhy didn't scientists trust atoms? Because they make up everything. Aha, good joke, yes. OK, cool, cool, cool. So anyway. Yep, so that's that, that's that. \r\n\r\nLet's get back to here. And then we'll go in through these, Cursive Learner, Adaptive Toolmaker, On the Fly, Systems Thinking, Adaptive, you start using the AIs of that tool. component library. It's like, hey, do you know about this library or what library can we use to solve this problem? Yeah, I remember all that stuff. It gives you functions and snippets for the learner to assemble into their project. \r\n\r\nThen you become a recursive learner. That's an example of me. "
  },
  {
    "id": "report_source",
    "chunk": "remember all that stuff. It gives you functions and snippets for the learner to assemble into their project. \r\n\r\nThen you become a recursive learner. That's an example of me. Once I got the bug of these images and I realized, whoa, whoa, whoa, whoa, whoa, you can do text and it's not gibberish, I figured out the trick. The trick is you just put it in quotations. You tell, okay, first the trick is you understand the system you're working with, which is there's a diffusion model that is smaller and like an autistic savant that knows how, where every rivet on the battleship should be, but it doesn't know that ships go in the ocean. It'll put it on the land. \r\n\r\nSo like that, so like that, you know. And then there's the actual AI that you're talking with, like gemini . google . com, when you say make me a picture of a cat. Well, that Gemini will then make a tool call to the diffusion model with your request. And it's depending on how that Gemini wrote to the diffusion model is the image is going to come back. \r\n\r\nDid it put quotes? Did it put the words it wants the diffusion model to produce in quotes or not? Did you tell it to? Because they didn't fucking train it to. Maybe they have now, I don't know. But when I made all these, they didn't. \r\n\r\nThat's what all the gibberish came from. But I figured it out. I figured it out by making hundreds of images. Wait a minute, why does this one have words? This one did not. Wait a minute, why are these spelled correctly? \r\n\r\nThis thing can spell Kibana? Do you remember the training? That training with the one that it says Elasticsearch, Kibana, Logstash? That image was the first image I ever got in AI to produce. words spelled correctly. And it was Elasticsearch, Bogstash, and Kiban"
  },
  {
    "id": "report_source",
    "chunk": "hat it says Elasticsearch, Kibana, Logstash? That image was the first image I ever got in AI to produce. words spelled correctly. And it was Elasticsearch, Bogstash, and Kibana. \r\n\r\nAnd I was like, wow. And then now we're here. now. This is a nice. So I made 2 ,000 images That's the stage 3 recursive learner an engineer of one's own expertise Manda cognitive analysis of personal learning gaps building personalized learning accelerators Me making this making 2 ,000 images not because I needed 2 ,000. I could just make one for each page I wanted to I wanted to get perfection in these images the The key skills developed is advanced metacognition, recursive thinking, expertise modeling, self -regulated learning. \r\n\r\nThe AI serves as a meta -tool used to construct personalized tools that enhance learners' cognitive capabilities. What I was using it for is I would have it, you know, so for example, Solarpunk, which you see in front of you, is the Solarpunk dichotomy to Cyberpunk. See, I never knew the term Solarpunk. Maybe you're just learning it. You probably knew Cyberpunk. We all know Cyberpunk. \r\n\r\nBut I didn't know solarpunk. \r\n\r\nAll I had to do is know that term that AI brought it to me throughout this process. I then learned solarpunk. I'm like, oh, well, that's easy. That makes sense. That's the term. So now I have an image system prompt. \r\n\r\nI have an artifact that describes how we're making a report and how pick through the motif, the theme of anywhere between like early cyberpunk without cyber cybernetics augmentations modifications that kind of stuff and solar pump is the hopeful visual for the future and anywhere in between that that that sort of scale that's my instruction to the AI now and that's how we get them"
  },
  {
    "id": "report_source",
    "chunk": "f stuff and solar pump is the hopeful visual for the future and anywhere in between that that that sort of scale that's my instruction to the AI now and that's how we get thematic and then I eventually made a section in my system prompt that really articulated and I had the AI help me write that part of the system prompt, because it sees the system prompt. It can update its own system prompt with my articulation. And it made a section to spell out how to make, because it's one thing to have this with colors and things, versus just like white. See? Like white, right? \r\n\r\nSo then I started to get more consistency. I wanted to try to get more consistency in the titles so then I started making a section in my system prompt that contains like that grayish sort of that look sort of try to capture that sort of white gray kind of thing going on as you can see sort of it kind of this was before I sort of thematic -ing it out and this is after this gray and white it's gray and white see so yeah and I never and I didn't and I see gray and white and I have not gone back through and sort of standardized everything I don't need to. I'm on a newer project, right? You know, so okay, so we're getting ahead. Hold up, hold up. I skipped the UBA. \r\n\r\nThat's where you wanted to know. We're almost there now. We're almost there to the UBA. Okay, so three slides away. Okay, so I think that's all. So let's finish this one. \r\n\r\nThe DCIA, the step four. The learner's role is to be a master, practitioner, and mentor. The core activities are fluid, intuitive, Human AI collaboration. That is what it feels like. On -the -fly tooling. That is what I make. \r\n\r\nDesigning complex systems. I'm going to make up my own PCTE, guys. That's the first project th"
  },
  {
    "id": "report_source",
    "chunk": "tion. That is what it feels like. On -the -fly tooling. That is what I make. \r\n\r\nDesigning complex systems. I'm going to make up my own PCTE, guys. That's the first project that I'm going to make with my own tool. I already have a plan. Yeah, yeah, yeah, I'm going to do it. So, key cognitive skills developed, true intuition, strategic foresight, effortless execution. \r\n\r\nThe AI's role is to be a cognitive exoskeleton that augments the expert's intent, speed, and reach. \r\n\r\nIt really does not read well. \r\n\r\nOkay. Okay. Okay. Yeah, so that's sort of so this is kind of my idea The way it would work. We'll get to how it works But yeah, well and this is a good example This is also a good example of why to run multiple products because when I did this one I did four different research proposals on UBA and have it go try to find stuff and one of them was like Uh, start starting to like, um, Oh, AI credit. \r\n\r\nNo, not UBA. \r\n\r\nIt was on AI credits. Let me tell the story when we get to AI credits. Okay. So, uh, yeah. Um, yeah. So which may be next or soon. \r\n\r\nThere was a moment where I was riding in my cycles and I was, and I was like, wait a minute. I literally wrote, wait a minute. I'm like, wouldn't the value of these AI credits go up over time? Like no matter what, like, because if you have a thousand credits today and you don't use them, you wait five years. You have thousand credits five years from now and now you got Gemini like 17. Those are thousand credits. \r\n\r\nare gonna make you a fucking, fucking, you wouldn't download a car. \r\n\r\nFucking maybe you will with Gemini 7, maybe it's got a 3D printer, you can connect it to your fucking 3D printer, you know what I'm saying? \r\n\r\nLike, you know what I'm saying? So like, so th"
  },
  {
    "id": "report_source",
    "chunk": "ou will with Gemini 7, maybe it's got a 3D printer, you can connect it to your fucking 3D printer, you know what I'm saying? \r\n\r\nLike, you know what I'm saying? So like, so those 1 ,000 credits are gonna go a lot farther because the same, you know, the AI today can maybe just source your email, but then the AI next year can maybe make you a web application, okay? And so, yeah, I've had this thought, I was like, wait a minute. So I'm like, wait a minute. Yeah. And so part of it is the AI, but a lot of it is the driver as well. \r\n\r\nI have to come up with these ideas and bring them, but I'm doing all this reading of this research, where do you think the ideas are coming from? Because I'm reading this good research that this AI is bringing to me. It's Bitcoin. I call it Bitcoin without the stress. And so here's a little graph. This part is a little wonky on the graph. \r\n\r\nThis part makes this part larger. Sorry about that. But once you get over that, you can actually see what this is trying to say. You know, the first year, you can give someone 100 credits or $100. On the second year, that $100 is worth $97 because of inflation. But that credit, just one year later, is worth 167 credits, the equivalent. \r\n\r\nbased off this math of Wright's Law and the drop in performance and the price of AI over time that we have measured so far. \r\n\r\nAnd so extrapolate that out 10 years, those 100 credits are worth 6 ,000 credits, while the $100 is now worth $73. \r\n\r\nWe need to start this yesterday. \r\n\r\nSo, okay, how can we start this? \r\n\r\nKinda, now it's minutia. \r\n\r\nLet's see, let's skip. \r\n\r\nYeah, now minutiae. \r\n\r\nWell, that's more minutiae. \r\n\r\nHa ha ha \r\n\r\nYeah, so you made it to the end almost no one literally y 'all are literally Lite"
  },
  {
    "id": "report_source",
    "chunk": "t's see, let's skip. \r\n\r\nYeah, now minutiae. \r\n\r\nWell, that's more minutiae. \r\n\r\nHa ha ha \r\n\r\nYeah, so you made it to the end almost no one literally y 'all are literally Literally, I think the only people now we have skipped a bit as I was going through I saw the Chinese China part. \r\n\r\nWe actually did skip some of the real juice here, especially We did we skipped this we skipped intelligent eyes warfare we skipped we skipped the open -source drug and horse We skipped we skip this stuff, but I remembered that as it was talking I was like, ah, we didn't we didn't we clearly skip some of this stuff because we didn't capture that but that's okay. That's okay So so now that you've gotten the notion like any any idea any thoughts questions? What do you guys think of this? I mean dude, like what I do it everyone should be seeing this website dude, like everyone needs to learn this Yeah Cool. All right, so then let's do this. \r\n\r\nI will share the extension with you guys, and then I will start the project. Or hey, we've been going for hours, so let me know how y 'all, but that will be the next step. Whenever you're, dude, I am ready. At the moment, we're done here. All I'm gonna do is keep cooking. I'm gonna just make that thing I was talking to you about. \r\n\r\nIt's actually gonna be pretty fucking cool, this thing. Yeah, yeah, go ahead. \r\n\r\nOkay, I was going to ask for a break or anything, but just showing you. \r\n\r\nUm, yes, I'm getting hungry too. The wife is hungry. Yep. Yep. \r\n\r\nAny day later on this afternoon. \r\n\r\nTomorrow's fine. Yeah. \r\n\r\nAnything, dude. \r\n\r\nAgain, like I said, I'm just on this because again, you saw the path, you know that I'm on that scale. I'm going up. \r\n\r\nI'm at a hundred. I'm going for a thousand nex"
  },
  {
    "id": "report_source",
    "chunk": "de. \r\n\r\nAgain, like I said, I'm just on this because again, you saw the path, you know that I'm on that scale. I'm going up. \r\n\r\nI'm at a hundred. I'm going for a thousand next dude. Uh, okay. So, um, if you want to join, like you got to get started to, um, just showing you, I'd see I'm starting my next project. So I will. Do that do this in front of you, but with your idea and then you will just do it with me That's part of the scaffolding process That's the step one through four and I'm just saying it to you now so that you kind of get a picture of when we do get there You'll know what to do. \r\n\r\nBut for now, I'll go ahead and get this file out to you both the current version of it I will demons you won't be using it, but I'll just demonstrate one tiny aspect of it Not on this window because I haven't started it over on this window over on this window. I have started it so it should be um yeah in my cycles so see this see this right here see it's all gone luckily luckily i have that this working um save and load stuff so well i thought i had it working so at least i got uh part of it oh hold on wait so okay i'm working on that so it's a process um i have it um in here i have to reconstruct it now see So I guess I'm gonna work on this, right? Actually, I'm kind of working on that right now. So this was a good test to see when it failed when I restarted this thing, to see that it failed. So now I can construct it to it and work on that. \r\n\r\nI've been waiting to, that's another thing you'll learn is I'll wait to observe a failure more so I can describe it more to the AI. Because if you actually are describing the error incorrectly, You actually get your you can really get yourself up shits creep because it starts making a"
  },
  {
    "id": "report_source",
    "chunk": "be it more to the AI. Because if you actually are describing the error incorrectly, You actually get your you can really get yourself up shits creep because it starts making a lot of changes to your files On something that's not even really honestly and then you later realize all the problems over here. Fuck You know those so so i'm taking my time, but it was good for me. Anyway, yeah So, uh, what when when do you think is good for you guys? Um, okay. Sure. \r\n\r\nUh, cool Um, then you don't need my extension if you want to start curating like anything you think you might need for jqrs You can just make a folder and start dropping shit in the folder And then when we get together, yep, no problem. Understood. No problem. Understood. I got you. Yep. \r\n\r\nSo you know. Yeah. Yep. So then there you go. So then, yeah, that'll be the plan. And then we'll just, I'll get, we can start the Slack bot. \r\n\r\nCause you don't need that. Cause again, that was the last piece of my Slack bot, right? I got everything up and working before that. And then I said, okay, now let's do the PDF thing. So you can do all of that. And then when, when it's ready, you, you can do that with test PDFs. \r\n\r\nThe guy in the video was using the U S constitution. So he could ask questions on the U S constitution. You know, you can test it for anything. And then when you're ready, you just go get your, we'll make it. We'll make the, we'll make your knowledge base. Cool. \r\n\r\nOkay. Thanks for taking the attention. I think everyone needs to know. And the more people we can get on board over time, the better it starts with showing them the product. So yeah, you're on the right track. Cool. \r\n\r\nCool. Well, nice to meet your friend, your roommate. See you guys. Perfect."
  },
  {
    "id": "report_source",
    "chunk": ", the better it starts with showing them the product. So yeah, you're on the right track. Cool. \r\n\r\nCool. Well, nice to meet your friend, your roommate. See you guys. Perfect. Yep. I'll look for it. \r\n\r\nYou too. Bye.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-9.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nAnd now it looks good too. When I, so you export it, uh, you can, you can see the Excel in the flatten and then it tells you the, the, the token size. Like it's, it's really nice. Um, now I'm planning out the phase two, which is the basically a clone of notepad plus plus the way I use that multiple tabs. If the reason I'm bringing this up is because I'm doing the planning phase now. So it may be beneficial to look at some of the documentation that I'm creating, because it's mirroring the same documentation creation process that I did when I started this VS Code, like from phase 0, like phase 1. \r\n\r\nBut again, the reason why we wouldn't do that is because there's something more pertinent, which would be helping y 'all get y 'all's documentation in order. Talking about putting the chicken before the egg would just help you give you more perspective forward thinking for putting the documentation together, or if you think you're good, you just want some guidance based off that, I'll just let, I'm just sort of, you know, spitballing. \r\n\r\nAgreed. \r\n\r\nmm -hmm yeah the version of the are you do you need excels do you have Excel documents all right let me just package this then and then I'll send actually yeah let me just package what I have I'll give you a new one anyway because it'll be Excel friendly your guess is as good as mine I'll ask my colleague my frien"
  },
  {
    "id": "report_source",
    "chunk": " send actually yeah let me just package what I have I'll give you a new one anyway because it'll be Excel friendly your guess is as good as mine I'll ask my colleague my friend my discord friend who has tried to install it and he's had success I'll see if he has to reinstall it I haven't actually installed it I just, the way I do it, I just click the run button and I'm in the, I get it, the dev environment. I haven't tried the, yeah, the package. The file, it's 41 kilobytes. \r\n\r\nHow big? \r\n\r\nWell, it was, I thought it was smaller before even, but I'll just upload. That's what I'll do. \r\n\r\nGoogle Drive. \r\n\r\nI wonder if I ever do like a full -fledged official publish where it gets on the extension store or whatever, then it would become more universal. And then I just dropped a link that should you may in general chat in discord. You may or may not need a permission I don't know if if you do I'll get the request check its shares when people with yeah, there we go. Okay Oh and I added I made sure delete worked so you could press the delete button Little things the less you have to switch tabs the better. I'll be right back. I have a coffee ready \r\n\r\nUh, I sent a URL in the general chat in Discord. Delete immediately. Yeah. Uh, maybe it was some, I don't know what it was that made it bigger. \r\n\r\nLet me double check. \r\n\r\nIt did get bigger. \r\n\r\nI'm double checking like, oh no. \r\n\r\nOkay. \r\n\r\nYeah. Yeah. The second one was about the same size. Probably the libraries I added like, uh, to parse the PDF and to get to the XLS. \r\n\r\nYeah. \r\n\r\nThat would be my guess. \r\n\r\nUm, yeah. you be able to double click it? Share your screen. Yeah, so it's in the very bottom. It's on the left. It's all I see the tab on the left, far further left o"
  },
  {
    "id": "report_source",
    "chunk": "ss. \r\n\r\nUm, yeah. you be able to double click it? Share your screen. Yeah, so it's in the very bottom. It's on the left. It's all I see the tab on the left, far further left on the bottom. \r\n\r\nYep. There it is. Yeah. \r\n\r\nSo, okay. No, it wouldn't be there. It would be something where you could get into that extension to either delete it or into that extensions settings. \r\n\r\nCould you uninstall that? Yeah. Is there? \r\n\r\nI haven't ever uninstalled it. And I can also just see if my friend knows. Oh, I can, I'll see if I can find instructions in this book. \r\n\r\nOkay, cool. \r\n\r\nPerfect. \r\n\r\nYeah, I can't read, I can't read that text or else I would have caught it. You're a gloriously large monitor. No, it doesn't, it doesn't, that doesn't, it's a book. pixel issue. Cool. \r\n\r\nDid you double -click it it or it didn't? \r\n\r\nExtensions manage manage extensions. Do you see that or so like extensions tab or anywhere manage extensions? So I thought I saw something up here Okay, is that it below the cat on the left a bit up one up one? Oh, you can click and drag those cool already. So now it handles it'll handle XLS So you just click the button and then you get your flattened XLS you get your flattened PDF without any You won't even get a have to manage the markdown file. \r\n\r\nIt's just there. \r\n\r\nIt's just done So all you've got to do the only thing you can't do from here is click and drag new files in just use the regular Explorer for that. \r\n\r\nOkay, I organized a lot of documentation as like reference documents and they were sort of like documents that I figured would be like a rock that wouldn't change, that would be like a starting point, like point A. And then you can create a second artifact once you get started, that will be thi"
  },
  {
    "id": "report_source",
    "chunk": " would be like a rock that wouldn't change, that would be like a starting point, like point A. And then you can create a second artifact once you get started, that will be this working living document, but you've always got your rock behind. \r\n\r\nAnd then while you're moving it to a completed deliverable, which would be your artifact three, your third deliverable artifact that you'll just produce and then like leave a carbon copy of, because the deliverable ends up going into Confluence or going somewhere else. it's beneficial for you to make that third extra copy. So I only see this, uh, VS code. Okay. Sorry. You were, you were also duplicating your audio. \r\n\r\nI've muted it. Can you, yes. So what was your question on this document? Yeah. You could literally wrap it as a artifact, uh, create a new artifact. Um, it's a, it's a PDF, so just name it, you know, um, initial starting point, literally just name it that dot PDF and drop it in. \r\n\r\nAnd then you can reference that as your initial starting point. Anything within it, you can talk about it as a thing. And then produce from that rock, from that rock you can grow, right? \r\n\r\nMaybe it's a seed, not a rock. \r\n\r\nMaybe a seed is a better analogy. Perfect. Initial starting point is essentially what you're building. An overall initial starting point, yeah. You won't break out, you'll break up. \r\n\r\nSo you'll produce documents from your initial, you get what I'm saying? \r\n\r\nAnd then your new documents will be like, haha, excellent. I'm planning ahead what you should, uh, I got a plan. I know what you'll do once based on the way you're getting started. Uh, so no worries. Uh, keep, keep cooking, keep cooking them. I have an idea whenever you're ready to listen. \r\n\r\nOkay. As you'r"
  },
  {
    "id": "report_source",
    "chunk": " once based on the way you're getting started. Uh, so no worries. Uh, keep, keep cooking, keep cooking them. I have an idea whenever you're ready to listen. \r\n\r\nOkay. As you're doing this, the, uh, the file structure, what we'll do is we'll just click expand all when you're finally done and we'll take a screenshot of that. And then we'll just let AI turn that into an initial. documentation artifact that'll be your initial structure, and that'll be artifact zero, or artifact number one. Artifact zero is a master list, which will list artifact one and all subsequent artifacts that we create. From that, with that list, artifact one, what you're gonna add in there is what is the significance of the folders, so that it is known from the get -go, like it'll help keep the organization structure that you've already started, and then it'll build upon that notion of organization that you've already got. \r\n\r\nOtherwise, I foresee some misalignment with the model and your structure. I think just, yep, so that'll be something you just would want to add in, a description, basically, under each file, or folder, I should say. Even file, file on the initial list, yes, that might even be a good idea. Why did you add this file? file what is your intent to use yes uh like a one sentence two sentence that's not even two sentence you know And then you're really ready for cycle one at that point. \r\n\r\nWell, that's what I was thinking. Control Z. Pillar 2 allows us KUI, which would be nice. \r\n\r\nAnd let me tell you one thing as well. \r\n\r\nAs you're going, you'll see some missing, maybe you'll notice that the AI is hallucinating somewhere. \r\n\r\nWith this process, what that really is enlightening you of is you're missing some documentation. You're mis"
  },
  {
    "id": "report_source",
    "chunk": "ng, maybe you'll notice that the AI is hallucinating somewhere. \r\n\r\nWith this process, what that really is enlightening you of is you're missing some documentation. You're missing something that the AI, if it had in its context, it wouldn't be having to hallucinate. Yeah, keep that in the back of your mind as well. So you can kind of pull the trigger when you feel 80%. You know what I mean? That's what I mean. \r\n\r\nSo don't feel like you have to be afraid. \r\n\r\nwell yeah and then you'll start making living documents that can turn into templates later because you've got you let's say you've already got templates for state at state a listen then we'll get a template at state Z yeah oh yeah \r\n\r\nAnd check this out. This is pretty meta. What you can also do is once you've got, let's just say you're done with this project and you're moving on to another, you can take this entire prompt and wrap it as example one and then just move on and then you don't have to sort of regurgitate all the boilerplate. It serves as training data. It's pretty epic. It's pretty epic. \r\n\r\nYep. Okay, so we will build into this. The way we will build into this is the first thing we will build into is that files list I was talking about, because those are the two things that I manually add. There's a files list section right there, and then the files, because that's what changes. And then the cycles, I add a new cycle, obviously, that comes from me. But that's it, that's all the manual sort of changes. \r\n\r\nSo the way we'll do the files list is just click the expand all up a bit. Yep, it should be the first one or the right one. The first one on the right. Oh, and let's also turn that one on too. Yeah, yeah. And then, yeah, sorry, that one. \r\n\r\nOkay, so "
  },
  {
    "id": "report_source",
    "chunk": "p, it should be the first one or the right one. The first one on the right. Oh, and let's also turn that one on too. Yeah, yeah. And then, yeah, sorry, that one. \r\n\r\nOkay, so we'll try to get, we want to capture basically two screenshots, it looks like. One of the top half and one of the bottom half. And then you're just gonna, just so you can send it to AI and let AI transcribe it into a text for you. Yep, precisely. Yeah, put it into the text. studio. \r\n\r\nAnd then at the same time, I will give you a template that it will follow to create. So let me send that just a basic, and that'll give a jumpstart to your whole solution. Drop them both as just copy, paste, or what have you, into a chat, a new prompt, a fresh AI prompt. And then we're going to be giving instructions shortly. I'm going to help you construct some of the initial instructions. Yeah, let's use AI studio. \r\n\r\nYeah, and then we'll ask shortly I'm getting an example of a master artifact list and I'm getting an example of a file tree put together and I'll just send you those two and then okay I got one done. Let me find the right file for the next one. There we go, this one, dot, dot, dot, okay. Example file tree structure, cool. And then just say dot, dot, dot, cool. And then I'll do the dot, dot, dot up here, okay. \r\n\r\nOkay, so please take the two screenshots and turn them into a, let me see, artifact zero, master artifact list. for this new project. Then, please, for the files, let me say, I'll explain that part second. So I've already got that written. So please take this two screenshots and turn them into an artifact one file tree structure list, and then a master zero artifact list for this new project. you can follow the structure in the two examples b"
  },
  {
    "id": "report_source",
    "chunk": "hots and turn them into an artifact one file tree structure list, and then a master zero artifact list for this new project. you can follow the structure in the two examples below. \r\n\r\nCool. So I'm going to send you this in Discord somehow. General chat, I guess. It's too long. Let me try a private message here. Still too long. \r\n\r\nI'll cut it and I'll just delete the first. Yep. And then I'll give you the second example. Yep. So you'll take, you'll take from please and then everything below. And then if it gives you any like, you know, Pac -Man said, just delete that. \r\n\r\nAh, so see the thinking budget on the right. We'll go ahead and turn that on as well. It's a toggle. it allows you to manually set it much much just max that slider out to the right yeah and it's fine for this it's for now we gave it all the instruction it needs it doesn't need the whole thing is less than 8 000 tokens so and then so now so this if if if you wanted to have an easy way to compare this you could do two three four of the same copy and paste it in for the purposes of now if you want to eyeball to see if that list is acceptable you see what i'm saying but if you had If you had multiple, then you could literally diff and see which one's longer. You would just know, like, at a glance, if you knew one was 55 or 56, you could see, diff it, and see, well, where's the extra line? So that's the immediately, like, you could, see what I'm saying? \r\n\r\nYeah, yeah, but this should be fine, yeah. And it'll, oh, it'll be fine, because the script'll share any new ones that come up. Yes, this is a perfect initial. Okay, so let's see what it looked like as well. Yeah, that's right, that's right. I would just, let me see. \r\n\r\nYeah, actually, it's fine, I jus"
  },
  {
    "id": "report_source",
    "chunk": "es, this is a perfect initial. Okay, so let's see what it looked like as well. Yeah, that's right, that's right. I would just, let me see. \r\n\r\nYeah, actually, it's fine, I just wish it, oh, I know why it didn't do it. I'm going to write you a sentence to write back and we'll see if it fixes. Can you also wrap the two artifacts in tags which correspond to their file names? I'll be adding these in as artifacts into a docs folder in this repository. See what I'm saying? It missed that notion. \r\n\r\nThere you go. Copy and paste errors and all what you can do is is if it if it it should be just fine AI studio is quite good with the context above But I just yes, there we go See now now you can just copy the artifact a1 up there for the file name when you're creating a new file. \r\n\r\nWhat have you? You see it's much and oh and then if you cop I don't like the way this looks I copy this out into notepad But that's just probably personal preference as long as you're able to get this data out Did it miss C? Do you see artifact one? Do you see? Oh, the header. I just want to do a spot, a validation check. \r\n\r\nCan you check the header where it says artifact one of the initial output? Yes. You see, I want to compare with what you just highlighted visually. Yep. Yep. For the artifact one header. \r\n\r\nOkay. It's literally verbatim. Cool. \r\n\r\nGood, good, good, good, good, good. \r\n\r\nFor some reason I thought it was different. Yeah. First I would create a new file in your repo. Cause now you're getting sort of, this is the, You've essentially done cycle zero, because you're using AI to organize your data at the very basic starting point. You're, yes, yes. Artifacts, you've just created artifact zero and artifact one. \r\n\r\nEverything can go in "
  },
  {
    "id": "report_source",
    "chunk": "'re using AI to organize your data at the very basic starting point. You're, yes, yes. Artifacts, you've just created artifact zero and artifact one. \r\n\r\nEverything can go in artifacts, one folder, just A1, A2, A3, A4, and we can worry about organizing in like another, when you want to create set of content for some other process. Yeah, don't put, yeah, yeah. It gave you the name, it's a markdown file. It wants it, yep. Yeah, see I don't I wouldn't trust the download because I don't know what it'll name it. It'll probably just name a code Yeah, I would do that and put it into a notepad and you'll see why especially if you have a notepad plus notepad plus plus if you have Yes. \r\n\r\nYeah. Okay. Good. Let's do this the right way. It's fine. We'll do this. \r\n\r\nThis is good because notepad plus plus will if you save a notepad file as a markdown file, then when you paste this in, it'll have the beautiful colors and it'll really help you visually know what part is the code and what part to copy out. So we'll get you set up in the way. \r\n\r\nAnd this will only be temporary until I make the whole actual integrated thing. \r\n\r\nYou're just too fast. I was hoping it would be ready before. So now save this. Exactly. Perfect. Now just save this thing. \r\n\r\nas markdown okay yeah now click in the drop down for the file type oh when you get to the right spot so this so this is not an artifact this is a working document that you're dumping responses into yep so you could literally name it response one because you may have a response to tab later if you do parallel tasks in the future so you can just save this as response one literally it now this to your point to your question should we have multiple prompt files Yes. \r\n\r\nShould we have multip"
  },
  {
    "id": "report_source",
    "chunk": "n the future so you can just save this as response one literally it now this to your point to your question should we have multiple prompt files Yes. \r\n\r\nShould we have multiple response 1s? \r\n\r\nNo. You don't need multiple response 1s. Does that make sense? Okay. All the very bottom should be marked down. All the way. \r\n\r\nThe very, very, very... \r\n\r\nIs that it? \r\n\r\nI can't read it. If you're not dark mode, do the one just above it. \r\n\r\nSo you have to... You just have to change your Notepad++ to dark mode. \r\n\r\nIt's not that big of a deal. But... Or else... Or else some tints might be hard to read with the light color. So if... You are already light mode. \r\n\r\nSo if you want to just choose that or if you want to make the change, you can. okay yeah go ahead and see and if it bothers your eyes then just go find where in notepad plus plus i can help you just response anywhere else anywhere outside of your project no i literally yeah i save it in my downloads it doesn't it has no it's arbitrary it's free it's so you can find it again at that point beyond that as long as you can find it again it doesn't matter yeah you don't want it to be not this one yeah this is your process document yeah Response one, yeah. Because you just organized your tabs, so you keep, yeah, response one. And then it's clean. Okay, cool. So now you should be able to see cleanly what you should name your thing, and then everything within the brackets is that artifact. \r\n\r\nThat's what you're gonna copy into the file you create. Yeah, cool. You found it? Perfect, that's what it looks like to me. And then there's some minor tweaking choices, but yeah, there you go. Now you're in hacker mode, man. \r\n\r\nIt's not just cool, man. When you're scrolling through tho"
  },
  {
    "id": "report_source",
    "chunk": "ike to me. And then there's some minor tweaking choices, but yeah, there you go. Now you're in hacker mode, man. \r\n\r\nIt's not just cool, man. When you're scrolling through thousands of lines, bro, it's necessary. I would do this in VS Code, and you'll see why in a second. \r\n\r\nLet's go to VS Code, and then let's go to the Explorer, not my extension, unfortunately. \r\n\r\nYeah, go to the explorer, the top left, almost, the two, yes. Now go to where artifacts are, where you're gonna drop all these documents that the AI is creating. Right click, is that where? Is it gonna be in the documents or artifacts? Okay, then new file, right? Okay, well, before you do new file, go get that file name in your clipboard, so go copy the file name, and then now go new file, and then name it. \r\n\r\nThat what you pasted what you paste it does need to be exact. Yeah, you know, it has nothing. \r\n\r\nNo, no, no, no, no, no, no Rename that to what you the name of the artifact. \r\n\r\nShow me the Notepad. Yep. So you see artifact 1 a 1 pillar tree. Yes markdown file. Yes, sir Yes, sir. That is the name and everything in between. \r\n\r\nYes. No problem. \r\n\r\nHey, don't be sorry man. \r\n\r\nNo, no, no Don't worry. Don't worry. Don't worry. The shit is all over. I'm all over the place There's no lesson, there's no, and then just delete the extra markdown. Cool, now everything that's in that bracket, in those bracket, within the, from the response one notepad, everything in between the artifact one title, where it's orange, yeah, all the way down to where the artifact one ends, which is right basically. \r\n\r\nUp one, up a little bit. Up a little bit? Yeah, I would get the three back ticks as well. Get those as well. Those are, down one more line. yep yep yep trust me t"
  },
  {
    "id": "report_source",
    "chunk": "sically. \r\n\r\nUp one, up a little bit. Up a little bit? Yeah, I would get the three back ticks as well. Get those as well. Those are, down one more line. yep yep yep trust me trust me it'll help your it when you flatten it everything will be clean if you don't if you unless you don't do that okay yep paste it right in there all right you've literally created your first artifact that is the process that you will be rinsing repeating so now you know the name of the second artifact now anytime you need to update this it's very easy you just copy paste it But now you need to create artifacts. \r\n\r\nI guess artifact, yep. \r\n\r\nArtifact zero, is that correct? Yeah, just make sure you don't get those carets. Just delete them if they show up. \r\n\r\nAnd then paste those. \r\n\r\nI find going from the bottom up works in this copying process. \r\n\r\nI don't know why. It's, I don't know why. \r\n\r\nYeah, it's easier to control. \r\n\r\nAnd then delete that last little, yeah, yep. \r\n\r\nBecause the script will do that. The script will create those for you based off the file name, okay? Oh, it's even got descriptions for you. You see, and you can qualify those and update them. And that would be a very important task for you to make sure those descript... And not only that, not only that, but it'll be kept up to date over time by the AI. \r\n\r\nIt'll update this artifact. Okay? I would, so you don't have to, but I would go through sometime or go back to the other artifact and review those descriptions and especially the descriptions for the files because, yeah, because the AI did not have those files at the time, it just had the screenshots. Well, this is gonna be basically how you can help build context for the overall project for the AI. Because, for example"
  },
  {
    "id": "report_source",
    "chunk": " those files at the time, it just had the screenshots. Well, this is gonna be basically how you can help build context for the overall project for the AI. Because, for example, why did you create this folder? There you go. \r\n\r\nYeah. Checkbox. \r\n\r\nYes, sir. \r\n\r\nYes, sir. And then, yes. Yeah. That's an important thing. And see, precisely what you're doing is super important. \r\n\r\nAnd you're learning the right vocabulary. \r\n\r\nCertain things like draft means expect this to get changed. \r\n\r\nVersus reference document, don't expect this to be changing this. You see what I'm saying? That's truly, truly, truly what you're doing right now. It will pay in dividends to do this because you're putting the knowledge the institutional knowledge to yeah to deposit Institutional knowledge you can say it like that is what you're doing. Yeah, let's see how it does it at this point though Do you want to save everything save all your dot make sure everything's saved and then you can go back to my My extension and then make sure everything's selected minus the prompt if that was one thing I wreck I realize is don't I stopped saving my prompt in my repo because if I selected it, then it would duplicate in my flattening. You don't want to flatten your own prompt because the repo goes in the prompt. \r\n\r\nSo select everything except your prompt if your prompt file is in here. So you should just be able to, yes, select all instantly like that. And then what's so big, first of all? And what's the total size? That's fine. If it's under a million, you're Gucci at the very, very bottom. \r\n\r\nJust great. Yeah, you're good. You're totally good. Yeah, yeah, deselect that one for sure. Oh, I thought I... that! \r\n\r\nOh, I'm so sorry. \r\n\r\nOh, man, hold on. \r\n\r\nO"
  },
  {
    "id": "report_source",
    "chunk": "tom. \r\n\r\nJust great. Yeah, you're good. You're totally good. Yeah, yeah, deselect that one for sure. Oh, I thought I... that! \r\n\r\nOh, I'm so sorry. \r\n\r\nOh, man, hold on. \r\n\r\nOkay, so just select it all, and then if you just remove it from below, it'll work. And I'll show you how to do it easily. Click on the prompt where you're looking at it. Click on it to open it. Double click, I guess. Oh, there's no prompt file? \r\n\r\nOkay, yeah, yeah. No, I can fix it. I can show you. There's an alternative to do that. I swore I fixed that. But just, yes. \r\n\r\nNow, now, hmm. Over on the, okay, just look on the left in the selected item section, you should be able to click on prompt in there, prompt in B. I think I see it, is it number 12, number 13 down there? At the very, yes. You should, yeah, you can, yeah, just click the X there. Yeah, click, yeah, see? Yeah, yeah, yeah. \r\n\r\nSorry about that, that was, that's annoying. But that works, okay. Those are folders, it's fine. That's fine, yeah. Yeah, then it was working fine. Oh, it was working fine. \r\n\r\nThat's okay, that's okay. I'm not, I'm not worried about that. It'll auto add anything new, don't worry. We turn that on. Yeah, you can flatten context. And then creates this file. \r\n\r\nNow let's glance through to make sure there's no encoded data. So you can click and drag on the right. But I don't, yeah, what does all that say? 21 items. Yeah, I see that. \r\n\r\nOkay, that's fine. \r\n\r\nWhat kind of document is that? What's the name of that file? They're probably related to the, I mean, that's a bug for me to fix for sure. I can fix that. I just, I can't read. Don't worry. \r\n\r\nYeah, give me the error codes and then we will, I will look at them really quick. I already know some plans. I alrea"
  },
  {
    "id": "report_source",
    "chunk": "re. I can fix that. I just, I can't read. Don't worry. \r\n\r\nYeah, give me the error codes and then we will, I will look at them really quick. I already know some plans. I already know. I just need to get the same files from you and then get the same error. Yes. So it's all very generic, unfortunately. \r\n\r\nOkay. Yeah, it did. Now, this first... I think those are... I honestly think those are not... Actually, I think it did its job. \r\n\r\nThis is what I... \r\n\r\nSee how it's popping up like this? I think because as you're scrolling. I actually think it's because it's encoding. \r\n\r\nAnd this encoding is an easy fix. \r\n\r\nThis is normal. \r\n\r\nIt just needs to be handled. Yep. So, don't worry. I totally got this. So, only certain files this would have happened. The other files it would have been fine with. \r\n\r\nI just need to get the file that this happened for. and then process it accordingly. So what we can do is go to the very top to where it started, and then what is this file that's ruined? Oh, it's a docx file. Oh, I didn't handle docx yet. Sorry, dude. \r\n\r\nThat's why it's happening, because I handled PDF, I handled XLS, I haven't handled docx yet. \r\n\r\nRight. That'll be next, I guess, on the list. I'll do that for you tomorrow. But in the meantime, if you open it and copy it, yeah. Oh, man. Oh, don't, don't bop. \r\n\r\nOh, man. One, two, you got at least 10. \r\n\r\nOh, if you open them and turn them into PDFs and replace it, same difference. \r\n\r\nAnd you'll be able to move forward without waiting for me to make an update. \r\n\r\nYeah. Okay. \r\n\r\nThen you can wait. \r\n\r\nIt's up to you. \r\n\r\nIt won't take me but a day. \r\n\r\nI didn't even think about that. That's all I needed to know. \r\n\r\nI'll fix it. \r\n\r\nEasy. I just need to code it. Yep. Easy."
  },
  {
    "id": "report_source",
    "chunk": "\nIt's up to you. \r\n\r\nIt won't take me but a day. \r\n\r\nI didn't even think about that. That's all I needed to know. \r\n\r\nI'll fix it. \r\n\r\nEasy. I just need to code it. Yep. Easy. I've done it already. yeah that's right yeah yeah so if you just uh anything that says docx in that list if you just remove it won't you know it won't you just remake your selection and we know the spacebar work does yeah oh and you can also save your selections in the future up in the top there's a save button by the way yes those will be fine just the docx i believe oh you can sort by file type by the way as well by the way You can just click the icon. \r\n\r\nNot that one. That's file name. Click on the one above. Click on the one above. The one to the left. It's an icon. \r\n\r\nThat one. There you go. Now you're sorting by file type. Yes. And it autosort. Yes. \r\n\r\nDude, I... Yes. Yep. Flatten again. \r\n\r\nAnd then it'll clean it up. \r\n\r\nThere you go. No errors because it was related to the thing. Oh wait, hold on. Did those pop back? Oh no they didn't, you scrolled up, okay. Zero tokens, interesting. \r\n\r\nI see more down there, keep going. What's that garbage? See on the right, I'm looking on the right, you see the red? Ooh, what file is that? Another . docx, it really didn't? \r\n\r\nI'll have to experiment with that. Yep, but it should still take things out. I'll fix that as well, I'll test that. Can we, ah, so the colors are token count, yep. Yeah, so hover over the token count, you'll actually get a little bit of a... No, it's not off. \r\n\r\nI made a poor choice in my decision making. I made a... I honestly made a poor decision. I thought orange would look more severe. So I... Yep, my bad. \r\n\r\nSo this is.. . It's already saved. It gets saved when it gets c"
  },
  {
    "id": "report_source",
    "chunk": " I made a... I honestly made a poor decision. I thought orange would look more severe. So I... Yep, my bad. \r\n\r\nSo this is.. . It's already saved. It gets saved when it gets created. Ah, at the very bottom it should be. Right there. \r\n\r\nIn your, yep, yep. And so when it's not messy, you can just copy and paste that into your prompt file at the bottom. That'll be, remember how I said you're copying and pasting two things? The files list, which is actually just your artifact zero. Yeah, you see? You would be, I wouldn't do it now, let's not copy these stupid encoded symbols. \r\n\r\nBut yeah, that's, yes, open the template we had, the prompt template. Yeah, it would go in the, at the very bottom, file section. \r\n\r\nJust in between there, in between files. \r\n\r\nBecause it's all your files. It's your flattened repo, basically. It'll always get created and placed there. It doesn't have to. I can make an option where you can direct where you would like it to go. \r\n\r\nThat's a good idea. \r\n\r\nBut right now, there's only going to be one. \r\n\r\nNo, it's only that. \r\n\r\nIt just gets updated. \r\n\r\nWhen you click flatten, it's recreated. So let's imagine a workflow. Let's say you get a new artifact back in Artifact 2. You create the new file. When you create the new file, because you've got it checked, it'll auto do the checkbox for you. And then you drop in your artifact that you got from the AI. \r\n\r\nthen you just save your file normal so you didn't do anything you don't do anything different and then you just click flatten that will pick up the new file because of the checkbox was automatic and then you just see so you don't change your your workflow you you copy you create the file copy it in click the button copy and copy uh copy in the fla"
  },
  {
    "id": "report_source",
    "chunk": "eckbox was automatic and then you just see so you don't change your your workflow you you copy you create the file copy it in click the button copy and copy uh copy in the flattened repo because it was just updated so this is um yeah this is Each project gets its own prompt file because that prompt file is the process. And so you could just imagine, yeah, just imagine, so the flattened repo for now is, you can only see it now because it hasn't been fully automated. It'll disappear just like how, as you can see in here, there's no copies of your PDF files. There's just the PDF file, but clearly we have it in Markdown because you can go dig through the flattened repo. It's in there as Markdown, not the broken ones. The prompt, so the flattened repo is basically just part of the prompt file and the prompt file is the overall process to create the NC doc. \r\n\r\nIt will only be there. And then you can mine from that prompt file the necessary information to create some second static content. Because you see what I'm saying? Yeah. What is it? The flattened repo? \r\n\r\nI would be putting it in the prompt file that I'm currently building the static content for. It is project specific. Okay. Okay. Okay. Oh, okay. \r\n\r\nSure. That's fine. Now that I know what you're doing, it's fine. Yeah, there's the PDF stuff or text file stuff. Yes, sir, it does. And then I just find, see right there. \r\n\r\nSo I just find, I find I get better performance when I also put the files list at the top. I would recommend doing it. Yes, it does show up in here, but I would recommend, I haven't, it hasn't hurt me. See, I have the files list section, right? Down a bit. So that's in the inner, up a bit. \r\n\r\nYes, in between. No, no, it should be in both places. Rem"
  },
  {
    "id": "report_source",
    "chunk": "en't, it hasn't hurt me. See, I have the files list section, right? Down a bit. So that's in the inner, up a bit. \r\n\r\nYes, in between. No, no, it should be in both places. Remember I said there are two things that I, yep. Oh, because yeah, it just makes, okay. So the files are all the files. It just so happens that the files list is a file as well. \r\n\r\nIt's just self -referential. It's not, it's not the end of the world. And, and if you don't check this out, if you don't want to put the files list at the top of your prompt, because let me say it like this, AI, large language models, they read one token at a time from the start. And so in my mind, in my mind, giving it the files list up early is beneficial so it can plan ahead. It can think about while it's reading your cycles because it's already got the files list in its mind as it's going through the cycles. That's the way I think about it. \r\n\r\nIt hasn't steered. I have no research, but I feel it works better. If you don't want to copy and paste, you don't have to. But that is what that is. The reason that is the root, the driving factors to why I did that in the first place. \r\n\r\nWas getting bad performance. \r\n\r\nThat was one of the things I changed at the time I was getting bad performance and I stopped getting bad and I was able to move forward in my projects in in which I Would definitely keep that net. \r\n\r\nI don't know why you want to remove the metadata at the top. I don't know I wouldn't touch it Yeah, I did it for a reason. I did it that way for a reason. \r\n\r\nYeah, just copy it into the file section If you don't, again, if you don't want to, you can just delete files list and you can delete the reference of it in the interaction schema and you don't have to worry"
  },
  {
    "id": "report_source",
    "chunk": " section If you don't, again, if you don't want to, you can just delete files list and you can delete the reference of it in the interaction schema and you don't have to worry about it. \r\n\r\nAnd you can see if you don't get bad performance because you know what? When I started doing that, it was a year ago, two years ago. There were older AIs. Maybe you don't need it. Maybe it's overhead you don't need to worry about. But that is the route. \r\n\r\nI got better performance doing it this way. \r\n\r\nAnd right now it's manual. \r\n\r\nI haven't found a way to... I don't have a way to parse it in right there yet. I'm going to build that in. You won't have to worry about it soon. And only up in the files... No, no, not the whole thing. \r\n\r\nControl Z. I get it. I get it now. \r\n\r\nI get it now. \r\n\r\nNow go over to Artifact. \r\n\r\nNo, in your Artifacts list, it'll be easier if you just go to the M... Is it M0 or M1? I forget. I'm sorry. Your Artifact 1 or Artifact 0. So it should be over there on the left. \r\n\r\nYou see your Artifacts tab? \r\n\r\nYep. Down a bit. Up. Yep. Which one of those is your... Oh, it's Artifact 0, 0, 0, 0, 0. \r\n\r\nYeah, click on that. copy this whole thing. That's all your copy. Yes. That's yeah. My mistake. \r\n\r\nI'm sorry. Yeah. No, we weren't clear. That goes in your files list. And that is because that is your file because that became that is your files list. Yes, sir. \r\n\r\nRight there. Yeah, sorry. Yeah. See that? See that? \r\n\r\nSee what I mean? So so it no, so it's a see, that's all your project metadata is what's easy. \r\n\r\nThat's I truly firmly believe I don't have any evidence, but I truly firmly believe when it's reading your context, we will excuse me when it's reading your cycle, And that's why my cycles go in the ord"
  },
  {
    "id": "report_source",
    "chunk": "ve I don't have any evidence, but I truly firmly believe when it's reading your context, we will excuse me when it's reading your cycle, And that's why my cycles go in the order that they do. \r\n\r\nThey don't go 0, 1, 2, 3. They go 35, 34, 33. They go all organized because I'm thinking like the AI reads. That's all. And another thing about how the attention works is every word it's reading. it looks for every other mention. \r\n\r\nThink of it like it does a complete search for that word through the whole document. And it gets like key value pairs of related information around every time that word shows up. So as it goes through every time. \r\n\r\nSo that's kind of as it reads every word. \r\n\r\nSo if you say, you know, like, you know, that, you know, one of those keywords right there, it'll just go, yeah, yeah. That's how the attention mechanism works. And make sure it's not checked, though. Uncheck your prompt files, yes. Now that, yeah, now that you get it in a clearer, clearer, yep. That was just a one -off. \r\n\r\nIt could have been your cycle zero. It totally, we could have done that as well. You get what I'm saying? You could have just wrote the same message, and then no difference. But it's fine now. You had literally no extra metadata to include. \r\n\r\nYou were creating it from scratch. Now that you're in this position, you can go to your cycles section. That's correct. Is there already a cycle one? Yep. So then that's fine. \r\n\r\nPerfect. That's fine. That's fine. Yep. Above it, you'll be making cycle one that looks just like cycle zero. So you can copy the two lines for cycle zero, paste them, and then name the one above one. \r\n\r\nAnd that'll give you the mental structure. \r\n\r\nYep. \r\n\r\nAnd then change the ones above the zeros abo"
  },
  {
    "id": "report_source",
    "chunk": "two lines for cycle zero, paste them, and then name the one above one. \r\n\r\nAnd that'll give you the mental structure. \r\n\r\nYep. \r\n\r\nAnd then change the ones above the zeros above to one because you go upwards. But then, yeah, before you send it, delete that. Right. But this mentally, that's how you're going to construct them. That's right. Your cycles go up. \r\n\r\nThat's right. Yep. Like a history. It's reading. It's reading from top, like a book. It's reading a book. \r\n\r\nAnd it needs to know what to work on now. Everything else is in the history. Yes, sir. Yes, sir. That's my, this is it. This is it, man. I don't know if this is like hard to conceptualize or easy or what, but this is it. \r\n\r\nThis is how it works for me. This is how I keep the situational awareness. It's this order. Yes. No, no, no, no. That's right. \r\n\r\nThat's right. That's how AI works. That's how AI works. That's how large language models work. They read one token at a time. From the first token you give it to the last token. \r\n\r\nRight. Well, you would do one cycle at a time because you would analyze the results. It's what you want to ask for. Yeah, right. \r\n\r\nYeah, that's right. \r\n\r\nSo you just correct. \r\n\r\nAbsolutely. 100%. Now you're thinking like key value pair. Yes, that's exactly how it works. And that's only shorthand for you. That's only shorthand for you. \r\n\r\nYou could just say your chat GPT summary and just be done with it. But yeah, absolutely. That is you're you're you're getting it. This is the transferring. This is exactly the basic, straightforward, not rocket science. Yes. \r\n\r\nNo problem. Delete Cycle 1, we'll do Cycle 0 first, and then you'll write Cycle 1 when you're ready to write Cycle 1, and it'll make more sense as we go through it."
  },
  {
    "id": "report_source",
    "chunk": ". Yes. \r\n\r\nNo problem. Delete Cycle 1, we'll do Cycle 0 first, and then you'll write Cycle 1 when you're ready to write Cycle 1, and it'll make more sense as we go through it. It's just I just wanted to illustrate it goes up. Yeah, what you got? Yep. So in cycle 0 you could start saying like No, put it in between it's in between it's everything in between the tag. \r\n\r\nYep, like DNA. I think of it like DNA. I don't know So so no, no, no, no, you're even already too. No, no totally abstract, bro Totally high level totally like what is it that you're trying to do here? What do you want to do? What is it? \r\n\r\nWhy did you bring all these documents together? I'm trying to make a training for these people. I currently have these pieces together. I'm looking to plan out further. Let's go ahead and get some initial documentation planned out. Let's turn the list of ELOs into something. \r\n\r\nNow you see where I'm going? And then analyze those results. \r\n\r\nTrust me. \r\n\r\nAnd then we'll see. Yeah, he knows a lot. Nope, you're talking to, you think like you're writing to a colleague. It'll get you, it'll get your typos, don't worry. You're assigning a junior a research task. It gets easier. \r\n\r\nIt gets easier as you do it as much as you feel comfortable, honestly, as much as you feel it's once you feel like you've hit sort of writer's block. That's again, that's the beauty of this is it solves the blank page problem. You're going to get something back in line and it'll help spur the next cycle. And then what you could ask, maybe what are some of the artifacts you're going to need? \r\n\r\nYou could start with an outline. \r\n\r\nWhat would be an outline of what the static content based off of our requirement or what we're the ask is. It can sta"
  },
  {
    "id": "report_source",
    "chunk": "e going to need? \r\n\r\nYou could start with an outline. \r\n\r\nWhat would be an outline of what the static content based off of our requirement or what we're the ask is. It can start helping create an outline, which then, you know, you've got a section one, okay, you can now build out section one. Now build out section two. Those, yeah, thinking and just, if there is a particular one, yes. Perhaps manually. \r\n\r\nIf it's a PDF, it's already in the context. \r\n\r\nYep, so you can talk, yeah, it's in there. \r\n\r\nThat's the hard part's done. \r\n\r\nYou can just speak about it. \r\n\r\nIt'll be, yeah, to get started like that. \r\n\r\nThat's right, yeah. Also, if they're all in the same directory, you can just reference the director. Yes, sir, yes, sir. Templates directory, UKI templates, yeah, sorry, what was that? Oh, put it in single backticks, put it in one backtick. Whenever you're talking about a specific item, that's what I do. \r\n\r\nDo one backtick and then do the, because that's actually how Markdown accepts it as code, inline code. So just type backtick, which is right next to one, and then type UKI templates as is, as it appears, and then backtick, and then you can say directory, there you go. \r\n\r\nThat's how I do it now. \r\n\r\nAnd I didn't start doing it that way. \r\n\r\nI do it now. Me either, man, until, yeah. \r\n\r\nuntil like maybe two months ago. \r\n\r\nYes. Yeah, the tilde, yeah. Yeah, directories, names, file names, it's just anything that is like defined. Yeah, and yeah, so you can totally, oh, go ahead. \r\n\r\nAbsolutely, yeah, and that's absolutely, and we're gonna have a lot of fun. In this file, do we have any garbage? \r\n\r\nScroll down in there. \r\n\r\nIn this file in the right, yes. \r\n\r\nYeah, any encoded? \r\n\r\nActually, no. \r\n\r\nOh, you pasted "
  },
  {
    "id": "report_source",
    "chunk": " a lot of fun. In this file, do we have any garbage? \r\n\r\nScroll down in there. \r\n\r\nIn this file in the right, yes. \r\n\r\nYeah, any encoded? \r\n\r\nActually, no. \r\n\r\nOh, you pasted it in one and not this one? \r\n\r\nIs that what happened? \r\n\r\nNope. So let's try to do this. Let's try to do this. Is it just one file? \r\n\r\nIs it just one file that got left? \r\n\r\nThat's what might be happening. \r\n\r\nDid you see? Yeah. Yep. \r\n\r\nEncoded. Yeah. I see. I remember. Because you pasted the whole thing once. Yep. \r\n\r\nYou can. You can. You could also try to completely unselect everything and then make a selection. There's all kinds. Yeah. Because I don't think that's a permanent bug. \r\n\r\nYeah. \r\n\r\nWell, you could delete. \r\n\r\nYou could delete the flattened repo file. \r\n\r\nYou could. \r\n\r\nYes. \r\n\r\nDelete should work. \r\n\r\nIt's fine. \r\n\r\nIt'll. Yeah. Well, hold on. Are you sure? Yeah. Are you sure there's no bugs? \r\n\r\nSort by file type, if it isn't. Yeah, that's fine. Cool. Fingers crossed. Okay. Looks good. \r\n\r\nYes, this is your PDFs. This is all good. Yep. \r\n\r\nThis is expected behavior. \r\n\r\nYes. Oh, is it getting errors again? Or is that old? Okay, don't stress me. Okay. Yeah, docx errors I can handle, because that's expected, but more? \r\n\r\nYeah, see? See, there you go. See? This is it. This is it. That's the \r\n\r\nyes it does when you see it yep so everything yes yeah yeah I wouldn't delete the files just because that's what you called it in your interaction schema unless you want to rename it to flattened repo because you're just tagging things and right now this is all tagged files okay the reason why I put it at the bottom is because I would put a little tag I would write ASDF So I would do a Control -F, ASDF, and the ASDF was at the top of where "
  },
  {
    "id": "report_source",
    "chunk": "files okay the reason why I put it at the bottom is because I would put a little tag I would write ASDF So I would do a Control -F, ASDF, and the ASDF was at the top of where the files start. So I could just hold Control -Shift and press End, and it would select everything down in one keystroke. So once you delete it, I'll help you, I'll help you do it. I'll help you do it once you delete. You're up there, right there, right there, right there. There it is, delete all that. \r\n\r\nAll the way up to the top. Now hold on, hold on, click at your line. Now hold on, I'll help you out, help you out. Hold on, let go. Click, just click right there, yep, exactly. And then now hold Shift and Control and press End. \r\n\r\nOh, no. Are you on a Mac? What is this? No, you're Windows. Hold on. Yeah, it works. \r\n\r\nYeah. Shift. Yeah. I just did it. It does it. \r\n\r\nIt does the thing. \r\n\r\nIt's supposed to. It's supposed to hot. Okay, whatever. \r\n\r\nWhatever. You can now. Okay, that's fine. Leave that there. Oh, no, no, no, no. I understand. \r\n\r\nE -N -D. E -N -D. Not the letter N. The button. Yes. Yes. Above the keyboard. \r\n\r\nYeah, there it is. And then now, there you go. That was quicker. And then, yeah, you just deleted, the only thing you deleted was the files, but that's okay, that's okay. You can delete it as well up here and we'll fix it permanently so you never have to worry about it. Before you, yeah, before you paste it in, we wanna do one thing. \r\n\r\nGo back to where it was. Delete the last three lines. Don't paste it in just yet. Go ahead and delete the last three lines. Yep. Now, so just one point before we move forward. \r\n\r\nYou're removing the files tag. And we're just replacing it for simplicity with the flattened repo. But before you"
  },
  {
    "id": "report_source",
    "chunk": "ines. Yep. Now, so just one point before we move forward. \r\n\r\nYou're removing the files tag. And we're just replacing it for simplicity with the flattened repo. But before you paste it in, right at the top, you want to type in ASDF. ASDF. Just so you can trust me. And then below that, you can press enter. \r\n\r\nAnd now you can paste in your flattened repo. \r\n\r\nBecause this is the manual process, the pasting. \r\n\r\nSo it's quick if you can just control F, ASDF, you'll jump right to that spot. yeah that's that's it promise that's the fastest way i found to do this that's the one thing i will yeah explain yep yep and then you just control shift end to the bottom and then paste yep easy easy peasy yep that's it that's easy straight most yeah and then yeah so basically we can send so here's the fun part here's the fun part because once you're ready to send your prompt You're gonna see the response and you can see how it vibes with you. Once you read it, you're gonna realize, I should have asked for this, I should have asked for that. You can just change your cycle zero. \r\n\r\nJust change your cycle. \r\n\r\nAh, so everything in your prompt file, prompt markdown. \r\n\r\nI would not give it the file. \r\n\r\nAnd the reason why is because they will do all kinds of trickery. They will parse and slice and contextualize the shit out of files. If you don't worry about any of that. Yep. Right there. And just paste it in right there. \r\n\r\nYep. You can run that and do a second one. Yep. Go ahead and send it and then just duplicate your tab. Let's just do it. Well, let's just see. \r\n\r\nLet's just see. Let's just see. \r\n\r\nAnd then that's it. \r\n\r\nYou give guidance based on it. It's see it. Listen, listen. You're building the mental model of the model right "
  },
  {
    "id": "report_source",
    "chunk": "\n\r\nLet's just see. Let's just see. \r\n\r\nAnd then that's it. \r\n\r\nYou give guidance based on it. It's see it. Listen, listen. You're building the mental model of the model right now. You're getting an idea of what one. \r\n\r\nSo this is an important analogy. Think of your prompt as an input output as a single page because it reads and produces every token, but even its output. \r\n\r\nSo after it produces, starts producing output, every time it produces a new token, it's rereads everything behind it. Again, every time it rereads everything, every time you see something pop up, it's rereading everything before it. all right so it it depends every um so for that reason if you just conceptualize it as one big page both the input and the output then what you're doing is you're you're you're building a new alphabet because you now know what the input will produce the output and that's one page like one japanese letter okay so what do we have yes and if we do yes, I wouldn't. Yeah. This is trash. \r\n\r\nHonestly, this is trash. This is experimenting because once we get real context, then you're cooking. Yes. But this is, you're, you're building the mental model of the model right now. Yes. It does do a search and you can turn that on or off on the right. \r\n\r\nDo you see grounding with Google search on the right? You can turn that on or off if you want. Yeah. You can do that as well. Do you see URL context? So if you give it a URL and you turn that on, it'll, it can read the URL. \r\n\r\nWhat's on the URL as well. So you can be more controlled about it. In your cycle, you would link something in your cycle and then it would read it. Oh, you can make an artifact that's just those links. That's a good idea. Now, just really quick, one caveat is no"
  },
  {
    "id": "report_source",
    "chunk": "ould link something in your cycle and then it would read it. Oh, you can make an artifact that's just those links. That's a good idea. Now, just really quick, one caveat is not all websites are machine readable. \r\n\r\nLike for example, especially a website that's like an app where you have to navigate within an app and like click certain things to see database records. Because it's web crawler, they're just basic web crawlers. Yeah, good. Yeah, more or less flat static content. They're good old -fashioned web crawlers. It's not like an AI is intelligently looking at the website you gave it. \r\n\r\nyes sir so yeah yeah yes right yes and that's okay it's your first project yes this is good mm -hmm and then you can check a project as you task switch yeah you have to see it you have to see it you've gone from zero experiential blindness to oh dude I've been wanting to do this let me go ahead keep talking I'm gonna put put together a minor Mm -hmm. Yes. Mm -hmm. Let me, let me give you some guidance. Let me go ahead. \r\n\r\nI know what you would want. This is where your interaction scheme is going to come into play because you're going to need to specify, give me outputs as artifacts. And then that's where you, and I've already written it out. And that's where you just say, artifacts are enclosed in these tags that have the name and that's it. That's basically it. And then that is what the site, instead of what it gave you, it would have given you something with the name of it on it. \r\n\r\nAnd you can decide if I like this artifact, is this something I want to iterate on in the future, or is this garbage? I want to give it more guidance now that I know what this prompt is going to create. I'm going to share my screen. So what I've been"
  },
  {
    "id": "report_source",
    "chunk": "rate on in the future, or is this garbage? I want to give it more guidance now that I know what this prompt is going to create. I'm going to share my screen. So what I've been doing is type, okay, so right above cycle zero. put a return, make a space in between. Yep. \r\n\r\nNow write cycle zero response. Yep. Yep. We'll copy, yeah. Cycle zero response. And then copy that, put that in brackets and carets, open greater than, less than. \r\n\r\nYeah, just follow my lead. Yep. Just, yep. Put it in between, just like you're creating a new tag. Yeah. There you go. \r\n\r\nand then now copy that whole line and then paste it so you have two of them. \r\n\r\nNo, no, no, just the one you just created, yep. \r\n\r\nBelow it, right below it. \r\n\r\nCorrect, correct. \r\n\r\nYou're just, yep. \r\n\r\nAnd now, so assuming you're doing multiple responses, you're gonna choose one that you like, that you vibe with the most. \r\n\r\nOr you're just doing one, but that's the response, you put the whole response in there, minus any artifacts, because you take the artifact out and you put it in your file, so you don't have duplicates. your control X when you cut it out of the response from your notepad, right? Notepad++, maybe, was it? Or no, was it? You can copy it back again. \r\n\r\nIt should be down in your AI studio. \r\n\r\nYes, because you send your cycle 0, and then cycle 0 response comes back. In there, it should have artifacts that are enclosed. \r\n\r\nYou cut those out, the ones that you like, because you've selected the response. \r\n\r\nYou cut those out, and you put those in the actual artifacts, or you're creating new artifacts, and then you take the whole response, like what Gemini said to you, oh, this is what your blah, blah, blah, blah, and you just paste that in here. So"
  },
  {
    "id": "report_source",
    "chunk": "e creating new artifacts, and then you take the whole response, like what Gemini said to you, oh, this is what your blah, blah, blah, blah, and you just paste that in here. So you're creating an audit trail, almost, Yep, that basically yeah, so yep, so copy the whole thing and actually don't use this button Don't do that that way because it automatically doesn't mark down if you do it this way you see the hook Yes, click that and then copy it. Yes. Yes now put that in notepad plus plus No, I would do notepad plus plus in the middle ground. It's your live. Yes. \r\n\r\nIt's much easier It will be I will create an interface for you. But for now, I would use notepad plus plus Not no no in your response one. You're dumping it into response one over and over again because it's your response one It's your current response one. \r\n\r\nThis is a working document. \r\n\r\nYou never say this is ephemeral copy copy You know control a and control V to select all and paste over you don't need the old one anymore. \r\n\r\nYes Control a and then control V. Yeah, there you go. Yep. Yep So now the only thing is did this actually encapsulate things in artifacts for you or no? Yeah, I don't think so either No, no, no. We are looking at it now. You can go... \r\n\r\nSo I don't read it in here. It's so much easier to read the responses in Notepad++. I don't read it in here. Yeah. I don't know why. It's easier. \r\n\r\nIt's much easier. It did not. So that would be part of what you now know. Because it's like you see the future. You're literally seeing the future. Let me explain. \r\n\r\nHindsight is 20 -20. You now know what your cycle zero will produce. You didn't know that. Okay, do you see my screen? Do you see what that is? \r\n\r\nCan you tell me what that is? \r\n\r\nYe"
  },
  {
    "id": "report_source",
    "chunk": " is 20 -20. You now know what your cycle zero will produce. You didn't know that. Okay, do you see my screen? Do you see what that is? \r\n\r\nCan you tell me what that is? \r\n\r\nYeah, can you? \r\n\r\nYeah, basically, it's almost like a Rorschach test. \r\n\r\nOkay, I'm going to, I'm going to, you currently are in a state of experiential blindness, and I have the antidote. I'm going to cure you. Okay, are you ready? Are you looking at the screen? What is that? Well, you can clearly, now you can see the snake, right? \r\n\r\nYou didn't have that experience. That split second of experience, you didn't have it. Now you have it. That's all it took. That one split second of experience. Okay. \r\n\r\nOkay. Okay. Yeah. Now you have the experience. Yes, sir. You're welcome. \r\n\r\nYou're welcome. Okay. Yep. So you're, you're good to go. The only thing you would need to do is go. Now you, this is good to go to your interaction schema. \r\n\r\nI'm going to give you the one for the artifact. Yes. That's right. After cutting out the artifacts and putting them in my actual repo. Yes. Because you're growing the repo. \r\n\r\nSo they would be in the tags. They would be. Again, this is, we've learned we need to give it the instruction about the artifact tags or else it won't do it. I've already written that in my prompt. I can share that with you. And you've already got an interaction schema section. \r\n\r\nYou already have it. \r\n\r\nIt should be at the very top. \r\n\r\nYeah. Yeah. Okay, so this is my prompt. You would look at the interaction schema section, which is my main artifact three. So that's going to be right below my cycle overview. So you could write in your cycle overview, cycle zero, project initialization, just if you wanted to start building your cycle overview"
  },
  {
    "id": "report_source",
    "chunk": "ng to be right below my cycle overview. So you could write in your cycle overview, cycle zero, project initialization, just if you wanted to start building your cycle overview. \r\n\r\nBut this is basically what you want to write. \r\n\r\nSo I will give you the top two. It looks like that's all you need are these two, one that describes artifacts, And then one that describes that they are the sources of truth. \r\n\r\nJust two basically sentences, three total, four maybe max total. \r\n\r\nSo I'll just send you that. And those can be yours. \r\n\r\nI would change them just a smidge. \r\n\r\nYou'll see why. Because I referenced some Artifact 106 or something. \r\n\r\nYou can just sort of, you know, like tweak it slightly for your use case. \r\n\r\nBut that can be your Artifact 1 and 2. \r\n\r\nAnd then just send your, just add those two and send your Cycle 0 again. Let's just see. Let's just see. And then you can also ask for something as well. You could ask for a list of modules, you could ask for a design of some kind, and it'll come back as an artifact. That would be up, so that's gonna be, you can think of this like your system message, your system instructions in like a project, a chat GPT project. \r\n\r\nSo you would be putting, let me see your screen again. \r\n\r\nCause you would be putting those near the top. \r\n\r\nLet me get my share working again. \r\n\r\nYes. \r\n\r\nYep. \r\n\r\nSo project plan, right up interaction schema a bit further. Okay. \r\n\r\nSo actually, so you don't have one yet. \r\n\r\nSo, um, oh wait, no, zero zero. So a zero and then project plan. What do I have it called? Hold on one second. One second. I see. \r\n\r\nI can fix it for you. Ah, because what you have called Interaction Schema, I have called Artifact Schema. And then if you change, yeah, old, that"
  },
  {
    "id": "report_source",
    "chunk": "econd. One second. I see. \r\n\r\nI can fix it for you. Ah, because what you have called Interaction Schema, I have called Artifact Schema. And then if you change, yeah, old, that's old. If you just, no, no, I got an easier way to do it. If you just undo that, and then highlight Interaction Schema together, copy it, and then do a Control F. Oh, it's already, Control F is already open in the top right. Paste that in there, and then click the down arrow. \r\n\r\nSee, this is helpful because if there's 50, you're learning. That's okay, but if there were 50, you're learning how to do it quick. So click the down arrow right there. I'm so sorry, the bracket to the left further. It's still in that section. Left a little bit more, just a smidge. \r\n\r\nA little bit more, a little bit more, a little bit, that one. This is replacing that. I didn't know, I didn't know you didn't know. I didn't know, I didn't know, I didn't know, sorry. Okay, yeah, that's what we're doing in this instance, yeah. That's right, and then, because you're gonna have a real interaction schema now. \r\n\r\nYep, there's some but, there it is. and now you can actually put a real one in here build one out Yep, build one out for yourself. \r\n\r\nI'm gonna it'll be a little experiment for yourself a little Home homework homework. \r\n\r\nI would put it in between project plan and files list. \r\n\r\nYou've already changed it No, you need to now create a new artifact It's not even an artifact because an artifact so start in the artifact schema section and look because that's a list I would do right below project plan. \r\n\r\nI would make a new line No, no, no. \r\n\r\nThat is where it will go. \r\n\r\nBut before you do it there, up even further. Because it's a self -refer... Even further. The line "
  },
  {
    "id": "report_source",
    "chunk": "n. \r\n\r\nI would make a new line No, no, no. \r\n\r\nThat is where it will go. \r\n\r\nBut before you do it there, up even further. Because it's a self -refer... Even further. The line number three or four? You see, that's the self -referential list. Yeah, yeah, yeah. \r\n\r\nIt takes a minute. It takes a minute. Our interaction schema. Because that's how the AI and you will be interacting. Yep. Now you have a name for your tag. \r\n\r\nCopy that. You can copy the whole line. Yep. \r\n\r\nSee? \r\n\r\nSee, see, see? See, see, see? It's just tags and tags and tags, man. Tags within tags within tags within tags. Ugh. I... \r\n\r\nYeah. Yeah, down one more. Nope, one more. There's a closing, the closing tag. No, no, no, you're under project plan. You gotta put an extra space, but yeah, below that, because you're still, no, you're riding inside the project plan right there. \r\n\r\nNow you're outside of the project plan. \r\n\r\nThat's right. \r\n\r\nYou were about to, weren't you? Yeah. Yes. \r\n\r\nYeah. Paste that. \r\n\r\nThere you go. \r\n\r\nPerfect. Perfect. Whatever it's doing. Yeah. Oh, is there a artifact at the tip at the, at the end of it? \r\n\r\nYeah. \r\n\r\nNow in here, put those number one and number two. \r\n\r\nYou've done it. \r\n\r\nYou've created, you've created the, see, this was me doing, I did all this manually, bro. \r\n\r\nI built, you know, I fit over three freaking years, bro. \r\n\r\nSo right, right there. And yeah, including the flattened repo as well. See, cause it says files. And in fact, we need to update that tag. \r\n\r\nWe need, We need to, well, we need to update that tag. \r\n\r\nRemember we deleted it? You can call it flattenedrepo . markdown if you want, instead of files. Everything's in perfect order. \r\n\r\nI don't see, there's nothing wrong. \r\n\r\nOh, I see that. \r\n\r\nI c"
  },
  {
    "id": "report_source",
    "chunk": " it? You can call it flattenedrepo . markdown if you want, instead of files. Everything's in perfect order. \r\n\r\nI don't see, there's nothing wrong. \r\n\r\nOh, I see that. \r\n\r\nI couldn't see, you're correct. \r\n\r\nBut everything's good. \r\n\r\nEverything largely is good. I see that. \r\n\r\nI see that. \r\n\r\nI see that. I see that. Yep. Now, and right there, press enter and you can put what I gave you and then you can clean it. It's just barely like, don't reference 106 or whatever. Just I'll put a number one. \r\n\r\nYeah, there you go. And then our documents. Yeah. \r\n\r\nYep. \r\n\r\nThat's it. That's literally the only change. Everything else is fine. So now I would just, now that, now that you have, you can think of something to ask if you want to add another sentence or two to the end of your cycle zero to please start with the word, please, not because it's polite, but because in, in English, what often follows the word please is a command. So that's what Sam Altman gets wrong when he says, stop being polite to the model. You're causing more tokens or whatever. \r\n\r\nNo, it's not that you're being polite. \r\n\r\nIt's simply that in parlance, English parlance, please do something. That's what comes after please. So it's trained to... You can say, you can just... Because we're giving it a whole shitload of context, aren't we? Where's the ask, dude? \r\n\r\nThe AI is going to just say, where's the ask? What do you want me to do here? please oh okay please do this oh everything makes sense now here's where the ask is okay so please is useful for that purpose yeah please create some initial art of documentation artifacts for us to get started um precisely yep now that you're enhanced with some hindsight well you can just say directive colon is same shit"
  },
  {
    "id": "report_source",
    "chunk": "e initial art of documentation artifacts for us to get started um precisely yep now that you're enhanced with some hindsight well you can just say directive colon is same shit. Initial planning artifacts so we can get started. \r\n\r\nYeah. \r\n\r\nTo help get started. \r\n\r\nYeah. And then if you want to make a mention about help me solve the blank page problem, there's no reason not to say that. That's called metacognition. Metacognition is thinking about thinking. And that is the training data that is missing right now. And so you will be adding metacognition into your AI by simply writing this. \r\n\r\nIt becomes your training data. Help me solve the blank page problem, exclamation point. Yep. \r\n\r\nCause it's going to know you're talking about you. \r\n\r\nYou're, you're the person having this problem. \r\n\r\nMetacognition level status. Okay. Uh, yeah, go ahead and remove the response. \r\n\r\nYep. \r\n\r\nYep. Yep. And remove the, uh, tag as well. Just so it's just not. Yep. Yeah. \r\n\r\nAnd you can copy it and then you're good to go. It honestly, I think it's valuable. Um, no, uh, actually yes as well. Um, yes. because you selected the response that you liked. Yep. \r\n\r\nAnd now's a good time. Yeah. Yeah. I'll get the docs working. No, that's it. You're going to get more organized and you're seeing how, what truly, what helps when you learn to use AI is it helps you cut through the fluff, cut through the extra garbage that is human garbage. \r\n\r\nBecause we've been reading our own books, writing our own books with our own, you know, as good at English as maybe the dude's dyslexic. The dude's writing a book, nothing wrong with dyslexia, but now we have to read it. Right. So now the AI is going to write everything for us. That's good. We just have to val"
  },
  {
    "id": "report_source",
    "chunk": " The dude's writing a book, nothing wrong with dyslexia, but now we have to read it. Right. So now the AI is going to write everything for us. That's good. We just have to validate it. \r\n\r\nIt's seriously, this is, Peak future, bro. \r\n\r\nThis is Star Trek level status. Did it wrap it as an artifact? \r\n\r\nThat's what I want to do. \r\n\r\nI think so. It did. I saw it. Yep. So you would copy this out. Don't you see how messy it looks in here? \r\n\r\nI don't even know. Copy this out in our same process. Right there, the... Yep. Markdown. Copy as markdown. \r\n\r\nPut it in Notepad. And then, yeah. Now it should be clean and... Yeah. What do we have? How's it look? \r\n\r\nOh, what you can do, what you can do, just do, see where the three backticks are? Just add three more backticks. Up at the top on line 19. Yeah, just add three more, or delete those. That's all, yeah. It's just that? \r\n\r\nYep. There you go, see? Yes, sir. So you, read that, yeah, read through that. That you're building the picture of what's missing. You see how, you see? \r\n\r\nYou see how the hallucinations tell you what's missing? I told you. It's easy. It's so easy, dude. This is so easy. Especially with the structure. \r\n\r\nYes. Uh, yeah. Yeah, however you want to do it. Each one. Yes. \r\n\r\nAnd it's task -based. \r\n\r\nHold on. It's task -based. And task can be like, I did the beacon course, and then I did the in -game course. That's the same task. So I just started at cycle 30 when I started to make in -game. I just said, now it's time to make in -game. \r\n\r\nYou see what I'm saying? \r\n\r\nYeah. Because it's the same task. It's the same task. I just started building new artifacts. Yeah. Yes. \r\n\r\nEvery project gets simpler, bro. Yes, sir. Yes, sir. every time you restart a project st"
  },
  {
    "id": "report_source",
    "chunk": "e same task. It's the same task. I just started building new artifacts. Yeah. Yes. \r\n\r\nEvery project gets simpler, bro. Yes, sir. Yes, sir. every time you restart a project structure yes sir because you had to take the lessons learned yet you haven't seen nothing yet yeah yeah yeah yeah yeah yeah it's gonna be a fun hopefully i can build it soon before you have to worry about that, yeah. I'm gonna make it just a button eventually, dude, don't worry. \r\n\r\nUse this prompt, use this prompt. Cycle one, yeah. In cycle one, write it out in cycle one. Write it all out, bro, that's the gold. That's the golden information. Put it, say, now that, put in the response zero. \r\n\r\nSay, I've thought about what you've given me, I've thought about what I want, here's now what I think. Literally, put that's metacognition, that's the missing piece. then the AI will go to work for you, bro. But if it doesn't know what you want, truly, because you never truly told it, yep, it's the missing piece. That's a good question. Rarely do I do the alteration what we just did. \r\n\r\nThat's very rare. Sometimes if it's easier or more beneficial, I will do that. But honestly, like 95 % of the time, I will not do that. I will just make a new cycle one, a cycle two, because my process makes it easy of where I should put this information. If you fall out of the process, you start editing cycles, you start realizing, oh, I've got this file. Where should I put it? \r\n\r\nWhere does it go? Oh, God, I don't know where to put it. Should it go in this place? Should it go here? My process sort of alleviates all that. Once you realize where things should go, it's just like you put this here, you fall \r\n\r\nthe next cycle. Oh, well, I now, I, I, okay. I now, it's the same t"
  },
  {
    "id": "report_source",
    "chunk": "lleviates all that. Once you realize where things should go, it's just like you put this here, you fall \r\n\r\nthe next cycle. Oh, well, I now, I, I, okay. I now, it's the same thing with the now seeing the future. I now know what, where I'm at with the cycle. I know what this will produce. I can produce, just write a cycle one. \r\n\r\nI like, oh, especially, let me say it like this. The reason why I do the 95, it's 95, is because you just give feedback. It's all feedback. It's one big feedback loop, both for you and for it. You're giving it the feedback, and it kind of breaks down if you're just editing the same cycle. You're not quite giving it a full feedback, if that makes sense. \r\n\r\nIf you could just continue through the cycle process, then you are truly giving it feedback, and that's the virtuous cycle when you're giving non -hallucinogenic feedback. You're the human in the loop, eliminating that hallucination. You'll be surprised how far you can get with just that little virtuous cycle. You're getting close to chain of thought. You're getting close to chain of thought reasoning. which is what cycle is doing. \r\n\r\nSo the cycles are doing chain of thought. And so all I'm going to say is that sometimes you need to let it respond and then take its response and then process that which you didn't have before. So you can't, there are some steps you can't skip, right? Do you know about the program called Life? I forget the guy who wrote it, Carl Haraway or something, but Game of Life, basically the guy made on a grid a few simple rules the square is filled in if it's alive and this square is not filled in if it's dead and if it has Two neighbors then the third one will become alive as if reproduction and if there's just one alon"
  },
  {
    "id": "report_source",
    "chunk": "in if it's alive and this square is not filled in if it's dead and if it has Two neighbors then the third one will become alive as if reproduction and if there's just one alone it will die as if to starvation or whatever and just simple rules like three of those rules and then if depending on the initial starting conditions, you get these massive, amazing structures that come out of it. \r\n\r\nAnd you can even get like these living things that seem like organisms that can like produce objects and shoot out and travel and they can move and things like that. And it's called the Game of Life. And it's a very interesting, deep dive on Wikipedia. But the moral of the story of the Game of Life is that some calculations, you cannot get to the, you have to literally run the calculation. You just got to run it to see what the result will be. There's no skipping of the steps. \r\n\r\nSo that's kind of what I feel is going on here. There are some of that you can consolidate. And I have done that when I did the beacon. And then when I did the end game, it was like 30 cycles to 10 cycles, because I knew where I was going wrong. And I knew I already had the cycles written. I could literally reference my own words, how I wrote it the last time. \r\n\r\nNow's the time to do the Excel document. How did I ask for it last time? Yeah, it's refining. Yes, sir. Yes. Me too, me too. \r\n\r\nIt'll help you in dividends in the future, man. This is not just for work. This is amazing. Okay, take care, man. All right, bye.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/audio-transcripts/1-on-1-training/transcript-11.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nYeah So yeah, give me a bit about your background tell me a bit about where you're fro"
  },
  {
    "id": "report_source",
    "chunk": "/v2v/audio-transcripts/1-on-1-training/transcript-11.md\">\nTranscribed with Cockatoo\r\n\r\n\r\nYeah So yeah, give me a bit about your background tell me a bit about where you're from that way I know who I'm talking to and I can talk to you rather than at you so So basically, let's see this. Basically, so actually just I have it downloaded and then if I open it that way. \r\n\r\nOkay, cool. \r\n\r\nOkay, so three years ago, I was working at Palo Alto Networks. I was initially hired to be a customer success engineer and because I had just gotten my bachelor's in cloud computing and I was hired in a I'm with 18 other academy members, and we were put in the Prisma Cloud Academy, which is like a six -week training course at Palo Alto, their internal enablement team put together. And so I was the top student in that academy. And then the team that was putting the training together actually offered me a position on that team. So I got a full, yeah, I got a full, I got, I've worked hard. I got a full -time, I earned a full -time position at Palo Alto Networks, kind of my first, first stint in cybersecurity. \r\n\r\nAnd that was about four years ago. And I supported Prism Cloud and XOR. And near the end of me working there, Chad GPT came out. And, you know, technical enablement. At the time, I knew exactly how it could be helpful for learning and education. I was in education at the time. \r\n\r\nAnd I was in technology. So technical enablement. this is the, what's the most technically enabling tool, the freaking AI, right? That can answer all those questions. Yeah, and that's another thing, that's another thing, the fear was there as well, there was fear, but for me it was fear of missing out, because I felt like it was gonna be a big wave, it was bi"
  },
  {
    "id": "report_source",
    "chunk": "other thing, that's another thing, the fear was there as well, there was fear, but for me it was fear of missing out, because I felt like it was gonna be a big wave, it was big, I didn't feel like it, I knew it was gonna be a big wave. Actually, hold on, let me click, let me see here. \r\n\r\nThere should be a button here I can click, yeah. \r\n\r\nOkay, hi, yeah. \r\n\r\nSo I basically heard two stories. I heard people were starting companies with AI, which I understood that to mean they were basically getting all the questions answered that they needed, like all the hurdles, all the legal issues, everything, just all the paperwork. And then people were also writing code with AI. \r\n\r\nWow. \r\n\r\nIf it can, because I asked the question, what's the most valuable thing that AI can write, if it can write? And the answer to me was code, because code is, objectionable, it's not subjective, like an essay is. You can write me the perfect essay, I can find you some editor who will find something to criticize about it. Versus code, it's functional, you can write a perfect function versus a not so efficient function, but all things being equal, it either does the job or doesn't do the job. And so you can verify, it's objective, verifiably objective. And so that's what I set out to do back then. \r\n\r\nWith GPT 3 .5, I created a Slackbot. I created a Slackbot. I basically created a multiplayer GPT. something that still doesn't quite exist yet. Because in Slack, you know, anyone can start a thread and then anyone can see the thread. And then so anyone can also read what the AI says to you and then can also reply and ask. \r\n\r\nSo it's like multiplayer, right? And you can customize for each channel it's in. Like I made a sales enablement channel. And so"
  },
  {
    "id": "report_source",
    "chunk": "ays to you and then can also reply and ask. \r\n\r\nSo it's like multiplayer, right? And you can customize for each channel it's in. Like I made a sales enablement channel. And so I gave it a persona with the channel's system message, adopt the persona of sales enablement specialist inside our security field, focusing on managed security services providers and palliative networks products, your audience is a team of sales professionals, blah, blah, blah. Prospective client is asking, why did you go with our solution over Zscaler? Sure, David, here are some common questions we encounter. \r\n\r\nAnd then some talking points for the sales enablement specialist, for answering the customer's question. \r\n\r\nYeah. \r\n\r\nAbsolutely, absolutely. Yeah, and I'll tell you exactly how it's going to work. It's just missing a few more pieces, so glasses. Imagine when, let's go with hair stylists. I use this analogy all the time. Very soon, everyone's going to have those glasses that have a camera in them, and people are going to be basically live streaming like to Twitch their entire lives, basically. \r\n\r\nAnd there'll be a viewership of two. It'll be everyone watching their own stream and then their AI. it as well. Then what's going to happen is that's when you're going to get hours and hours and hours of cutting hair, a hairstylist cutting hair. Then he's going to start annotating that data. Or not even just annotating, he's going to have that data as raw data. \r\n\r\nHe's a good hairstylist, so it's recorded how to cut a good haircut, right? Bada bing, bada boom, that's training data that we didn't have before we had the recording platform. So you can't skip the step. You can't have an AI that can help you learn to cut hair with your glasses, you"
  },
  {
    "id": "report_source",
    "chunk": "ning data that we didn't have before we had the recording platform. So you can't skip the step. You can't have an AI that can help you learn to cut hair with your glasses, you know, augmented reality superimposed like the right angle or the right clipper or detecting that you've picked up the wrong clipper or the wrong size and saying, uh -uh, they've asked for this haircut and this is the right one you're supposed to be using. That's in situ learning. That's not possible without the training data set and you can't get the training data set until you have the need for it. \r\n\r\nOkay, so and here's an example of I also created a RAG system before I even knew the term RAG. Because you see here I'm adding a knowledge base file. I'm adding the administrator guide for XIM and it turns it into an embedding. And I actually store the embedding in the Slack channel. So Slack instantly became my vector database. Um, um, but, uh, so I asked, this is the reset of the GIF. \r\n\r\nSo what is Cortex XIM? And XIM was a software that came out in January of 2022. And the cutoff date for training was December, 2021. And so when you ask about XIM, it's like, oh, XOR it's, uh, and I'm like, no, not XOR, XIM. It's a new product. It's not in the training data. \r\n\r\nI apologize. However, I'm not familiar with XIM. It might be some confusion or a typo. No brother. I didn't make a typo. So I drop in the advert. \r\n\r\nguide, I upload it to the Slack channel, and then I just use the slash command to upload a PDF. I choose which PDF to make into a knowledge base for this channel. It's processed. Now I'm going to ask the exact same question. And this was what it said the first time, EXOR. So here's the exact same question. \r\n\r\nI get a response. Thinking. Cor"
  },
  {
    "id": "report_source",
    "chunk": "processed. Now I'm going to ask the exact same question. And this was what it said the first time, EXOR. So here's the exact same question. \r\n\r\nI get a response. Thinking. Cortex XIM is a comprehensive security platform with XIM, a gainful visibility in the assets, a tech emerging set. \r\n\r\nYes, yes, yes. \r\n\r\nFollow -up questions. \r\n\r\nYou see? \r\n\r\nHow hard was that to set up? Yeah, and I, yeah, yeah. That's what, that's what taught me. See, that's great. You're very clever, so check this out. That's how I came up with my RAG idea was I first asked, chat GPT, do you know what XOR is? \r\n\r\nYeah, I do, blah, blah, blah, generics. I said, make me training on playbooks, how to make a playbook in XOR. And it was garbage. That's when I thought, well, I went through the whole admin. The admin guide itself was too big at the time to fit, so I went through and I just did a control F, playbooks. Every single paragraph that had the word playbook in it, I made my own file, and then that was basically like my playbook. \r\n\r\nyou know, data set, right? That's right. And then I just asked the exact same question, but I just added that in with my prompt. And it was like, magic. It was damn near almost usable. \r\n\r\nI only had to like format for like the use case, right? But it was literally like whole, it was like night and day difference. And I was like, wow, if I could just like automate this somehow. And so I found a YouTube video. Some dude made a 70 line script where he could rag the constitution and ask questions on it. followed his YouTube, made the 70 -page script. \r\n\r\nI had already made my AI bot without the rag, and so I took the two scripts, I showed them to the AI, because I can't write it. I can't code. I can't code. I'm not a cod"
  },
  {
    "id": "report_source",
    "chunk": ". \r\n\r\nI had already made my AI bot without the rag, and so I took the two scripts, I showed them to the AI, because I can't write it. I can't code. I can't code. I'm not a coder, I'm not a developer. I can't write enough statements to save my life. I also could never learn another foreign language. \r\n\r\nI failed Spanish every year before I passed it, every year of high school, every year of college, because it's a required course. That's right. I know. I know. Thank you. And this is it's I'm kind of I'm chicken little over here and I'm screaming the sky is falling. \r\n\r\nAlright, so let's fast forward. Let's fast forward. Because if I could do this, China can do this. And if no one's paying attention, if no one's paying attention, I know they are paying attention in China, they see this as their golden ticket. If you look at just optimism levels, if you just look at optimism levels of AI, AI in China, and in in America, it's like 39 % optimism in AI in America and 70 or 81 % optimism in China and if you just look at the adoption rate of any technology throughout history, a leading indicator as to the adoption rate is the optimism rate as well. \r\n\r\nOne of which is measurable prior. You see what I'm saying? So, like, they are, and not, and so, in March of this year, March 25, Gemini 2 .5 Pro came out. Before that, in May, November, I had reactivated a game that I had launched over a decade ago. \r\n\r\nNo, no, no, no, no. Where's the damn history? \r\n\r\nAh, yes. My videos, yes. \r\n\r\nThat's what I'm looking at. \r\n\r\nA game called Lineage 2. It'll be fine. And so I made... It's a L2J server. It's a Java server from over a decade ago where I got hacked. Someone wiped my database. \r\n\r\nBut I kept my code, because I always thought in the b"
  },
  {
    "id": "report_source",
    "chunk": "I made... It's a L2J server. It's a Java server from over a decade ago where I got hacked. Someone wiped my database. \r\n\r\nBut I kept my code, because I always thought in the back of my mind I could maybe reverse engineer the custom part of the database from if you could look at the code, because the code is going to call directly the right tables and columns, and if you just put it all together, you could do that. And so I kept it for 12 years. And then finally AI comes about, right? And then O1 Preview comes out, which was the first thinking model. And that's what made it really code extraordinarily well. And that's when I sort of learned my parallel processing. \r\n\r\ntrick. And one of the things that I did was once I got that server back up and running and everything, I made a website and everything, I wanted to start making new things. That was sort of the holy grail was making something new versus tweaking something that already exists. So I had played on a server way back in the day where they had this fantastic custom PVP event in a specific dungeon that was perfect red versus blue because the dungeon itself was colored red and blue. And so I basically recreated that from memory in this game with AI. This was kind of like the, huh? \r\n\r\nNo, this was before Gemini. I used O1 preview. So I'm giving you the real long back story because you sound like, oh, yeah, no, it's chat TBT. Now it's like O3 or whatever. Yep, but it was the first version of the o1 o2 their strength their thinking models They had you know chat GPT 3 .5 and 4 and then I think they just got to 5 and they're not going to do that anymore They're doing thinking models. They're doing 4o and other in that but they're doing o1. \r\n\r\nIt's so confusing But yeah"
  },
  {
    "id": "report_source",
    "chunk": "y just got to 5 and they're not going to do that anymore They're doing thinking models. They're doing 4o and other in that but they're doing o1. \r\n\r\nIt's so confusing But yeah, this the first one was o1 preview. That was in november of last year. So it literally hasn't even been a year since the first thinking LLM has been in existence. So like, that's right. And so this is all very fresh. What I'm able to do, I was able to do from the very first version of thinking. \r\n\r\nIt's only going to be uphill from here. you know what I mean? So what this event is, basically, you got your scores. I even had a whole, yeah, I'll show that as well. So you destroy the flags and you push around here. Ah, so a thinking model is basically just a model that talks to itself before it talks to you. \r\n\r\nSo it's basically accessing the latent space in its memory as it thinks, right? And then it can make a plan. It can make a plan for you in the thinking, see? So you can prompt it to think in a certain way. And then there's all kinds of like thinking strategies like plan, act, do, reason, you know, those things they make you learn in like business school. But you can just have your AI do that as long as you, right? \r\n\r\nAnd then you can make that into a, you can make that, it's called chain of thought as well, so. But they do that automatically. It's not like you have to do it. It's done automatically, sort of. That's right. It can plan. \r\n\r\nAnd then it can find a solution, right? And then it can give that one to you, right? Actually, yeah. Yes. 2 .5 Pro is a thinking model. Yeah. \r\n\r\nSo, I trained Gemini before working at UKI. I was a RLHF trainer, basically. It's actually part of my whole story, part of this situation. This isn't loading, but "
  },
  {
    "id": "report_source",
    "chunk": ". Yeah. \r\n\r\nSo, I trained Gemini before working at UKI. I was a RLHF trainer, basically. It's actually part of my whole story, part of this situation. This isn't loading, but maybe we could, ah, there we go, okay. So, this was my website. I still have it all. \r\n\r\nI just flipped it off to do this game instead. But this was kind of the first time using an AI to like make SQL statements, servicing data. on the website. This is data from the game server. So who owns what boss jewelry? Where is it? \r\n\r\nOne of my players said this is like CIA level status. One of the things in the game that's very fun is over enchanting a weapon and then you can break the weapon. But when you do that, that story is gone. But that's part of the story of the servers who has what over enchanted equipment. And so now it's captured. It's actually stored. \r\n\r\nAnd so you can see the history. You know top clan list all that kind of stuff and then for the battlegrounds they have stats as well So yeah, yeah, so it's um, it's an open source project called L2j and yeah, I just got basically my own version of it with it that has some pretty sophisticated Customization that I actually got one of one of the world's best Developers of this game to make for me at his people at his peak when he was he was making $10 ,000 a weekend off of his off of his \r\n\r\nservers from donations. \r\n\r\nYeah, I was I Was just barely scrapped punk dude. Oh, man. Oh, yeah. This is yeah, I still have him Actually, this is him just a full circle This is him right here. Jeremy Eskins. That was that's the guy. \r\n\r\nYeah, that's the guy So anyway, um, so this was a replay. So I I record everything I made a whole season Because every single game gets recorded, and so you can have ELO, pers"
  },
  {
    "id": "report_source",
    "chunk": "Yeah, that's the guy So anyway, um, so this was a replay. So I I record everything I made a whole season Because every single game gets recorded, and so you can have ELO, persistent ELO, persistent kill death. And then each kill, depending on what kind of class you kill. So it's all dynamic ELO scoring. And then I put it all on the website. It was wild. \r\n\r\nBut then, so, Autofarm. I made my own bot in the game. \r\n\r\nMy own botting system. \r\n\r\nLet's find it. \r\n\r\nI should have a video of that, actually. Maybe not. Oh, I love that game, yeah. Okay, but, okay. I have a little bit, but not too much. It'll be nice when it's ready for VR. \r\n\r\nOkay, so that was... I was making... Now, 01 has a context window limit of 128 ,000, which when it came out was an extraordinary leap. It went from 20 ,000 to 120 ,000. And then when 2 .5 Pro came out, that one had a million. So that's a huge jump, that's right. \r\n\r\nHuge jump. And even still now, the latest quad code just came out, 4 .5 or whatever, it's got 200 ,000 still. So a million is a lot. And this game, Yeah, now I hear there's some, yeah, on the super expensive plans, I think you can get more, but it's extremely expensive. Like we're talking like, you can get a million with Quad, but it's like $15 prompt, a $15. Good question. \r\n\r\nDivide character count by four. And I'll show you what, I'll, I'll, no, no, no. So just rule of thumb, and we can get deeper into it, but rule of thumb, the token count is just the character count divided by four. Yep. I'm showing, no it's a great question and that's how I know when the student is tracking, is that question always comes up. So this is what a token is, is, is, is, is, is. \r\n\r\nSo this is what a token is, is, is, is, is, is. So we got an is "
  },
  {
    "id": "report_source",
    "chunk": "the student is tracking, is that question always comes up. So this is what a token is, is, is, is, is, is. \r\n\r\nSo this is what a token is, is, is, is, is, is. So we got an is and an is. See there all the different colors are signifying which one is a token. So this is one token, this is one token, this is one token, this is one token. It's just the colors are showing that. Now you can see, there's 12 tokens and 39 characters. \r\n\r\nIt's a bit off of that. It's repetitive, so that's cheating. Anyway, so what's happening is these are what the actual token numbers are. So these are the actual tokens. It's 382. That's IS. \r\n\r\nBecause I can tell, because look at all the 382s. See? So this is, they're just numbers. Brother, they're just numbers. You're looking at a number. this is what an embedding looks like. \r\n\r\nThis is what an embedding file, that file I showed you that comes back, I press in the PDF. When you actually look at that file, because I can see it in the raw text as it streams back in, even though it's binary, when it's in the code and processing, I can see it. And it's just this shit. It's just strings, it's just chunks. Because that's what a rag does, right? It chunks out your document into smaller pieces. \r\n\r\nEach chunk then gets turned into this vector. That's what they look like. Huh? Ah, bro, bro, why? Oh, so there's a whole field of study called tokenomics. It's actually a whole, yeah, dude, it's a whole thing. \r\n\r\nIt's basically just symbology. It's basically just about compression. It's basically just how you use, it's basically just another language. It's like another base. Base 27, base 10, base 2. It's just, it's just, that's all it is, dude. \r\n\r\nIt's just numbers. It's just, that's it. Divide by four. "
  },
  {
    "id": "report_source",
    "chunk": "another language. It's like another base. Base 27, base 10, base 2. It's just, it's just, that's all it is, dude. \r\n\r\nIt's just numbers. It's just, that's it. Divide by four. There's nothing else you need to worry about at all whatsoever. And that's it. Limits and costs. \r\n\r\nThat's right. That's right. Now, that's right. That's right. That's right. Yeah. \r\n\r\nThat's where it matters for us. \r\n\r\nYeah. \r\n\r\nWhere's my AI studio? I don't know why. Oh, what is going on here? Why is all my history? Oh, I'm in Chrome right now. That's why. \r\n\r\nOkay. \r\n\r\nI understand. So AI studio. is free. No one offers an analog. OpenAI does not offer an AI studio equivalent where they just give you damn near unfettered access to their smartest models. Claude, same thing. \r\n\r\nYes. Yeah, so that's unfortunately our company is not ready for that yet, not for lack of trying on my part. I had a very nice long talk with the CTO, but apparently no, he never wanted to follow up. But basically, it's like a repeat. It's like a repeat. I gem these guys up about AI, but then they don't pull the trigger and do the one thing that they need to do, which is to get us a CUI safe API or get us our own endpoint that we can call. \r\n\r\nI've got an LLM running in my damn closet. What is their excuse? right like let's you know it's really not that and it's not rocket science and i can help them shut it all up you know but it's just they they go off and don't whatever anyway so um so that's what that's yeah yeah well we'll look uh talk to who i don't know who he is all right so let me just do a quick demo of where i'm at with my DCE. I'm in the process of working on this, so I'll just have to close that. Yeah, yeah. \r\n\r\nDude, it's wild. I've never done it either, bro."
  },
  {
    "id": "report_source",
    "chunk": "uick demo of where i'm at with my DCE. I'm in the process of working on this, so I'll just have to close that. Yeah, yeah. \r\n\r\nDude, it's wild. I've never done it either, bro. That's the fucking point, bro. Dude, I didn't even know how to get the goddamn logs. How do you develop when you don't know where the air logs come from, right? It took me like four hours to figure that out and then even then like, you know There's a certain thing you have to do or else like you won't really refresh your environment even with your new code is saved or whatever And so I'm sitting here testing the same damn environment eight times not knowing I have to refresh it into a certain way It's all learning but the AI is helping me learn every step of the way my process, dude Oh my god. \r\n\r\nNo, I'm like chicken little over here, dude. It's it's wild. Okay, so 34. I'm just gonna make a new older I know you saw this, but there's one piece of the puzzle that... Yeah, there was one piece of the puzzle that you didn't see. Because this is the development version. \r\n\r\nAlright, so, watch this. Oh no, that's right, it broke. That's right, okay. I have this... It's okay, I have a GIF of it. I'm in the middle of fixing it, and I've made some really good progress. \r\n\r\nBut let me just show you a GIF of it. Yeah, yeah, yeah, it would, but you would have to coax it a bit. All you would have to do, though, is you would have to make your, it's the same process, though. That's what it is, it's this, you create the artifacts, you just create the artifacts that describe the thing that you're after, and you don't know what they look like, the AI does, right? It'll come up with, like, user stories. I didn't ask for user stories, but I get user stories, right? \r\n"
  },
  {
    "id": "report_source",
    "chunk": "'re after, and you don't know what they look like, the AI does, right? It'll come up with, like, user stories. I didn't ask for user stories, but I get user stories, right? \r\n\r\nYou just have to work with it, And then you start getting artifacts and you start vibing with it. And you're like, yeah, I like this. No, I don't like that. And with multiple responses, you know, you like this. And when you get a choice, you're like, oh, I want it. I want this direction. \r\n\r\nI like this direction. And you can go that direction. It's do it. So I got a demo mode that I'm building out right now, because once I'm done with demo mode, then API mode is just built automatically built. \r\n\r\nDemo mode is using a local LLM, my local LLM. \r\n\r\nSo it doesn't matter how many responses that you generate. And then they come streaming in. This is, so this is from my local LLM, streaming them in parallel. I'm getting about 500. tokens per second from just my shitty -ass little 3890. I'm just running OpenAIs at GPT -OSS. \r\n\r\nYes, yes. The same, it's running my server, it's running RISC -AIM as well. It's hosting the, no, it's all free. No, no, yeah, that's right, it's free, that's right. It's free. That's right. \r\n\r\nI'm just paying for electricity. I'm just that's right. And that's what I'm saying. That's what this is over here That's what this is. That's what so look at this. \r\n\r\nLook at this. \r\n\r\nThat's what this one is. All right, that's this choice Like we can do this like we can that's on premise. We make our own LLM. That's pillar three. It's more expensive No, no, this is all my personal stuff. Yeah. \r\n\r\nNo, I'll share this as well. Not sure. We'll try open that one Okay, and then that one the prep this one So, yeah, well, this is how you get "
  },
  {
    "id": "report_source",
    "chunk": " all my personal stuff. Yeah. \r\n\r\nNo, I'll share this as well. Not sure. We'll try open that one Okay, and then that one the prep this one So, yeah, well, this is how you get AI, and this is how you get AI in your company. It's so, I understand completely how blinding it is to not even know where to start, but this is where you start. You either get commercial API, which is you go to ChatGBT and start using it, which is not good for us for a myriad of reasons, or you get your own AWS Bedrock solution with SageMaker, like I said in the meeting, which is in here as well. \r\n\r\nThat's pillar two. And then pillar three is running your own local model. And then so certain tasks will be good for local, and certain tasks you're going to want the foundational models because they're smarter. Yes, that's Bedrock. \r\n\r\nNo, so you're talking two different things. \r\n\r\nSo there's one is API access to foundational models through Bedrock, which is CUI safe. \r\n\r\nSo it's API calls, so no local. \r\n\r\nOr you can still in the cloud set up your own, now what you're talking about, get your own GPU in the cloud and then put your local model on that GPU. That's different, that's different. Or you can get the third, which is your own damn GPU. I'm advocating for the API, and then what'll happen is we'll start to discover functions that we would love to make API calls for. Like, do you remember that in the demo I gave, the Intel chip, where I highlighted a paragraph and I got the key Intel out of it? Okay. \r\n\r\nBasically, I could get it up again, but that is an example of like a refined, defined function. Right? I send it a paragraph, and then it reviews that paragraph, but then it also reviews the context of the scenario, and then decide, because then"
  },
  {
    "id": "report_source",
    "chunk": "efined, defined function. Right? I send it a paragraph, and then it reviews that paragraph, but then it also reviews the context of the scenario, and then decide, because then it knows what the users are going to need to do, because the users are going to need to ultimately type five different commands. Right? It boils down to like five different commands. And so ultimately, the user needs to know which of the five commands should they, you know, and so just find some relevancy there. \r\n\r\nSo whatever the user's copying. And so right in the beginning, the key intel is telling them how to log in to get the drone manifest. And so the AI knows those two things. And so the AI understands and knows just by those two things, oh, the user's in the beginning, they're looking for the drone manifest, here are the two things they're gonna need to copy and paste in order to get access to it. And it just creates that nice little chip for the user. \r\n\r\nNow, you don't need to, once you've got that refined and you've fine -tuned that process, you don't need, you know, you can use a local call, that's a free, that's free AI, because it's so clean and refined use case, yeah. \r\n\r\nthose are the big boy models. No, that's what they are. They're the foundational. \r\n\r\nThat's your biggest, strongest models available that need massive server farms to run. \r\n\r\n2 .5 Pro? Yes. Yes. Yes. That's right. That's right. \r\n\r\nYeah. That's okay. Yeah, that's right. Yes, sir. Now you're catching it. See? \r\n\r\nThere's nothing stopping us from just getting this started. But they're going about it the wrong way. They're trying to like define the, huh? So that's what I'm trying, that's what I'm building out right now. That's what you just saw with the GIF where it"
  },
  {
    "id": "report_source",
    "chunk": "out it the wrong way. They're trying to like define the, huh? So that's what I'm trying, that's what I'm building out right now. That's what you just saw with the GIF where it was streaming in, right? See, so it's a GIF. \r\n\r\nIt's the exact same. Yeah. And then you get a choice. Just look at the spread. Look at this. Did you just see that spread? \r\n\r\nSo yeah, I'm doing eight. I'm doing eight at once, but now it's just restarting. Yes. Okay. Yeah, because think about it. Think about how different they are. \r\n\r\nThink about the question I ask. I ask, I want to create a tower defense game. Maybe one of them goes a cybernetic route. Maybe one of them goes like a plant -based route. You see what I'm saying? Like they could be so completely different and now I get to choose. \r\n\r\nThat's what I mean by I flip the script when you do this. But then also one could have an error and one could not have an error. One could have a good idea that the others did not have. Yes, that's what a lot of people don't do as well. Is they think they want to use is not wrong, it's just not what I'm doing. \r\n\r\nWhat they do is they do one to Grock and one to Claude and one to Gemini, which is fine. It's still sort of the same thing, but it's apples to oranges sort of. This is very standard and you still get the gains that I've been just espousing over and over again from my process. Yes, yeah. And look, yeah, it is, and look at the difference. It's about to finish, when the last one finishes, there. \r\n\r\nSo the spread, see, one to eight, and then over on the right, I'm gonna click sort, and now the biggest one is 3 .1, and the shortest one is 1 .3. So it's almost double the size. And I got, you see, so I got more planning, I got more planning out of it"
  },
  {
    "id": "report_source",
    "chunk": " sort, and now the biggest one is 3 .1, and the shortest one is 1 .3. So it's almost double the size. And I got, you see, so I got more planning, I got more planning out of it, okay? So that's just, now this is just local, this is all just local. \r\n\r\nThe smarter AIs, the better AI you use, the better planning it can give you. \r\n\r\nAnd again, that's the beauty of my extension, is all when a new AI comes, I just point to the new AI. So, okay, so now let's kind of back up a little bit, because now we're basically at the very tip of today, which is my extension and connecting it with the local LLM. Because it's the moment that, the moment that UKI has a KUI safe API, all they need is my extension that's API friendly, which I'm coding it out right now. \r\n\r\nAnd then it's, you can just use it with our repos. \r\n\r\nAnd then the code created a whole new Ansible role instantly. \r\n\r\nI've done it. This is phenomenal. But this is actually where I want to go. I want to pivot to this. go over the game a little bit because once March 23 25 came around This I have a good idea. \r\n\r\nYeah, this is uh, this is the game I made and then I made a report about the game So this is sort of I skipped into section 2 the origin story. \r\n\r\nLet's see. \r\n\r\nCheck this out. Actually It's it's it's right here 120 days This is the prompt for my game. \r\n\r\nI did it manually. I did it manually. That's right. Before I had the extension. See? So, this is the way I would do it. \r\n\r\nI'm just going to scroll down to, and start with one of the cycles. Let me just search open bracket cycle. There we go. Cycle 1. I want to fill this out before I use it. Something is bothering me. \r\n\r\nOh, that's why I did it differently. That's why I did it differently. Okay, so you see I"
  },
  {
    "id": "report_source",
    "chunk": "e go. Cycle 1. I want to fill this out before I use it. Something is bothering me. \r\n\r\nOh, that's why I did it differently. That's why I did it differently. Okay, so you see I just wrote cycle 1, 3, 3, 7. And I said, we're done with reports one and two. Please continue. I was building out a report. \r\n\r\nI was building out my reports, this report, basically. And the image, yeah, working on the image generation and stuff. And then, see, here was the previous cycle summary of action. So this was just part of the AI's response that I clicked out to keep the context. See, it was all manual. And I would put my own tags like this. \r\n\r\nAnd then, great work. Let's fix the script. And I just built this over time. \r\n\r\nThis is the prompt file. \r\n\r\nAnd this is where I would put all the responses, in these eight different tabs. \r\n\r\nIt's all manual, in Notepad++. I'm not a developer, bro. But I am, uh, I know. It's impressive that I just never stopped, even though everyone tells me that this is stupid, you know what I mean? Dude, this is like, just, you know, no one listens, man. Like, everyone should... \r\n\r\nWhen I show this to someone, they should do what you did. Fucking stop, and turn, and start asking some fucking questions. Just like the thing just said, it's something that demands an explanation. Legit, you know, like yeah, and then I would look that's right. I need to talk to the right billionaire dude. \r\n\r\nYeah. Yeah, I haven't met that person Yeah, I could make some waves trust me and I'm just getting more and more refined Oh, and also let me tell you as well. Let me just mention this is to you as well when I did talk to dr Wells I didn't have my DCE extension He doesn't know about he doesn't know about that. And and in fact wh"
  },
  {
    "id": "report_source",
    "chunk": "ell. Let me just mention this is to you as well when I did talk to dr Wells I didn't have my DCE extension He doesn't know about he doesn't know about that. And and in fact why I started making it it's a direct replacement and competition between Not in a bad way, in honestly a good way. As to what is he making right now? You know he's making a content development studio, right? \r\n\r\nThat's what him and Ben, and they're all jazzed up about it. They're going about it the wrong way. This is the content, we already develop content in a studio. It's called Visual Studio. Stop inventing, reinventing the wheel. I did it so well on accident with an extension, yes. \r\n\r\nLet's keep going, man. I already love where your head's at. Let's just keep going, because I need to fill your head with all these ideas. Alright? I love it. Seriously. Let's skip a bit. \r\n\r\nLet's skip a bit. \r\n\r\nAnd we can go quicker. Because what needs to happen, let me tell you why. I need to create a training. Imagine every senator, every decision maker in our country receiving reports of this magnitude. It took me days to put this together. Days. \r\n\r\nthe brother brother no no no no no no let me look at this look to pick it's a picture book okay it's a picture book it's an adult picture book it's the printing press 2 .0 it's read to you by Scarlett Johansson okay dude I mean I could do however I could mix match the voices I can give her I can give her an accent if you'd like all right it's crazy all right but this this delivery of knowledge like knowledge transfer is unprecedented and available today. It cost me zero to put this together. It's zero dollars. If I can do this, China is using these tools to do the same thing to stab us in the back. That's their M "
  },
  {
    "id": "report_source",
    "chunk": "lable today. It cost me zero to put this together. It's zero dollars. If I can do this, China is using these tools to do the same thing to stab us in the back. That's their M . O. \r\n\r\n, dude. That's their M . O. They have a whole, yes. Okay, so here's where sort of it starts to get more like, so the way that they train AIs is this fissured workforce. Basically, Google, OpenAI, Mena. \r\n\r\nThey break out the, they subcontract out the work to these contractors like Globalogic, Majoral, ScaleAI, and then they even subcontract it out further to even more subcontractors like Synet, Ravens, and Digitiv. And basically it's a whole army of ghost workers that are doing this essential work, by the way, so they should not be coming in. They should be full -fledged employees, just off that fact alone. But so, it's a critical, you can't get an AI. An AI, once it's pre -trained and it's trained, it's useless until you do the reinforcement learning with human feedback where you evaluate the helpfulness and the harmfulness and you write, you get two responses back and you say, well, this one's better than the other one. And you create that reinforcement learning. \r\n\r\nThat's what makes a model actually usable. And so, that's what this army actually creates. And so, without this army, yeah, and so, that becomes a problem though. It used to be the way it works is it's labor arbitrage. So Globalogic, which is a Hitachi Group company, they're a Japanese conglomerate. It's not even American. \r\n\r\nThey make money via labor arbitrage, so the split in between, obviously, from what Google pays them and what they can pay the workers. So the more they can pay, keep the wages down. And so the job title is a content writer. In America, content writer. N"
  },
  {
    "id": "report_source",
    "chunk": "what Google pays them and what they can pay the workers. So the more they can pay, keep the wages down. And so the job title is a content writer. In America, content writer. No one listens to a content writer. Ask me how I know. \r\n\r\nThat was my title and no one will listen to me talk about AI. Now if my title were pacing threat, what is China doing? I'll just jump down to that. They have an entire training. They've done professionalization. It's state -sanctioned. \r\n\r\nThey started it over five and a half years ago. They have a whole job career ladder. Whereas in America, I hit it. I hit the glass ceiling. I'm a go -getter. If you can't tell already, hence the story about Palo Alto. \r\n\r\nAnd then so it the same thing happened, huh? Yes. Yeah. Um, yep I have all the research that I used Gemini to do research OSET I don't know Mandarin. Okay, but I use Gemini I said to Gemini deep research I said your English is pretty good, but how's your Mandarin and I sent it and I asked it How is China's AI playing? What are they doing? \r\n\r\nHow it and that's how I got all my Intel. Yes wild Dude, oh my God, they're doing it on us. They use DeepSea for OSINTs, of course. That's in here as well. But so here, so what we have, so here's what I'm saying, is what I am doing is I have this skill set that the Chinese are cultivating. That's, thank you, thank you, and then no one will listen to me because I'm deprofessionalized, all right? \r\n\r\nThere was no career path for me to go up, okay? And that's what's missing in all of America AI right now is, The AI deployments fail. I'm sure, I don't know if you've seen those statistics right now, but Gartner and everything, they're putting out these, there's only like 1 % of AI deployments are making li"
  },
  {
    "id": "report_source",
    "chunk": " I'm sure, I don't know if you've seen those statistics right now, but Gartner and everything, they're putting out these, there's only like 1 % of AI deployments are making like million dollar returns. And the vast majority of them are failing and not doing good. And everyone's gonna ask why, maybe go into an AI winter, probably not. Because too many people like me are just saying this is way too ridiculous to get AI winter. \r\n\r\nEven if AI stopped today, we've got a decade of work ahead of us. and AI is not gonna stop today. So, the glass ceiling, I hit it, dude, I hit it. In fact, just check this out. I'm in the union for Alphabet Workers Union. I just met with the organizing committee. \r\n\r\nI gave them a short spiel, but I blew their minds. Also, at Global Logic, I'm still in communication with the training manager. She's right here. And she's been there. She knows it's a revolving door. She knows exactly. \r\n\r\nShe might even be ex -military. Because she said, when I showed her my virtual side of the proving ground, she said, imagine military using a crane. It reminded me of the Arnold Whitehall simulations I did in grad school. So I'd love to hear more about what she's talking about here. She was the one who promoted it. So let me actually share this as well. \r\n\r\nIt's probably quicker if I go over here. So, basically, this, they could care less, dude, they could care less. I basically, because it's, you know, it's basically my responsibility, honestly, to let them know when I discovered this, the fact that the job is a de facto national security asset. Because we're training the I mean you use gymnastics and people in the NSA use Gemini. And when your workers training Gemini are up here in this section, the cognitive co"
  },
  {
    "id": "report_source",
    "chunk": "set. Because we're training the I mean you use gymnastics and people in the NSA use Gemini. And when your workers training Gemini are up here in this section, the cognitive consequences of scarcity are all underpaid. \r\n\r\nThey're ghost workers. I wasn't even allowed to say I worked and trained Gemini. I'm creating the most celebrated technology, yet I can't even say that I am doing it. It's either, I get a little emotional sometimes because of that. And so, it's institutionalized garbage in, garbage out. Because Hitachi Globalogic does not care about the quality of the product, only so much as Google doesn't complain, all right? \r\n\r\nAnd people say, oh, well, they have, Reviewers, they have to make sure that the data is good. You're talking to the senior reviewer. Okay, I got promoted. I was promoted to reviewer. First, I was moved from the non -technical to technical. That's when I tried to get a pay raise. \r\n\r\nI never got it. And then I was promoted again to reviewer and then promoted again to senior reviewer. When I was promoted to senior reviewer, I got English grammar training. That was the training. We were all put in English grammar. We were given grammar worksheets. \r\n\r\nEnglish grammar, so no training whatsoever for, you know, chain of thought, yeah, nothing, because they don't know how to, and the size of the tasks, because in the beginning, the AI could only have a thousand tokens, it just, LLMs didn't have context windows. And so you could only have to review 1 ,000 tokens max, right? They're small tasks, right? But over time, it grows exponentially. Now we're dealing with a million token context windows. The size of the tasks we were reviewing went from 1 ,000 to 40 ,000 on track to 120. \r\n\r\nAnd the pay didn't "
  },
  {
    "id": "report_source",
    "chunk": "nentially. Now we're dealing with a million token context windows. The size of the tasks we were reviewing went from 1 ,000 to 40 ,000 on track to 120. \r\n\r\nAnd the pay didn't change. Nothing changed. It's just more work. And then they give you three hours to do it. That's nearly a book, actually. Okay? \r\n\r\nSo garbage in, garbage out. That's all you're going to get. And so institutionalized garbage in, garbage out. It's the cause of Ouroboros effect, which is the model collapse. That's my theory. It's why AI sort of hit a plateau. \r\n\r\nBecause the people training them. We're not given any training. Imagine if I had my DCE system doing grading validation. That never allowed to be innovative whatsoever. So that is a problem in and of itself as well in such a fast -moving field. Anyway, so this is basically what's going on is the higher the tech rises, the harder the fall will be in this current deprofessionalized situation where all the learning that's down here actually on the unseen battlefield. \r\n\r\nLet's skip down here. Oh, what is this? I forgot about this. Okay. \r\n\r\nAnyway, I forgot what I was looking for. \r\n\r\nWell, obviously I'll find it. Yeah, let's go there. I like this picture a lot, actually. This is fun. So I made over 2 ,000 images for this. And you can see the difference. \r\n\r\nLook at this image. Versus, this was the first one I created. It is, however, it's the image for cognitive capital. And cognitive capital is the collective intellectual capacity, skill, and problem -solving potential of a workforce or population. Now, would you get that from this? Absolutely not, right? \r\n\r\nYeah, right? Versus like this, when I got better, and I learned, oh, it can do words, right? You can tell what this is all about. No, n"
  },
  {
    "id": "report_source",
    "chunk": "t from this? Absolutely not, right? \r\n\r\nYeah, right? Versus like this, when I got better, and I learned, oh, it can do words, right? You can tell what this is all about. No, no, this is Gemini. This is foundational. Yeah, see? \r\n\r\nSo this is like, you can tell exactly what I'm trying to communicate in this section. And I learned how to do it over time. That's the vibe coding to virtuosity. You can literally see the, now I can take this with me for the rest of my life. This quality, you know, because I put in the two weeks it took to learn how to, and what do I ask for? I ask for, it's about knowing how the system you're interacting with, because you're talking to Gemini 2 .5 Pro, and Gemini 2 .5 Pro can send a message to the diffusion model, the image model. \r\n\r\nSo when you understand you're working with it like that, you can tell, because you don't send the message to the diffusion model, Gemini does. Gemini creates the tool call. So you've got to coax Gemini to do something good for you. You get what I'm saying? You've got to gin up Gemini. You've got to gin up Gemini. \r\n\r\ngin up, it's actually for real. And so, you, no, this is, no, no, absolutely not, no. And I told you, I trained Gemini. And I learned this stuff myself, everything I learned was, yes, from three years ago, the first project I made was the Slackbot. No one could be vibe coding longer than me, I was the original, I was an OG vibe coder. Because, are you in your car, Pat? \r\n\r\nNo, that's fine, that's fine. It's got a history from March or something. Vibe Code, yeah, February, not March. Andrew Karpathy, one of the guys, one of the OpenAI, original OpenAI guys. In 2025, he wrote a blog post. \r\n\r\nOh, no, no, no, no, no, no. \r\n\r\nHe wrote a tweet or whatever"
  },
  {
    "id": "report_source",
    "chunk": "rch. Andrew Karpathy, one of the guys, one of the OpenAI, original OpenAI guys. In 2025, he wrote a blog post. \r\n\r\nOh, no, no, no, no, no, no. \r\n\r\nHe wrote a tweet or whatever. Tweet, tweet. There's a new kind of coding I call vibe coding, where you fully give in to the vibe, express exponentials, and forget that code even exists. It's possible because, yeah, dude, I can't write code. What is he talking about? It means, honestly, seriously, it's crazy. \r\n\r\nIt should mean nothing coming from a real developer, and it should mean everything coming from someone like me. Do you see what I'm saying? The fact that I can't code makes it completely... Dude. And so, he comes up with this idea this year. This year. \r\n\r\nI've been doing it since 3 .5 came out. It was the first thing I thought, like I told you. I asked the fucking question. What's the most valuable thing you can write if you can write code? The answer is code. I told you why. \r\n\r\nIt's an object. I just put the two dog brain cells together. That was it. I did it three years ago and I never stopped. I never stopped because I got the results, dude. If I didn't get the results, I wouldn't have thrown it all. \r\n\r\nI would have gone, you know, played my video games, whatever. But I got the results. and it just changes everything. I felt like the wave is coming. You know, we gotta learn this before it's, I can capture as much as I can, and I didn't know I'd be riding it. I also didn't know that no one would even recognize, like, that I'm riding the wave. \r\n\r\nI'm gonna appear up right in the wave, and no one even recognizes. It's pretty, okay, so, all right. Anyway, yes, thank you, thank you. So, I'd love to make it huge. Yeah, so negative feedback loop, that's Ouroboros effec"
  },
  {
    "id": "report_source",
    "chunk": "one even recognizes. It's pretty, okay, so, all right. Anyway, yes, thank you, thank you. So, I'd love to make it huge. Yeah, so negative feedback loop, that's Ouroboros effect, the snake eating its own tail. In China, what they're doing, I mean, they're only five years away from the completion of their plan to dominate in AI, okay? \r\n\r\nAnd they started this plan in 2017. So how they're doing it, how they're doing it, they're doing inland sourcing, so whereas we're outsourcing our cognitive capital, they're insourcing, so they're using it as a form of poverty alleviation. If they have done in Yizhou, the poorest region in all of China, because it is the most mountainous, they have turned it into their premier prime data labeling base that they're going to use as a case study to expedite delivery throughout the rest of their nation. So while people on Reddit are all like, ooh, ah, look at this cool, interesting this bug, interesting this bug, ooh, I'm sitting here realizing the only reason that they could possibly have. \r\n\r\nbe cutting mountains to build a highway as fast as they fucking can in this fucking place that's ass because of the mountains is for AI is for AI they built this they built this for AI so yes and people are like oh cool is it less work than building a tunnel guys you're asking why did they build this in the poorest region because that's where their AI base is right and yeah and yeah and so So they're gonna have people like me. \r\n\r\nArmies of people like me. And it's just data, it's data curation. That's the skill set. Data labeling is the skill set. And it's like this, they're gonna be, dude, they're gonna be like, they're gonna be like sleeper agents, dude. And they won't even know it. \r\n\r\nBecause they"
  },
  {
    "id": "report_source",
    "chunk": " is the skill set. And it's like this, they're gonna be, dude, they're gonna be like, they're gonna be like sleeper agents, dude. And they won't even know it. \r\n\r\nBecause they're gonna be gaining these insane skills of the future and they won't even know it until China activates them, I'm telling you. \r\n\r\nAnd how does that, what do I mean by that? \r\n\r\nThat's what I mean down here. Like the call to action, like so, when you do this vibe coding virtuosity, Basically, you just find some cool project and you code it out, you know, I love baking so I'm going to make a website for my bakery, or I love fishing so I'm going to make an app that helps me find the best fishing spots, whatever. And then, after you code it out, you make your website, you do whatever. And then you live your life, and then you're walking through your community, and all of a sudden, you know, your neighbor, X, Y, Z, someone in your community is having a problem that you realize, wait a minute, I have the skill set. I can make them a website, an intake system, a blah, blah, blah, accounts, and I can solve that problem for them. I've got this skill set. \r\n\r\nYou get what? You get activated. and you didn't even know it you're so you're you're I think you can we can create sleeper agents in our country of these people who just become these experts and they don't even know it because this because the AI will get better under their feet that it's all about and why is it why is it data curation I use the analogy of the human eye The human eye has a focal point of 2 degrees, and everything else is, for lack of a better word, hallucinated. Your brain is basically concocting that which it thinks is around, but you only get focus here. Why? Because there's so much "
  },
  {
    "id": "report_source",
    "chunk": "else is, for lack of a better word, hallucinated. Your brain is basically concocting that which it thinks is around, but you only get focus here. Why? Because there's so much information that your brain would be overloaded if everything was in perfect focus. \r\n\r\nSame thing here, it's context window. Always, I argue, the context window will never be as big as the universe. Therefore, we will always have to filter or funnel somehow into our context the data which we need to use for the task at hand, and the tasks will always change and evolve over time as we explore and spread throughout the solar system. Everywhere AI has not been, it will hallucinate. We will have to go first and create the data sets, annotate, label, transform the data into something that the AI can then come with and use, and we're the explorers. We're going to be the eyes for the brain. \r\n\r\nThat's it. So, and just look at how much data we produce as humanity. It grows exponentially the moment we got more data to store, right? So, we'll never have a need, a lack of data. We'll always be exponential. Ah, and then you can rise to meet the moment. \r\n\r\nwhich is basically here. As AI gets better, the capability threshold to use it to reach your 100x moment will go down over time. The expert will be able to reach their 100x moment sooner than a novice would. But what you can do is you can become an active learner and you can accelerate that intersection. You can accelerate that. But you know all about technology, so this is where I want to, you started talking sort of got me thinking about this because this is what my skill set plus your skill set, right, is the peak archetype because it's one thing to, a lot of people don't know how to get data together, ri"
  },
  {
    "id": "report_source",
    "chunk": "out this because this is what my skill set plus your skill set, right, is the peak archetype because it's one thing to, a lot of people don't know how to get data together, right? \r\n\r\nAnd I think these skills help these skills. \r\n\r\nLike I know this data is important, not that data. I know I need this data. I think I say it like this, the internet is your hard drive. So the more you know that's out there on the internet, the more you can think, oh, I need this data set. I can pull this data set into my project and use it. So it's more or less, yeah, live coding virtuosity. \r\n\r\nThe AI sort of helps you learn. You start out basically trying to dissect everything, untangling knots to building blocks. After a little while, you start to be able to bring pieces together and put them together. Then what happens is at a certain point, you kind of get stuck somewhere. it's because you don't know something. Like maybe you don't have like cloud skills and so like a serverless function is like very abstract to you, right? \r\n\r\nYou know, you're talking to an AI that can make you an artifact that can explain exactly how it works and like you can give it, you know, errors and build out this AI as a meta tool and explain all those learning gaps. It becomes this learning accelerator. That's this recursive learner stage. Yes, exactly. That's it. Exactly. \r\n\r\nThat's right. That's this stage, right, precisely. And then at a certain point, you become sort of an adaptive toolmaker in this recursive learning stage. And the Apex skill is on -the -fly tooling. That's literally me making the DCE. And it's here. \r\n\r\nIt says, a competent user asks the AI, how do I solve problem X? While the expert asks or says, build me a tool that solves problem X. "
  },
  {
    "id": "report_source",
    "chunk": "y me making the DCE. And it's here. \r\n\r\nIt says, a competent user asks the AI, how do I solve problem X? While the expert asks or says, build me a tool that solves problem X. It's the same AI. You just have to think how, what you're doing is you're building, you have to build a mental model of the model so that every prompt is a lesson. Because you send a message, you get a response, you now know what it can create. And maybe if you ask it differently next time, you'll get closer to what you're after. It's building that mental model of the model. \r\n\r\nOr you can even game that out with the AI as well. Yeah. Because even that might be so abstract, you don't even know what it is, what it should look like. So, yes. And you know when it's solved. That's crucial as well. \r\n\r\nThat's key as well. Because I don't look at the code, right? I look at this and I say, this button doesn't do what it's supposed to do. Fix it, right? This is what it does, and this is what I want it to do. So I'm at step A, and there's step Z. Get me B to Y, right? \r\n\r\nAnd then, also, another thing, you don't know how many in -between steps, because again, you're not a coder, and you can't instantly come up with a solution to every problem in your brain to know that, oh, this is going to take one cycle, or this is a big problem, this is going to take five cycles. You know what I'm saying? Just throw it at the AI. You'll get there at the end sometime. Because that's what I do at DCE. I give 10 problems. \r\n\r\nI don't know which one is going to be the hard one. \r\n\r\nIt'll solve seven in one go. \r\n\r\nThose were probably easy. Two made some progress, and one it didn't even touch. That doesn't matter. I'm asking about three problems this time, not seven or ten, ri"
  },
  {
    "id": "report_source",
    "chunk": "ne go. \r\n\r\nThose were probably easy. Two made some progress, and one it didn't even touch. That doesn't matter. I'm asking about three problems this time, not seven or ten, right? So I have a solution as well. I came up with something called universal basic access. \r\n\r\nIt's not universal basic income. It's better than that because you're giving people AI credits. You're not giving people dollars. You're giving people AI credits. So how much does it cost to give a person a dollar? \r\n\r\nIt's not a trick question. \r\n\r\nThat's right. \r\n\r\nHow much does it cost to give someone an AI credit? Fucking nothing until they spend it, yeah, right? \r\n\r\nAnd then when they spend it, what are they doing? They're prompting they're producing. That's right They produced something out input output response. That was a something was produced an image a digital asset, right? Yeah, that's right. That's right That's you got it. \r\n\r\nI don't have to I don't have to walk you. I don't have to hold your hand through it Yeah, that's absolutely right. And that's what we do with the Rural Electrification Act We needed electricity in the country, but no no But no one would, no electrical company would build it. Likewise, we need AI talent in the workforce, but no AI company, they keep it deprofessionalized. Yeah, Trump doesn't like AI spending. Trump doesn't like spending money on AI. \r\n\r\nThe money's not moving and the factories aren't getting built. So, you know, show me the factories, you know, show me the results. So by Google's own admission, by Google's own research, they predict billion data labelers in the future right now think about that number so currently you're right let's listen to this one That's my job. What is DLA accounts in your table? I'm"
  },
  {
    "id": "report_source",
    "chunk": "billion data labelers in the future right now think about that number so currently you're right let's listen to this one That's my job. What is DLA accounts in your table? I'm not sure. \r\n\r\nThat was a thought I had. I wanted to mention it. It'll probably come back to me. It's a way to explain the significance of this situation, right? I remember, I remember. Okay, so machine learning training has always been a super data intensive task. \r\n\r\nAnd then in 2017, generative AI showed up. It was that research paper. So, but up until that, so up until that point, Machine learning was a sort of like, at most, like sentiment analysis, like is this paragraph, you know, positive sentiment, negative sentiment? By and large, it was like, you know, maybe like, you know, data, like drawing bounding boxes around like a pedestrian and saying pedestrian, you know, labeling a dog a dog, a cat a cat. You don't need to be a rocket scientist to do that, much less speak much English to do that, and this is a globalized economy. And so it makes sense that largely a lot of that work is outsourced. \r\n\r\nYou almost can't fault the big companies for doing that. \r\n\r\nBut then 2017 creates a new tool, the LLM, which requires a new data set, a critical thinking kind of data set. \r\n\r\nAnd that kind of leads to this hidden curriculum, which is here. That's this hidden curriculum. Because when you spend eight hours a day critical thinking and writing down your critical thinking, see, when people would do work, they wouldn't write down their thoughts, they would write down the product. It's only now that we have the tool that we actually need to write down our thoughts. Exactly, you see? \r\n\r\nAn AI without knowing how to think won't be able to, right? You've "
  },
  {
    "id": "report_source",
    "chunk": "It's only now that we have the tool that we actually need to write down our thoughts. Exactly, you see? \r\n\r\nAn AI without knowing how to think won't be able to, right? You've got to put the thoughts down in words and then it can do it. So when you spend eight hours a day, five days a week critically thinking about thinking, you get what? You get smarter. It's just because you're black. What a surprise, right? \r\n\r\nIt's a hidden curriculum. The mind is a muscle. Every click is a rep, you know? Sense making is basically critical thinking, bias detection, AI validation. You're building these insane skills. This is the same skill set, right? \r\n\r\nOkay, so That's right. Yeah, that's right. Yeah. Yeah, basically so because cognitive capital is more powerful than economic capital now because look what I can do with no money a 3090 I just went for the cheapest route. I just went for the cheapest route to 24 gigs 100 % Yeah, because you can't you just can't load a model and VRAM if you don't have the VRAM because then it goes into CPU RAM and then it's just dogshit slow using GPT OSS. \r\n\r\nThey have two model. It's open AI's open source model. They have two models. They have a 20 billion model and 120 billion model and I'm using the 20 billion. Parameters. Yeah, the size of the model. \r\n\r\nHow big is its brain? \r\n\r\nYeah, and it directly correlates with that's how much VRAM you need. You can fit 20. And then now quantization comes in. So quantization basically halves the amount of VRAM you need, but then AI gets stupider. So for 20 billion at like Q4 or whatever, you cut it in half or something. I think eight, I don't know. \r\n\r\nI think an unquantized is FP16, and then I think the first layer of quantization is Q8, and then the second "
  },
  {
    "id": "report_source",
    "chunk": " you cut it in half or something. I think eight, I don't know. \r\n\r\nI think an unquantized is FP16, and then I think the first layer of quantization is Q8, and then the second layer is this Q4, which is what basically everyone's going towards. It's this happy medium, and then there's Q2, which is just dog shit. So first my process is the copy and paste to AI Studio. And that's free. API calls cost money. So I first, I'm designing my DCE in phases. \r\n\r\nThe first phase is complete, where the whole thing fucking works and I can, you know, create the whole project. and then I can, I have that file that I can then manually send it to the AI of my choice for whatever service I have purchased, $20 a month or whatever. Now that that's all built out, it's a much smaller lift to then build the API piece of the puzzle, you know what I'm saying? Now, well not even, not even cost, yeah, because in order, because now I'll have to build out this, the API calls and the functions and stuff. So I can just use my model as a toy to build out, yes, yes, as a test bed. No, no, no, no, so, no, so I use Gemini 2 .5 Pro to actually write my DCE code because I need the smartest dude. \r\n\r\nIf I'm going to, I'm not going to, I would not be wasting my time, you know, trying, because that's where I'm doing real work, the real work, the cooking. I'm going to use the smartest model available to me, right? Why wouldn't I? It's also, but also, no, again, no one has anything like AI Studio. Only Google has this, which is literally damn near unfettered access to their smartest model. My prompts do $15, bro, if I were to pay API. \r\n\r\nLet me tell you the math per cycle. Yes, per cycle. Yes, in my game. No, per prompt. I have a shortcut here. I just go here and"
  },
  {
    "id": "report_source",
    "chunk": " prompts do $15, bro, if I were to pay API. \r\n\r\nLet me tell you the math per cycle. Yes, per cycle. Yes, in my game. No, per prompt. I have a shortcut here. I just go here and I click this, click this. \r\n\r\nYeah, I have this button right here. Yeah, see, it counts it up for me, see? I actually do the math. See, so this is my game, AI Ascent, my project. My whole prompt would be about 747. ,000 tokens. \r\n\r\nAnd it would cost me to send it four times, but I actually, I usually do eight, $15. And did you see how many cycles I'm in? Let's go to the top, 1 ,408. So let's do the math, let's do the math. That's just to make the game. That's how much, nah, we'll get there. \r\n\r\nSo that's $21 ,000 of API calls. And that's a, that's a, that's a, Conservative because not every cycle is just one and done all that would be beautiful now many times a cycle year Yeah, yeah, you have to reiterate and change and realize you made a mistake and fix and send it again Yeah, so yeah This is what basis this is the minimum of what it would have cost to make this game Via API and I did it for free. I took that money. I put in my pocket basically because it's yeah, I got the tokens the tokens Yeah Okay, so but now now you're asking some questions that actually get to sort of like are important in terms of making development decisions like so So I made this game. Let's sort of look at what did I make so I made a game where? you research Yeah, so I've got two researchers in my my founder on research right now, so that's researching We'll just do a little building So I just got basic in the concepts that gave me some more components and I can get some vision tech Oops, did you see that? \r\n\r\nOh, what do I need? need gpu oh i need cpu so let's just add s"
  },
  {
    "id": "report_source",
    "chunk": "sic in the concepts that gave me some more components and I can get some vision tech Oops, did you see that? \r\n\r\nOh, what do I need? need gpu oh i need cpu so let's just add some more cpus to my cluster the research is going again i'm playing the game right now i'm showing you the game yeah it's a tycoon game yep it's a simulation game you you make your own ai company and so this is just sort of the research tree that we're going through right here right now i can actually queue dude it's so meta no research nodes yet so we'll get there later all right so well all right so now i've got some components i can make I'm gonna assign my founder to build that one. I've got some machine learning engineers. Hire some. I only got two right now. \r\n\r\nSo they're building some components. \r\n\r\nOur old training gears, agent sensor unit, agent logic cores. I think they'll build those up. Yeah, yeah, I sent that in. And my report is in here, see? \r\n\r\nI mean, yeah, the game is the proof and the report is the theory, right? \r\n\r\nSo I made this game. \r\n\r\nThree months into making the game, that's when I decided to pause and I'm like, because I'm showing you just the pieces. \r\n\r\nI'm showing you what I made. And then after I made it all, I'm like, Hold the phone, man. This is just wild. And then I, because everything that's in the report was in my brain. It was too much man. I had to get it out Yeah, I think I think it's just gonna change yeah, so simple pathfinding algorithm implement basic pathfinding for the game AI agent Okay, cool, and then we can train the game AI agent. We're ready to train it. \r\n\r\nI've got the agent modules I needed see the agent modules. They needed those core logics that I was making so to make these so now I can trai"
  },
  {
    "id": "report_source",
    "chunk": "ent. We're ready to train it. \r\n\r\nI've got the agent modules I needed see the agent modules. They needed those core logics that I was making so to make these so now I can train the game AI agent. I need a cluster first. Let me make a cluster. Make a cluster. ClusterFuck to add some resources to it. \r\n\r\nI'll do it this way. Put it over here. Do it this way. I just changed that. Okay. Now I'm looking at the cluster and adding resource to that cluster. \r\n\r\nI think that should be enough right there. Okay. Back to the training. Yes. See? I require, I need 100 and I have 250 in the cluster, in the selected cluster, which is ClusterFuck. \r\n\r\nAnd then I have enough GPU and I can start the training. It starts a training cycle. I have a nice little simulated loss function, you can see it's sucking up all the GPU to do the training. General pool's not in use right now. Okay, so that training is done, now I can do the benchmark for the game AI agent. Oh, I need compute, I have no compute in my general pool, I forgot, I took it all out. \r\n\r\nTook it all out, general pool, let's put one in general pool. \r\n\r\nprobably just that, probably just that. \r\n\r\nOkay. Yeah, that was it. \r\n\r\nThat was what I needed. \r\n\r\nI took all my GPUs out and I didn't really need it. Okay, benchmark. Now the benchmark is running. So loading the opponent, a medium bot. So my AI is playing a bot and my AI beat the bot. So now I can finalize, name it OpenAI5. \r\n\r\nThat's what they called their bot. Okay, so now I have a bot. I can add some features like basic heuristics. Simple rules for decision making, some lane control, oh my CPU is junk. And some predictive aiming. Oh, I'll deal with the CPU, I'm stuck in a second, let me upgrade. \r\n\r\nI need a certain amount of "
  },
  {
    "id": "report_source",
    "chunk": "on making, some lane control, oh my CPU is junk. And some predictive aiming. Oh, I'll deal with the CPU, I'm stuck in a second, let me upgrade. \r\n\r\nI need a certain amount of ELO, I need more, I need more components, and I need more compute. \r\n\r\nSo let's, I can hold shift to do five at a time, cook and knees again, in order to upgrade. See, now I can upgrade again. Once I get, I think it's 1650, and I can hold shift to upgrade five at a time, so it's faster. Oh, they're getting built, they're getting built. I've got my engineers building. This guy actually, let me reassign. \r\n\r\nThere we go. There we go. Okay. Almost there. We need 1650. There we go. \r\n\r\nOkay. Now I have enough ELO to enter the... i need 1640 so i can compete my game ai agent against their game ai agent oh so they're just kicking that guy's out they just kill that guy basically they're probably yeah they're probably yeah so i mean bro right dude dude okay how how crazy is what you're looking at right now all right so i followed history because open ai before they made chat gbt they were making a dota bot and i got the dota map So you make the first AI you make as a game AI, and then once you win your first match, the attention is all you need, paper gets released, and then you can do more research, because I've done all the research already for this stage of the game, and then unlock more research, and I can do more research, and I can make an LLM API, and then I can make a chatbot, and then I can make an audio model, and an image model, and a video model, a robotics model, a multimodal model, and then finally a world model, and that's how you beat the game is you get all seven billion people to play your world model. Everyone's living in your simulation "
  },
  {
    "id": "report_source",
    "chunk": "dal model, and then finally a world model, and that's how you beat the game is you get all seven billion people to play your world model. Everyone's living in your simulation at that point. So, I have an idea. You saw my virtual cyber proofing round. \r\n\r\nI literally made that from scratch, dude. It honestly sounded kind of corny, I'll be honest with you, when the AI came up with that scenario. Because it came up with four different scenarios. And it sounded corny, but I didn't care. I just had the AI pick which one would be easiest to make. And I just went with it, dude. \r\n\r\nAnd it came out pretty damn good. A month. Not the scenario. the whole vcpg and then you can just make scenario after scenario after scenario because i've got the whole environment you see i've got the platform made that's right with my extension it was the first project that's right i made with my extension because i just needed to test i needed to test it was it's a throw it's a throwaway project dude it's a genuine throwaway it but it's god it's glory it's a billion dollar thing dude and and and also look at the look at this consistency like that's what's really key is i had this image then I could say I need a yellow one and you know blue one but it's yeah that's the AI's at that point now and then I just had a bunch of image and I think I like whatever I use this one or whatever right and then I just map it and then you saw up here this was just I said I drew this out in paint and I sent this image to the AI And I said, this is the plan. And I put my mouse over it to get the X, Y coordinates, right? \r\n\r\nBecause it's 10, 24, 10, 24. I just used paint, because paint, wherever you put your mouse, it'll show you the coordinate of your mouse. So I ju"
  },
  {
    "id": "report_source",
    "chunk": "e X, Y coordinates, right? \r\n\r\nBecause it's 10, 24, 10, 24. I just used paint, because paint, wherever you put your mouse, it'll show you the coordinate of your mouse. So I just needed one, two, three, four, five coordinates to make my game logic, basically. Yeah, which is just an image also AI generated. easy easy easy yeah great well let's go let me yeah so the so there so the four scenarios that were planned out one of them was this forward base blackout basically it's early morning like 4 a . m and then at 6 a . \r\n\r\nm the big off is about to go off but right before the whole base gets shut down and then you have two hours to get the base back online Ghost Fleet is the one, is the drone one. Silent Running, that one's about you're in a submarine and you're in, you know, silent ops or whatever. So, and all of a sudden the reactor starts acting erratically and you've got to figure out what the heck is going on with the outside support. So, breaking, you know, radio silence and using internet or anything like that. And then Operation Stolen Scepter, I don't remember. I didn't read that one too carefully. \r\n\r\nThat was like the first one I suggested. But I could just make hundreds of them, each one. Also, some of those artifacts are worth just glancing at, because that's what we can do is we can just build a little bit of this. vcpg together And that'll just open your eyes. So I always do this with people. I'll show I'll I'll give them those so all the theory That's what we just talked about all theory like it's all great. \r\n\r\nIt's all talk right? Um until the next time you're gonna see it. Um, You're gonna see it. So let me get in here and just uh, yeah, let me just cut by coding it out with the dce um, so in here artifac"
  },
  {
    "id": "report_source",
    "chunk": "until the next time you're gonna see it. Um, You're gonna see it. So let me get in here and just uh, yeah, let me just cut by coding it out with the dce um, so in here artifacts, so The team intelligence and flags, the scenario, tactical map integration, UI plan, collaborative intelligence system, those little Intel chips, Jane AI integration, so like how we're going to get the AI. I called it Jane from Indra's game. The tactical map, you know, so like zooming in on it. \r\n\r\nI didn't, we didn't do that yet, right? \r\n\r\nIf I ever want to, I have an artifact made for it. The offensive gameplay, so I added that to it after we had all the defensive stuff. I had, so then that means that most of the scenario three planning is going to be up here a bit. There it is, S003, ghost fleet, narrative, and event flow. So, aha, this artifact, because I had it all split up. This artifact is deprecated as of cycle 104. \r\n\r\nContents of this document have been consolidated into artifact 59. That's where we want to go. Okay, so there we go. I had, so I had to ask for this. I had to ask, I had to recognize that, okay, my scenario three is sort of getting split up between these artifacts, and it's like, you know, I've got some scenario three at artifact 30, I've got some scenario three at artifact 70, and I decided to ask the AI. a cycle on that, reorganization. \r\n\r\nThat's part of being the curator, the human in the loop. It's called context rot. It's a known thing. This allows you to spend a cycle to keep your context. from Roddy, that's right, it's real. So, but yeah, that's it. \r\n\r\nSee, I'm glad that's what you're seeing by just getting into, now we're transitioning a bit to the, from theory to practice. Now you're seeing still theory, but b"
  },
  {
    "id": "report_source",
    "chunk": "at's it. \r\n\r\nSee, I'm glad that's what you're seeing by just getting into, now we're transitioning a bit to the, from theory to practice. Now you're seeing still theory, but because you didn't see it create this, maybe I wrote this. Oh, good God, Jesus Christ, look at this. \r\n\r\nI did not write this. \r\n\r\nSo, but yeah, all these, yeah, all AI studio, yep, every copy paste. \r\n\r\nAnd then so it starts with the master artifact list. \r\n\r\nWhich has every single artifact organized by the way look at this organized, dude, dude That's insane because yeah the first yeah, it keeps it up to date. Yeah. Yeah, so I So I write I want to make a tower defense game click create the prompts that gives me the whole prompt markdown file Which is just in the root directory down here at the bottom prompt markdown and see I was at cycle 125 on this project And see all my cycles are recorded because DCE every single cycle is in here So I have my own company, that's another thing. I have my own AI company. This is, DC is mine, dude. Okay, so let's just keep that in the back, keep, I, dude, I am the. \r\n\r\nmost generous motherfucker you'll ever meet. But let's just keep, let's just, yeah. No, no, I'm happy to share, but this motherfucker is mine. And because here's the deal, here's the deal. I am happy to share because I am going exponential. I am going parabolic. \r\n\r\nAnd so if you wanna try to cut me dry, that's short -sighted thinking, bro. You wanna take my DCE and cut me dry? You're not gonna get the next version, bro. That's only two months old. Imagine what it looks like in four months, bro. Wait until I'm, wait until I'm making it, wait until I'm making it with Gemini 3. \r\n\r\nGemini 3's on the horizon. \r\n\r\nIt's on the horizon. There's, there's, "
  },
  {
    "id": "report_source",
    "chunk": " four months, bro. Wait until I'm, wait until I'm making it, wait until I'm making it with Gemini 3. \r\n\r\nGemini 3's on the horizon. \r\n\r\nIt's on the horizon. There's, there's, there's, there's rumors. I'm just gonna code faster when I got 3. It's because it's my process, dude, right? Yeah, yeah, yeah. No, no, I wanted to, I wanted to get that, oh no, no, no, it's a fair, it's important, and it's very important that you know where I'm coming from, right? \r\n\r\nYeah, yeah, yeah. Yeah, the way I would want it the way I'm thinking about monetizing it is um so over in the Version of building in the settings I have I have these choices, so I think there'll be a split right here, so if you want to get API Access you need to pay like you know five dollars a month. I don't care. It doesn't matter money is nothing But you get the free mode which is the manual copy and paste version, and then there's this demo mode, which can just be my local LLM, I don't, I could care less. It'll stream in, right, whatever, the users can, and then that'll, because then that will show them how the API works. work, right? \r\n\r\nSo that the moment, just use the, no, yeah, pick us. Because then the moment they just, they love it, they want it, they're done copying and pasting, they want API, just show off the five bucks a month, right? I don't care. And then they can get the API, and then it's all straight. So that's how I think about it, I'll just make a website, right, you know? Then just that's that, you know. \r\n\r\nI've never been able to monetize anything, I'm not very good at it. \r\n\r\nMaybe this will be the thing I can monetize, right? \r\n\r\nI don't know. Maybe, maybe I can get some people to help me. Maybe I can get some people to help me. I don't know. "
  },
  {
    "id": "report_source",
    "chunk": "\r\n\r\nMaybe this will be the thing I can monetize, right? \r\n\r\nI don't know. Maybe, maybe I can get some people to help me. Maybe I can get some people to help me. I don't know. Who knows, right? Okay, because I'm, yeah, yeah. \r\n\r\nNo, you're right. \r\n\r\nOkay, so check, no, I know you said you gotta go. Maybe five minutes and then we'll, okay. So, finish this. Now I can start a company because I beat my first one. Let's just call it OpenAI for, just to get it over with. And then intention is all you need, paper's been published, this revolutionary transformer architect, you can change everything. \r\n\r\nBut also, training. I could retrain now because I have a win replay data, so I could retrain my game AI agent. But also, I got new research available, see? \r\n\r\nA whole bunch of new research now. \r\n\r\nBut now let's just fast forward, just unlock all research, so you can get a kind of glimpse, right? Researcher, data science, training optimization. I made a whole, and this isn't Angry Birds, right? This is not Angry Birds. This is not Angry Birds. So these are all the different AIs you can make. \r\n\r\nThese are all the different components you can make. \r\n\r\nAnd they filter, so you can just see what the advanced image API needs. \r\n\r\nIt just needs these. Yep, yep, yep. All the different compute, different data types. Text, coding, image, audio, video, robotics. You do data enrichment, actually. Raw web text, synthetic web text. \r\n\r\nAnd that's how you keep your data quality high. \r\n\r\nOh, it's multiplayer. So I made the whole game before I even plugged an LLM into it. And then about three months into it, I was like, oh, let's just try to make a multiplayer. So I made a multiplayer. And then once I made a multiplayer, yeah, just some peopl"
  },
  {
    "id": "report_source",
    "chunk": ". And then about three months into it, I was like, oh, let's just try to make a multiplayer. So I made a multiplayer. And then once I made a multiplayer, yeah, just some people, mostly people I know. A few people are from the internet, genuine. \r\n\r\nYeah, he's my friend. He's a good friend. So yeah, he's a really smart guy too. Okay, so I'm just gonna go. \r\n\r\nOh yeah, yeah, yeah. \r\n\r\nSo once I had the chat window, That was when I had the idea to make my chatbot, because I was like, well, I already made a Slackbot. So I had my whole game, I got my Slackbot script, and I just added it as an artifact. I said, now let's make Ascentia. I call my AI Ascentia. \r\n\r\nAscent AI, you put AI at the end, Ascentia. \r\n\r\nSo that's my AI. It's turned off. Yeah, it's good. I turned it off right now because I'm actually pivoting to use VLLM, which is much more potent than LM Studio. And so I had not switched over the game to use the LLM. The game still uses LLM Studio, so I would have to turn off the AI over there, turn it on over here. \r\n\r\nI don't want to bother with it. the AI questions about the game and it will tell you how to play the game. You can also ask an AI in here about the page, or you can ask anything about the report, because I have over 100 ,000 tokens of report, or 300 ,000 that are also an embedding, so when you ask a question about that, you get all my data in the response from the AI. Dude, basic. \r\n\r\nThat's actually yes. And that's so funny you said that. \r\n\r\nNo, you're right. I said that to someone that thought I was being cheeky. They thought I was being snarky. I'm like, no, legit. Because she said, well, what do you think about it? I'm like, you can ask the AI what I think about it. \r\n\r\nAnd she's like, no, I want to "
  },
  {
    "id": "report_source",
    "chunk": "s being snarky. I'm like, no, legit. Because she said, well, what do you think about it? I'm like, you can ask the AI what I think about it. \r\n\r\nAnd she's like, no, I want to know what you think. And I'm like, all the research was I painstakingly put it together. I read it. And if I didn't like it, I changed it. Because I would critique the model I would say this paragraph is wrong and here's why right so you're getting my answers You're getting my thought. Yeah, so like she and then she and then she's like, oh I get what you're saying She actually I see what you did there. \r\n\r\nShe got it. She got it. Yeah, she's part of the union Yeah, okay. So, um, yeah, so next time absolutely. \r\n\r\nI'm glad we got this to make this connection Yeah, it'll be forever man because this is just gonna you know Parabolic man, and you will grow with it Once you get entwined with it the next model comes out you get more capable all your tricks will work Okay, so yeah, I'll just kind of leave it at that \r\n\r\nYes, absolutely. I love that idea. So I gave you the extension already, so let me just show you how you would install it. I'm glad you asked that before we disconnect. \r\n\r\nAll you would have to do with that file that you download, it's a v6 file, you just go into the extension section, and then a VS Code, it doesn't matter if it's Windows or Linux or Mac, you just click this button right here, as long as you've got like real VS Studio and you don't have like Community Edition, you'll have this option right here. \r\n\r\nThen you just you just shoot you point you point to the v6 file and then you'll get this little button right here And you're in yeah, the AI just made a spiral. \r\n\r\nYeah That's right. That's right. That's right. We never that's r"
  },
  {
    "id": "report_source",
    "chunk": "e v6 file and then you'll get this little button right here And you're in yeah, the AI just made a spiral. \r\n\r\nYeah That's right. That's right. That's right. We never that's right. \r\n\r\nThat's talking about my DCE. \r\n\r\nThat's right So so what's important? Yeah, so by all means by all means and maybe probably everyone has this you'll get stuck You'll like you won't even know where to click. It's confusing sometimes and I'm telling you like there's parts where I'm on my DC. Let me pull it over I'm over here in my DC and I'm like, shit, wait, do I right here? Do I need to start a new cycle? Wait, shit, wait, I forgot. \r\n\r\nLike, where am I at? You start to get into a flow and I'll help you. Once you get into the flow, you're in the flow. But there's, yeah, so, yeah, see? The solution in the accuracy environment. Because the problem, right? \r\n\r\nRevising something, dude? Oh my God, dude. Oh my God. \r\n\r\nWhat a nightmare. \r\n\r\nAlso, you know, getting a little work done. \r\n\r\nOh, you did read this. Great. Okay, good. Yes. I put this together in one evening. After I showed Eric, Nell, my DCE, he got to sit next to me and see it, right? \r\n\r\nBut again, it's sort of falling on deaf ears. No shade. So, no, no, no. He, no, no, no. Yeah, he knows. Not in any meaningful way, right? \r\n\r\nEveryone can see and agree it's cool. Everyone can see and get that. But we need action, brother. We need to make movement. We need to start walking the walk. Yeah, and it's fresh, it's brand new. \r\n\r\nDude, I literally just made it. I literally just made this thing. And I only made it because I showed the whole team before you showed up, the last demo day, two demo days ago. I showed, that was the first time the whole team saw my AI gig. And so they were asto"
  },
  {
    "id": "report_source",
    "chunk": "cause I showed the whole team before you showed up, the last demo day, two demo days ago. I showed, that was the first time the whole team saw my AI gig. And so they were astounded, but then they were like, what does this mean for us? And then, that's what I'm trying to say. \r\n\r\nIt's content, bro. I created content. What do we do? So, but yeah, yeah, yeah, it helps. \r\n\r\nSo yeah, I'm not a coder. \r\n\r\nI just know a lot about tech, because I grew up, I'm a gamer, right? So I have that edge, right? I think gamers all have an edge at this. Yeah, I could literally talk all day to you about that. But yeah, so you saw this. I made this for Eric in an evening because he suggested it. \r\n\r\nHe suggested you should make a white paper. And so I literally that evening put this entire thing together. for him So this was a one evening thing because because how because I have my entire Context already brother and I just pivoted I said, okay, we're making a white paper on this extension. It's already got all the context It knows it knows all my artifacts. It's got all of the code and it's got all of the cycles of me inventing inventing this thing so this so So the way I do that as well is I take the, once I get the white paper written, it's basically, you know, it's basically this paragraphical form. And then I just basically for each page, for each section, I create an image prompt. \r\n\r\nAnd let's actually do it. Let's do it. Let's go to my DCE. \r\n\r\nLet's go to my artifacts. Let's go to my search image. \r\n\r\nImage. I got it. White paper generation plan. Yeah, where are the images? \r\n\r\nProcesses asset. \r\n\r\nOkay, so here is the actual. \r\n\r\nwhite paper before it has images. Okay, there's one for the AISN game. Actually, no, let's look at this "
  },
  {
    "id": "report_source",
    "chunk": " the images? \r\n\r\nProcesses asset. \r\n\r\nOkay, so here is the actual. \r\n\r\nwhite paper before it has images. Okay, there's one for the AISN game. Actually, no, let's look at this one. Here, yes. Image generation system prompt. I have a file like this somewhere for each project. \r\n\r\nIt's a master system prompt for an image generation to create a consistent and thematically appropriate set of visual assets for whatever the project is. And so whatever sort of the theme of the images I want, like high tech, military, cyber security, you know, environment, technology, lighting, color palette, dominant, dark, amber, gold, cyan, it's going to have all the same sort of theme to it. And so all I do when it's image creation time, whatever I'm asking for, I just copy and paste this in with it. It's that simple. And then there was one in here, image generation system prompt, and then the CVPG banner image prompt. So this was, at some point, Original home page I felt a little bland, but I was like you know what we should have a banner image So I just said one of the cycles make an artifact to make an image banner to ask for an image banner So I can get an image banner, and it just broke this up And I just I literally just literally just copied that and dropped it into the to the running conversation I had and it came out with the banner. \r\n\r\nI just picked the one out of the ten I liked hyper -realistic cinematic ultra -wide aspect image of futuristic cemented earth or whatever And it tells me where I should put it, where I should name it when I get it and save it, right? You see, you build out all the structure, all that content, and then the book will write itself. Okay, let's write chapter one. And then you can read eight different cha"
  },
  {
    "id": "report_source",
    "chunk": "ight? You see, you build out all the structure, all that content, and then the book will write itself. Okay, let's write chapter one. And then you can read eight different chapter ones. Yeah, which one tickled your fancy? \r\n\r\nWhich one got your goosebumps, bro? It's exactly what it is. I love that analogy. Choose your own adventure. What does OCO stand for? Offensive Cyber Company. No, I get you. \r\n\r\nYeah, the bad guys. Yep. Yep. Here's the scenario one. A critical segment of the Combatant Command Headquarters network has been compromised. The SOC received high -fidelity alerts indicating unusual outbound traffic and potential data staging from the server in the J2 Directorate. \r\n\r\nPreliminary analysis suggests the activity aligns with DTPs of a known nation -state, cozy bearer, CPT, activated, conduct immediate alerting objectives. See? And if we had KSATs, see that's what Ben was asking in the meeting, right? He's like, how could we map this? I'm like, and that's what I said, this is all my own shit. Like what I meant was this is all from my own head. \r\n\r\nI haven't bought, why would I care to map to KSATs? I could care less about that. But if that's what you're interested in, yeah, drop the Excel in here, bro. Check the box. And then when you ask for learning objectives, you ask for learning objectives mapped to the KSATs. Guess what you're gonna get? \r\n\r\nGuess what you're gonna get? That's right, that's right. Look at this, dude. This is what it's going to make for this scenario. I need a DC, I need a seam, I need a file share, I need two workstations, a firewall, and the AI will help me build this whole network. \r\n\r\nYes, dude, bare bones. \r\n\r\nYeah, yes, actually, actually, yes, actually, yes. \r\n\r\nBut also another thi"
  },
  {
    "id": "report_source",
    "chunk": "tions, a firewall, and the AI will help me build this whole network. \r\n\r\nYes, dude, bare bones. \r\n\r\nYeah, yes, actually, actually, yes, actually, yes. \r\n\r\nBut also another thing is a lot of that is a lot of heavy lifting that we might not need to do, but also a lot of it, the AI knows Ansible, actually, and can just start helping make those as well. \r\n\r\nSo my, yeah, yes, the Ansible rules, that's right, yep, I know. Scenario index, so as these scenarios grow, Bunny rabbit on the pancake bunny rabbit with a pancake on its head man. I don't know what what do you people need to see? So here's a bunny rabbit with a pancake on its head. Um, I think I think I think over time I think it's more people. I just hope you know sooner rather than later Oh, I already sent it to you. You already have it. \r\n\r\nYou already have it. That's right. Yeah, basically, yeah, so That's right, that's right. This is the skill of the future. That's another thing I didn't say to you. Everyone, so that billion person workforce, this is what I'm trying to say. \r\n\r\nThis is what I was trying to put in perspective. I got it now, I remember. This is the secondary skill set that everyone is gonna have, data curation. Because if you're a radiologist, if you're a hairstylist, if you're XYZ, it's about data labeling, data annotation. \r\n\r\nA reporter, a news reporter, or a stock analyst, or an accountant, it doesn't matter. \r\n\r\nAll of them will have their own AI that Just like you said, it's my brain out, right? \r\n\r\nEveryone's gonna do the same thing. \r\n\r\nIt's too valuable not to. You give everyone a chance. and then what when one person doesn't give a rat's ass about them they're just gonna what they're gonna accumulate government doesn't care about it they're "
  },
  {
    "id": "report_source",
    "chunk": "everyone a chance. and then what when one person doesn't give a rat's ass about them they're just gonna what they're gonna accumulate government doesn't care about it they're gonna see someone oh look someone made a baking app for their bakery I have a bakery I have credits I never spent my credits oh I wonder what GPT -7 can do now with my credits ah strategically saving and you know this is They're appreciating assets. Like, there's a reason to save them and then there's a reason to use them strategically. Anyway, so yeah. That's the billion person workforce. \r\n\r\nHuh? Let's see. I think I just clicked here, right? \r\n\r\nShare, copy. \r\n\r\nYeah, there it is. Yeah, so version 1 .10 is the final version of the one before I started integrating local. \r\n\r\nThis is probably the one you were saying you couldn't download before. \r\n\r\nYeah, because I can't just click and drag it. It's too big for Discord. I can email it to you. Oh, someone messaged me on my, literally my catalyst AI, probably a spammer. What the hell, dude? What are the odds? \r\n\r\nNo one messaged me over there. Okay, one hour ago. Literally, what are the odds, dude? Talking about it one hour ago. Anyway, who cares? \r\n\r\nSeriously, what the fuck? \r\n\r\nI haven't touched that website for three fucking years, dude. Okay. Yeah, me too, man. Yeah, I agree, and it's just gonna get better, you know? Oh, that's another thing I wanna do, is I bet you, I bet you that's gonna be a real takeoff. is the moments people start using AI to make VR, because it's extremely difficult to make VR. \r\n\r\nAI, AI, AI's gonna make it easy. And we're gonna have it once, yeah, so. um, I just sent, yeah. So see if that link works. Yeah. Cause it still did turn it into a, um, Google drive link anyway, "
  },
  {
    "id": "report_source",
    "chunk": "make it easy. And we're gonna have it once, yeah, so. um, I just sent, yeah. So see if that link works. Yeah. Cause it still did turn it into a, um, Google drive link anyway, but, um, maybe it'll still work this way. \r\n\r\nYes, it is exactly that. Yeah. Just drop me a message on discord. Yeah. When you're dicking and dicking around with it and then I'll just, you know, I can look over your shoulder. \r\n\r\nSo that's sort of the, uh, cognitive apprenticeship model. \r\n\r\nUh, let's actually, yeah, yeah. Basically it's, uh, I remember what it is. I remember this. Yes, yes, yes, yes. Modeling. coaching scaffolding and fading. So basically I do it, I'll show it to you and then you do it and I look over your shoulder while you do it. \r\n\r\nThat's basically kind of this little, I forget the name of it. It starts with a D or something. Oh no, it was a car, it was a race car. It was some race car. I don't know if I'll find it. Anyway, I'll let you go, man. \r\n\r\nYeah, yeah. No, it's fine. This is the only thing that's really important. You're not taking away my weekend. The more people that I empower turn into citizen architects, it's one more out of the 330 million. Yeah, no, for real, for real. \r\n\r\nAbsolutely. That's where my headspace is at, so. Yeah, so you pick a project. You pick a project, something you're just passionate about, and ideally something you have intimate knowledge with. My friend said, you know, he's got a 60 -year -old aunt, she's an accountant, accountant all her life, he lives in Romania, he's saying, what is she going to do with the rest of her life? I said, make an accounting game, because it's something that she knows internally, she can go, what that allows you to do is you can go deep in, like many cycles deep, "
  },
  {
    "id": "report_source",
    "chunk": " life? I said, make an accounting game, because it's something that she knows internally, she can go, what that allows you to do is you can go deep in, like many cycles deep, and you can, without hallucinations. \r\n\r\nBecause you can gut check those hallucinations the moment it shows up because you know it counting like the back of your hand. So you're gaining, that's the skill set. You're gaining the gut check ability so that the moment the AI is going off, you're gonna see, you're gonna be like, why? Then you're gonna learn the true lessons. So that puts you in a position to gut check, by coding everything, having that intimate knowledge, picking a project that you have intimate knowledge in. And then you just go deep, go deep, go deep, and you learn all the side skills, the secondary skill set. \r\n\r\nYep. that's right. That's feedback, that's right. So that's another part of the equation is in order to, because you don't know if it's a hallucination without the accurate feedback. And if you're an expert, you can give accurate feedback, like that's the wrong cybersecurity solution. That's expert feedback. \r\n\r\nBut if you aren't an expert, you cannot give expert feedback. \r\n\r\nSo then you can't go deep with the AI. \r\n\r\nBut then if you get a code error, that's expert feedback that you don't have to create. It's created by the system. The code error, that's right. \r\n\r\nAnd you take that and you give that, that's expert feedback of the code that the AI just wrote. \r\n\r\nThere's your feedback loop. There's your feedback loop, yeah. And because that's your feedback loop and you're witnessing it, you learn coding because you're in that feedback loop. \r\n\r\nAnd so, yes, yes, yes. \r\n\r\nIt's already here. This is Star Trek level status. It'"
  },
  {
    "id": "report_source",
    "chunk": "ack loop and you're witnessing it, you learn coding because you're in that feedback loop. \r\n\r\nAnd so, yes, yes, yes. \r\n\r\nIt's already here. This is Star Trek level status. It's just not evenly distributed. \r\n\r\nAnd that's again, that's why I'm actually so gung -ho, dude. Why, David? What is your motivation? What's your selfishness? I want to be Star Trek, bro. I want to be Captain Kirk. \r\n\r\nI want to travel through space. And we're not going to fucking do it if we're fucking shooting each other for fucking Nikes, bro. It's so stupid. Look at the skills. Look at the tools we have, dude. We could solve every problem. \r\n\r\nWe could explore this universe. Like, get your shit together. I want to do it in my lifetime. So there's my selfishness. I'm selfish as fuck, dude. I want to see it myself. \r\n\r\nAlright? So there we go. Yeah. Yeah. Yeah, yeah. Yeah, fold space, man. \r\n\r\nYeah, fold that shit. Yeah, let's go. Yeah, man. All right. Now, all right. Anytime. \r\n\r\nI'm glad we got to connect like this. Yeah. Cool, man. All right. Have a good night. Bye.\r\n\r\n\r\nTranscribed with Cockatoo\r\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/01-V2V Academy Content Research Plan.md\">\n\n\n# **From Instruction to Environment: A Comprehensive Analysis and Curriculum Blueprint for Context Engineering**\n\n## **Part I: The Foundational Shift: Defining the New Discipline**\n\nThe advent of large language models (LLMs) has catalyzed a rapid evolution in the practices of human-AI interaction and application development. Initially, the dominant skill was perceived to be \"prompt engineering\"a craft focused on eliciting desired behaviors through carefully worded instructions. However, as the ambition of AI applications has grown from simple, sing"
  },
  {
    "id": "report_source",
    "chunk": "ompt engineering\"a craft focused on eliciting desired behaviors through carefully worded instructions. However, as the ambition of AI applications has grown from simple, single-turn tasks to complex, multi-step, and stateful workflows, the limitations of this linguistic-centric approach have become increasingly apparent. A new, more robust paradigm has emerged from the demands of production-grade systems: **Context Engineering**. This report provides a comprehensive analysis of this paradigm shift, establishing context engineering not as a mere rebranding of old techniques, but as a formal, systematic engineering discipline. It deconstructs the core methodologies, architectural patterns, and practical workflows that define this field and concludes with a detailed blueprint for a curriculum module designed to cultivate expertise in this critical domain.\n\n### **Beyond the Prompt: The Evolution from Linguistic Tuning to Systems Thinking**\n\nThe transition from prompt engineering to context engineering represents a fundamental shift in perspectivefrom the art of crafting a single instruction to the science of designing an entire informational environment.1 This evolution mirrors the maturation of the AI field itself, moving from novel demonstrations of capability to the development of reliable, scalable, and enterprise-ready systems.  \nPrompt engineering is best understood as a practice of **linguistic tuning**. It involves the iterative process of adjusting the phrasing, structure, and content of a single input to an LLM to guide its output for a specific, immediate task.1 Well-established practices include techniques such as role assignment (\"You are a professional translator\"), the imposition of formatting and output con"
  },
  {
    "id": "report_source",
    "chunk": "ific, immediate task.1 Well-established practices include techniques such as role assignment (\"You are a professional translator\"), the imposition of formatting and output constraints (\"Provide the answer in JSON format\"), the use of step-wise reasoning patterns like Chain-of-Thought, and the inclusion of few-shot examples to illustrate the desired input-output transformation.1 While powerful for localized tasks, this approach is fundamentally a single-turn optimization. Its primary focus is on \"what you say\" to the model in a given moment.2 The core limitation of this paradigm is its inherent brittleness; small, often imperceptible variations in wording or example placement can lead to significant and unpredictable changes in output quality and reliability.1 This sensitivity, coupled with a general lack of persistence and generalization across tasks, makes systems built solely on prompt engineering difficult to scale and maintain in production environments.2 This has led to a perception in some technical communities that prompt engineering is a superficial skill, with some dismissing it as a \"cash grab manufactured by non-technical people\".4  \nIn stark contrast, context engineering adopts a holistic, **systems-thinking** perspective. It is formally defined as \"the systematic discipline of designing, organizing, orchestrating, and optimizing the complete informational payload provided to a Large Language Model at inference time\".2 This definition moves beyond the user's immediate query to encompass the entire information ecosystem that an AI system requires to perform complex tasks accurately and consistently. As articulated by AI researcher Andrej Karpathy, it is \"the delicate art and science of filling the context wind"
  },
  {
    "id": "report_source",
    "chunk": "requires to perform complex tasks accurately and consistently. As articulated by AI researcher Andrej Karpathy, it is \"the delicate art and science of filling the context window with just the right information for the next step\".2 This payload is not a static string of text but a dynamically assembled composite of multiple components: system-level instructions, user dialogue history, memory stores, real-time data, retrieved documents from external knowledge bases, and definitions of available tools.1  \nThis terminological and conceptual shift is not accidental; it represents a deliberate professionalization of the field. The initial adoption of generative AI was characterized by the accessibility of prompt engineering, which was often framed as a \"magic\" skill. However, as organizations began to build industrial-strength applications, the fragility of this approach became a significant bottleneck.2 The emergence of \"context engineering\" signals a maturation, borrowing its lexicon directly from established software engineering disciplines\"systems,\" \"architecture,\" \"pipelines,\" \"orchestration,\" and \"optimization\".1 This strategic reframing aligns AI development with rigorous, proven engineering practices, making it more integrable for enterprise teams that were often skeptical of the less formal, more artisanal nature of prompt engineering.5 Anthropic, a leading model provider, explicitly views context engineering as the \"natural progression of prompt engineering,\" essential for building the more capable, multi-turn agents that are now in demand.9 It is the shift from writing a single command to designing the entire recipea playbook that enables reliable, multi-turn performance.11\n\n| Dimension | Prompt Engineering | Cont"
  },
  {
    "id": "report_source",
    "chunk": " is the shift from writing a single command to designing the entire recipea playbook that enables reliable, multi-turn performance.11\n\n| Dimension | Prompt Engineering | Context Engineering |\n| :---- | :---- | :---- |\n| **Scope** | Single-turn, localized interaction. | Multi-turn, session-long, and persistent interactions. |\n| **Core Skillset** | Linguistic creativity, natural language expression, instruction design. | Systems architecture, data engineering, information retrieval, process design. |\n| **Time Horizon** | Immediate, stateless. | Persistent, stateful. |\n| **Key Artifacts** | A single, well-crafted text prompt. | An automated pipeline integrating memory, retrieval (RAG), and tools. |\n| **Analogy** | Finding the perfect \"magic word\".11 | Writing the entire \"recipe\" or \"playbook\".11 |\n| **Primary Goal** | Elicit a specific, high-quality response to a single query. | Create a reliable, consistent, and scalable task environment for the AI. |\n| **Failure Mode** | Brittle, inconsistent, or incorrect output due to phrasing. | Context rot, hallucination, or system failure due to poor data management. |\n\n### **The Anatomy of the Context Window: A Finite and Strategic Resource**\n\nAt the heart of context engineering lies a fundamental technical and economic constraint: the LLM's context window. The context window refers to the set of tokens an LLM can \"see\" and consider at any given time when generating a response.9 It is the model's working memory. The engineering challenge is to optimize the utility of the tokens within this finite space to consistently achieve a desired outcome.9 This requires a shift in thinking, from simply providing information to strategically managing a scarce resource.  \nThe \"complete informat"
  },
  {
    "id": "report_source",
    "chunk": "istently achieve a desired outcome.9 This requires a shift in thinking, from simply providing information to strategically managing a scarce resource.  \nThe \"complete informational payload\" that a context engineer manages is a composite of several distinct elements, each serving a specific purpose 1:\n\n* **System Instructions:** High-level directives that define the AI's role, persona, operational rules, and behavioral guardrails.  \n* **User Dialogue History:** The record of the current conversation, providing immediate short-term memory.  \n* **Real-time Data:** Dynamic information such as the current date, time, or user location.  \n* **Retrieved Documents:** Chunks of text sourced from external knowledge bases via Retrieval-Augmented Generation (RAG) to ground the model in facts.  \n* **Tool Definitions:** Descriptions of external functions or APIs that the model can call to interact with the outside world.  \n* **Structured Output Schemas:** Predefined formats (e.g., JSON) that constrain the model's output for reliable parsing by downstream systems.\n\nThe critical constraint is that this context window is a finite resource with diminishing marginal returns. LLMs, like humans, possess a limited \"attention budget\" that they draw upon when parsing large volumes of information.9 Each new token introduced into the context window depletes this budget by some amount. This leads to a well-documented phenomenon known as **context rot**: as the number of tokens increases, the model's ability to accurately recall and utilize specific pieces of information from within that context decreases.9 This is often referred to as the \"lost-in-the-middle\" problem, where information placed at the beginning or end of a long context is recalled mo"
  },
  {
    "id": "report_source",
    "chunk": "ithin that context decreases.9 This is often referred to as the \"lost-in-the-middle\" problem, where information placed at the beginning or end of a long context is recalled more reliably than information buried in the middle.14 A study by Microsoft and Salesforce quantified this degradation, demonstrating that when information was sharded across multiple conversational turns instead of being provided at once, model performance dropped by an average of 39%.7  \nThis performance degradation establishes context engineering as a fundamental optimization problem with economic dimensions. Every token included in the context window incurs a cost across three axes:\n\n1. **Financial Cost:** Most proprietary LLM APIs are priced on a per-token basis for both input and output, making larger contexts directly more expensive.14  \n2. **Latency Cost:** Processing a larger number of tokens takes more computational time, increasing the latency of the response.14  \n3. **Attention Cost:** As established by the concept of context rot, every token dilutes the model's limited attention, increasing the risk of critical information being overlooked.9\n\nFrom this, a central principle of the discipline emerges: the goal of the context engineer is not to maximize the *amount* of information provided to the model, but to maximize the *signal-to-noise ratio* within a constrained token budget. The objective is to find the \"smallest possible set of high-signal tokens that maximize the likelihood of some desired outcome\".10 Every technique within the context engineer's toolkitfrom retrieval and summarization to data structuring and agentic designcan be understood as a method for improving the economic efficiency of the context window.\n\n## **Part II: Core"
  },
  {
    "id": "report_source",
    "chunk": "trieval and summarization to data structuring and agentic designcan be understood as a method for improving the economic efficiency of the context window.\n\n## **Part II: Core Methodologies and Architectural Patterns**\n\nWith the foundational principles established, the focus now shifts to the core technical methodologies and architectural patterns that constitute the practice of context engineering. These are the tools and frameworks used to design, build, and optimize the informational environments in which LLMs operate. They represent the transition from abstract theory to concrete implementation, providing systematic solutions to the challenges of knowledge grounding, state management, and logical reasoning.\n\n### **Retrieval-Augmented Generation (RAG): The Cornerstone of External Knowledge**\n\nRetrieval-Augmented Generation (RAG) is not merely a technique but the foundational architectural pattern for modern, knowledge-intensive AI applications. It addresses one of the most significant limitations of LLMs: their knowledge is static, limited to the data they were trained on, and can become outdated or contain inaccuracies (hallucinations).15 RAG overcomes this by dynamically augmenting the model's internal, parametric knowledge with external, non-parametric knowledge retrieved from a specified corpus at inference time.16 This process of systematically supplying relevant information is a cornerstone of context engineering.7  \nThe formal introduction of RAG in a 2020 NeurIPS paper by Lewis et al. marked a pivotal moment, demonstrating that combining a pre-trained retriever with a sequence-to-sequence generator could achieve state-of-the-art results on open-domain question answering tasks.16 Since then, the field has evolv"
  },
  {
    "id": "report_source",
    "chunk": "a pre-trained retriever with a sequence-to-sequence generator could achieve state-of-the-art results on open-domain question answering tasks.16 Since then, the field has evolved rapidly, moving beyond simple document retrieval to encompass a range of sophisticated architectures, including modular, agentic, and graph-enhanced RAG systems.8 An advanced RAG system is best understood as a complete data lifecycle with two primary phases:\n\n1. **The Ingestion Phase:** This offline process prepares the external knowledge source for efficient retrieval. It involves a series of data engineering tasks, including content preprocessing (standardizing formats, handling special characters), developing a sophisticated chunking strategy (optimizing chunk size, using overlapping windows, or employing advanced methods like \"Small2Big\"), and designing an effective indexing architecture (using hierarchical, specialized graph-based, or hybrid indexes to store the chunk embeddings).18  \n2. **The Inference Phase:** This online process occurs in real-time when a user query is received. It begins with query preprocessing, where the user's input may be rewritten for clarity (e.g., using Hypothetical Document Embeddings or HyDE), broken into subqueries, or routed to the most appropriate index.18 After the initial retrieval of relevant chunks, a post-retrieval processing step is often applied. This can include re-ranking the chunks to place the most relevant information at the beginning and end of the context (to combat the \"lost-in-the-middle\" problem) and compressing the retrieved information to fit within the token budget before it is finally passed to the LLM for generation.18\n\nThe rise of RAG signifies a crucial shift in the landscape of applie"
  },
  {
    "id": "report_source",
    "chunk": "ieved information to fit within the token budget before it is finally passed to the LLM for generation.18\n\nThe rise of RAG signifies a crucial shift in the landscape of applied AI. As powerful base models from providers like OpenAI, Anthropic, and Google become increasingly commoditized and accessible alongside high-quality open-source alternatives, the primary source of competitive advantage is no longer the proprietary model itself.2 An LLM, regardless of its parameter count, cannot solve specific, high-value enterprise problems without access to an organization's internal knowledge bases, real-time databases, user histories, and business rules.2 While fine-tuning can imbue a model with domain-specific knowledge, it is an expensive and static process that cannot account for information that changes in real-time.7 RAG provides the architectural solution, enabling the \"just-in-time\" injection of this dynamic, proprietary, and highly valuable information into the context window.7 Consequently, the most defensible and valuable component of a modern enterprise AI application is often not the LLM but the sophisticated RAG pipelinethe context engineering systemthat sources, processes, and feeds it information.\n\n### **Managing Long-Horizon Tasks: Strategies for Coherence and Memory**\n\nWhile RAG addresses the challenge of external knowledge, another critical domain of context engineering focuses on internal state: managing memory and maintaining coherence over long-horizon tasks that span multiple conversational turns and may exceed the capacity of a single context window. These techniques are essential for building stateful applications like sophisticated chatbots, coding assistants, and autonomous agents.  \nA foundational c"
  },
  {
    "id": "report_source",
    "chunk": "gle context window. These techniques are essential for building stateful applications like sophisticated chatbots, coding assistants, and autonomous agents.  \nA foundational concept in this domain is the use of **memory hierarchies**, which distinguish between different types of memory based on their persistence and scope 1:\n\n* **Short-Term Memory:** This typically refers to the immediate dialogue history stored within the context window. It is managed using simple strategies like a \"conversational buffer,\" which keeps the last N turns of the conversation. As the conversation grows, older messages are truncated to make space for new ones.14  \n* **Long-Term Memory:** This provides persistence across sessions, allowing an application to remember user preferences or past interactions. It is almost always implemented using an external storage system, typically a vector database, where summaries of past interactions or key facts can be stored and retrieved semantically.2\n\nTo manage the finite context window during a single, long-running task, a suite of **context window optimization techniques** has been developed. These move beyond simple truncation to more intelligently process and condense information 14:\n\n* **Summarization and Compression:** This involves using an LLM (often a smaller, faster model) to create a concise summary of the conversation history or large retrieved documents. This summary then replaces the original, longer text in the context window, preserving key information while significantly reducing the token count.1  \n* **Chunking Patterns for Large Documents:** For processing single documents that are larger than the context window, several patterns are common. The **Map-Reduce** approach involves summariz"
  },
  {
    "id": "report_source",
    "chunk": "erns for Large Documents:** For processing single documents that are larger than the context window, several patterns are common. The **Map-Reduce** approach involves summarizing each chunk independently and then summarizing the summaries. The **Refine** approach iteratively builds a summary, passing the summary of the first chunk along with the second chunk to be refined, and so on. The **Map-Rerank** approach processes each chunk to see how relevant it is to a query and then focuses only on the highest-ranked chunks for the final answer generation.19\n\nFor building truly autonomous agents capable of complex, multi-day tasks, even more advanced strategies are required. Research from Anthropic outlines a set of powerful techniques for maintaining long-term agentic coherence 10:\n\n* **Compaction:** This is an intelligent form of summarization where the agent periodically pauses to distill the conversation history, preserving critical details like architectural decisions and unresolved bugs while discarding redundant information like raw tool outputs. The art of compaction lies in selecting what to keep versus what to discard.10  \n* **Structured Note-Taking:** This technique involves giving the agent a tool to write notes to an external \"scratchpad\" or memory store (e.g., a text file or database). The agent can then offload its working memory, tracking progress, dependencies, and key findings with minimal token overhead. This persistent memory can be retrieved and loaded back into the context window as needed.10  \n* **Sub-agent Architectures:** For highly complex tasks, a single agent can become overwhelmed. This architecture involves a main \"orchestrator\" agent that manages a high-level plan and delegates focused sub-tasks "
  },
  {
    "id": "report_source",
    "chunk": "ly complex tasks, a single agent can become overwhelmed. This architecture involves a main \"orchestrator\" agent that manages a high-level plan and delegates focused sub-tasks to specialized sub-agents. Each sub-agent operates with its own clean, focused context window, performs its task (e.g., deep research or complex tool use), and then returns a condensed, distilled summary of its work to the main agent. This creates a clear separation of concerns and prevents the main agent's context from being cluttered with low-level details.10\n\nThese advanced strategies reveal a profound principle: the most effective AI agents are being designed to mimic human cognitive offloading. Humans do not hold all information for a complex project in their working memory. Instead, we use external toolsnotebooks, file systems, calendars, and delegation to colleaguesto manage complexity.10 Structured note-taking is the agent's notebook; a sub-agent architecture is its method of delegation. This indicates that the path toward more capable, long-horizon agents is not simply a brute-force race to build ever-larger context windows.14 Rather, it is about engineering intelligent systems that can effectively manage and interact with externalized knowledge, overcoming the inherent limitations of their \"working memory\" through well-designed architecture.\n\n### **The Power of Structure: Imposing Order for Enhanced Reasoning**\n\nThe final core methodology of context engineering recognizes that the *format* of information within the context window is as important as its content. LLMs are not just processing a \"bag of words\"; they are sensitive to the structure and organization of the tokens they receive. By imposing explicit, machine-readable structure on"
  },
  {
    "id": "report_source",
    "chunk": " are not just processing a \"bag of words\"; they are sensitive to the structure and organization of the tokens they receive. By imposing explicit, machine-readable structure on the context, engineers can significantly enhance a model's ability to parse, comprehend, and reason about the provided information, leading to more reliable and predictable behavior.  \nThis principle applies at multiple levels of the context payload:\n\n* **Structuring Input Prompts:** When constructing a complex prompt that includes instructions, examples, and retrieved data, using structural separators can dramatically improve the model's ability to distinguish between different parts of the context. Techniques like wrapping distinct sections in XML tags (e.g., \\<instructions\\>, \\<document\\>) or using Markdown headers (\\#\\# Instructions, \\#\\# Retrieved Data) provide clear delimiters that guide the model's attention and reduce ambiguity.10 While the exact formatting may become less critical as models improve, it remains a best practice for ensuring clarity.  \n* **Enforcing Structured Outputs:** For applications where an LLM's output must be consumed by another piece of software (e.g., a tool-using agent that needs to generate API call parameters), enforcing a structured output format like JSON is essential for reliability.1 Approaches to achieve this range from simple instructions in the prompt to more advanced techniques like constrained decoding or using a fine-tuned, model-agnostic post-processing layer like that proposed in the SLOT (Structured LLM Output Transformer) paper, which transforms unstructured outputs into a precise, predefined schema.21  \n* **Context Structurization for Comprehension:** The benefit of structure extends beyond simple "
  },
  {
    "id": "report_source",
    "chunk": ", which transforms unstructured outputs into a precise, predefined schema.21  \n* **Context Structurization for Comprehension:** The benefit of structure extends beyond simple parsing to deeper comprehension. Research has shown that transforming a flat block of plain text into a hierarchical structure (e.g., a document organized by Scope \\-\\> Aspect \\-\\> Description) can help LLMs better grasp intricate and long-form contexts.22 This process is believed to mimic human cognitive processes, where we naturally organize information into structured knowledge trees to facilitate understanding and retrieval.22  \n* **Training on Structured Data:** The impact of structure is so profound that it can be leveraged during the model training process itself. The SPLiCe (Structured Packing for Long Context) method demonstrates that fine-tuning a model on training examples that are intentionally structured to increase semantic interdependencefor instance, by collating mutually relevant documents into a single training contextleads to significant improvements in the model's ability to utilize long contexts effectively during inference.23\n\nThese techniques collectively suggest that a key role of the context engineer is to act as an API designer for the LLM. In traditional software engineering, developers rely on strongly-typed schemas (like OpenAPI specifications) to create reliable, predictable contracts for communication between services. An LLM, as a non-deterministic, natural-language-based component, is inherently unreliable from a traditional software perspective.5 Imposing structure on its input and output is an attempt to create a machine-readable \"contract\" that reduces ambiguity, improves parseability, and makes the model's beha"
  },
  {
    "id": "report_source",
    "chunk": "ve.5 Imposing structure on its input and output is an attempt to create a machine-readable \"contract\" that reduces ambiguity, improves parseability, and makes the model's behavior more predictable and integrable. The context engineer's job is not just to provide raw information but to act as a data architect, structuring that information in a way that the LLM can most effectively consume and act upon. While natural language is the medium, structured data is often the most effective message.\n\n## **Part III: Context Engineering in Practice: From Systems to Agents**\n\nThis section transitions from methodological principles to their practical application, providing actionable blueprints and case studies for building real-world systems. It demonstrates how the core concepts of RAG, memory management, and data structuring are synthesized to create production-grade applications and enable advanced modes of human-AI collaboration, moving from the theoretical \"what\" to the operational \"how.\"\n\n### **Architecting Production-Grade RAG Systems: A Lifecycle Approach**\n\nBuilding a robust RAG system that performs reliably in a production environment is a complex engineering endeavor that extends far beyond a simple \"retrieve-then-prompt\" script. A comprehensive, lifecycle approach is required, treating the system as a sophisticated data processing pipeline. This lifecycle can be broken down into three distinct phases: Ingestion, Inference, and Evaluation.18  \nPhase 1: The Ingestion Pipeline  \nThis is the foundational, offline phase where the external knowledge corpus is prepared for retrieval. The quality of this phase directly determines the quality of the entire system. Best practices include:\n\n* **Content Preprocessing and Extraction:"
  },
  {
    "id": "report_source",
    "chunk": "s is prepared for retrieval. The quality of this phase directly determines the quality of the entire system. Best practices include:\n\n* **Content Preprocessing and Extraction:** This initial step ensures data quality and consistency. It involves standardizing text formats, handling special characters and tables, extracting valuable metadata (e.g., source, creation date), and tracking content versions.18  \n* **Chunking Strategy:** This is one of the most critical decisions. It involves more than just splitting documents by a fixed token count. Advanced strategies include optimizing chunk size based on content type, using overlapping chunks to preserve context across boundaries, and implementing hierarchical approaches like \"Small2Big,\" where small, distinct sentences are retrieved first, but the system then expands the context to include the surrounding paragraph to provide the LLM with richer information.18  \n* **Indexing and Organization:** The processed chunks are converted into vector embeddings and stored in a vector database. The organization of these indexes is crucial for performance. Techniques include using **hierarchical indexes** (a top-level summary index for coarse filtering followed by a detailed index), **specialized indexes** (e.g., graph-based indexes for data with inherent relationships), and **hybrid indexes** that combine multiple methods.18  \n* **Alignment Optimization:** To improve retrieval relevance, a powerful technique is to generate a set of hypothetical questions that each chunk is well-suited to answer. These question-chunk pairs can then be used to fine-tune the retrieval model or to perform semantic search against the user's query, effectively creating labels that guide the retrieval algori"
  },
  {
    "id": "report_source",
    "chunk": "unk pairs can then be used to fine-tune the retrieval model or to perform semantic search against the user's query, effectively creating labels that guide the retrieval algorithm.18  \n* **Update Strategy:** Production knowledge bases are rarely static. A robust update strategy is needed to keep the vector database current. This can range from periodic batch updates to real-time, trigger-based re-indexing of only the changed content (selective re-indexing).18\n\nPhase 2: The Inference Pipeline  \nThis is the real-time pipeline that executes when a user submits a query. It is a sequence of orchestrated steps designed to produce the most accurate and relevant response:\n\n* **Query Preprocessing:** The raw user query is refined before retrieval. This can involve a **policy check** to filter for harmful content, or **query rewriting** to expand acronyms, fix typos, or rephrase the question using techniques like step-back prompting. An advanced method is **Hypothetical Document Embeddings (HyDE)**, where an LLM first generates a hypothetical answer to the query, and the embedding of this answer is used for the retrieval search, often yielding more relevant results.18  \n* **Subquery Decomposition and Routing:** For complex questions, the system may break the query into multiple subqueries. A **query router** can then analyze each subquery and direct it to the most appropriate data source or index (e.g., a vector index for semantic questions, a SQL database for structured data queries).18  \n* **Post-Retrieval Processing:** After an initial set of chunks is retrieved, they are further processed before being sent to the LLM. This includes **filtering** out irrelevant results, **re-ranking** the chunks to place the most important infor"
  },
  {
    "id": "report_source",
    "chunk": "ieved, they are further processed before being sent to the LLM. This includes **filtering** out irrelevant results, **re-ranking** the chunks to place the most important information at the top and bottom of the prompt to mitigate the \"lost-in-the-middle\" effect, and **prompt compression** to summarize and combine the chunks into a token-efficient format.18\n\nPhase 3: The Evaluation Pipeline  \nContinuous evaluation is critical for maintaining and improving a production RAG system. This goes beyond simple accuracy metrics:\n\n* **User Feedback and Assessment:** Implementing mechanisms to capture user feedback (e.g., thumbs up/down) is crucial. An **assessment pipeline** can then analyze this feedback, perform root cause analysis on poor responses, and identify gaps in the knowledge corpus.18  \n* **Golden Dataset:** A curated set of representative questions with validated, \"golden\" answers should be maintained. This dataset serves as a regression test suite to ensure that system updates do not degrade performance on key queries.6  \n* **Harms Modeling and Red-Teaming:** A proactive approach to safety involves identifying potential risks and harms (e.g., providing dangerous advice, leaking private information) and systematically testing the system's safeguards. **Red-teaming**, where testers actively try to break the system's safety rules (a practice known as \"jailbreaking\"), is an essential part of this process.18\n\nThe exhaustive detail involved in these three phases underscores a critical reality: a production-grade RAG system is composed of approximately 90% data engineering and 10% LLM prompting. The majority of the complexity, potential failure modes, and optimization effort lies within the data ingestion and processing pip"
  },
  {
    "id": "report_source",
    "chunk": " 90% data engineering and 10% LLM prompting. The majority of the complexity, potential failure modes, and optimization effort lies within the data ingestion and processing pipelines. Issues like poor chunking, stale indexes, or irrelevant retrieval cannot be fixed by simply tweaking the final prompt sent to the LLM. Therefore, building a successful RAG system requires a data-centric, systems-thinking approach, where the LLM is treated as the final, powerful component in a much larger and more intricate data processing machine.\n\n### **Enabling Agentic Workflows: Context as the Engine for Autonomy**\n\nThe principles of context engineering are the fundamental enablers of the next frontier in AI: autonomous agents. Agentic software development is a paradigm where autonomous or semi-autonomous AI agents work alongside human developers, undertaking complex tasks throughout the software development lifecycle (SDLC), from planning and coding to testing and deployment.24 For an agent to operate effectively, it must be able to interpret high-level goals, decompose them into executable steps, utilize tools, and maintain context over long periodsall of which are core challenges of context engineering.26  \nThe recent industry trend away from unstructured \"vibe coding\"an intuitive, free-form process of prompting an AI to generate large amounts of codetowards more structured, agentic workflows is a direct consequence of the need for reliable context.27 While vibe coding is useful for rapid prototyping, it breaks down for complex, real-world projects because intuition does not scale; structure does.28 Context engineering provides this necessary structure. A key practice emerging in this space is **spec-driven development**, where the "
  },
  {
    "id": "report_source",
    "chunk": "ition does not scale; structure does.28 Context engineering provides this necessary structure. A key practice emerging in this space is **spec-driven development**, where the human's primary role is to create high-level specification documents (e.g., a REQUIREMENTS.md file outlining product goals and functional requirements) that serve as the grounding context and source of truth for the AI agent's work.29  \nThis evolution is fundamentally changing the nature of the human-AI interface for software development. The \"prompt\" is no longer a transient instruction in a chat window; it is expanding to become the entire **project directory**. The locus of interaction is shifting to a collection of structured, persistent files that collectively define the agent's working environment and task. Developers are now creating files like CLAUDE.md or GEMINI.md at the root of their projects to provide the AI with a high-level overview, architectural constraints, and coding conventions.29 This file, combined with formal specification documents and the source code itself, forms a rich, multi-faceted context that the agent can refer to throughout its execution.  \nIn this model, the human's role shifts from that of a micro-manager, providing step-by-step instructions, to that of an architect, who carefully prepares the blueprints and engineers the environment. The collaboration becomes asynchronous, mediated by a shared, structured file system. The human engineers the context; the AI executes within it. This is a more scalable and robust model for collaboration, leveraging the strengths of both parties: the human's capacity for high-level reasoning, planning, and goal-setting, and the AI's ability to execute well-defined tasks at high speed"
  },
  {
    "id": "report_source",
    "chunk": "ging the strengths of both parties: the human's capacity for high-level reasoning, planning, and goal-setting, and the AI's ability to execute well-defined tasks at high speed.\n\n### **Human-AI Collaboration as Cognitive Apprenticeship**\n\nThe most powerful mental model for understanding and guiding this new mode of collaboration is that of **Cognitive Apprenticeship**. This pedagogical framework, traditionally used to describe how a human expert (a master) guides a novice (an apprentice), provides a rich and effective lens through which to view the relationship between a human engineer and an AI agent.31 In this model, the human is the expert mentor, and the AI is the tireless apprentice.  \nThe core of cognitive apprenticeship is making the expert's implicit thought processes explicit and providing the apprentice with scaffolding to support their learning and performance. Context engineering is the practical mechanism for implementing this model in a human-AI context. The \"curriculum\" for the AI apprentice is the engineered context provided by the human mentor.\n\n* **Making Thinking Visible:** The expert human's plan, domain knowledge, constraints, and goals for a task are encoded into the context window. A well-written system prompt or a PROJECT\\_CONTEXT.md file is the equivalent of the master explaining the high-level strategy to the apprentice.29  \n* **Providing Scaffolding:** The various techniques of context engineering are forms of scaffolding that guide and support the AI apprentice. Providing few-shot examples is akin to demonstrating a technique. Curating high-quality documents for a RAG system is like giving the apprentice access to a well-organized library. Defining a clear set of tools is like providing a well-"
  },
  {
    "id": "report_source",
    "chunk": "que. Curating high-quality documents for a RAG system is like giving the apprentice access to a well-organized library. Defining a clear set of tools is like providing a well-maintained workshop.\n\nWhen a developer meticulously engineers the context for an AI agent, they are not merely \"using a tool\"; they are actively teaching, mentoring, and guiding an apprentice for a specific, complex task. This reframes the interaction from one of command-and-control to one of collaboration and empowerment. The Cognitive-AI Synergy Framework (CASF) further formalizes this by suggesting that the level of AI integration and autonomy can be aligned with the \"cognitive development stage\" of the task or the user, ranging from using the AI for simple editing assistance to deploying it as a full co-pilot.32 This model provides a powerful, human-centric vision for the future of work, where the goal is not to replace human expertise but to augment and scale it by leveraging AI as a capable cognitive partner.\n\n## **Part IV: Blueprint for the V2V \"Context Engineering\" Module**\n\nThis final section translates the preceding analysis into a direct, actionable blueprint for a new module within the \"Vibecoding to Virtuosity\" (V2V) curriculum. It outlines a structured learning path, complete with objectives, lesson plans, and practical exercises, designed to equip learners with the skills and mental models necessary to excel in the discipline of context engineering.\n\n### **Proposed Curriculum Structure and Learning Objectives**\n\n**Module Title:** From Prompting to Partnership: Mastering Context Engineering  \n**Overall Objective:** Upon completion of this module, students will be able to design, build, and evaluate robust, production-grade AI systems b"
  },
  {
    "id": "report_source",
    "chunk": "stering Context Engineering  \n**Overall Objective:** Upon completion of this module, students will be able to design, build, and evaluate robust, production-grade AI systems by systematically managing the informational context provided to LLMs. They will transition from simple instruction-giving to architecting sophisticated human-AI collaborative workflows, grounded in the principles of systems thinking and the cognitive apprenticeship model.  \n**Proposed Structure:** A 4-week, intensive module.\n\n* **Week 1: Foundations \\- Thinking in Context.** This week establishes the fundamental paradigm shift. Students will learn to identify the limitations of prompt engineering and adopt the systems-thinking mindset of a context engineer, focusing on the context window as a finite, strategic resource.  \n* **Week 2: The RAG Lifecycle \\- Grounding AI in Reality.** This week provides a deep, practical dive into the cornerstone of context engineering: Retrieval-Augmented Generation. Students will learn the end-to-end lifecycle of a production RAG system, from data ingestion to inference and evaluation.  \n* **Week 3: Advanced Context Management \\- Memory, Agents, and Structure.** This week covers the techniques required for building complex, stateful, and long-horizon applications. Students will learn to manage memory, impose structure on inputs and outputs, and design architectures for autonomous agents.  \n* **Week 4: Capstone \\- The AI as Cognitive Apprentice.** This final week synthesizes all the technical skills under a powerful conceptual framework. Students will learn to apply the cognitive apprenticeship model to structure and manage complex, multi-step projects in collaboration with an AI agent.\n\n### **Core Lessons, Key Concept"
  },
  {
    "id": "report_source",
    "chunk": "will learn to apply the cognitive apprenticeship model to structure and manage complex, multi-step projects in collaboration with an AI agent.\n\n### **Core Lessons, Key Concepts, and Illustrative Examples**\n\n**Week 1: Foundations \\- Thinking in Context**\n\n* **Lesson 1.1: The Limits of the Prompt.**  \n  * **Key Concepts:** Brittleness, scalability challenges, the \"magic word\" fallacy, single-turn vs. multi-turn interactions.  \n  * **Illustrative Example:** Students will be given a well-crafted prompt for a text classification task. They will then be tasked with finding edge cases and subtle input variations that cause the prompt to fail, leading to a discussion on why this approach is not robust enough for production systems.1  \n* **Lesson 1.2: The Context Engineer's Mindset.**  \n  * **Key Concepts:** Systems thinking vs. linguistic tuning, the context window as a finite resource, the \"attention budget,\" context rot, and the \"lost-in-the-middle\" problem.  \n  * **Illustrative Example:** A detailed analysis of the Microsoft/Salesforce study on performance degradation in long-context scenarios. Students will calculate the potential cost (latency, financial) of an inefficiently packed context window versus a concise, high-signal one.1\n\n**Week 2: The RAG Lifecycle \\- Grounding AI in Reality**\n\n* **Lesson 2.1: The Ingestion Pipeline: Preparing Knowledge.**  \n  * **Key Concepts:** Content preprocessing, chunking strategies (fixed-size, recursive, Small2Big), vector embeddings, and indexing patterns (hierarchical, hybrid).  \n  * **Illustrative Example:** Students will build a Python script using a library like LlamaIndex or LangChain to ingest a small corpus of mixed-format documents (e.g., markdown, txt), process them using an ad"
  },
  {
    "id": "report_source",
    "chunk": "ents will build a Python script using a library like LlamaIndex or LangChain to ingest a small corpus of mixed-format documents (e.g., markdown, txt), process them using an advanced chunking strategy, and create a local vector index.18  \n* **Lesson 2.2: The Inference Pipeline: Answering with Evidence.**  \n  * **Key Concepts:** Query transformation (HyDE), re-ranking algorithms, and prompt compression techniques.  \n  * **Illustrative Example:** Students will implement a post-retrieval re-ranking step in their RAG pipeline to explicitly move the most relevant retrieved chunks to the beginning and end of the final prompt, and then measure the difference in response quality on a test query.18\n\n**Week 3: Advanced Context Management \\- Memory, Agents, and Structure**\n\n* **Lesson 3.1: Structuring for Success: The API for the LLM.**  \n  * **Key Concepts:** Using XML/Markdown tags for prompt organization, enforcing structured outputs with JSON schemas (e.g., using Pydantic models), and the principles of hierarchical context structurization.  \n  * **Illustrative Example:** Students will refactor a complex, unstructured \"mega-prompt\" into a well-organized, multi-section prompt using XML tags. They will then modify it to require the LLM to output its response in a specific JSON format and validate the output programmatically.10  \n* **Lesson 3.2: Building Agents with Memory and State.**  \n  * **Key Concepts:** Short-term vs. long-term memory, context compaction, structured note-taking (\"scratchpad\"), and the sub-agent architectural pattern.  \n  * **Illustrative Example:** Students will design a simple, multi-step planning agent. The agent must solve a problem that requires intermediate calculations. Students will implement a \"scratch"
  },
  {
    "id": "report_source",
    "chunk": "Example:** Students will design a simple, multi-step planning agent. The agent must solve a problem that requires intermediate calculations. Students will implement a \"scratchpad\" tool that allows the agent to write down its intermediate results, thus preserving its state across multiple LLM calls without cluttering the main context window.10\n\n**Week 4: Capstone \\- The AI as Cognitive Apprentice**\n\n* **Lesson 4.1: The Cognitive Apprenticeship Model.**  \n  * **Key Concepts:** The human as mentor, the AI as apprentice, context as the curriculum, making expert thinking visible, and providing cognitive scaffolding.  \n  * **Illustrative Example:** A lecture synthesizing the theoretical framework, drawing parallels between traditional apprenticeship and the context engineering techniques learned throughout the module. The lesson will analyze case studies of effective human-AI collaboration through this lens.31  \n* **Lesson 4.2: Engineering an Agentic Workflow.**  \n  * **Key Concepts:** Spec-driven development, the role of AGENT.md files, and scaffolding a project directory for optimal AI collaboration.  \n  * **Illustrative Example:** Students will be given a simple software development task. They will structure a mini-project directory, creating a REQUIREMENTS.md file and a CONTEXT.md file designed to provide a coding agent with all the necessary context to begin the task autonomously.28\n\n### **Practical Exercises and Capstone Project Recommendations**\n\n**Weekly Exercises:**\n\n* **Week 1 Exercise: \"Prompt Breaking.\"** Students are given a seemingly \"perfect\" prompt and are challenged to act as adversarial testers, finding inputs that cause it to fail. They must then write a brief analysis explaining why a context-based approach"
  },
  {
    "id": "report_source",
    "chunk": "t\" prompt and are challenged to act as adversarial testers, finding inputs that cause it to fail. They must then write a brief analysis explaining why a context-based approach (e.g., using RAG or a system prompt) would be more robust.  \n* **Week 2 Exercise: \"RAG Pipeline Debugging.\"** Students are provided with a malfunctioning RAG system and a small knowledge base. They must diagnose the root cause of its poor performance, which could be an issue in the ingestion pipeline (e.g., suboptimal chunking) or the inference pipeline (e.g., irrelevant retrieval), and then implement a fix.  \n* **Week 3 Exercise: \"Long-Form Q\\&A Agent.\"** Students must build an agent capable of answering detailed questions about a single document that is significantly larger than the model's context window. This will force them to implement an advanced context management technique, such as the Refine pattern or Structured Note-Taking, to process the document in pieces while maintaining coherence.\n\n**Capstone Project: The AI Apprentice Code Refactor**\n\n* **Objective:** This project synthesizes all module concepts. Students will assume the role of a Senior Software Engineer tasked with mentoring an AI apprentice to refactor a small but poorly written legacy codebase into a clean, modular, and well-documented version.  \n* **Deliverables:**  \n  1. **A PROJECT\\_CONTEXT.md File:** A comprehensive document placed at the root of the repository. This file will serve as the primary \"briefing\" for the AI apprentice, outlining the high-level purpose of the codebase, key architectural principles to follow (e.g., SOLID principles), coding style guidelines, and specific \"do's and don'ts\" for the refactoring process.  \n  2. **A REFACTOR\\_PLAN.md Specification:** "
  },
  {
    "id": "report_source",
    "chunk": "nciples to follow (e.g., SOLID principles), coding style guidelines, and specific \"do's and don'ts\" for the refactoring process.  \n  2. **A REFACTOR\\_PLAN.md Specification:** A detailed, step-by-step plan for the refactoring task. This document will break down the high-level goal into a series of smaller, verifiable sub-tasks (e.g., \"1. Extract the database logic from main.py into a new database.py module. 2\\. Add docstrings to all public functions.\"). This serves as the agent's explicit task list.  \n  3. **A Transcript of the \"Mentoring\" Session:** A log of the prompts and interactions used to guide the AI agent through the refactoring plan. This transcript must demonstrate the application of context engineering principles, such as providing specific code snippets for context, referring the agent back to the specification documents, and correcting its course when it deviates.  \n  4. **A Final Reflection Report:** A short (1-2 page) report where the student analyzes their process through the lens of the Cognitive Apprenticeship model. They will discuss which context engineering strategies (scaffolding techniques) were most and least effective for \"teaching\" the AI apprentice and reflect on how their role shifted from a simple \"prompter\" to a \"mentor\" and \"architect.\"\n\n#### **Works cited**\n\n1. Understanding Prompt Engineering and Context Engineering, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  \n2. Context Engineering: Techniques, Tools, and Implementation \\- iKala, accessed October 15, 2025, [https://ikala.ai/blog/ai-trends/context-engineering-techniques-too"
  },
  {
    "id": "report_source",
    "chunk": "ering)  \n2. Context Engineering: Techniques, Tools, and Implementation \\- iKala, accessed October 15, 2025, [https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/](https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/)  \n3. Master Advanced Prompting Techniques to Optimize LLM Application Performance, accessed October 15, 2025, [https://medium.com/data-science-collective/master-advanced-prompting-techniques-to-optimize-llm-application-performance-a192c60472c5](https://medium.com/data-science-collective/master-advanced-prompting-techniques-to-optimize-llm-application-performance-a192c60472c5)  \n4. Prompt engineering, Context Engineering, Protocol Whatever... It's all Linguistics Programming... : r/OpenAI \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt\\_engineering\\_context\\_engineering\\_protocol/](https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt_engineering_context_engineering_protocol/)  \n5. Context Engineering : r/LocalLLaMA \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1lnldsj/context\\_engineering/](https://www.reddit.com/r/LocalLLaMA/comments/1lnldsj/context_engineering/)  \n6. davidkimai/Context-Engineering: \"Context engineering is ... \\- GitHub, accessed October 15, 2025, [https://github.com/davidkimai/Context-Engineering](https://github.com/davidkimai/Context-Engineering)  \n7. Context Engineering: A Guide With Examples \\- DataCamp, accessed October 15, 2025, [https://www.datacamp.com/blog/context-engineering](https://www.datacamp.com/blog/context-engineering)  \n8. Context Engineering. What are the components that make up | by Cobus Greyling, accessed Octobe"
  },
  {
    "id": "report_source",
    "chunk": "og/context-engineering](https://www.datacamp.com/blog/context-engineering)  \n8. Context Engineering. What are the components that make up | by Cobus Greyling, accessed October 15, 2025, [https://cobusgreyling.medium.com/context-engineering-a34fd80ccc26](https://cobusgreyling.medium.com/context-engineering-a34fd80ccc26)  \n9. Effective Context Engineering for AI Agents Anthropic | PDF | Computer File \\- Scribd, accessed October 15, 2025, [https://www.scribd.com/document/927990263/Effective-Context-Engineering-for-AI-Agents-Anthropic](https://www.scribd.com/document/927990263/Effective-Context-Engineering-for-AI-Agents-Anthropic)  \n10. Effective context engineering for AI agents \\\\ Anthropic, accessed October 15, 2025, [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)  \n11. Beyond the Prompt: AI's Great Shift to Process Design | by Umakshi Sharma | Oct, 2025, accessed October 15, 2025, [https://ai.plainenglish.io/beyond-the-prompt-ais-great-shift-to-process-design-5fa2e1503bf0](https://ai.plainenglish.io/beyond-the-prompt-ais-great-shift-to-process-design-5fa2e1503bf0)  \n12. AI Prompting (3/10): Context Windows ExplainedTechniques Everyone Should Know : r/PromptEngineering \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1iftklk/ai\\_prompting\\_310\\_context\\_windows/](https://www.reddit.com/r/PromptEngineering/comments/1iftklk/ai_prompting_310_context_windows/)  \n13. Meirtz/Awesome-Context-Engineering: Comprehensive survey on Context Engineering: from prompt engineering to production-grade AI systems. hundreds of papers, frameworks, and implementation guides for LLMs"
  },
  {
    "id": "report_source",
    "chunk": "eering: Comprehensive survey on Context Engineering: from prompt engineering to production-grade AI systems. hundreds of papers, frameworks, and implementation guides for LLMs and AI agents. \\- GitHub, accessed October 15, 2025, [https://github.com/Meirtz/Awesome-Context-Engineering](https://github.com/Meirtz/Awesome-Context-Engineering)  \n14. 6 Techniques You Should Know to Manage Context Lengths in LLM Apps \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LLMDevs/comments/1mviv2a/6\\_techniques\\_you\\_should\\_know\\_to\\_manage\\_context/](https://www.reddit.com/r/LLMDevs/comments/1mviv2a/6_techniques_you_should_know_to_manage_context/)  \n15. A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2507.18910v1](https://arxiv.org/html/2507.18910v1)  \n16. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, accessed October 15, 2025, [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)  \n17. A Survey of Context Engineering for Large Language Models \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2507.13334v1](https://arxiv.org/html/2507.13334v1)  \n18. Build Advanced Retrieval-Augmented Generation Systems ..., accessed October 15, 2025, [https://learn.microsoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation](https://learn.microsoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation)  \n19. Context Window Optimizing Strategies in Gen AI Applications, accessed October 15, 2025, [https://cloudkitect.co"
  },
  {
    "id": "report_source",
    "chunk": "ure/developer/ai/advanced-retrieval-augmented-generation)  \n19. Context Window Optimizing Strategies in Gen AI Applications, accessed October 15, 2025, [https://cloudkitect.com/context-window-optimizing-strategies-in-gen-ai-applications/](https://cloudkitect.com/context-window-optimizing-strategies-in-gen-ai-applications/)  \n20. CONTEXT ENGINEERING Explained With Examples \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=seU-C6lbuTA](https://www.youtube.com/watch?v=seU-C6lbuTA)  \n21. \\[2505.04016\\] SLOT: Structuring the Output of Large Language Models \\- arXiv, accessed October 15, 2025, [https://arxiv.org/abs/2505.04016](https://arxiv.org/abs/2505.04016)  \n22. Enhancing LLM's Cognition via Structurization \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2407.16434v1](https://arxiv.org/html/2407.16434v1)  \n23. Structured Packing in LLM Training Improves Long Context Utilization \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2312.17296v6](https://arxiv.org/html/2312.17296v6)  \n24. Agentic Software Development Patterns and Feature Flag Runtime ..., accessed October 15, 2025, [https://www.getunleash.io/blog/agentic-software-development-patterns-and-feature-flag-runtime-primitives](https://www.getunleash.io/blog/agentic-software-development-patterns-and-feature-flag-runtime-primitives)  \n25. Ultimate Guide to Agentic AI and Agentic Software Development | Blog, accessed October 15, 2025, [https://www.codiste.com/agentic-ai-software-development-guide](https://www.codiste.com/agentic-ai-software-development-guide)  \n26. Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI, accessed October 15, 2025, [https://arxiv.org/html/2505.19443v1](https://arx"
  },
  {
    "id": "report_source",
    "chunk": "uide)  \n26. Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI, accessed October 15, 2025, [https://arxiv.org/html/2505.19443v1](https://arxiv.org/html/2505.19443v1)  \n27. To all vibe coders I present : r/LocalLLaMA \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1msr7j8/to\\_all\\_vibe\\_coders\\_i\\_present/](https://www.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/)  \n28. Context Engineering is the New Vibe Coding (Learn this Now) \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=Egeuql3Lrzg](https://www.youtube.com/watch?v=Egeuql3Lrzg)  \n29. 5 Pillars of Augmented Agentic Software Development \\- Liran Tal, accessed October 15, 2025, [https://lirantal.com/blog/five-pillars-augmented-agentic-software-development](https://lirantal.com/blog/five-pillars-augmented-agentic-software-development)  \n30. Karpathy: \"context engineering\" over \"prompt engineering\" \\- Hacker News, accessed October 15, 2025, [https://news.ycombinator.com/item?id=44379538](https://news.ycombinator.com/item?id=44379538)  \n31. (PDF) Cowboys and Aliens in the Digital Frontier: The Emergence of ..., accessed October 15, 2025, [https://www.researchgate.net/publication/380098993\\_Cowboys\\_and\\_Aliens\\_in\\_the\\_Digital\\_Frontier\\_The\\_Emergence\\_of\\_Techno-Social\\_Learning\\_in\\_AI-Enhanced\\_Communities\\_of\\_Practice](https://www.researchgate.net/publication/380098993_Cowboys_and_Aliens_in_the_Digital_Frontier_The_Emergence_of_Techno-Social_Learning_in_AI-Enhanced_Communities_of_Practice)  \n32. Integrating Generative AI with the Dialogic Model in ... \\- Preprints.org, accessed October 15, 2025, [https://www.preprints.org/frontend/manuscript/92390"
  },
  {
    "id": "report_source",
    "chunk": "s_of_Practice)  \n32. Integrating Generative AI with the Dialogic Model in ... \\- Preprints.org, accessed October 15, 2025, [https://www.preprints.org/frontend/manuscript/923901d1ed2e2582e2746eafffe9c58d/download\\_pub](https://www.preprints.org/frontend/manuscript/923901d1ed2e2582e2746eafffe9c58d/download_pub)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/02-V2V Context Engineering Research Plan.md\">\n\n\n# **The New Engineering Paradigm: A Formal Research Proposal on the Transition from Prompt Engineering to Context Engineering and V2V Methodologies**\n\n## **Section 1: Introduction: From Linguistic Tuning to Systems Architecture in AI Interaction**\n\nThe advent of large language models (LLMs) has catalyzed a rapid evolution in the methodologies used to build intelligent applications. Initially, the primary interface for eliciting desired behavior from these models was **prompt engineering**, a practice centered on the meticulous crafting of linguistic instructions. This approach, while foundational, is increasingly being subsumed by a more mature, robust, and scalable discipline: **context engineering**. This report posits that the evolution from prompt engineering to context engineering is not a mere terminological shift but a fundamental paradigm change, representing the maturation of applied AI from a craft-based, linguistic art to a formal, systems-design discipline. It marks a transition from focusing on \"what you say\" to a model in a single turn to architecting \"what the model knows when you say it\".1  \nThis research plan proposes a comprehensive discovery, analysis, and synthesis of public content to map this paradigm shift. The analysis will be anchored by the influential definition of context engineeri"
  },
  {
    "id": "report_source",
    "chunk": "comprehensive discovery, analysis, and synthesis of public content to map this paradigm shift. The analysis will be anchored by the influential definition of context engineering provided by researcher Andrej Karpathy: \"the delicate art and science of filling the context window with just the right information for the next step\".3 This framing moves the focus from the user's immediate query to the carefully curated informational environment the model operates within, ensuring it receives the right data, in the right format, at the right time.3\n\n### **Defining the Paradigms**\n\nTo establish a clear framework, this report will define the two paradigms as distinct points on a continuum of AI system design.6  \n**Prompt Engineering as \"Linguistic Tuning\"** will be characterized as the practice of influencing an LLM's output through the precise phrasing of instructions, the provision of illustrative examples (few-shot prompting), and the structuring of reasoning patterns (chain-of-thought).6 It is an iterative process of adjusting language, role assignments (e.g., \"You are a professional translator\"), and formatting constraints to guide the model's response within a single interaction.6 While powerful, these methods are often brittle, with small variations in wording leading to significant differences in output quality.6  \n**Context Engineering as \"Systems Thinking\"** will be defined as the systematic discipline of designing, organizing, orchestrating, and optimizing the complete informational payload provided to an LLM at inference time.3 This holistic perspective transcends a single instruction to encompass the entire information ecosystem an AI system requires for reliable and consistent performance. It involves constructing a"
  },
  {
    "id": "report_source",
    "chunk": "pective transcends a single instruction to encompass the entire information ecosystem an AI system requires for reliable and consistent performance. It involves constructing automated pipelines that aggregate and filter context from diverse sources, including system prompts, user dialogue history, real-time data, retrieved documents, and external tools.6 It is a discipline focused on building stateful, multi-turn reliability.6\n\n### **The Scope of \"V2V Methodologies\"**\n\nThe reference to \"V2V methodologies\" within the user query is interpreted as the spectrum of advanced techniques that serve as the technical underpinnings of the context engineering paradigm. This report will systematically deconstruct these methodologies, which include but are not limited to:\n\n* **Advanced Retrieval-Augmented Generation (RAG):** The foundational technology for grounding LLMs in external knowledge.  \n* **Self-Correcting and Reflective RAG Variants:** Methodologies like Self-RAG and Corrective RAG that introduce evaluation and feedback loops into the retrieval process.  \n* **Structured Knowledge Retrieval:** Techniques such as GraphRAG that leverage structured data representations for more complex reasoning.  \n* **Agentic Frameworks:** Systems that orchestrate memory, tools, and retrieval to enable autonomous, multi-step task execution.\n\nThe emergence of context engineering as a formal discipline is not merely a technical evolution; it is a direct economic and competitive response to a fundamental shift in the AI landscape. As powerful foundational models from providers like OpenAI, Anthropic, and Google become increasingly commoditized and accessible, the competitive advantage in building AI applications no longer derives solely from posse"
  },
  {
    "id": "report_source",
    "chunk": "s like OpenAI, Anthropic, and Google become increasingly commoditized and accessible, the competitive advantage in building AI applications no longer derives solely from possessing a superior proprietary model.3 The technological playing field in terms of raw model capability has been leveled. Consequently, sustainable differentiation must come from another source. This new competitive moat is the ability to effectively apply a general-purpose model to an organization's unique, proprietary data and operational logic at runtime.3 An LLM, regardless of its power, cannot solve specific enterprise problems without access to internal knowledge bases, user histories, and business rules. Context engineering is the formal practice that operationalizes this differentiation, providing the architectural patterns necessary to reliably integrate this proprietary information into the model's reasoning process, thereby creating defensible, value-added AI applications.3\n\n## **Section 2: The Brittle Limits of Prompt Engineering at Scale**\n\nThe transition toward context engineering is necessitated by the inherent and systemic limitations of prompt engineering when applied to the demands of industrial-strength AI applications. While a valuable skill for prototyping and single-turn tasks, prompt engineering's reliance on static, handcrafted instructions proves insufficient and brittle for systems that are inherently dynamic, stateful, and involve multiple interactions over time.3 This section deconstructs these limitations, arguing that they are not tactical shortcomings but fundamental architectural constraints that mandate a new approach.\n\n### **Analysis of Limitations**\n\nThe failures of prompt engineering at scale can be categorized acro"
  },
  {
    "id": "report_source",
    "chunk": "gs but fundamental architectural constraints that mandate a new approach.\n\n### **Analysis of Limitations**\n\nThe failures of prompt engineering at scale can be categorized across several key dimensions:\n\n* **Brittleness and Lack of Generalization:** The core practice of prompt engineering is highly sensitive to minor variations in wording, phrasing, and example placement, a characteristic frequently described as \"brittle\".6 A meticulously crafted prompt that performs well for a specific input can fail unexpectedly when faced with a slight semantic or structural deviation. This lack of generalization means that prompts require constant, manual adjustment and fail to create a persistent, reliable system behavior across a wide range of inputs.6 Traditional prompt engineering produces outputs that are prone to failure during integration, deployment, or when business requirements evolve, because a prompt without deep system context amounts to educated guesswork.8  \n* **Failure of Scope for Stateful Applications:** The fundamental limitation of prompt engineering is one of scope.3 A static, single-turn instruction is architecturally incapable of managing the complexities of modern AI applications. A cleverly worded prompt, in isolation, cannot manage conversation history, retrieve real-time data from an API, or maintain a persistent understanding of a user's preferences across multiple sessions.3 These stateful requirements are central to creating coherent and useful user experiences, and they lie outside the purview of a single prompt.  \n* **The \"Failure of Context\" Diagnosis:** A critical reframing of the problem is the recognition that most failures in complex AI applications are not failures of prompting but \"failures of co"
  },
  {
    "id": "report_source",
    "chunk": "of Context\" Diagnosis:** A critical reframing of the problem is the recognition that most failures in complex AI applications are not failures of prompting but \"failures of context\".3 A customer service bot that forgets a user's issue mid-conversation or an AI coding assistant that is unaware of a project's overall structure has not failed because of a poorly worded instruction. It has failed because its underlying system did not provide it with the necessary contextual informationthe conversation history or the repository structureat the moment of inference.3 This diagnosis correctly shifts the focus of debugging and design from linguistic tweaking to systems architecture.  \n* **Inherent Scalability Issues:** The reliance on manual prompt tweaking for every edge case is fundamentally unscalable.3 In a production environment with diverse user inputs and evolving requirements, this approach leads to an ever-expanding and unmanageable set of custom prompts, resulting in inconsistent and unpredictable system behavior.3 In contrast, context-engineered systems are designed for consistency and reuse across many users and tasks by programmatically injecting structured context that adapts to different scenarios.1  \n* **The \"Vibe Coding\" Honeymoon:** The initial excitement around LLMs led to a culture of what has been termed \"vibe coding,\" where developers intuitively craft prompts to achieve a desired result.10 This approach, while accessible, completely falls apart when attempting to build real, scalable software because intuition does not scalestructure does.10 This has also fueled a perception in some engineering circles of prompt engineering as a \"cash grab\" or a non-technical skill focused on finding \"magic words,\" creat"
  },
  {
    "id": "report_source",
    "chunk": "e does.10 This has also fueled a perception in some engineering circles of prompt engineering as a \"cash grab\" or a non-technical skill focused on finding \"magic words,\" creating a cultural barrier to its integration with rigorous software development practices.11\n\nThe culture of \"magic words\" and arcane prompt-craft that characterized early prompt engineering created a significant barrier to collaborative and scalable development. This practice, often based on individual intuition and opaque trial-and-error, is antithetical to modern software engineering principles of clarity, maintainability, version control, and teamwork. It is difficult to document, test, or scale the \"art\" of a perfect prompt across a large engineering organization. The shift to context engineering represents a necessary professionalization of the field. By replacing the quest for magic words with transparent and auditable system design, it aligns LLM application development with established engineering practices. Context engineering uses the language of software architecture\"pipelines,\" \"modules,\" \"orchestration,\" and \"state management\"which are standard concepts that promote collaboration, automated testing, and long-term maintainability.1 This transition effectively democratizes the development of complex AI systems, transforming it from the domain of individual \"prompt whisperers\" into a structured, collaborative engineering discipline.\n\n## **Section 3: Context Engineering: A Formal Discipline for Industrial-Strength AI**\n\nIn response to the limitations of prompt engineering, context engineering has emerged as a formal, multi-faceted discipline. It moves beyond the simple metaphor of \"filling the context window\" to establish a comprehensive se"
  },
  {
    "id": "report_source",
    "chunk": "ing, context engineering has emerged as a formal, multi-faceted discipline. It moves beyond the simple metaphor of \"filling the context window\" to establish a comprehensive set of principles and practices for architecting the flow of information to an LLM. This section provides a formal definition of context engineering, outlines its core principles, and details its essential practices, establishing it as the foundational discipline for building reliable, industrial-strength AI applications.\n\n### **Formal Definition and Core Principles**\n\nContext engineering is formally defined as **the systematic discipline of designing, organizing, orchestrating, and optimizing the complete informational payload provided to a Large Language Model at inference time**.3 It is a systems-level practice that treats the LLM's entire input window not as a simple instruction field, but as a dynamic \"workspace\" that is programmatically populated with the precise information needed to solve a given task.5 This discipline is built upon three fundamental principles:\n\n1. **Information Architecture:** This principle involves the deliberate organization and structuring of all potential contextual data to ensure optimal comprehension by the AI. A key practice is the establishment of a clear information hierarchy, distinguishing between **primary context** (mission-critical information for the immediate task), **secondary context** (supporting details that enhance understanding), and **tertiary context** (broader background information).13 This structured approach ensures that the most vital information is prioritized and not lost in a sea of irrelevant data.  \n2. **Memory Management:** This principle addresses the strategic handling of temporal inform"
  },
  {
    "id": "report_source",
    "chunk": " most vital information is prioritized and not lost in a sea of irrelevant data.  \n2. **Memory Management:** This principle addresses the strategic handling of temporal information to create stateful and coherent interactions. It involves designing systems that can manage different \"memory slots,\" such as **short-term memory** (e.g., a conversation buffer for recent exchanges), **long-term memory** (e.g., a persistent vector store for user preferences or key facts from past sessions), and **user profile information**.6 Effective memory management is what allows an AI application to maintain continuity across multiple turns and sessions.14  \n3. **Dynamic Context Adaptation:** This principle focuses on the real-time assembly and adjustment of the context based on the evolving needs of the interaction. Rather than relying on a static system prompt, a dynamically adapted system can aggregate and filter context from multiple sources on the fly, such as user dialogue history, real-time data from APIs, and retrieved documents.6 This ensures the context is always as relevant and up-to-date as possible.13\n\n### **Core Practices and Components**\n\nThe principles of context engineering are implemented through a set of core practices and architectural components:\n\n* **Context Retrieval:** The practice of identifying, fetching, and ranking the most relevant information from external knowledge sources like documents, databases, or knowledge base articles. This is the primary domain of Retrieval-Augmented Generation (RAG) and its advanced variants.6  \n* **Context Processing and Summarization:** Techniques for condensing large volumes of text, such as long documents or conversation histories, into compact and high-utility summaries. This "
  },
  {
    "id": "report_source",
    "chunk": "ocessing and Summarization:** Techniques for condensing large volumes of text, such as long documents or conversation histories, into compact and high-utility summaries. This is crucial for managing the finite context window of LLMs, reducing noise, and improving computational efficiency.6  \n* **Tool Integration:** The practice of defining and describing external functions or APIs that the model can invoke to perform actions or retrieve information beyond its internal knowledge. This includes defining the tool's purpose, parameters, and expected output format.6  \n* **Structured Templates and Output Formatting:** The use of predictable and parsable formats (e.g., JSON schemas, XML tags) to organize the different elements of the context provided to the model. This is often paired with constraints on the model's output to ensure it generates data in a reliable, machine-readable form for downstream processing.6\n\nIt is crucial to understand that prompt engineering and context engineering are not competing practices; rather, **prompt engineering is a subset of context engineering**.1 A well-engineered promptthe clear, specific instruction of *what to say*remains a vital component. However, its ultimate effectiveness is determined by the broader context architecture that defines *what the model knows* when it receives that instruction. A brilliant prompt can be rendered useless if it is drowned in thousands of tokens of irrelevant retrieved data, a failure that context engineering is designed to prevent.1  \nThe establishment of this discipline imposes a new, proactive development methodology that can be described as a \"context-first\" pattern. This approach fundamentally inverts the traditional software development workflow fo"
  },
  {
    "id": "report_source",
    "chunk": "new, proactive development methodology that can be described as a \"context-first\" pattern. This approach fundamentally inverts the traditional software development workflow for AI applications. In a traditional prompt-centric model, a developer has a task, writes code, and then attempts to craft a prompt to make an AI understand or generate that code, often reactively debugging failures.16 This frequently leads to production failures like \"hallucinated API calls\" or \"architectural blindness\" because the AI lacks a systemic understanding of the codebase it is operating on.8  \nThe context-first paradigm addresses this by requiring the developer to first architect the AI's understanding of the system *before* asking it to perform a task. This initial step involves creating a comprehensive context layer, which may include indexing the entire code repository, mapping dependencies, and defining existing architectural patterns.8 Only after this context has been engineered can the developer pose a high-level architectural challenge to the AI (e.g., \"How should authentication be refactored to support new requirements?\") rather than a simple procedural request.8 This workflowarchitecting the context, posing a challenge, receiving a plan for approval, and then executingmakes the AI's knowledge base a primary development artifact, not an afterthought. This has profound implications for tooling, which must now support repository-level indexing, and for the role of the developer, who becomes a context architect first and a prompter second.\n\n## **Section 4: Architectural Pillars of Modern Context Engineering**\n\nA robust, context-engineered AI system is not a monolithic entity but a composition of distinct yet interconnected architect"
  },
  {
    "id": "report_source",
    "chunk": "itectural Pillars of Modern Context Engineering**\n\nA robust, context-engineered AI system is not a monolithic entity but a composition of distinct yet interconnected architectural pillars. These pillars work in concert to dynamically manage the LLM's context window, providing it with the necessary information to reason effectively and perform complex tasks. This section deconstructs the modern context engineering stack into its three core pillars: advanced Retrieval-Augmented Generation (RAG), Memory and State Management systems, and Tool Integration frameworks.\n\n### **Pillar 1: Advanced Retrieval-Augmented Generation (RAG)**\n\nRAG serves as the foundational pillar for grounding LLMs in external reality. Its primary function is to connect the model to up-to-date, proprietary, or domain-specific knowledge sources, thereby mitigating hallucinations and moving the model's capabilities beyond its static, pre-trained knowledge.14  \nThe naive RAG process consists of three main stages:\n\n1. **Indexing:** Raw documents are loaded, cleaned, and segmented into smaller, manageable chunks. Each chunk is then passed through an embedding model to create a numerical vector representation, which is stored in a vector database.17  \n2. **Retrieval:** At inference time, a user's query is also converted into a vector embedding. A semantic similarity search is performed against the vector database to find the chunks whose embeddings are closest to the query embedding.17  \n3. **Generation:** The retrieved text chunks are prepended to the user's original query and fed into the LLM as part of the prompt. The LLM then generates a response that is \"augmented\" with this retrieved context.18\n\nWhile revolutionary, this basic RAG pipeline suffers from "
  },
  {
    "id": "report_source",
    "chunk": "the LLM as part of the prompt. The LLM then generates a response that is \"augmented\" with this retrieved context.18\n\nWhile revolutionary, this basic RAG pipeline suffers from significant challenges in production environments. Common failure modes include **bad retrieval** (low precision, where retrieved chunks are irrelevant, or low recall, where relevant chunks are missed) and **bad generation** (the model hallucinates or produces an irrelevant response despite being provided with the correct context).7 These limitations have spurred the development of the more sophisticated RAG methodologies that form the core of modern context engineering and are discussed in detail in Section 5\\.\n\n### **Pillar 2: Memory and State Management Systems**\n\nThe second pillar is dedicated to providing the AI system with continuity and personalization. Memory systems enable an application to maintain state across multiple interactions, allowing it to remember past conversations, learn user preferences, and build a coherent understanding over time.6  \nMemory is typically architected into distinct types:\n\n* **Short-Term Memory:** This functions as a conversational buffer, holding the history of recent exchanges within a single session. It is essential for maintaining the coherence of a multi-turn dialogue, allowing the model to understand references to previous statements.9 This is often implemented as a simple list of messages that grows with the conversation.  \n* **Long-Term Memory:** This provides a mechanism for persistent storage of information across different sessions. It can store key facts, summaries of past conversations, or detailed user profiles. Long-term memory is typically implemented using an external storage system, such as a "
  },
  {
    "id": "report_source",
    "chunk": "ons. It can store key facts, summaries of past conversations, or detailed user profiles. Long-term memory is typically implemented using an external storage system, such as a vector database, where information can be retrieved when needed to provide continuity and personalization.9  \n* **Hierarchical Memory:** Advanced systems may employ more complex memory hierarchies that include mechanisms for compression, prioritization, and optimization. This allows the system to manage vast amounts of historical context efficiently, deciding what information is critical to retain and what can be summarized or discarded.14\n\n### **Pillar 3: Tool Integration and Function Calling**\n\nThe third pillar extends the LLM's capabilities from a pure text processor into an active agent that can interact with the external world. By integrating tools, the model can perform actions like querying a database, calling an API, running a piece of code, or searching the web.6  \nThe mechanism for tool integration involves several steps:\n\n1. **Definition:** A set of available tools is defined and described in natural language, including each tool's name, a description of what it does, and the parameters it accepts (e.g., in a JSON schema).9  \n2. **Provision:** These tool definitions are provided to the LLM as part of its context.  \n3. **Invocation:** When faced with a query it cannot answer from its internal knowledge or retrieved context, the LLM can generate a structured output (e.g., a JSON object) requesting a call to one of the available tools with specific parameters.  \n4. **Execution:** The application code parses this request, executes the corresponding function or API call, and receives a result.  \n5. **Observation:** The output from the tool exe"
  },
  {
    "id": "report_source",
    "chunk": "ecution:** The application code parses this request, executes the corresponding function or API call, and receives a result.  \n5. **Observation:** The output from the tool execution is then fed back into the LLM's context, allowing it to use this new information to formulate its final response.9\n\nFrameworks like LangChain and standards such as the Model Context Protocol (MCP) play a crucial role in simplifying and standardizing this process, providing abstractions that make it easier to define tools and manage the interaction loop.5  \nThese three pillarsRetrieval, Memory, and Toolsdo not operate in isolation. They form a deeply interconnected and synergistic \"cognitive architecture\" for the LLM. The effectiveness of a context-engineered system lies in the orchestration of the interplay between these components. For instance, a user might ask a complex question that requires multi-step reasoning.7 The system would first consult its **Memory** to check if a similar query has been resolved before. Finding no existing answer, it might invoke a planning **Tool** to decompose the complex query into a series of simpler sub-queries.20 For each sub-query, the system would then perform **Retrieval** from a dedicated knowledge base.21 If a retrieved document proves ambiguous or outdated, the system could trigger another **Tool**, such as a web search, to gather more current information.22 Throughout this entire process, the system continuously updates its short-term **Memory** (often called a \"scratchpad\") with intermediate findings and the results of tool calls, building up a comprehensive context before synthesizing the final answer.9 This dynamic orchestration is the hallmark of an agentic system, where the challenge is not me"
  },
  {
    "id": "report_source",
    "chunk": "alls, building up a comprehensive context before synthesizing the final answer.9 This dynamic orchestration is the hallmark of an agentic system, where the challenge is not merely implementing each pillar, but designing the sophisticated logic that governs their interaction.\n\n## **Section 5: The Evolution of Retrieval: A Comparative Analysis of Advanced RAG Methodologies**\n\nThe technical engine driving the context engineering paradigm is the rapid evolution of Retrieval-Augmented Generation. Moving beyond the limitations of the naive RAG pipeline, a new class of advanced methodologies has emerged. These approaches introduce sophisticated mechanisms for self-correction, reflection, and structural awareness, transforming RAG from a simple data-fetching process into an intelligent, robust, and adaptable component of the AI cognitive architecture. This section provides a detailed comparative analysis of these cutting-edge retrieval methodologies.\n\n### **Methodology 1: Self-Correction and Reflection (Self-RAG & Corrective RAG)**\n\nThe first major advancement in RAG involves introducing a self-evaluation loop to critically assess the quality and relevance of retrieved information *before* it is used for generation. This principle of self-correction significantly enhances the system's robustness against the common failure mode of inaccurate retrieval.\n\n* **Self-RAG (Self-Reflective Retrieval-Augmented Generation):** This framework trains a language model to generate special \"reflection tokens\" that actively control the retrieval and generation process.23 Instead of retrieving blindly, the model learns to make several critical decisions. First, it decides *when* retrieval is necessary by predicting a \\`\\` token, enabling adaptive"
  },
  {
    "id": "report_source",
    "chunk": "stead of retrieving blindly, the model learns to make several critical decisions. First, it decides *when* retrieval is necessary by predicting a \\`\\` token, enabling adaptive, on-demand retrieval that can be skipped for simple queries or repeated for complex ones.24 Second, after retrieving documents, it assesses their relevance by generating an ISREL (Is Relevant) token for each passage. Finally, it critiques its own generated response to ensure it is factually supported by the evidence, using an ISSUP (Is Supported) token.25 This end-to-end training for self-reflection allows the model to balance versatility with a high degree of factual accuracy and control.  \n* **Corrective RAG (CRAG):** This methodology offers a more modular, \"plug-and-play\" approach to improving retrieval robustness.22 It employs a lightweight, fine-tuned retrieval evaluatorseparate from the main LLMto assess retrieved documents and assign them a confidence score. Based on this score, the system triggers one of three distinct actions:  \n  1. **Correct:** If confidence is high, the documents are deemed relevant and used for generation.  \n  2. **Incorrect:** If confidence is low, the documents are discarded, and the system supplements its knowledge by performing a large-scale web search to find better information.  \n  3. Ambiguous: If confidence is intermediate, the system uses both the retrieved documents and the web search results.22  \n     CRAG's design, which includes a decompose-then-recompose algorithm to filter noise from documents, makes it an effective add-on for enhancing the reliability of existing RAG pipelines.22\n\n### **Methodology 2: Structured Knowledge Integration (GraphRAG)**\n\nThe second major evolutionary path for RAG moves beyon"
  },
  {
    "id": "report_source",
    "chunk": "ancing the reliability of existing RAG pipelines.22\n\n### **Methodology 2: Structured Knowledge Integration (GraphRAG)**\n\nThe second major evolutionary path for RAG moves beyond processing unstructured text chunks to leveraging structured knowledge representations. GraphRAG constructs a knowledge graph from the source documents, capturing not just isolated pieces of information but also the intricate relationships between them. This enables more complex, multi-hop reasoning that is difficult to achieve with standard semantic search.28\n\n* **Mechanism:** The GraphRAG indexing process involves using an LLM to extract key entities (nodes) and their relationships (edges) from the text, building a comprehensive knowledge graph.29 When a query is received, instead of performing a simple vector search, the system can traverse this graph. For example, it can find entities mentioned in the query and then explore their multi-hop neighbors to gather a rich, interconnected context.29 This approach is particularly effective for answering questions that require synthesizing information from multiple sources or understanding the overall structure of the knowledge base.29  \n* **Variants:** Several approaches to GraphRAG exist. **KG-based GraphRAG** focuses on retrieving and traversing triplets (head, relation, tail) from the graph.29 **Community-based GraphRAG**, a method developed by Microsoft, goes a step further by applying community detection algorithms to the graph to create hierarchical summaries. This allows for both **Local Search** (exploring specific entities and their immediate connections) and **Global Search** (querying high-level community summaries to understand broad themes), providing a multi-resolution view of the data.2"
  },
  {
    "id": "report_source",
    "chunk": "es and their immediate connections) and **Global Search** (querying high-level community summaries to understand broad themes), providing a multi-resolution view of the data.29\n\n### **Synthesis and Other Advanced Techniques**\n\nThe RAG landscape is rich with other innovative techniques that complement these major methodologies:\n\n* **Query Transformation:** Before retrieval, the user's initial query can be improved. Techniques like **multi-query retrieval** use an LLM to generate several variations of the original query to cast a wider net.18 **Hypothetical Document Embedding (HyDE)** involves having the LLM generate a hypothetical ideal answer to the query, embedding that answer, and then searching for documents that are semantically similar to this ideal response.7  \n* **Advanced Chunking and Re-ranking:** The quality of retrieval is highly dependent on how documents are indexed. **Semantic chunking** splits documents based on conceptual coherence rather than fixed character counts.36 The **small-to-big retrieval** technique involves retrieving small, precise chunks for high-accuracy matching but then passing larger, parent chunks to the LLM to provide more context for generation.7 After initial retrieval, a **cross-encoder re-ranker** can be used to apply a more computationally expensive but accurate model to re-order the top-k results, pushing the most relevant documents to the top.7  \n* **Agentic RAG:** This represents the convergence of RAG with autonomous agents. Instead of a fixed pipeline, an AI agent orchestrates the entire retrieval process, making dynamic decisions about which retrieval strategy to use, whether to transform the query, or when to use a tool like a web search, based on the specific query and the "
  },
  {
    "id": "report_source",
    "chunk": ", making dynamic decisions about which retrieval strategy to use, whether to transform the query, or when to use a tool like a web search, based on the specific query and the state of the task.37\n\nThe following table provides a synthesized comparison of these advanced RAG methodologies, designed to help technical leaders map their specific business problems to the most appropriate RAG architecture.\n\n| Methodology | Core Principle | Key Challenge Addressed | Strengths | Limitations/Trade-offs | Ideal Use Case |\n| :---- | :---- | :---- | :---- | :---- | :---- |\n| **Naive RAG** | Static Retrieval | Basic factual grounding from external knowledge. | Simple to implement; provides baseline grounding. | Brittle; prone to \"lost in the middle\" problem; sensitive to retrieval quality. | Simple Q\\&A over a clean, well-structured knowledge base. |\n| **Corrective RAG (CRAG)** | Retrieval  Evaluate  Act | Irrelevant or inaccurate document retrieval. | Improves robustness against bad retrieval; plug-and-play with existing systems; uses web search to augment knowledge. | Increased latency due to evaluation and potential web search steps; web results can introduce new noise. | High-stakes applications where factual accuracy is paramount (e.g., finance, legal, medical). |\n| **Self-RAG** | Adaptive Retrieval & Self-Critique | The trade-off between versatility (always generating) and factuality (always retrieving). | High factual accuracy and citation precision; controllable and adaptive retrieval frequency; efficient at inference time. | Requires specialized model training or fine-tuning; more complex to implement than modular approaches. | Open-domain Q\\&A; long-form generation requiring verifiable citations and high factuality. |\n| **G"
  },
  {
    "id": "report_source",
    "chunk": "raining or fine-tuning; more complex to implement than modular approaches. | Open-domain Q\\&A; long-form generation requiring verifiable citations and high factuality. |\n| **GraphRAG** | Relational Retrieval | Multi-hop reasoning and understanding contextual relationships between data points. | Captures deep relationships within data; excels at complex queries requiring synthesis; can be more token-efficient. | High upfront indexing cost and complexity; performance is dependent on the quality of the generated graph. | Analyzing interconnected data like research paper networks, codebases, or complex business intelligence reports. |\n\n## **Section 6: The Agentic Paradigm: Orchestrating Context for Autonomous Task Execution**\n\nThe architectural pillars and advanced retrieval methodologies discussed previously converge in the **agentic paradigm**, which can be seen as the ultimate expression and application of context engineering. An AI agent is a system that leverages a continuously managed contextcomprising memory, tools, and retrieved knowledgeto autonomously plan, reason, and execute complex, multi-step tasks that go far beyond a single question-and-answer exchange.5 This section will analyze the shift from linear RAG pipelines to cyclical agentic workflows and survey the frameworks that enable this new mode of AI-powered automation.\n\n### **From RAG Pipelines to Agentic Workflows**\n\nTraditional RAG applications, even advanced ones, typically follow a linear, sequential pipeline: a query is received, documents are retrieved, context is augmented, and a response is generated. Agentic systems fundamentally alter this flow by introducing a cyclical reasoning loop, often described as **Think  Act  Observe**.\n\n1. **Think:**"
  },
  {
    "id": "report_source",
    "chunk": "d a response is generated. Agentic systems fundamentally alter this flow by introducing a cyclical reasoning loop, often described as **Think  Act  Observe**.\n\n1. **Think:** The agent analyzes the current goal and the state of its context (including the user's request, its memory, and available tools) to form a plan or decide on the next action.  \n2. **Act:** The agent executes the chosen action. This could be invoking a tool (e.g., running a search query, calling an API), updating its internal memory, or generating a response to the user.  \n3. **Observe:** The agent takes the result of its action (e.g., the output from a tool call, a new message from the user) and incorporates it back into its context. This updated context then serves as the input for the next \"Think\" step.\n\nThis loop continues until the agent determines that the overall goal has been achieved. Effective context engineering is the prerequisite for this entire process. For example, agents often use a \"scratchpad\" or working memorya form of short-term, dynamically updated contextto record their intermediate thoughts, the results of tool calls, and their evolving plan.9 This scratchpad is a direct implementation of context management that allows the agent to maintain a coherent \"thought process\" throughout a complex task.\n\n### **Analysis of Agent Frameworks and Design Patterns**\n\nThe rise of the agentic paradigm has been accelerated by the development of specialized frameworks that provide abstractions for building and orchestrating these complex systems. These frameworks are, in essence, toolkits for context engineering at an agentic level.\n\n* **LangChain / LangGraph:** LangChain provides high-level abstractions for creating chains and agents that man"
  },
  {
    "id": "report_source",
    "chunk": "essence, toolkits for context engineering at an agentic level.\n\n* **LangChain / LangGraph:** LangChain provides high-level abstractions for creating chains and agents that manage context through built-in memory classes and tool integrations.5 Its more recent extension, LangGraph, is explicitly designed for building cyclical, stateful agentic workflows. LangGraph represents the agent's logic as a graph where nodes are functions (e.g., call a tool, generate a response) and edges are conditional logic that directs the flow based on the current state. This makes it a powerful tool for implementing complex, multi-step reasoning and self-correction loops.5  \n* **CrewAI:** This framework specializes in the orchestration of multi-agent systems. It introduces the concepts of \"Crews\" (teams of specialized agents) and \"Flows\" (workflows).5 The core idea is to break down a complex problem and assign sub-tasks to different agents, each with its own specific role, tools, and isolated context. A controller then manages the communication and collaboration between these agents.5 This approach is a powerful context engineering pattern, as it uses separation of concerns to prevent context overload in any single agent.  \n* **DSPy (Declarative Self-improving Python):** DSPy takes a different, more programmatic approach. Instead of having developers write explicit prompts, it allows them to define the logic of an LLM program as a series of Python modules (e.g., dspy.ChainOfThought, dspy.Retrieve). DSPy then acts as a \"compiler\" that automatically optimizes these modules into highly effective, context-aware prompts for a given LLM.5 It abstracts away the raw prompt engineering, allowing developers to focus on the high-level program structure w"
  },
  {
    "id": "report_source",
    "chunk": "into highly effective, context-aware prompts for a given LLM.5 It abstracts away the raw prompt engineering, allowing developers to focus on the high-level program structure while the framework handles the low-level context management.\n\nThese frameworks enable sophisticated agentic design patterns that are direct applications of context engineering. **Multi-agent collaboration**, as seen in CrewAI and proposed in frameworks like RepoTransAgent 21, isolates context by function, allowing a \"RAG Agent\" to focus solely on retrieval while a \"Refine Agent\" focuses on code generation, improving the effectiveness of each.21 **Reflection and self-correction**, a key feature of agentic RAG, is implemented by creating cycles in the agent's logic where the output of one step is evaluated and used to decide the next, such as re-querying if initial retrieval results are poor.21  \nThe proliferation of these agentic frameworks signifies a new, higher layer of abstraction in AI application development. The engineering focus is rapidly shifting away from managing individual LLM prompt-completion pairs and toward designing the interaction protocols, state management systems, and collaborative workflows for teams of autonomous agents. This evolution mirrors previous shifts in the history of software engineering, such as the move from assembly language to high-level procedural languages, and more recently, the transition from monolithic applications to orchestrated microservice architectures. In this new paradigm, context engineering provides the essential infrastructurethe \"network\" and \"state management\" layersfor what can be conceptualized as \"AI-native microservices.\" Here, autonomous agents are the services, each with a specialized ro"
  },
  {
    "id": "report_source",
    "chunk": "ethe \"network\" and \"state management\" layersfor what can be conceptualized as \"AI-native microservices.\" Here, autonomous agents are the services, each with a specialized role and API (its tools). The primary engineering challenge is no longer just prompt design, but the orchestration, state synchronization, and inter-agent communication required to make these services collaborate effectively to solve complex business problems.\n\n## **Section 7: Human-in-the-Loop: Redefining Collaboration in Context-Aware Systems**\n\nThe paradigm shift from prompt engineering to context engineering does more than just alter the technical architecture of AI systems; it profoundly redefines the role of the human developer and the nature of human-AI collaboration. As AI moves from a simple instruction-following tool to a context-aware partner, the developer's role evolves from that of a \"prompter\" or \"vibe coder\" to a \"context architect\" and \"AI orchestrator.\" This section explores these new models of collaboration and the practical workflows that emerge in a context-first development environment.\n\n### **New Models of Collaboration**\n\nThe relationship between a human and a context-aware AI is more nuanced and collaborative than the simple command-response dynamic of prompt engineering.\n\n* **Cognitive Apprenticeship with AI:** In this model, the AI acts as an expert apprentice or intelligent tutor within the development process.42 The human developer takes on the role of the master practitioner, providing the strategic direction, architectural constraints, and domain knowledge that form the AI's context. The AI, guided by this rich context, then handles the tactical implementation, such as generating code, suggesting refactoring, or identify"
  },
  {
    "id": "report_source",
    "chunk": "owledge that form the AI's context. The AI, guided by this rich context, then handles the tactical implementation, such as generating code, suggesting refactoring, or identifying potential bugs.42 The AI can provide cognitive scaffolding, offering insights based on its analysis of the entire codebase, a task that would be too complex for a human to perform in real-time.42  \n* **AI-Assisted Software Architecture:** With a deep understanding of the entire system's context, AI can transcend mere code generation and become a participant in high-level architectural decision-making. Instead of being given procedural requests like \"Write a login function,\" an AI with full repository context can be posed architectural challenges: \"How should the authentication service be refactored to support OAuth2 while maintaining backward compatibility with our existing JWT implementation?\".8 This elevates the AI from a simple coder to a co-architect that can reason about system-wide implications, dependencies, and established patterns.16\n\n### **\"Context-First\" Development Workflows**\n\nThese new collaborative models are enabled by a set of \"context-first\" development patterns that prioritize architecting the AI's understanding before asking it to perform tasks. These workflows stand in stark contrast to the reactive, trial-and-error nature of traditional prompt engineering.8\n\n* **The Flipped Interaction Pattern:** In a traditional workflow, the developer provides a prompt and hopes the AI understands it, often leading to incorrect implementations due to unstated assumptions. The Flipped Interaction Pattern inverts this. The AI, positioned as an intelligent collaborator, proactively asks clarifying questions to resolve ambiguity *before* begi"
  },
  {
    "id": "report_source",
    "chunk": "ions. The Flipped Interaction Pattern inverts this. The AI, positioned as an intelligent collaborator, proactively asks clarifying questions to resolve ambiguity *before* beginning implementation.8 For the authentication refactoring example, the AI might ask: \"Should OAuth2 replace JWT entirely or integrate alongside it? Which OAuth2 providers need to be supported?\" This dialogue prevents silent errors and significantly reduces rework.8  \n* **The Agentic Plan Pattern:** For complex tasks that span multiple files or services, this pattern introduces a crucial human review step. The AI first analyzes the request and the system context to generate a detailed, multi-step implementation plan. This plan outlines which files will be modified, what new dependencies will be introduced, and how the changes will be tested. The human developer then reviews and approves this plan, ensuring it aligns with the project's architectural goals, before the AI autonomously executes it.8 This prevents the AI from making unilateral architectural decisions that could introduce \"surprise dependencies\" during integration.8  \n* **Human-in-the-Loop (HITL) for Safety and Quality:** Beyond the development process, HITL remains a critical component for the ongoing operation of context-engineered systems, especially in high-stakes domains. Human oversight is essential for validating AI outputs, mitigating algorithmic bias that may be present in the data sources, ensuring ethical decision-making, and providing a final layer of accountability.43 Regulations like the EU AI Act mandate this level of human oversight for high-risk systems, formalizing the need for humans to be able to intervene and override AI-driven decisions.44\n\nThe adoption of a context-f"
  },
  {
    "id": "report_source",
    "chunk": "e this level of human oversight for high-risk systems, formalizing the need for humans to be able to intervene and override AI-driven decisions.44\n\nThe adoption of a context-first approach leads to the creation of a new and critical development artifact: the **\"Context Manifest\"** or **\"System Prompt Notebook\"**.4 This is a formal, structured document or set of configuration files that explicitly defines the AI's operating environment. It contains the AI's role and persona, definitions of available tools, pointers to knowledge sources, examples of desired behavior, and high-level architectural constraints.45 This manifest is not a one-off, disposable prompt; it is a persistent, engineered resource that is as vital to the application's behavior as the source code itself.10 The logical conclusion of this trend is the formalization of **\"AI Configuration as Code.\"** This Context Manifest will be stored in version control systems, subjected to the same rigorous code review and testing processes as the application code, and deployed as part of the CI/CD pipeline. This represents a fundamental shift in the definition of a software project, where the explicit and auditable configuration of the AI's \"mind\" becomes a first-class citizen of the engineering lifecycle.\n\n## **Section 8: Strategic Implications and Future Research Directions**\n\nThe transition from prompt engineering to context engineering is more than a technical upgrade; it is a strategic inflection point for any organization seeking to build meaningful and defensible AI capabilities. Mastering this new paradigm is not just an engineering goal but a business imperative. This concluding section synthesizes the report's findings to outline the strategic importance of co"
  },
  {
    "id": "report_source",
    "chunk": "his new paradigm is not just an engineering goal but a business imperative. This concluding section synthesizes the report's findings to outline the strategic importance of context engineering and identifies the key open challenges and future research frontiers that will shape the next generation of AI systems.\n\n### **Strategic Imperatives**\n\n* **A New Source of Competitive Advantage:** The central strategic implication is that in an era of powerful and widely accessible foundational LLMs, the primary driver of competitive advantage has shifted. It is no longer about who owns the best model, but who can most effectively connect a model to their unique, proprietary data and complex business workflows.3 The context layerthe sophisticated architecture of retrieval, memory, and toolsis the new competitive moat. Organizations that invest in building robust context engineering capabilities will be able to create AI applications that are more accurate, more personalized, and more deeply integrated into their core operations, creating a defensible advantage that cannot be easily replicated by competitors with access to the same base LLMs.  \n* **A Fundamental Shift in Required Skillsets:** The value of an AI engineer is no longer measured by their ability to wordsmith the perfect \"magic prompt.\" The most critical skill is now systems design for context.46 This requires a cross-functional expertise that blends data architecture (designing knowledge bases and retrieval strategies), software engineering (building scalable pipelines and tool integrations), and deep business domain knowledge (understanding the specific information and logic required to solve a problem). Organizations must foster these hybrid skillsets to move beyond"
  },
  {
    "id": "report_source",
    "chunk": " deep business domain knowledge (understanding the specific information and logic required to solve a problem). Organizations must foster these hybrid skillsets to move beyond simple AI demos and build production-critical infrastructure.8  \n* **The Bridge from Demos to Production:** Context engineering is the set of principles and practices that enables AI applications to graduate from interesting but brittle prototypes to reliable, scalable, and maintainable production systems.8 By replacing manual, ad-hoc prompting with structured, repeatable, and auditable systems, context engineering provides the engineering rigor necessary for enterprise-grade deployment.\n\n### **Challenges and Open Frontiers**\n\nDespite its rapid advancement, the field of context engineering faces several significant challenges that represent active areas of research and development.\n\n* **Managing Context Window Limitations:** While the context windows of LLMs are expanding, they remain a finite and expensive resource. Effectively managing this space is a critical challenge. Active research is focused on advanced strategies such as intelligent **context summarization** to distill key information, heuristic-based **context trimming** to prune less relevant data, and architectural patterns like **multi-agent systems** that **isolate context** by splitting a complex task across multiple agents, each with its own smaller, focused context window.14  \n* **Evaluation and Observability:** Evaluating the performance of a complex, context-engineered system is a significant challenge. Simple output accuracy is insufficient. A new class of evaluation metrics is needed to assess the quality of the intermediate steps: Was the retrieval relevant? Was the correct to"
  },
  {
    "id": "report_source",
    "chunk": "le output accuracy is insufficient. A new class of evaluation metrics is needed to assess the quality of the intermediate steps: Was the retrieval relevant? Was the correct tool chosen? Was the memory state updated properly? This has led to the emergence of specialized AI observability platforms (e.g., Langfuse, Trulens, Ragas) that provide deep traces into the agent's reasoning process, allowing developers to debug and optimize the entire context pipeline, not just the final output.48  \n* **Context Security:** As the context window becomes the primary interface for controlling an LLM, it also becomes a new attack surface. Emerging threat vectors include **context poisoning**, where malicious or misleading information is deliberately injected into the knowledge base that an agent retrieves from, and sophisticated **prompt injection** attacks that can be delivered through retrieved documents or tool outputs, potentially causing the agent to leak data or perform unauthorized actions.39 Developing robust defenses against these context-based attacks is a critical research frontier.\n\n### **Future Directions**\n\nLooking forward, the continued evolution of context engineering points toward several exciting future directions:\n\n* **Automated Context Engineering:** The next logical step is to use AI to automate the design and optimization of its own context architecture. Frameworks like AutoRAG, which can automatically test and select the best combination of chunking strategies, embedding models, and retrieval parameters for a given dataset, are early indicators of this trend.48  \n* **Multi-Modal Context:** Current systems predominantly focus on textual context. A major frontier is the development of unified frameworks that can sea"
  },
  {
    "id": "report_source",
    "chunk": "rs of this trend.48  \n* **Multi-Modal Context:** Current systems predominantly focus on textual context. A major frontier is the development of unified frameworks that can seamlessly ingest, index, and reason over multi-modal context, including images, audio, video, and structured data, to provide a more holistic understanding of the world.  \n* **Cognitive Architectures:** The long-term vision of context engineering is the development of increasingly sophisticated, human-like cognitive architectures for AI. The pillars of retrieval (accessing knowledge), memory (retaining experience), and tools (acting on the world) are the foundational building blocks. Future research will focus on creating more advanced systems for reasoning, learning, and planning that are built upon these context-engineered foundations, moving us closer to more general and capable artificial intelligence.\n\n#### **Works cited**\n\n1. Context Engineering vs Prompt Engineering | by Mehul Gupta | Data ..., accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d)  \n2. What Is Context Engineering And Why Should You Care? | In The Loop Episode 23, accessed October 15, 2025, [https://www.mindset.ai/blogs/in-the-loop-ep23-what-is-context-engineering](https://www.mindset.ai/blogs/in-the-loop-ep23-what-is-context-engineering)  \n3. Context Engineering: Techniques, Tools, and Implementation \\- iKala, accessed October 15, 2025, [https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/](https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-imp"
  },
  {
    "id": "report_source",
    "chunk": ", 2025, [https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/](https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/)  \n4. Context Engineering : r/LocalLLaMA \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1lnldsj/context\\_engineering/](https://www.reddit.com/r/LocalLLaMA/comments/1lnldsj/context_engineering/)  \n5. Context Engineering in LLMs and AI Agents | by DhanushKumar | Stackademic, accessed October 15, 2025, [https://blog.stackademic.com/context-engineering-in-llms-and-ai-agents-eb861f0d3e9b](https://blog.stackademic.com/context-engineering-in-llms-and-ai-agents-eb861f0d3e9b)  \n6. Understanding Prompt Engineering and Context Engineering, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  \n7. Retrieval-augmented Generation: Part 2 | by Xin Cheng \\- Medium, accessed October 15, 2025, [https://billtcheng2013.medium.com/retrieval-augmented-generation-part-2-eaf2fdff45dc](https://billtcheng2013.medium.com/retrieval-augmented-generation-part-2-eaf2fdff45dc)  \n8. How Context-First Prompt Engineering Patterns Actually Ship ..., accessed October 15, 2025, [https://www.augmentcode.com/guides/how-context-first-prompt-engineering-patterns-actually-ship-production-code](https://www.augmentcode.com/guides/how-context-first-prompt-engineering-patterns-actually-ship-production-code)  \n9. What is Context Engineering? (And Why It's Really Just Prompt ..., accessed October 15, 2025, [https://mirascope.com/blog/context-engineering](https://mirascope.com/blog/context-engin"
  },
  {
    "id": "report_source",
    "chunk": "Context Engineering? (And Why It's Really Just Prompt ..., accessed October 15, 2025, [https://mirascope.com/blog/context-engineering](https://mirascope.com/blog/context-engineering)  \n10. Context Engineering is the New Vibe Coding (Learn this Now) \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=Egeuql3Lrzg](https://www.youtube.com/watch?v=Egeuql3Lrzg)  \n11. Beyond the Prompt: AI's Great Shift to Process Design | by Umakshi Sharma | Oct, 2025, accessed October 15, 2025, [https://ai.plainenglish.io/beyond-the-prompt-ais-great-shift-to-process-design-5fa2e1503bf0](https://ai.plainenglish.io/beyond-the-prompt-ais-great-shift-to-process-design-5fa2e1503bf0)  \n12. Prompt engineering, Context Engineering, Protocol Whatever... It's all Linguistics Programming... : r/OpenAI \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt\\_engineering\\_context\\_engineering\\_protocol/](https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt_engineering_context_engineering_protocol/)  \n13. Context Engineering in AI: Principles, Methods, and Uses \\- Code B, accessed October 15, 2025, [https://code-b.dev/blog/context-engineering](https://code-b.dev/blog/context-engineering)  \n14. Context Engineering. What are the components that make up \\- Cobus Greyling \\- Medium, accessed October 15, 2025, [https://cobusgreyling.medium.com/context-engineering-a34fd80ccc26](https://cobusgreyling.medium.com/context-engineering-a34fd80ccc26)  \n15. Enhancing AI Prompts with XML Tags: Testing Anthropic's Method and o4-mini-high / Mike Levin AI SEO GEO AEO AIO in NYC, accessed October 15, 2025, [https://mikelev.in/futureproof/ai-prompts-xml-tags/](https://mikelev.in/futureproof/ai-prompts-xml-tags/) "
  },
  {
    "id": "report_source",
    "chunk": " / Mike Levin AI SEO GEO AEO AIO in NYC, accessed October 15, 2025, [https://mikelev.in/futureproof/ai-prompts-xml-tags/](https://mikelev.in/futureproof/ai-prompts-xml-tags/)  \n16. How to Use AI to Modernize Software Architecture \\- DZone, accessed October 15, 2025, [https://dzone.com/articles/ai-modernize-software-architecture](https://dzone.com/articles/ai-modernize-software-architecture)  \n17. Retrieval-Augmented Generation for Large Language ... \\- arXiv, accessed October 15, 2025, [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997)  \n18. Advanced RAG Techniques: Upgrade Your LLM App Prototype to Production-Ready\\!, accessed October 15, 2025, [https://levelup.gitconnected.com/advanced-rag-techniques-upgrade-your-llm-app-prototype-to-production-ready-74839342e9c0](https://levelup.gitconnected.com/advanced-rag-techniques-upgrade-your-llm-app-prototype-to-production-ready-74839342e9c0)  \n19. 13+ Popular MCP servers for developers to unlock AI actions \\- DronaHQ, accessed October 15, 2025, [https://www.dronahq.com/popular-mcp-servers/](https://www.dronahq.com/popular-mcp-servers/)  \n20. Context Engineering Guide, accessed October 15, 2025, [https://www.promptingguide.ai/guides/context-engineering-guide](https://www.promptingguide.ai/guides/context-engineering-guide)  \n21. RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation, accessed October 15, 2025, [https://arxiv.org/html/2508.17720v1](https://arxiv.org/html/2508.17720v1)  \n22. Corrective RAG (CRAG) \\- Kore.ai, accessed October 15, 2025, [https://www.kore.ai/blog/corrective-rag-crag](https://www.kore.ai/blog/corrective-rag-crag)  \n23. Self-Rag: Self-reflective Retrieval augmented Generation \\- arXiv, accessed October 15, 202"
  },
  {
    "id": "report_source",
    "chunk": "re.ai/blog/corrective-rag-crag](https://www.kore.ai/blog/corrective-rag-crag)  \n23. Self-Rag: Self-reflective Retrieval augmented Generation \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2310.11511v1](https://arxiv.org/html/2310.11511v1)  \n24. Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflection, accessed October 15, 2025, [https://selfrag.github.io/](https://selfrag.github.io/)  \n25. Self-RAG \\- Learn Prompting, accessed October 15, 2025, [https://learnprompting.org/docs/retrieval\\_augmented\\_generation/self-rag](https://learnprompting.org/docs/retrieval_augmented_generation/self-rag)  \n26. SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI \\- Medium, accessed October 15, 2025, [https://medium.com/@sahin.samia/self-rag-self-reflective-retrieval-augmented-generation-the-game-changer-in-factual-ai-dd32e59e3ff9](https://medium.com/@sahin.samia/self-rag-self-reflective-retrieval-augmented-generation-the-game-changer-in-factual-ai-dd32e59e3ff9)  \n27. Corrective Retrieval Augmented Generation (CRAG)  Paper ..., accessed October 15, 2025, [https://medium.com/@sulbha.jindal/corrective-retrieval-augmented-generation-crag-paper-review-2bf9fe0f3b31](https://medium.com/@sulbha.jindal/corrective-retrieval-augmented-generation-crag-paper-review-2bf9fe0f3b31)  \n28. Advanced RAG techniques \\- Literal AI, accessed October 15, 2025, [https://www.literalai.com/blog/advanced-rag-techniques](https://www.literalai.com/blog/advanced-rag-techniques)  \n29. RAG vs. GraphRAG: A Systematic Evaluation and Key Insights \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2502.11371v1](https://arxiv.org/html/2502.11371v1)  \n30. Four retrieval techniques to improv"
  },
  {
    "id": "report_source",
    "chunk": "tion and Key Insights \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2502.11371v1](https://arxiv.org/html/2502.11371v1)  \n30. Four retrieval techniques to improve RAG you need to know | by Thoughtworks \\- Medium, accessed October 15, 2025, [https://thoughtworks.medium.com/four-retrieval-techniques-to-improve-rag-you-need-to-know-f641e2b1db6c](https://thoughtworks.medium.com/four-retrieval-techniques-to-improve-rag-you-need-to-know-f641e2b1db6c)  \n31. GraphRAG: The Practical Guide for Cost-Effective Document Analysis with Knowledge Graphs \\- LearnOpenCV, accessed October 15, 2025, [https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/](https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/)  \n32. Intro to GraphRAG \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=f6pUqDeMiG0](https://www.youtube.com/watch?v=f6pUqDeMiG0)  \n33. Getting Started \\- GraphRAG \\- Microsoft Open Source, accessed October 15, 2025, [https://microsoft.github.io/graphrag/get\\_started/](https://microsoft.github.io/graphrag/get_started/)  \n34. Advanced RAG Techniques in AI Retrieval: A Deep Dive into the ..., accessed October 15, 2025, [https://medium.com/@LakshmiNarayana\\_U/advanced-rag-techniques-in-ai-retrieval-a-deep-dive-into-the-chroma-course-d8b06118cde3](https://medium.com/@LakshmiNarayana_U/advanced-rag-techniques-in-ai-retrieval-a-deep-dive-into-the-chroma-course-d8b06118cde3)  \n35. Advanced RAG Techniques \\- Guillaume Laforge, accessed October 15, 2025, [https://glaforge.dev/talks/2024/10/14/advanced-rag-techniques/](https://glaforge.dev/talks/2024/10/14/advanced-rag-techniques/)  \n36. Four data and model quality challenges tied to generative AI \\- Deloitte, accessed October 15"
  },
  {
    "id": "report_source",
    "chunk": "echniques/](https://glaforge.dev/talks/2024/10/14/advanced-rag-techniques/)  \n36. Four data and model quality challenges tied to generative AI \\- Deloitte, accessed October 15, 2025, [https://www.deloitte.com/us/en/insights/topics/digital-transformation/data-integrity-in-ai-engineering.html](https://www.deloitte.com/us/en/insights/topics/digital-transformation/data-integrity-in-ai-engineering.html)  \n37. Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.09136v1](https://arxiv.org/html/2501.09136v1)  \n38. Retrieval-Augmented Generation (RAG) with LangChain and Ollama \\- Medium, accessed October 15, 2025, [https://medium.com/@tschechd/retrieval-augmented-generation-rag-in-practice-implementing-a-chatbot-with-langchain-and-ollama-79d6d19642f7](https://medium.com/@tschechd/retrieval-augmented-generation-rag-in-practice-implementing-a-chatbot-with-langchain-and-ollama-79d6d19642f7)  \n39. Context Engineering \\- LangChain Blog, accessed October 15, 2025, [https://blog.langchain.com/context-engineering-for-agents/](https://blog.langchain.com/context-engineering-for-agents/)  \n40. Context Engineering Clearly Explained \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=jLuwLJBQkIs](https://www.youtube.com/watch?v=jLuwLJBQkIs)  \n41. crewAIInc/crewAI: Framework for orchestrating role-playing ... \\- GitHub, accessed October 15, 2025, [https://github.com/crewAIInc/crewAI](https://github.com/crewAIInc/crewAI)  \n42. (PDF) Cowboys and Aliens in the Digital Frontier: The Emergence of ..., accessed October 15, 2025, [https://www.researchgate.net/publication/380098993\\_Cowboys\\_and\\_Aliens\\_in\\_the\\_Digital\\_Frontier\\_The\\_Emergence\\_of\\_Tec"
  },
  {
    "id": "report_source",
    "chunk": "The Emergence of ..., accessed October 15, 2025, [https://www.researchgate.net/publication/380098993\\_Cowboys\\_and\\_Aliens\\_in\\_the\\_Digital\\_Frontier\\_The\\_Emergence\\_of\\_Techno-Social\\_Learning\\_in\\_AI-Enhanced\\_Communities\\_of\\_Practice](https://www.researchgate.net/publication/380098993_Cowboys_and_Aliens_in_the_Digital_Frontier_The_Emergence_of_Techno-Social_Learning_in_AI-Enhanced_Communities_of_Practice)  \n43. What is Human-in-the-Loop (HITL) in AI & ML? \\- Google Cloud, accessed October 15, 2025, [https://cloud.google.com/discover/human-in-the-loop](https://cloud.google.com/discover/human-in-the-loop)  \n44. What Is Human In The Loop (HITL)? \\- IBM, accessed October 15, 2025, [https://www.ibm.com/think/topics/human-in-the-loop](https://www.ibm.com/think/topics/human-in-the-loop)  \n45. Context Engineering : r/PromptEngineering \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1me36vh/context\\_engineering/](https://www.reddit.com/r/PromptEngineering/comments/1me36vh/context_engineering/)  \n46. The New Skill in AI is Not Prompting, It's Context Engineering : r/ArtificialInteligence \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/ArtificialInteligence/comments/1m973yp/the\\_new\\_skill\\_in\\_ai\\_is\\_not\\_prompting\\_its\\_context/](https://www.reddit.com/r/ArtificialInteligence/comments/1m973yp/the_new_skill_in_ai_is_not_prompting_its_context/)  \n47. Manage context window size with advanced AI agents | daily.dev, accessed October 15, 2025, [https://app.daily.dev/posts/manage-context-window-size-with-advanced-ai-agents-m2mil9dwq](https://app.daily.dev/posts/manage-context-window-size-with-advanced-ai-agents-m2mil9dwq)  \n48. Andrew-Jang/RAGHub: A community-driven colle"
  },
  {
    "id": "report_source",
    "chunk": "with-advanced-ai-agents-m2mil9dwq](https://app.daily.dev/posts/manage-context-window-size-with-advanced-ai-agents-m2mil9dwq)  \n48. Andrew-Jang/RAGHub: A community-driven collection of ... \\- GitHub, accessed October 15, 2025, [https://github.com/Andrew-Jang/RAGHub](https://github.com/Andrew-Jang/RAGHub)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/03-AI Research Proposal_ V2V Pathway.md\">\n\n\n# **From Vibecoding to Virtuosity: A Framework for Developer Mastery in the Age of Context Engineering**\n\n## **Part I: Defining the New Paradigm of AI-Driven Development**\n\nThe integration of Large Language Models (LLMs) into the software development lifecycle has catalyzed a profound transformation in how developers interact with technology. This shift has given rise to a spectrum of practices, ranging from nascent, intuition-driven experimentation to highly structured, architectural design. This report delineates a developmental journeythe 'Vibecoding to Virtuosity' (V2V) pathwaythat maps a developer's progression from novice exploration to systemic mastery. It establishes that this journey is not merely an accumulation of skills but a fundamental paradigm shift, culminating in the discipline of Context Engineering. This initial section defines the two poles of this pathway, characterizing the initial, widespread approach of 'Vibecoding' and contrasting it with the systematic discipline of 'Virtuosity,' which is the technical and philosophical embodiment of Context Engineering.\n\n### **The Age of 'Vibecoding': Intuition, Artistry, and Inefficiency**\n\nThe initial and most accessible mode of interaction with LLMs can be characterized as 'Vibecoding.' This approach represents a necessary but ultimately limited starting"
  },
  {
    "id": "report_source",
    "chunk": "ncy**\n\nThe initial and most accessible mode of interaction with LLMs can be characterized as 'Vibecoding.' This approach represents a necessary but ultimately limited starting point on the path to mastery, defined by its reliance on intuition, creative exploration, and conversational interaction.  \nAt its core, Vibecoding is a practice of interaction characterized by trial-and-error, linguistic intuition, and treating the LLM as a conversational partner rather than a deterministic system component.1 Developers in this phase engage in what has been described as an \"artful way to 'speak AI',\" combining curiosity and experimentation to coax desired outputs from the model.1 The process is often unstructured, relying on the developer's ability to \"vibe\" with the model and adjust their natural language inputs in an iterative, often unpredictable, fashion.3  \nA hallmark of this stage is the use of \"mega-prompts\"large, monolithic prompts that attempt to inject a vast amount of context, instructions, and examples into a single turn.4 These prompts are often complex, multi-part constructions assembled from various sources, designed to guide the AI through a complete task in one go.6 The seven pillars of a strong promptdefining the action, outlining steps, assigning a role, providing examples, offering context, adding constraints, and specifying the output formatare all packed into one comprehensive command.5 While this technique can produce impressive initial results and feels powerful in the moment, it is fundamentally brittle and suffers from low retention. The context provided in a mega-prompt is transient, existing only within the immediate conversational window; it is not committed to any form of durable memory, leading to"
  },
  {
    "id": "report_source",
    "chunk": "on. The context provided in a mega-prompt is transient, existing only within the immediate conversational window; it is not committed to any form of durable memory, leading to the common experience of the model \"forgetting\" the instructions in subsequent interactions.6  \nThe limitations of Vibecoding become apparent when moving from exploratory tasks to building robust, scalable applications. This approach is frequently described as a \"quick-and-dirty hack\" 8 that remains \"more art than science\".4 Its primary weaknesses are a profound lack of repeatability and scalability. When a mega-prompt fails, the debugging process is often reduced to simply rewording phrases and guessing what went wrong, rather than systematically inspecting a system's components.8 This makes it wholly unsuitable for production systems that demand predictability, consistency, and reliability across a multitude of users and edge cases.6 As applications grow in complexity, the Vibecoding approach begins to \"fall apart,\" revealing its inadequacy for building anything beyond simple, one-off tools or creative content.8\n\n### **The Emergence of 'Virtuosity': The Discipline of Context Engineering**\n\nThe destination of the V2V pathway is a state of mastery defined by systematic design, architectural thinking, and repeatable processes. This state, termed 'Virtuosity,' is achieved through the practice of Context Engineeringthe discipline of designing and managing the entire environment in which an LLM operates.  \nThe fundamental shift from Vibecoding to Virtuosity is a move from focusing on the \"surface input\" of a single prompt to architecting the \"entire environment\" of the LLM.9 Context Engineering is defined as the science and engineering of organizing, "
  },
  {
    "id": "report_source",
    "chunk": "g on the \"surface input\" of a single prompt to architecting the \"entire environment\" of the LLM.9 Context Engineering is defined as the science and engineering of organizing, assembling, and optimizing all forms of context fed into an LLM to maximize its performance.10 It is a paradigm shift away from merely considering *what to say* to the model at a specific moment, and toward meticulously designing *what the model knows* when you say it, and why that knowledge is relevant.8 This moves the developer's role from that of a prompt crafter to a systems architect.11  \nThis architectural approach is built upon several technical pillars that constitute the LLM's operational environment. These pillars transform the LLM from a standalone conversationalist into a component of a larger, more capable system.\n\n* **Dynamic Information and Tools:** A core principle of Context Engineering is providing the LLM with the right information and tools, in the right format, at the right time.11 This involves dynamically retrieving data from external sources such as knowledge bases, databases, and APIs at runtime, rather than attempting to stuff all possible information into a static prompt.13 Tools are well-defined functions that allow the agent to interact with its environment, extending its capabilities beyond text generation.15  \n* **Memory Systems:** To support stateful, multi-turn interactions, a virtuoso developer architects explicit memory systems. This includes short-term memory, such as the immediate conversation history and current task state, and long-term memory, which stores persistent information like user profiles, preferences, and past interactions across sessions.8 This allows an application, such as a customer support bot, "
  },
  {
    "id": "report_source",
    "chunk": "mory, which stores persistent information like user profiles, preferences, and past interactions across sessions.8 This allows an application, such as a customer support bot, to maintain context and provide personalized, consistent responses over time.  \n* **Retrieval-Augmented Generation (RAG):** RAG is identified as the \"foundational pattern of context engineering\".12 It is the primary mechanism for grounding the LLM in external, proprietary, or real-time information. By retrieving relevant document chunks from a vector database and injecting them into the context, RAG mitigates common LLM failure modes like hallucination, lack of domain-specific knowledge, and outdated information.16\n\nAchieving this level of systemic control requires a corresponding shift in the developer's mindset. The effort type transitions from \"creative writing or copy-tweaking\" to \"systems design or software architecture for LLMs\".8 It becomes a cross-functional discipline that necessitates a deep understanding of the business use case, the desired outputs, and the most effective way to structure and orchestrate information flows to guide the LLM toward its goal.11 Virtuosity is not about finding the perfect words; it is about building the perfect system.\n\n### **The Inevitable Evolution from Instruction to Architecture**\n\nThe transition from the ad-hoc artistry of Prompt Engineering (the practice underlying 'Vibecoding') to the systematic discipline of Context Engineering (the foundation of 'Virtuosity') is not an optional specialization for advanced developers. It is an inevitable and necessary evolution driven by the fundamental requirements of building reliable, scalable, and complex AI-powered applications. As an organization's ambitions mat"
  },
  {
    "id": "report_source",
    "chunk": "evitable and necessary evolution driven by the fundamental requirements of building reliable, scalable, and complex AI-powered applications. As an organization's ambitions mature from simple demonstrations to production-grade systems, the inherent limitations of the former paradigm force an adoption of the latter.  \nThe available evidence clearly establishes Prompt Engineering as the entry point into LLM interaction. It is described as \"how we started,\" the \"quick-and-dirty hack to bend LLMs to your will,\" and the \"artful way to 'speak AI'\" that characterized early experimentation.1 This positions it as a foundational but ultimately primitive stage, sufficient for one-off tasks, copywriting variations, and \"flashy demos\".8  \nHowever, the limitations of this stage are explicitly and inextricably linked to the challenges of scale, complexity, and reliability. The literature consistently notes that Prompt Engineering \"starts to fall apart when scaled\" because more users introduce more edge cases that brittle, monolithic prompts cannot handle.8 It is deemed insufficient for \"complex applications\" or \"long-running workflows and conversations with complex state\" that require memory and predictability.8  \nContext Engineering is consistently presented as the direct solution to these scaling and reliability challenges. It is defined as \"how we scale\" and the \"real design work behind reliable LLM-powered systems\".8 Its methodologies are explicitly designed for \"production systems that need predictability,\" \"multi-turn flows,\" and \"LLM agents with memory\".8 A clear causal relationship thus emerges: the desire to build more sophisticated AI applications creates engineering requirements (reliability, statefulness, scalability) that P"
  },
  {
    "id": "report_source",
    "chunk": "clear causal relationship thus emerges: the desire to build more sophisticated AI applications creates engineering requirements (reliability, statefulness, scalability) that Prompt Engineering cannot meet. This failure compels a shift in practice toward the architectural robustness of Context Engineering.  \nThis evolutionary path has profound implications for the definition of a senior AI developer. The core competency is no longer centered on linguistic creativity or the clever wordsmithing of prompts. Instead, it is converging with the traditional skills of a senior software engineer: systems architecture, data modeling, state management, API integration, and debugging complex, distributed systems. The 'Vibecoding to Virtuosity' pathway, therefore, is not just a map of LLM-specific skills; it is a map of how a developer acquires these timeless engineering competencies and applies them to the unique context of building with and around large language models. The journey from a prompt crafter to a context architect mirrors the journey from a scriptwriter to a systems engineer.\n\n## **Part II: The V2V Pathway \\- A Cognitive Apprenticeship Model**\n\nTo structure the developer's journey from the intuitive exploration of 'Vibecoding' to the systematic mastery of 'Virtuosity,' this report adopts the pedagogical framework of Cognitive Apprenticeship. Developed by Collins, Brown, and Newman, this model is designed to make the implicit thought processes of experts visible to novices, guiding them through a structured sequence of learning stages.19 Unlike traditional apprenticeships focused on physical skills, the cognitive model emphasizes the thinking processes behind expert performance.19 Its six stagesModeling, Coaching, Scaffo"
  },
  {
    "id": "report_source",
    "chunk": "tional apprenticeships focused on physical skills, the cognitive model emphasizes the thinking processes behind expert performance.19 Its six stagesModeling, Coaching, Scaffolding, Articulation, Reflection, and Explorationprovide a powerful framework for mapping the developer's progression. Each stage of the V2V pathway corresponds to an evolution in technical skills, a shift in the developer's cognitive model, and a maturation of the human-AI collaboration pattern.\n\n### **Stage 1: The Intuitive Explorer (Modeling Phase)**\n\nThe V2V journey begins with the Modeling phase, where the developer's primary learning mechanism is observation and imitation. The pedagogical goal is for the novice to witness an expert performing a task while verbalizing their thought process, making the invisible thinking skills visible.21 In the context of AI development, this often involves watching tutorials, reading blog posts, or experimenting with shared prompts to internalize the basic patterns of interaction.  \nDuring this stage, the developer's mindset is one of pure 'Vibecoding.' They engage with the LLM through natural language, using intuition and trial-and-error to discover its capabilities.2 The LLM is perceived as a powerful but somewhat magical \"black box,\" and the primary goal is to achieve a desired output in a single, self-contained interaction. This leads directly to the primary technical skill of this phase: **mega-prompting**. The developer learns to assemble large, context-rich prompts that bundle together role assignments, contextual information (priming), structural specifications, and examples in an attempt to comprehensively guide the AI in one shot.6 They master the \"seven pillars of prompt wisdom\"defining the action,"
  },
  {
    "id": "report_source",
    "chunk": "ing), structural specifications, and examples in an attempt to comprehensively guide the AI in one shot.6 They master the \"seven pillars of prompt wisdom\"defining the action, outlining steps, assigning a role, providing examples, context, constraints, and output formatbut apply them within a single, monolithic command.5  \nThe human-AI collaboration model at this stage is best described as **AI as a Tool**. The interaction is unidirectional and transactional: the developer provides a set of instructions, and the AI executes them.22 There is little to no sense of partnership; the human is the sole strategist and creator, and the AI is a sophisticated instrument for text generation.\n\n### **Stage 2: The Structured Apprentice (Coaching & Scaffolding Phase)**\n\nAs the developer moves beyond simple exploration, they enter the Coaching and Scaffolding phase. The pedagogical goal here is to begin practicing skills with direct guidance from an expert (coaching) and to use support structures (scaffolding) that reduce cognitive load and make complex tasks more manageable.19 In modern AI workflows, the AI itself can serve as a powerful scaffolding agent, providing hints, feedback, and adaptive support that enables the learner to complete tasks that would otherwise be beyond their reach.24  \nThis structured support enables a crucial shift in the developer's mindset toward **Computational Thinking**. Instead of treating the problem as a single conversational turn, they begin to apply principles of decomposition, pattern recognition, and algorithmic design.27 This is manifested in a move away from mega-prompts and toward \"task-driven\" or \"sequential\" prompting, where a complex problem is broken down into a series of smaller, discrete p"
  },
  {
    "id": "report_source",
    "chunk": " is manifested in a move away from mega-prompts and toward \"task-driven\" or \"sequential\" prompting, where a complex problem is broken down into a series of smaller, discrete prompts, with the output of one step often becoming the input for the next.4  \nThis cognitive shift is supported by and enables the acquisition of more advanced technical skills. The developer masters **In-Context Learning (ICL)**, also known as \"few-shot prompting.\" This involves strategically embedding a small number of high-quality, canonical examples of input-output pairs directly into the prompt to guide the model's behavior without needing to update its parameters.15 They also begin to implement **basic Retrieval-Augmented Generation (RAG)** patterns, building simple systems that retrieve information from an external document store to ground the LLM's responses, thereby addressing knowledge gaps and reducing the frequency of hallucinations.12 Furthermore, their interaction with AI for coding becomes more formalized through **Structured AI Pair Programming**. They adopt distinct roles, with the human acting as the \"Navigator\"setting the high-level strategy and architectural directionand the AI acting as the \"Driver\"generating the specific code implementations.33  \nThe human-AI collaboration model evolves to **AI as an Assistant**. The AI is no longer a passive tool but an active participant in the development process. It can suggest alternative approaches, refine code, and co-create solutions, all within a structured workflow that is still defined and controlled by the human developer.33\n\n### **Stage 3: The Systems Builder (Articulation & Reflection Phase)**\n\nThe third stage of the V2V pathway is defined by Articulation and Reflection. Here, "
  },
  {
    "id": "report_source",
    "chunk": "uman developer.33\n\n### **Stage 3: The Systems Builder (Articulation & Reflection Phase)**\n\nThe third stage of the V2V pathway is defined by Articulation and Reflection. Here, the pedagogical imperative is for the learner to explain their reasoning and compare their performance and processes to those of experts.21 This act of making one's own thought processes explicit forces a deeper, more systemic level of understanding. It is no longer enough to get the right answer; the developer must be able to articulate *why* their system produced that answer.  \nThis requirement drives a further evolution in the developer's cognitive model, moving toward **Machine Learning Thinking (MLT)** and **Generative Thinking (GenT)**. With MLT, the developer recognizes they are not just giving deterministic instructions but are guiding a probabilistic system that learns from data. They begin to think in terms of training data, bias, and model evaluation.34 With GenT, they embrace their role as a curator and refiner of AI-generated content, focusing on guiding the generative process and selecting the best outputs from a multitude of possibilities.34 This is reflected in a significant shift in their debugging practices. A problem is no longer solved by simply \"rewording a prompt\"; instead, debugging becomes a systematic process of \"inspecting the full context window, memory slots, and token flow\" to understand the complete state of the system at the point of failure.8  \nThis systemic mindset is necessary to master the technical skills of this stage. The developer moves to **Advanced RAG Pipelines**, implementing sophisticated techniques to optimize the retrieval process. This includes query transformations like HyDE (Hypothetical Document Embe"
  },
  {
    "id": "report_source",
    "chunk": " **Advanced RAG Pipelines**, implementing sophisticated techniques to optimize the retrieval process. This includes query transformations like HyDE (Hypothetical Document Embeddings) to improve query relevance, strategic document chunking (e.g., sentence-level vs. semantic chunking), and re-ranking retrieved documents to prioritize the most salient information.35 They also learn **Strategic Context Window Management**, moving beyond naive truncation to employ methods like hierarchical summarization, context compression, and strategically placing critical instructions at the beginning and end of the prompt to mitigate the \"lost-in-the-middle\" effect where models tend to ignore information in the center of a long context.15 At a higher level, they begin to practice **AI in the Software Development Lifecycle (SDLC)**, systematically integrating AI tools across all phases, from AI-assisted requirements analysis and design prototyping to automated testing, deployment, and maintenance.22  \nThe collaboration model matures into **Human-Centric Collaboration**. In this mode, the human is the clear leader and orchestrator of the development process. However, the AI is a deeply integrated and indispensable partner that provides critical data, automates complex sub-tasks, and actively shapes the workflow, acting on the human's strategic intent.46\n\n### **Stage 4: The Symbiotic Virtuoso (Exploration Phase)**\n\nThe final stage of the V2V pathway is Exploration, where the developer achieves a state of 'Virtuosity.' Having internalized the expert's mindset and mastered the core technical skills, the pedagogical goal is for the developer to solve novel problems independently and apply their knowledge to open-ended challenges, pushing the b"
  },
  {
    "id": "report_source",
    "chunk": "ed the core technical skills, the pedagogical goal is for the developer to solve novel problems independently and apply their knowledge to open-ended challenges, pushing the boundaries of what is possible with the technology.19  \nThe developer's mindset fully crystallizes into **Agentic Thinking**. They are no longer just collaborating with an AI to perform a task; they are *orchestrating* systems of autonomous AI agents that can plan, make decisions, and take actions to achieve complex, high-level goals.34 Their role elevates from a hands-on creator or editor to that of an architect and supervisor of intelligent systems, defining the objectives and constraints while delegating the execution to a team of AI agents.49  \nThe technical skills at this stage represent the pinnacle of Context Engineering. The virtuoso designs and implements **Agentic Workflows**, building multi-agent systems where specialized AI agents collaborate to perform sophisticated tasks like conducting deep research, autonomously developing software features, or creating and executing marketing campaigns.50 A key methodology at this level is **AI-Driven Test-Driven Development (TDD)**. This practice inverts the traditional coding process: the developer (or an AI agent) first generates a comprehensive suite of tests from natural language requirements. Then, a coding agent is tasked with writing the implementation code with the sole objective of making all tests pass. This creates a rapid, high-quality development loop where the tests provide an unambiguous specification and an immediate feedback mechanism.3 This culminates in **Spec-Driven Development**, a paradigm where a detailed, human-validated specification becomes the central source of truth for t"
  },
  {
    "id": "report_source",
    "chunk": "diate feedback mechanism.3 This culminates in **Spec-Driven Development**, a paradigm where a detailed, human-validated specification becomes the central source of truth for the entire project. From this spec, AI agents can autonomously generate the technical plan, the development tasks, the code, and the corresponding tests, ensuring perfect alignment and quality from inception to deployment.55  \nAt this zenith of mastery, the human-AI collaboration model becomes a **Symbiotic Partnership**. The human and AI operate as a tightly integrated hybrid intelligence. The human sets the strategic direction, defines the ultimate goals, and provides critical oversight and ethical judgment. The AI, or system of AIs, autonomously executes complex, multi-step plans, adapting its strategy based on real-time feedback. The relationship is bidirectional, dynamic, and mutually reinforcing, with each partner augmenting the other's capabilities.47\n\n### **The Symbiotic Relationship Between Pedagogy, Technology, and Cognition**\n\nThe V2V pathway is more than a simple linear progression of skills. It reveals a tightly coupled, co-evolutionary relationship where the pedagogical model (Cognitive Apprenticeship), the technical competencies (Context Engineering), and the developer's underlying cognitive framework (from Computational to Agentic Thinking) are deeply intertwined. Advancement in one area both enables and necessitates advancement in the others, creating a powerful, self-reinforcing feedback loop that drives the developer toward mastery.  \nThe journey begins with the pedagogical stage of **Modeling**, which is perfectly suited for the imitative and exploratory nature of **Vibecoding**. A novice developer observes expert prompts and atte"
  },
  {
    "id": "report_source",
    "chunk": "he pedagogical stage of **Modeling**, which is perfectly suited for the imitative and exploratory nature of **Vibecoding**. A novice developer observes expert prompts and attempts to replicate them, using the AI as a simple **Tool**. This is the natural entry point. However, to progress, the developer requires **Coaching and Scaffolding**. These pedagogical supports are technically instantiated by methodologies like In-Context Learning, which scaffolds understanding by providing clear examples, and basic RAG, which scaffolds the LLM's knowledge with external information. The availability of this technical scaffolding makes it possible for the developer to adopt a more structured **Computational Thinking** approach, breaking problems down into manageable, sequential steps.  \nTo advance to the next stage, the developer must learn to **Articulate** their reasoning and **Reflect** on their process. This is impossible if the system remains a black box. This pedagogical demand drives the need to learn the internals of **Advanced RAG pipelines** and **Context Window Management**. The very act of debugging these complex, probabilistic systemsdiagnosing issues like context poisoning or retrieval failuresforces the developer to abandon a purely deterministic mindset and adopt a **Generative and Machine Learning Thinking** model. They are now reasoning about a data-driven system, not just a set of instructions.  \nFinally, to reach the state of Virtuosity and engage in true **Exploration**, the developer must have achieved a deep mastery of the underlying systems. This mastery enables them to design novel **Agentic Workflows** and employ sophisticated methodologies like **AI-driven TDD**. These tasks require the highest level of c"
  },
  {
    "id": "report_source",
    "chunk": "tems. This mastery enables them to design novel **Agentic Workflows** and employ sophisticated methodologies like **AI-driven TDD**. These tasks require the highest level of cognitive abstraction: **Agentic Thinking**, where the developer is no longer a direct participant but an orchestrator of autonomous systems.  \nThis interconnected progression demonstrates that training programs for AI developers must be holistic. They cannot treat pedagogical strategy, technical tooling, and cognitive skill development as separate domains. The pedagogical framework provides the structure to learn the technology. The technology, once learned, enables and necessitates a more advanced cognitive model. This cyclewhere pedagogy enables technology, and technology demands a new way of thinkingis the fundamental dynamic that propels a developer along the V2V pathway.\n\n### **The V2V Pathway Matrix**\n\nThe following table provides a consolidated overview of the Vibecoding to Virtuosity pathway, mapping each developmental stage to its corresponding mindset, key technical skills, dominant collaboration model, and core pedagogical support. This matrix serves as a high-level schematic for the entire framework, offering a clear rubric for assessing developer capabilities and charting a deliberate course for professional growth.\n\n| V2V Stage | Primary Mindset / Cognitive Model | Key Technical Skills & Methodologies | Dominant Human-AI Collaboration Model | Core Pedagogical Support |\n| :---- | :---- | :---- | :---- | :---- |\n| **1\\. Intuitive Explorer** | **Vibecoding** (Intuitive, Ad-Hoc) | Prompt Crafting, Mega-Prompting 5 | **AI as Tool** (Unidirectional command) | **Modeling** (Observing experts) |\n| **2\\. Structured Apprentice** | **Computatio"
  },
  {
    "id": "report_source",
    "chunk": "tive, Ad-Hoc) | Prompt Crafting, Mega-Prompting 5 | **AI as Tool** (Unidirectional command) | **Modeling** (Observing experts) |\n| **2\\. Structured Apprentice** | **Computational Thinking** (Decomposition, Sequencing) | ICL/Few-Shot 32, Basic RAG 12, Structured Pair Programming 33, Sequential Prompting 7 | **AI as Assistant** (Guided co-creation) | **Coaching & Scaffolding** (Guided practice) |\n| **3\\. Systems Builder** | **ML & Generative Thinking** (Guiding, Curating) | Advanced RAG 35, Context Window Management 40, AI in SDLC 43 | **Human-Centric Collaboration** (Human orchestrates) | **Articulation & Reflection** (Explaining the 'why') |\n| **4\\. Symbiotic Virtuoso** | **Agentic Thinking** (Orchestrating Autonomy) | AI-driven TDD 52, Agentic Workflows 50, Spec-Driven Development 55, Systems Design | **Symbiotic Partnership** (Bidirectional, adaptive) | **Exploration & Deliberate Practice** |\n\n## **Part III: The Principles of Deliberate Practice for AI Virtuosity**\n\nWhile the Cognitive Apprenticeship model provides the essential map for the V2V pathway, the principles of Deliberate Practice, as established by the research of Anders Ericsson, provide the engine for progression. Deliberate Practice is a specific and highly structured form of practice aimed at improving performance, distinct from mere repetition or \"naive practice\".57 By adapting these principles to the unique context of AI engineering, developers can consciously and systematically accelerate their journey toward virtuosity. This section operationalizes the V2V journey by outlining how to apply these core principles to the acquisition of Context Engineering skills.\n\n### **Principle 1: Setting Specific, Measurable Goals**\n\nThe first principle of Deliberate"
  },
  {
    "id": "report_source",
    "chunk": "ow to apply these core principles to the acquisition of Context Engineering skills.\n\n### **Principle 1: Setting Specific, Measurable Goals**\n\nThe first principle of Deliberate Practice dictates that improvement requires well-defined, specific goals rather than vague aspirations like \"get better at prompting\".57 For a developer on the V2V pathway, this means setting concrete, measurable objectives that are tied to the technical skills of each stage. These goals provide a clear target for practice and an objective benchmark for success.  \nFor example, a developer's goals could be structured according to their current stage in the V2V framework:\n\n* **Stage 2 (Structured Apprentice) Goal:** \"Implement a basic RAG system using our internal documentation that can accurately answer at least 80% of the top 20 most frequent Tier 1 support questions, as measured by a blind evaluation from the support team.\" This goal is specific (RAG on internal docs), measurable (80% accuracy on top 20 questions), and relevant to the skills of that stage.  \n* **Stage 3 (Systems Builder) Goal:** \"Reduce the average end-to-end latency of our existing RAG pipeline by 15%, from 2.5 seconds to \\~2.1 seconds, by experimenting with and optimizing document chunking strategies and implementing a more efficient re-ranking model.\" This goal targets a specific performance metric and focuses on the advanced optimization skills of Stage 3\\.  \n* **Stage 4 (Symbiotic Virtuoso) Goal:** \"Build an autonomous agent that can successfully execute a 'spec-to-code' workflow for a new API endpoint. The goal is for the agent to generate both the implementation code and the corresponding unit tests, achieving a 95% test pass rate on the first attempt with no human interven"
  },
  {
    "id": "report_source",
    "chunk": "The goal is for the agent to generate both the implementation code and the corresponding unit tests, achieving a 95% test pass rate on the first attempt with no human intervention in the code generation step.\" This sets a high bar for an agentic system, requiring mastery of the most advanced skills.\n\n### **Principle 2: Intense Focus and Escaping the Comfort Zone**\n\nDeliberate Practice is, by definition, mentally demanding. It requires intense focus and consistently pushing oneself beyond one's current capabilities into a zone of productive discomfort.59 For the AI developer, this means actively moving away from the comfortable and familiar patterns of \"vibe coding\" and engaging directly with the most challenging and complex aspects of Context Engineering.  \nThis involves a conscious effort to tackle difficult problems head-on. Instead of avoiding long documents, a developer in this mode would intentionally work on tasks that force them to confront the \"lost-in-the-middle\" problem, experimenting with techniques like summarization and strategic prompt structuring to ensure the model utilizes the entire context.40 Rather than sticking to simple RAG implementations, they would seek out use cases that are prone to \"context poisoning\"where irrelevant retrieved information confuses the modeland practice designing more robust retrieval and filtering mechanisms.16 For those at the Virtuoso stage, this means designing and debugging complex, multi-step agentic systems, focusing on building robust error handling, recovery mechanisms, and validation checks to ensure the agent's autonomous actions remain aligned with the user's intent.33 This sustained, focused effort on the edge of one's ability is what drives meaningful skill impr"
  },
  {
    "id": "report_source",
    "chunk": "ensure the agent's autonomous actions remain aligned with the user's intent.33 This sustained, focused effort on the edge of one's ability is what drives meaningful skill improvement.\n\n### **Principle 3: Immediate and Informative Feedback**\n\nThe most critical principle of Deliberate Practice is the need for a continuous loop of immediate and informative feedback. A practitioner must know, in real-time, whether their actions are correct and, if not, precisely how they are wrong.57 This is where modern, AI-native development workflows offer a revolutionary advantage over traditional learning methods, providing feedback loops that are tighter, faster, and more objective than ever before.  \n**AI-Driven Test-Driven Development (TDD)** stands out as the ultimate feedback mechanism for the AI developer. The classic Red-Green-Refactor cycle of TDD provides an immediate, binary, and unambiguous feedback signal: the test either passes or it fails.3 This transforms the abstract goal of \"writing good code\" into a concrete, measurable task. A developer can practice implementing a feature, receive instant validation from the automated test suite, and then confidently refactor the code, knowing that the tests act as a safety net against regressions.54 This cycle perfectly instantiates a deliberate practice loop, allowing for rapid iteration and correction.  \n**AI Pair Programming** also provides a powerful, real-time feedback channel. By adopting the structured \"Navigator\" (human) and \"Driver\" (AI) roles, the developer receives immediate feedback on their strategic and architectural decisions.33 When the human Navigator outlines a plan, the code generated by the AI Driver serves as an instant reflection of that plan's clarity and feasi"
  },
  {
    "id": "report_source",
    "chunk": "c and architectural decisions.33 When the human Navigator outlines a plan, the code generated by the AI Driver serves as an instant reflection of that plan's clarity and feasibility. If the AI produces incorrect or inefficient code, it provides an immediate signal that the Navigator's instructions were ambiguous or flawed, allowing for rapid clarification and iteration.\n\n### **Principle 4: Repetition and Refinement**\n\nFinally, mastery is not achieved through a single success but through repeated application of skills with a constant focus on refinement and improvement.59 In the context of AI development, this means moving beyond one-off projects and embracing a methodology of continuous improvement and the creation of reusable assets.  \nThis principle manifests in several key practices. It involves not just building one RAG pipeline, but building several for a variety of use casessuch as question-answering, summarization, and conversational agentsand, after each implementation, reflecting on the process to refine the architecture for the next iteration.12 It encourages the development of **prompt libraries**, where high-performing, reusable prompts are stored, versioned, and shared across teams, transforming a successful prompt from a personal \"hack\" into a reliable organizational asset.1 Most importantly, it fosters the mindset of treating **context as a product**. This involves applying rigorous software engineering principles to the components of the AI's environment: version-controlling system prompts, creating quality checks for retrieved data, and continuously monitoring and benchmarking the performance of the entire context assembly system.12 This disciplined approach ensures that learning is cumulative and that"
  },
  {
    "id": "report_source",
    "chunk": "a, and continuously monitoring and benchmarking the performance of the entire context assembly system.12 This disciplined approach ensures that learning is cumulative and that the quality of the organization's AI systems improves systematically over time.\n\n### **TDD as the Engine of Deliberate Practice in AI Development**\n\nWithin the domain of AI-driven software development, Test-Driven Development (TDD) transcends its traditional role as a quality assurance methodology. It becomes the primary mechanism for enabling Deliberate Practice. It achieves this by transforming the abstract and often subjective process of coding into a concrete, repeatable, and measurable feedback loop that is essential for rapid and effective skill acquisition.  \nThe foundational requirement of Deliberate Practice is the availability of \"continuous feedback on results\".59 Without this feedback, practice remains \"naive\" and does not lead to significant improvement; a developer may repeat the same mistakes without realizing it.57 However, the nature of LLM-generated code presents a unique challenge to traditional feedback mechanisms. LLMs are non-deterministic and have been shown to \"cheat\" by generating code that passes a specific test case without correctly implementing the underlying general logic.62 This makes post-hoc testing a less reliable feedback mechanism for evaluating the developer's *process* of guiding the AI.  \nTDD fundamentally inverts this dynamic and resolves the feedback problem. The process begins with the developer defining the desired behavior first, by writing a test that is designed to fail (the \"Red\" phase).61 This initial act is itself a form of deliberate practice, forcing the developer to hone the skill of precise, unam"
  },
  {
    "id": "report_source",
    "chunk": "writing a test that is designed to fail (the \"Red\" phase).61 This initial act is itself a form of deliberate practice, forcing the developer to hone the skill of precise, unambiguous specification. The AI is then tasked with a clear, singular goal: write the minimum amount of code required to make the failing test pass (the \"Green\" phase). The result of running the testa binary pass or failprovides an objective, non-negotiable, and immediate feedback signal on the quality of both the developer's specification (the test) and the AI's generated code. Finally, the \"Refactor\" phase allows the developer to practice the crucial skill of improving code design and maintainability, using the comprehensive test suite as a safety net to ensure that no functionality is broken in the process.  \nThis Red-Green-Refactor cycle directly maps to the core components of Deliberate Practice. It provides a specific goal (pass the test), requires intense focus (writing only the code necessary), and, most critically, delivers an immediate and informative feedback loop (the test result). This causal link establishes that for an organization aiming to cultivate virtuosity in its developers, the adoption of AI-driven TDD is not merely a best practice for production code. It is the central pedagogical tool for developer training and skill acceleration. The infrastructure that enables these rapid, test-based feedback loops is as vital to fostering mastery as access to the LLMs themselves.\n\n## **Part IV: Strategic Implementation and Future Outlook**\n\nThe 'Vibecoding to Virtuosity' pathway provides a comprehensive model for understanding and cultivating developer mastery in the age of AI. To translate this framework from a theoretical construct into"
  },
  {
    "id": "report_source",
    "chunk": "ity' pathway provides a comprehensive model for understanding and cultivating developer mastery in the age of AI. To translate this framework from a theoretical construct into a practical organizational advantage, a strategic and deliberate implementation plan is required. This concluding section synthesizes the report's findings into a set of actionable recommendations for aiascent.dev. It outlines a blueprint for creating an environment that actively fosters progression along the V2V pathway and provides an outlook on the future of human-AI collaboration in software development, where the principles of Context Engineering and symbiotic partnership become the standard.\n\n### **A Blueprint for Fostering Virtuosity**\n\nTo systematically move developers from intuition-driven exploration to architectural mastery, organizations must architect their training, tooling, and culture around the principles of the V2V framework. The following recommendations provide a strategic blueprint for this transformation.\n\n* **Formalize the V2V Pathway:** The first step is to officially adopt the V2V framework as the internal model for AI developer progression. This involves creating an internal \"V2V Playbook,\" based on the findings of this report, to be integrated into key organizational processes. This playbook should serve as a guide for onboarding new developers, structuring ongoing training programs, and informing performance reviews and career ladder definitions. By making the pathway explicit, the organization provides a clear map for growth and sets unambiguous expectations for what constitutes seniority and mastery.  \n* **Structure Training as a Cognitive Apprenticeship:** Learning programs should be redesigned to mirror the stages of"
  },
  {
    "id": "report_source",
    "chunk": "pectations for what constitutes seniority and mastery.  \n* **Structure Training as a Cognitive Apprenticeship:** Learning programs should be redesigned to mirror the stages of the V2V pathway. Initial training should focus on **Modeling**, where junior developers observe experts conducting live-coding sessions that demonstrate advanced Context Engineering workflows. This should be followed by **Coached** projects where developers practice these skills with support from scaffolding tools, such as pre-built RAG components or standardized prompt templates that reduce initial complexity. Training should culminate in capstone projects that require **Exploration** and the design of novel, agentic systems, allowing developers to apply their skills to open-ended, real-world problems.64  \n* **Invest in a Deliberate Practice Infrastructure:** An organization must prioritize the development and adoption of tools that facilitate the rapid, high-quality feedback loops essential for Deliberate Practice. This means investing in Integrated Development Environments (IDEs) that have seamless, first-class support for **AI-driven Test-Driven Development**, allowing a developer to move through the Red-Green-Refactor cycle with minimal friction.53 It also requires establishing platforms and protocols for **AI pair programming** that enforce the structured Navigator/Driver roles, ensuring that the collaboration is a disciplined practice rather than an ad-hoc conversation.33  \n* **Promote a Culture of Systems Thinking:** A cultural shift is necessary to support the V2V pathway. Leadership and peer review processes should evolve to celebrate not just clever \"prompt hacks\" or impressive one-off demos, but robust, well-documented, and reusable Con"
  },
  {
    "id": "report_source",
    "chunk": "hway. Leadership and peer review processes should evolve to celebrate not just clever \"prompt hacks\" or impressive one-off demos, but robust, well-documented, and reusable Context Engineering solutions. This involves championing the practice of **treating context as a product**a critical piece of infrastructure that is version-controlled, subjected to quality assurance checks, and continuously improved over time.12 This cultural emphasis signals that true value lies in building scalable, maintainable systems, not in transient conversational tricks.\n\n### **The Future of Human-AI Development: The Symbiotic Team**\n\nExtrapolating from the trends and methodologies identified in this report, the future of software development points toward an increasingly integrated and symbiotic relationship between human developers and AI systems. The role of the virtuoso developer will continue to shift up the stack of abstraction, focusing less on implementation details and more on strategic design and system-level orchestration.  \nThe evolution toward **AI-Native Software Development Lifecycles (SDLCs)** is already underway. Methodologies like the AI-Driven Development Lifecycle (AI-DLC) re-imagine the entire process, positioning AI not as an add-on tool but as a central collaborator that initiates and directs workflows.56 In such a model, the AI generates the initial project plan, breaks it down into tasks, writes the code and tests, and manages deployment, constantly seeking clarification and validation from a team of human experts who provide oversight and strategic guidance.  \nThis leads to a future where development moves **from code generation to system generation**. The primary role of the virtuoso developer will no longer be to w"
  },
  {
    "id": "report_source",
    "chunk": "gic guidance.  \nThis leads to a future where development moves **from code generation to system generation**. The primary role of the virtuoso developer will no longer be to write lines of code, but to create and refine the high-level specifications that guide autonomous AI agents.55 The developer's core task becomes defining the \"what\" and the \"why\" with precision and clarity, and then validating that the complex systems generated by the AI agents correctly and robustly fulfill that specification.  \nDespite this massive automation of the development process, the value of **uniquely human cognition** will not diminish; it will become more critical than ever. As AI handles the mechanical and tactical aspects of coding, the premium will be on skills that AI cannot replicate: deep domain expertise, nuanced understanding of user needs, ethical reasoning, creative problem-framing, and the critical thinking required to question and validate the outputs of an AI system.46 The virtuoso of the future is the ultimate \"human-in-the-loop,\" operating at the highest level of strategic abstraction and ensuring that the powerful autonomous systems being built are aligned with human values and goals.\n\n### **Final Analysis: Organizational Learning as a Competitive Advantage**\n\nIn the rapidly evolving landscape of artificial intelligence, the primary and most durable competitive advantage for a technology organization will not be privileged access to foundational models or proprietary data. Instead, it will be the organization's capacity to accelerate the collective journey of its developers along the 'Vibecoding to Virtuosity' pathway. The speed at which an organization, as a whole, learns to collaborate effectively with AI will be the ul"
  },
  {
    "id": "report_source",
    "chunk": "ourney of its developers along the 'Vibecoding to Virtuosity' pathway. The speed at which an organization, as a whole, learns to collaborate effectively with AI will be the ultimate determinant of its success.  \nThe evidence is clear that even the most capable AI models underperform significantly when provided with incomplete or poorly structured context.12 This fundamental truth means that the value of an AI system is unlocked not by the raw power of the model itself, but by the skill of the developer who architects its environment. The V2V pathway demonstrates that this skill is not a simple trick to be learned, but a complex, multi-layered competency that requires simultaneous shifts in technical methodology, pedagogical support, and cognitive frameworks.  \nThe principles of Cognitive Apprenticeship and Deliberate Practice are not merely academic concepts; they are proven, structured methods for accelerating this complex learning process. Therefore, an organization that systematically implements these learning frameworksby building a supportive culture, designing effective training programs, and investing in the right tooling for rapid feedbackwill enable its developers to progress from Vibecoding to Virtuosity far more quickly and effectively than its competitors.  \nThis leads to a final, critical conclusion: the role of R\\&D and engineering leadership must expand beyond technical strategy to include the intentional design of organizational learning systems. The primary function of a technical strategist in the age of AI is to architect an environment where the V2V pathway is not an accidental journey for a talented few, but a deliberate, supported, and accelerated progression for the entire engineering organizatio"
  },
  {
    "id": "report_source",
    "chunk": "ironment where the V2V pathway is not an accidental journey for a talented few, but a deliberate, supported, and accelerated progression for the entire engineering organization. This is the ultimate form of Context Engineeringengineering the context for human learning and mastery.\n\n#### **Works cited**\n\n1. The Evolution of Prompt Engineering: The Brain of Agentic AI Systems \\- Inclusion Cloud, accessed October 15, 2025, [https://inclusioncloud.com/insights/blog/the-evolution-of-prompt-engineering/](https://inclusioncloud.com/insights/blog/the-evolution-of-prompt-engineering/)  \n2. Prompt engineering \\- Wikipedia, accessed October 15, 2025, [https://en.wikipedia.org/wiki/Prompt\\_engineering](https://en.wikipedia.org/wiki/Prompt_engineering)  \n3. The complete guide for TDD with LLMs | by Rogrio Chaves | Medium, accessed October 15, 2025, [https://rchavesferna.medium.com/the-complete-guide-for-tdd-with-llms-1dfea9041998](https://rchavesferna.medium.com/the-complete-guide-for-tdd-with-llms-1dfea9041998)  \n4. Megaprompt vs Task Driven Prompting Ep.049 \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=T1g5eHV\\_rYE](https://www.youtube.com/watch?v=T1g5eHV_rYE)  \n5. Feeding the Beast: A Developer's Guide to Data Prep and Mega-Prompting for AI Code Assistants, accessed October 15, 2025, [http://flaming.codes/posts/feeding-the-beast-developers-guide-data-prep-mega-prompting-ai](http://flaming.codes/posts/feeding-the-beast-developers-guide-data-prep-mega-prompting-ai)  \n6. Mega prompts \\- do they work? : r/ChatGPTPro \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/ChatGPTPro/comments/1ley49z/mega\\_prompts\\_do\\_they\\_work/](https://www.reddit.com/r/ChatGPTPro/comments/1ley49z/mega_prompts_do_t"
  },
  {
    "id": "report_source",
    "chunk": " October 15, 2025, [https://www.reddit.com/r/ChatGPTPro/comments/1ley49z/mega\\_prompts\\_do\\_they\\_work/](https://www.reddit.com/r/ChatGPTPro/comments/1ley49z/mega_prompts_do_they_work/)  \n7. Manuel\\_PROMPTING\\_engl.docx, accessed October 15, 2025, [https://www.unileoben.ac.at/fileadmin/shares/ctl/Word\\_Dateien/Manuel\\_PROMPTING\\_engl.docx](https://www.unileoben.ac.at/fileadmin/shares/ctl/Word_Dateien/Manuel_PROMPTING_engl.docx)  \n8. Context Engineering vs Prompt Engineering | by Mehul Gupta | Data Science in Your Pocket, accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d)  \n9. nearform.com, accessed October 15, 2025, [https://nearform.com/digital-community/beyond-prompt-engineering-the-shift-to-context-engineering/\\#:\\~:text=Prompt%20engineering%2C%20the%20art%20of,%2C%20tools%2C%20and%20retrieval%20systems.](https://nearform.com/digital-community/beyond-prompt-engineering-the-shift-to-context-engineering/#:~:text=Prompt%20engineering%2C%20the%20art%20of,%2C%20tools%2C%20and%20retrieval%20systems.)  \n10. www.marktechpost.com, accessed October 15, 2025, [https://www.marktechpost.com/2025/08/03/a-technical-roadmap-to-context-engineering-in-llms-mechanisms-benchmarks-and-open-challenges/\\#:\\~:text=Context%20Engineering%20is%20defined%20as,%2C%20and%20real%2Dworld%20application.](https://www.marktechpost.com/2025/08/03/a-technical-roadmap-to-context-engineering-in-llms-mechanisms-benchmarks-and-open-challenges/#:~:text=Context%20Engineering%20is%20defined%20as,%2C%20and%20real%2Dworld%20application.)  \n11. The New Skill in AI is Not Prompting, It"
  },
  {
    "id": "report_source",
    "chunk": "nisms-benchmarks-and-open-challenges/#:~:text=Context%20Engineering%20is%20defined%20as,%2C%20and%20real%2Dworld%20application.)  \n11. The New Skill in AI is Not Prompting, It's Context Engineering, accessed October 15, 2025, [https://www.philschmid.de/context-engineering](https://www.philschmid.de/context-engineering)  \n12. What is Context Engineering? The New Foundation for Reliable AI and RAG Systems, accessed October 15, 2025, [https://datasciencedojo.com/blog/what-is-context-engineering/](https://datasciencedojo.com/blog/what-is-context-engineering/)  \n13. What is Context Engineering, Anyway? \\- Zep, accessed October 15, 2025, [https://blog.getzep.com/what-is-context-engineering/](https://blog.getzep.com/what-is-context-engineering/)  \n14. Context Engineering vs. Prompt Engineering: Smarter AI with RAG & Agents \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=vD0E3EUb8-8](https://www.youtube.com/watch?v=vD0E3EUb8-8)  \n15. Effective context engineering for AI agents \\- Anthropic, accessed October 15, 2025, [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)  \n16. What is Context Engineering? \\- Elasticsearch Labs, accessed October 15, 2025, [https://www.elastic.co/search-labs/blog/context-engineering-overview](https://www.elastic.co/search-labs/blog/context-engineering-overview)  \n17. What is Context Engineering for LLMs? | by Tahir | Medium, accessed October 15, 2025, [https://medium.com/@tahirbalarabe2/%EF%B8%8F-what-is-context-engineering-for-llms-90109f856c1c](https://medium.com/@tahirbalarabe2/%EF%B8%8F-what-is-context-engineering-for-llms-90109f856c1c)  \n18. A Gentle Introduc"
  },
  {
    "id": "report_source",
    "chunk": "8%8F-what-is-context-engineering-for-llms-90109f856c1c](https://medium.com/@tahirbalarabe2/%EF%B8%8F-what-is-context-engineering-for-llms-90109f856c1c)  \n18. A Gentle Introduction to Context Engineering in LLMs \\- KDnuggets, accessed October 15, 2025, [https://www.kdnuggets.com/a-gentle-introduction-to-context-engineering-in-llms](https://www.kdnuggets.com/a-gentle-introduction-to-context-engineering-in-llms)  \n19. What Is the Cognitive Apprenticeship Model of Teaching and Its Use ..., accessed October 15, 2025, [https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning](https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning)  \n20. Cognitive Apprenticeship and Instructional Technology \\- DTIC, accessed October 15, 2025, [https://apps.dtic.mil/sti/tr/pdf/ADA203609.pdf](https://apps.dtic.mil/sti/tr/pdf/ADA203609.pdf)  \n21. Understanding the Cognitive Apprenticeship Framework for Smarter Learning \\- Pooks.ai, accessed October 15, 2025, [https://www.pooks.ai/posts/understanding-the-cognitive-apprenticeship-framework-for-smarter-learning.html](https://www.pooks.ai/posts/understanding-the-cognitive-apprenticeship-framework-for-smarter-learning.html)  \n22. AI in Software Development \\- IBM, accessed October 15, 2025, [https://www.ibm.com/think/topics/ai-in-software-development](https://www.ibm.com/think/topics/ai-in-software-development)  \n23. Generative AI Meets Cognitive Apprenticeship \\- The EvoLLLution, accessed October 15, 2025, [https://evolllution.com/technology/tech-tools-and-resources/using-ai-and-cognitive-apprenticeships-to-upskill-and-retool-adult-learners](https://evolllution.com/technology/tech-tools-and-resources/using-ai-and-cogni"
  },
  {
    "id": "report_source",
    "chunk": "tools-and-resources/using-ai-and-cognitive-apprenticeships-to-upskill-and-retool-adult-learners](https://evolllution.com/technology/tech-tools-and-resources/using-ai-and-cognitive-apprenticeships-to-upskill-and-retool-adult-learners)  \n24. Developing Alice: A Scaffolding Agent for AI-Mediated Computational Thinking \\- HKU Scholars Hub, accessed October 15, 2025, [https://hub.hku.hk/bitstream/10722/357791/1/content.pdf?accept=1](https://hub.hku.hk/bitstream/10722/357791/1/content.pdf?accept=1)  \n25. www.txdla.org, accessed October 15, 2025, [https://www.txdla.org/scaffolding-for-ai/\\#:\\~:text=Scaffolding%20Applied%20to%20AI%20Instruction\\&text=Begin%20with%20Basic%20Prompts%3A%20Introduce,%2C%20comparisons%2C%20or%20deeper%20explanations.](https://www.txdla.org/scaffolding-for-ai/#:~:text=Scaffolding%20Applied%20to%20AI%20Instruction&text=Begin%20with%20Basic%20Prompts%3A%20Introduce,%2C%20comparisons%2C%20or%20deeper%20explanations.)  \n26. The effects of artificial intelligence-based interactive scaffolding on ..., accessed October 15, 2025, [https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2470319](https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2470319)  \n27. Computational Thinking: Be Empowered for the AI Age, accessed October 15, 2025, [https://www.computationalthinking.org/](https://www.computationalthinking.org/)  \n28. Leveraging Computational Thinking in the Era of Generative AI, accessed October 15, 2025, [https://cacm.acm.org/blogcacm/leveraging-computational-thinking-in-the-era-of-generative-ai/](https://cacm.acm.org/blogcacm/leveraging-computational-thinking-in-the-era-of-generative-ai/)  \n29. Why Learn to Code in the Age of Artificial Intelligence? | Codelearn.com, accessed October 15, 20"
  },
  {
    "id": "report_source",
    "chunk": "logcacm/leveraging-computational-thinking-in-the-era-of-generative-ai/)  \n29. Why Learn to Code in the Age of Artificial Intelligence? | Codelearn.com, accessed October 15, 2025, [https://codelearn.com/blog/why-learn-to-code-in-the-age-of-ai/](https://codelearn.com/blog/why-learn-to-code-in-the-age-of-ai/)  \n30. What is In-context Learning, and how does it work: The Beginner's ..., accessed October 15, 2025, [https://www.lakera.ai/blog/what-is-in-context-learning](https://www.lakera.ai/blog/what-is-in-context-learning)  \n31. What is In-Context Learning? How LLMs Learn From ICL Examples \\- PromptLayer Blog, accessed October 15, 2025, [https://blog.promptlayer.com/what-is-in-context-learning/](https://blog.promptlayer.com/what-is-in-context-learning/)  \n32. In Context Learning Guide \\- PromptHub, accessed October 15, 2025, [https://www.prompthub.us/blog/in-context-learning-guide](https://www.prompthub.us/blog/in-context-learning-guide)  \n33. Best practices for pair programming with AI assistants \\- Graphite, accessed October 15, 2025, [https://graphite.dev/guides/ai-pair-programming-best-practices](https://graphite.dev/guides/ai-pair-programming-best-practices)  \n34. From Computational to Agentic: Rethinking How Students Solve ..., accessed October 15, 2025, [https://medium.com/@antonioskarampelas/from-computational-to-agentic-rethinking-how-students-solve-problems-in-the-age-of-ai-adbc916edf96](https://medium.com/@antonioskarampelas/from-computational-to-agentic-rethinking-how-students-solve-problems-in-the-age-of-ai-adbc916edf96)  \n35. Best Practices for RAG Pipelines | Medium, accessed October 15, 2025, [https://masteringllm.medium.com/best-practices-for-rag-pipeline-8c12a8096453](https://masteringllm.medium.com/best-pr"
  },
  {
    "id": "report_source",
    "chunk": "s for RAG Pipelines | Medium, accessed October 15, 2025, [https://masteringllm.medium.com/best-practices-for-rag-pipeline-8c12a8096453](https://masteringllm.medium.com/best-practices-for-rag-pipeline-8c12a8096453)  \n36. Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2506.00054v1](https://arxiv.org/html/2506.00054v1)  \n37. Searching for Best Practices in Retrieval-Augmented Generation \\- ACL Anthology, accessed October 15, 2025, [https://aclanthology.org/2024.emnlp-main.981.pdf](https://aclanthology.org/2024.emnlp-main.981.pdf)  \n38. Searching for Best Practices in Retrieval-Augmented Generation \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2407.01219v1](https://arxiv.org/html/2407.01219v1)  \n39. Enhancing Retrieval-Augmented Generation: A Study of Best Practices, accessed October 15, 2025, [https://arxiv.org/abs/2501.07391](https://arxiv.org/abs/2501.07391)  \n40. 6 Techniques You Should Know to Manage Context Lengths in LLM Apps \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LLMDevs/comments/1mviv2a/6\\_techniques\\_you\\_should\\_know\\_to\\_manage\\_context/](https://www.reddit.com/r/LLMDevs/comments/1mviv2a/6_techniques_you_should_know_to_manage_context/)  \n41. LLM Prompt Best Practices for Large Context Windows \\- Winder.AI, accessed October 15, 2025, [https://winder.ai/llm-prompt-best-practices-large-context-windows/](https://winder.ai/llm-prompt-best-practices-large-context-windows/)  \n42. Quality over Quantity: 3 Tips for Context Window Management \\- Tilburg.ai, accessed October 15, 2025, [https://tilburg.ai/2025/03/context-window-management/](https://tilburg.ai/202"
  },
  {
    "id": "report_source",
    "chunk": "ty over Quantity: 3 Tips for Context Window Management \\- Tilburg.ai, accessed October 15, 2025, [https://tilburg.ai/2025/03/context-window-management/](https://tilburg.ai/2025/03/context-window-management/)  \n43. AI-Driven SDLC: The Future of Software Development | by typo | The ..., accessed October 15, 2025, [https://medium.com/beyond-the-code-by-typo/ai-driven-sdlc-the-future-of-software-development-3f1e6985deef](https://medium.com/beyond-the-code-by-typo/ai-driven-sdlc-the-future-of-software-development-3f1e6985deef)  \n44. The AI Software Development Lifecycle: A practical ... \\- Distributional, accessed October 15, 2025, [https://www.distributional.com/blog/the-ai-software-development-lifecycle-a-practical-framework-for-modern-ai-systems](https://www.distributional.com/blog/the-ai-software-development-lifecycle-a-practical-framework-for-modern-ai-systems)  \n45. What is the Software Development Lifecycle (SDLC)? \\- IBM, accessed October 15, 2025, [https://www.ibm.com/think/topics/sdlc](https://www.ibm.com/think/topics/sdlc)  \n46. A Framework for Human-Centric AI-First Teaching | AACSB, accessed October 15, 2025, [https://www.aacsb.edu/insights/articles/2025/02/a-framework-for-human-centric-ai-first-teaching](https://www.aacsb.edu/insights/articles/2025/02/a-framework-for-human-centric-ai-first-teaching)  \n47. HUMAN-CENTERED HUMAN-AI COLLABORATION (HCHAC) \\- arXiv, accessed October 15, 2025, [https://arxiv.org/pdf/2505.22477](https://arxiv.org/pdf/2505.22477)  \n48. (PDF) Human-AI Collaboration in Teaching and Learning \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/391277461\\_Human-AI\\_Collaboration\\_in\\_Teaching\\_and\\_Learning](https://www.researchgate.net/publication/391277461_"
  },
  {
    "id": "report_source",
    "chunk": "ctober 15, 2025, [https://www.researchgate.net/publication/391277461\\_Human-AI\\_Collaboration\\_in\\_Teaching\\_and\\_Learning](https://www.researchgate.net/publication/391277461_Human-AI_Collaboration_in_Teaching_and_Learning)  \n49. Human-AI Collaboration in Writing: A Multidimensional Framework for Creative and Intellectual Authorship \\- Digital Commons@Lindenwood University, accessed October 15, 2025, [https://digitalcommons.lindenwood.edu/cgi/viewcontent.cgi?article=1727\\&context=faculty-research-papers](https://digitalcommons.lindenwood.edu/cgi/viewcontent.cgi?article=1727&context=faculty-research-papers)  \n50. 17 Useful AI Agent Case Studies \\- Multimodal, accessed October 15, 2025, [https://www.multimodal.dev/post/useful-ai-agent-case-studies](https://www.multimodal.dev/post/useful-ai-agent-case-studies)  \n51. AI for Software Development Life Cycle | Reply, accessed October 15, 2025, [https://www.reply.com/en/ai-powered-software-engineering/ai-for-software-development-lifecycle](https://www.reply.com/en/ai-powered-software-engineering/ai-for-software-development-lifecycle)  \n52. Test Driven Development Meets Generative AI, accessed October 15, 2025, [https://www.btc-embedded.com/test-driven-development-meets-generative-ai/](https://www.btc-embedded.com/test-driven-development-meets-generative-ai/)  \n53. Automating Test Driven Development with LLMs | by Benjamin \\- Medium, accessed October 15, 2025, [https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1](https://medium.com/@benjamin22-314/automating-test-driven-development-with-llms-c05e7a3cdfe1)  \n54. TDD in the Age of Vibe Coding: Pairing Red-Green-Refactor with AI ..., accessed October 15, 2025, [https://medium.com/@rupeshit/td"
  },
  {
    "id": "report_source",
    "chunk": "en-development-with-llms-c05e7a3cdfe1)  \n54. TDD in the Age of Vibe Coding: Pairing Red-Green-Refactor with AI ..., accessed October 15, 2025, [https://medium.com/@rupeshit/tdd-in-the-age-of-vibe-coding-pairing-red-green-refactor-with-ai-65af8ed32ae8](https://medium.com/@rupeshit/tdd-in-the-age-of-vibe-coding-pairing-red-green-refactor-with-ai-65af8ed32ae8)  \n55. Spec-driven development with AI: Get started with a new open source toolkit \\- The GitHub Blog, accessed October 15, 2025, [https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/)  \n56. AI-Driven Development Life Cycle: Reimagining Software ... \\- AWS, accessed October 15, 2025, [https://aws.amazon.com/blogs/devops/ai-driven-development-life-cycle/](https://aws.amazon.com/blogs/devops/ai-driven-development-life-cycle/)  \n57. Learn Data Science (or any skills) with \"Deliberate Practice\", accessed October 15, 2025, [https://towardsdatascience.com/learn-data-science-or-any-skills-with-deliberate-practice-47eb21bd2c8/](https://towardsdatascience.com/learn-data-science-or-any-skills-with-deliberate-practice-47eb21bd2c8/)  \n58. 5 Principles of Deliberate Practice \\- INTRINSIC First, accessed October 15, 2025, [https://www.intrinsicfirst.com/blog/how-to-take-an-effective-mental-health-day-4kth5](https://www.intrinsicfirst.com/blog/how-to-take-an-effective-mental-health-day-4kth5)  \n59. 8 Keys to Deliberate Practice. \\- Mission to Learn \\- Lifelong Learning ..., accessed October 15, 2025, [https://missiontolearn.com/deliberate-practice/](https://missiontolearn.com/deliberate-practice/)  "
  },
  {
    "id": "report_source",
    "chunk": ". \\- Mission to Learn \\- Lifelong Learning ..., accessed October 15, 2025, [https://missiontolearn.com/deliberate-practice/](https://missiontolearn.com/deliberate-practice/)  \n60. Deliberate Practice \\- Datopian, accessed October 15, 2025, [https://www.datopian.com/playbook/deliberate-practice](https://www.datopian.com/playbook/deliberate-practice)  \n61. How to Handle TDD with AI \\- testRigor AI-Based Automated Testing Tool, accessed October 15, 2025, [https://testrigor.com/blog/how-to-handle-tdd-with-ai/](https://testrigor.com/blog/how-to-handle-tdd-with-ai/)  \n62. The Problem with LLM Test-Driven Development \\- Jazzberry, accessed October 15, 2025, [https://jazzberry.ai/blog/the-problem-with-llm-test-driven-development](https://jazzberry.ai/blog/the-problem-with-llm-test-driven-development)  \n63. Vibe Coding with Generative AI and Test-Driven Development \\- SAS ..., accessed October 15, 2025, [https://communities.sas.com/t5/SAS-Communities-Library/Vibe-Coding-with-Generative-AI-and-Test-Driven-Development/ta-p/968477](https://communities.sas.com/t5/SAS-Communities-Library/Vibe-Coding-with-Generative-AI-and-Test-Driven-Development/ta-p/968477)  \n64. Insights Gained from Using AI to Produce Cases for Problem-Based Learning \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2504-3900/114/1/5](https://www.mdpi.com/2504-3900/114/1/5)  \n65. Using AI to Enhance Project-Based Learning Units \\- Trevor Muir, accessed October 15, 2025, [https://www.trevormuir.com/blog/AI-project-based-learning](https://www.trevormuir.com/blog/AI-project-based-learning)  \n66. How Students Can Use AI in Project-Based Learning \\- Edutopia, accessed October 15, 2025, [https://www.edutopia.org/article/how-students-use-ai-pbl-units/](https://www"
  },
  {
    "id": "report_source",
    "chunk": ")  \n66. How Students Can Use AI in Project-Based Learning \\- Edutopia, accessed October 15, 2025, [https://www.edutopia.org/article/how-students-use-ai-pbl-units/](https://www.edutopia.org/article/how-students-use-ai-pbl-units/)  \n67. Test-Driven Development for Code Generation \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2402.13521v1](https://arxiv.org/html/2402.13521v1)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/04-AI Research Proposal_ V2V Pathway.md\">\n\n\n# **From Vibecoding to Virtuosity: A Synthesis of Research on Context Engineering, AI Pedagogy, and Structured Development Workflows**\n\n## **Part I: The Paradigm Shift from Prompting to Context Engineering**\n\nThe advent of large language models (LLMs) has catalyzed a rapid and ongoing evolution in human-computer interaction. The initial phase of this evolution has been dominated by the craft of \"prompt engineering\"the art of carefully phrasing natural language instructions to elicit desired outputs from a model. While this practice has unlocked significant capabilities, its inherent limitations become increasingly apparent as the complexity of tasks grows. A new, more rigorous discipline is emerging from both academic research and industry practice: Context Engineering. This report establishes the intellectual framework for this paradigm shift, arguing that the journey from novice to expert in AI collaboration is a progression from the ad-hoc, linguistic-centric world of prompting to the systematic, architectural discipline of Context Engineering. This transition is not merely a change in technique but a fundamental re-conceptualization of the user's rolefrom a conversationalist to an architect of the AI's cognitive environment.\n\n##"
  },
  {
    "id": "report_source",
    "chunk": "n is not merely a change in technique but a fundamental re-conceptualization of the user's rolefrom a conversationalist to an architect of the AI's cognitive environment.\n\n### **Section 1: Deconstructing the Prompt Engineering Landscape**\n\nPrompt engineering represents a spectrum of techniques aimed at \"linguistic tuning\"influencing an LLM's output through the careful construction of its input.1 Understanding this landscape is the first step toward recognizing its boundaries and the necessity of a more robust paradigm. The evolution of these techniques reveals a consistent, underlying drive to impose structure and state onto a fundamentally stateless interaction model. Each advancement, from providing simple examples to authoring complex, multi-part prompts, can be seen as an attempt to build a more reliable operating environment within the limited confines of the prompt itself. This trajectory logically culminates in the need for a discipline that externalizes and systematizes this ad-hoc process of environment-building.\n\n#### **1.1 Foundational Prompting Techniques**\n\nThe earliest and most fundamental prompting techniques are rooted in the discovery of In-Context Learning (ICL), the remarkable ability of LLMs to perform new tasks based solely on examples provided within the prompt, without any updates to the model's weights.2 This capability forms the bedrock of prompt-based interaction.  \nThe spectrum of ICL begins with **zero-shot learning**, where the model is given a task description without any examples (e.g., \"Classify the sentiment of the following review:...\"). This relies entirely on the model's pre-trained knowledge to understand and execute the task.2 Performance and reliability increase with **one-shot le"
  },
  {
    "id": "report_source",
    "chunk": " following review:...\"). This relies entirely on the model's pre-trained knowledge to understand and execute the task.2 Performance and reliability increase with **one-shot learning**, where a single example of an input-output pair is provided to demonstrate the desired format and logic. This is further extended in **few-shot learning**, where multiple examples are included in the prompt. This method mimics human reasoning by allowing the model to draw analogies from previous experiences, leveraging the patterns and knowledge learned during pre-training to dynamically adapt to the new task.3 The format and distribution of these examples are often as important as the content itself, signaling to the model the underlying structure of the desired output.3  \nA pivotal evolution beyond simple example-based prompting is **Chain-of-Thought (CoT) prompting**. This technique moves beyond providing just input-output pairs and instead demonstrates the intermediate reasoning steps required to get from input to output.3 By explicitly outlining the logical, sequential steps of a problem-solving process, CoT guides the model's internal cognitive process, significantly improving its performance on tasks requiring arithmetic, commonsense, and symbolic reasoning. This was the first major attempt to systematically structure not just the expected output, but the model's latent computational path to generating that output. For educators, CoT offers a method to delegate cognitive load to the LLM, allowing the AI to generate structured instructional sequences or materials by following a demonstrated logical progression.4\n\n#### **1.2 Advanced and Structured Prompting Methodologies**\n\nAs practitioners sought to tackle more complex tasks, the pro"
  },
  {
    "id": "report_source",
    "chunk": " by following a demonstrated logical progression.4\n\n#### **1.2 Advanced and Structured Prompting Methodologies**\n\nAs practitioners sought to tackle more complex tasks, the prompt itself evolved from a simple instruction into a complex, structured artifact. This gave rise to a family of techniques collectively known as **structured prompting**, which decomposes complex tasks into modular, explicit steps to improve alignment, reliability, and interpretability.5  \nA comprehensive taxonomy of these methodologies reveals a clear trend toward formalization. Techniques such as **Iterative Sequence Tagging** use a predict-and-update loop for incremental output, while **Structured Chains-of-Thought (SCoT)** employ programmatic or state-based decomposition for tasks like code generation.5 **Input-Action-Output (IAO) Templates** enforce a verifiable, auditable chain of reasoning by mandating per-step definitions, which has been shown to improve human error detection in the model's logic.5 Other methods, like **Meta Prompting**, provide an example-agnostic scaffold that outlines the general reasoning structure for a category of tasks, enabling the LLM to fill in specific details as needed.4 These approaches often adopt template-driven or formalized structures, such as JSON templates or function signatures, to constrain and organize the model's output into a predictable and parseable format.5  \nThe apotheosis of this prompt-centric approach is arguably the concept of **\"mega-prompting.\"** This methodology attempts to create a complete, self-contained task environment within a single, massive prompt. A prominent example is the six-part structure developed by Rob Lennon, which comprehensively prepares the AI system by defining its:\n\n1."
  },
  {
    "id": "report_source",
    "chunk": "nment within a single, massive prompt. A prominent example is the six-part structure developed by Rob Lennon, which comprehensively prepares the AI system by defining its:\n\n1. **Role:** Who or what the AI should simulate.  \n2. **Task/Activity:** What needs to be done.  \n3. **Work Steps:** The sub-steps to be performed in order.  \n4. **Context/Restrictions:** Additional conditions and constraints to consider.  \n5. **Goal:** The specific objective the dialogue should achieve.  \n6. **Output Format:** The desired structure of the response.6\n\nThis approach represents the ultimate expression of \"prompt-as-specification,\" where the user attempts to front-load all necessary information to guide the model through a complex task in one go. However, practitioner discussions reveal that while mega-prompts can yield impressive initial results, they are often brittle, require careful construction, and necessitate near-full regression testing for any modifications, as model updates can alter their behavior.7\n\n#### **1.3 The Inherent Limitations of a Prompt-Centric World**\n\nDespite their sophistication, even the most advanced prompt engineering techniques are built upon a fundamentally fragile foundation. Their limitations become insurmountable when dealing with complex, multi-turn, and stateful applications, creating a clear need for a more robust engineering discipline.  \nThe most significant limitation is **brittleness and lack of persistence**. Prompt-based interactions are highly sensitive to small variations in wording, phrasing, or example placement, which can cause notable differences in output quality and reliability.1 This makes the process feel more like an art than a science, a form of \"vibe coding\" that is difficult to repr"
  },
  {
    "id": "report_source",
    "chunk": "an cause notable differences in output quality and reliability.1 This makes the process feel more like an art than a science, a form of \"vibe coding\" that is difficult to reproduce consistently.8 Furthermore, any knowledge or context provided in a prompt is ephemeral. It exists only within the immediate context window and \"fades\" as the conversation progresses or the session ends.7 This \"prompt drift\" requires users to constantly refresh the AI's memory, a clear sign of a non-persistent system.8  \nThis ephemerality places an unsustainable **cognitive load on the human operator**. In a complex, multi-step task, the user must manually track the conversation history, manage relevant facts, decide what information to re-introduce, and synthesize outputs from previous turns. The human becomes the external memory and state manager for the AI. This manual orchestration is a significant bottleneck, preventing the development of scalable, automated, and repeatable workflows. The complexity of authoring and maintaining mega-prompts is a testament to this burden; the user is essentially programming in natural language, but without the robust tools for state management, modularity, and debugging that traditional software engineering provides.\n\n### **Section 2: Defining Context Engineering as a Systems Discipline**\n\nContext Engineering emerges as the systematic solution to the limitations of a purely prompt-centric approach. It reframes the challenge of interacting with LLMs from a problem of linguistic precision to one of architectural design. It is a discipline rooted in systems thinking, focused on constructing and managing a dynamic information environment that is fed to the model at each step of a task. This shift elevates the p"
  },
  {
    "id": "report_source",
    "chunk": "e rooted in systems thinking, focused on constructing and managing a dynamic information environment that is fed to the model at each step of a task. This shift elevates the practitioner's role from a \"prompt artist\" to a \"system architect,\" responsible for designing the data flows and cognitive resources the AI will use to reason effectively.\n\n#### **2.1 The Core Distinction: Linguistic Tuning vs. Systems Thinking**\n\nThe fundamental difference between prompt engineering and context engineering lies in their scope and metaphor. As articulated in industry analyses, prompt engineering is best understood as **Linguistic Tuning**. Its focus is on the micro-level of interaction: influencing a single output through the meticulous crafting of language, phrasing, examples, and reasoning patterns within the prompt itself.1 It is an iterative, often manual process of adjusting words and structure to guide the model's immediate response.  \nIn contrast, Context Engineering is **Systems Thinking**. Its focus is on the macro-level architecture of the entire interaction. It involves designing and automating pipelines that assemble a rich, task-specific environment composed of tools, memory, and external data.1 The goal is not just to craft one perfect prompt, but to build a system that dynamically generates the *right* prompt with the *right* information at every turn of a complex workflow. This distinction is pivotal, as it represents a move from a craft-based approach to a true engineering discipline.\n\n| Feature | Prompt Engineering (\"Linguistic Tuning\") | Context Engineering (\"Systems Thinking\") |\n| :---- | :---- | :---- |\n| **Core Metaphor** | A conversation with an expert; giving precise instructions. | Designing an operating syst"
  },
  {
    "id": "report_source",
    "chunk": "xt Engineering (\"Systems Thinking\") |\n| :---- | :---- | :---- |\n| **Core Metaphor** | A conversation with an expert; giving precise instructions. | Designing an operating system; managing an agent's memory and tools. |\n| **Primary Goal** | Elicit a high-quality response for a single turn. | Ensure reliable, stateful performance across a multi-step task. |\n| **Key Activities** | Word choice, phrasing, role assignment, few-shot examples, CoT. | Retrieval, summarization, tool integration, memory management, data pipelines. |\n| **Unit of Work** | The text of a single prompt. | The entire information pipeline that assembles the prompt. |\n| **Time Horizon** | Ephemeral; focused on the immediate interaction. | Persistent; maintains state and memory across sessions and tasks. |\n| **Failure Mode** | Brittle response to phrasing changes; \"prompt drift.\" | Systemic failure; context overload, retrieval errors, data leakage. |\n| **Required Skillset** | Linguistic creativity, logical reasoning, iterative refinement. | Systems architecture, information retrieval, data flow management, automation. |\n\n#### **2.2 Architectural Components of a Context-Engineered System**\n\nContext Engineering is defined by a set of core practices that treat the prompt not as a static document to be authored, but as a dynamic payload to be assembled. These practices form the architectural components of a modern AI system.  \nA central component is **dynamic information management**, which involves constructing automated pipelines to aggregate, filter, and structure various sources of information before they enter the model's context window. Key practices include:\n\n* **Context Retrieval:** This involves identifying and selecting the most relevant content from "
  },
  {
    "id": "report_source",
    "chunk": "ormation before they enter the model's context window. Key practices include:\n\n* **Context Retrieval:** This involves identifying and selecting the most relevant content from external knowledge bases based on the current task. The most prominent implementation of this is Retrieval-Augmented Generation (RAG), which grounds the model's responses in specific, verifiable documents.1  \n* **Summarization and Compression:** To manage the finite context window, systems must condense large documents, long conversation histories, or verbose tool outputs into compact, high-utility summaries.1 This preserves essential information while conserving valuable token space.  \n* **Tool Integration:** This practice involves defining and describing external functions or APIs that the model can call to perform actions in the world, such as querying a database, sending an email, or accessing real-time data. The descriptions of these tools become part of the context, enabling the model to reason about when and how to use them.1  \n* **Structured Templates and Memory Slotting:** Instead of a single block of text, context is organized into predictable, parseable formats. This includes maintaining distinct channels for different types of information, such as short-term memory (for the current conversation), long-term memory (for persistent facts), and user profile information.1\n\nThese practices collectively represent a fundamental shift from manually writing a prompt to designing an automated workflow that *assembles* the optimal prompt for each step of an agent's process.\n\n#### **2.3 Proactive Context Window Management**\n\nThe LLM's context window is its working memoryits RAM. Like the RAM in a traditional computer, it is finite, and its inefficie"
  },
  {
    "id": "report_source",
    "chunk": "## **2.3 Proactive Context Window Management**\n\nThe LLM's context window is its working memoryits RAM. Like the RAM in a traditional computer, it is finite, and its inefficient use leads to severe performance issues.10 Proactive context window management is therefore a critical sub-discipline of Context Engineering. Without it, even well-designed systems can fail.  \nA lack of careful management leads to a predictable set of problems. The most obvious is **running out of context**, where the maximum token limit is exceeded and older, potentially crucial information is truncated.10 This is common in multi-step agentic tasks like coding across multiple files or aggregating research from many sources. Even when the limit is not reached, performance can degrade. Long, cluttered, or badly structured context can lead to **context distraction**, where irrelevant information misleads the model; **context poisoning**, where a hallucination in the history is incorporated into new outputs; or **context clash**, where contradictory information confuses the model.10 Furthermore, stuffing the context window is inefficient, leading to **rising costs and latency**, as API calls are often priced per token and processing time increases with prompt length.10 Finally, poor management can lead to **unintended data leakage**, where private or irrelevant user information is naively pulled into a prompt where it doesn't belong.10  \nTo combat these issues, practitioners have developed advanced strategies for managing context in complex, multi-stage projects. These can be analogized to the memory management techniques of a modern operating system:\n\n* **Multi-Stage Context Architecture:** This involves treating a large project like a series of pro"
  },
  {
    "id": "report_source",
    "chunk": "alogized to the memory management techniques of a modern operating system:\n\n* **Multi-Stage Context Architecture:** This involves treating a large project like a series of processes. It uses **phase-based organization** to break the project into discrete stages with explicit context handoffs. **Context inheritance planning** ensures that each new phase inherits only the essential context from previous stages, preventing the accumulation of irrelevant data. **Strategic context points** are identified as critical junctures where a full context summary and refresh are necessary.12  \n* **The Context Budget Approach:** This is a practical heuristic for resource allocation within the context window. For example, a budget might reserve 20-30% of the window for instructions and formatting, allocate 40-50% for essential, persistent project context, and use the remaining 20-40% for current, phase-specific information and outputs.12  \n* **Context Efficiency Techniques:** This involves using more token-efficient data formats to represent information. Bullet point summaries, structured lists, and key-value pairs are often more easily parsed by the model and consume fewer tokens than verbose paragraphs.12\n\nThe discipline of Context Engineering, therefore, can be powerfully understood through the metaphor of designing a specialized operating system for an LLM agent. The context window is the RAM. External knowledge bases (vector databases, files) are the hard disk. The strategies of \"Write\" (storing information externally), \"Select\" (retrieving relevant information into the prompt), \"Compress\" (summarizing), and \"Isolate\" (using multi-agent systems) are direct analogues to fundamental OS concepts like writing to disk, memory paging, da"
  },
  {
    "id": "report_source",
    "chunk": "n into the prompt), \"Compress\" (summarizing), and \"Isolate\" (using multi-agent systems) are direct analogues to fundamental OS concepts like writing to disk, memory paging, data compression, and process sandboxing.10 This metaphor provides a robust mental model, elevating the practice from a collection of ad-hoc tricks to a true engineering discipline with a foundation in established computer science principles.\n\n## **Part II: Core Methodologies and Advanced Frontiers**\n\nBuilding on the foundational principles of Context Engineering, this section transitions to a detailed examination of its most critical implementation patterns. It begins with a deep dive into Retrieval-Augmented Generation (RAG), the quintessential practice that has become the bedrock of most production-grade AI applications. It then progresses to the current research frontier, analyzing the Agentic Context Engineering (ACE) framework, which represents a shift from passive context provision to active, self-improving context curation.\n\n### **Section 3: Retrieval-Augmented Generation (RAG) as a Foundational Practice**\n\nRetrieval-Augmented Generation is not merely one technique among many; it is the archetypal and most widely adopted pattern of Context Engineering. It directly addresses the core limitations of LLMstheir static, pre-trained knowledge and their propensity for hallucinationby grounding their responses in external, verifiable data sources. A production-ready RAG system, however, is far more than a simple \"vector search \\+ prompt\" pipeline. It is a complex, multi-stage information retrieval system that requires the same engineering rigor as a traditional search engine.\n\n#### **3.1 Principles and Implementation of RAG**\n\nAt its core, RAG is a "
  },
  {
    "id": "report_source",
    "chunk": "nformation retrieval system that requires the same engineering rigor as a traditional search engine.\n\n#### **3.1 Principles and Implementation of RAG**\n\nAt its core, RAG is a technique for enhancing the accuracy and reliability of generative AI models by providing them with information fetched from specific and relevant data sources at inference time.13 Instead of relying solely on the model's \"parameterized knowledge\" learned during training, RAG dynamically injects factual, up-to-date, or domain-specific information directly into the prompt. This process significantly improves factual accuracy, reduces the generation of incorrect or nonsensical information (hallucination), and allows the model to cite its sources, thereby increasing user trust.13  \nThe basic implementation pipeline for a RAG system provides a practical starting point for understanding its mechanics. The process typically involves four main steps:\n\n1. **Data Preparation (Chunking):** The external knowledge base (e.g., a collection of PDFs, markdown files, or database entries) is separated into smaller, manageable, fixed-size chunks of text.9  \n2. **Indexing (Vectorizing):** Each chunk is processed by an embedding model, which converts the text into a numerical vector representing its semantic meaning. These vectors are then stored in a specialized vector database, creating a searchable index of the knowledge library.9  \n3. **Retrieval (Searching):** At inference time, the user's query is also converted into a vector using the same embedding model. A vector search is then performed against the database to find the chunks whose vectors are most semantically similar to the query vector.9  \n4. **Generation (Augmenting):** The text of the most relevant retri"
  },
  {
    "id": "report_source",
    "chunk": "against the database to find the chunks whose vectors are most semantically similar to the query vector.9  \n4. **Generation (Augmenting):** The text of the most relevant retrieved chunks is then added to the LLM's prompt, along with the original user query. The LLM uses this augmented context to generate a final, grounded response.9\n\n#### **3.2 Best Practices for Production-Grade RAG Systems**\n\nWhile the basic pipeline is straightforward to implement for demonstration purposes, building a robust, production-grade RAG system requires addressing several complex engineering challenges. The quality of the final output is critically dependent on the quality of the retrieved information, demanding a sophisticated approach that integrates best practices from the field of Information Retrieval (IR).  \nFirst, **advanced retrieval techniques** are necessary to ensure the most relevant documents are found. A simple vector search can be insufficient. **Hybrid search**, which combines semantic (vector) retrieval with traditional lexical (keyword-based) retrieval, often yields drastically better results by capturing both conceptual similarity and exact term matches.9 Furthermore, a **re-ranking** step is often added after the initial retrieval. A more powerful but slower model, such as a cross-encoder, can be used to re-evaluate the top N retrieved documents and re-order them based on a more nuanced understanding of their relevance to the query.9  \nSecond, **data preprocessing and cleaning** is a critical but often overlooked step. Data for RAG systems frequently comes from multiple sources in various formats (e.g., PDF, HTML, Word), which can introduce artifacts like logos, navigation bars, special symbols, or code blocks that can co"
  },
  {
    "id": "report_source",
    "chunk": " comes from multiple sources in various formats (e.g., PDF, HTML, Word), which can introduce artifacts like logos, navigation bars, special symbols, or code blocks that can confuse the LLM.9 A dedicated data cleaning pipeline that standardizes formats, filters out noise, and properly extracts clean text is essential for reliable performance.  \nThird, **systematic evaluation** is non-negotiable for building and maintaining a high-quality RAG system. This requires implementing repeatable and accurate evaluation pipelines that assess both the individual components and the system as a whole. The retrieval component can be evaluated using standard search metrics like Normalized Discounted Cumulative Gain (nDCG), which measures the quality of the ranking. The generation component can be evaluated using an \"LLM-as-a-judge\" approach, where another powerful LLM scores the quality of the final response. End-to-end evaluation frameworks like RAGAS provide a suite of metrics to assess the full pipeline.9  \nFinally, a production system must incorporate a loop for **continuous improvement**. As soon as the application is deployed, data should be collected on user interactions, such as which results were helpful and which were not. This data can be used to fine-tune the retrieval models on pairs of queries and relevant chunks, fine-tune the generator LLM on high-quality outputs, and run A/B tests to quantitatively measure the impact of changes to the pipeline.9\n\n#### **3.3 Real-World Applications of RAG**\n\nThe power and versatility of RAG have led to its adoption across a wide range of industries, transforming how organizations leverage their internal and external knowledge. These applications provide concrete, relatable examples of Co"
  },
  {
    "id": "report_source",
    "chunk": "across a wide range of industries, transforming how organizations leverage their internal and external knowledge. These applications provide concrete, relatable examples of Context Engineering in practice.  \nIn **customer support**, RAG-powered chatbots and virtual assistants are replacing static, pre-scripted response systems. They can dynamically pull information from help centers, product documentation, and policy databases to provide personalized and precise answers, leading to faster resolution times and reduced ticket escalations.16  \nWithin the enterprise, **knowledge management** has been revolutionized. Employees can now ask natural language questions and receive grounded answers synthesized from disparate internal sources like wikis, shared drives, emails, and intranets, all while respecting user access controls. This significantly improves employee onboarding and reduces the time spent searching for information.16  \nSpecialized professional domains are also seeing significant impact. In **healthcare**, RAG systems provide clinical decision support by retrieving the latest medical research, clinical guidelines, and patient-specific data to inform diagnoses and treatment plans.17 In **financial services**, RAG helps analysts navigate complex regulatory changes and supports compliance by retrieving and contextualizing guidelines in real-time.17 Similarly, **legal research** and contract review are streamlined by systems that can instantly pull relevant case law, precedent, and contract clauses from trusted legal databases.17 Other applications include sales automation, where platforms use RAG to provide personalized lead recommendations based on CRM data, and content creation, where RAG automates the research pha"
  },
  {
    "id": "report_source",
    "chunk": "ns include sales automation, where platforms use RAG to provide personalized lead recommendations based on CRM data, and content creation, where RAG automates the research phase by pulling from market data and internal documents.16\n\n### **Section 4: The Apex of Context Management: Agentic Context Engineering (ACE)**\n\nWhile RAG represents the foundational practice of providing passive context to an LLM, the current research frontier is exploring how to make the context itself active, dynamic, and self-improving. The Agentic Context Engineering (ACE) framework, emerging from recent academic research, embodies this vision. It transforms context creation from a static, one-time authoring task into a continuous learning process, applying principles analogous to the scientific method to empirically refine the information an AI uses. ACE represents the programmatic embodiment of \"deliberate practice\" for an AI system, providing a powerful parallel to how human experts achieve virtuosity.\n\n#### **4.1 A Paradigm Shift: Contexts as Evolving Playbooks**\n\nThe ACE framework introduces a fundamental paradigm shift: it treats contexts not as concise, static instructions, but as comprehensive, evolving \"playbooks\".21 This approach argues that instead of compressing information into brief summaries, contexts should be rich, detailed, and inclusive, accumulating domain-specific heuristics, strategies, and tactics over time.22  \nThis philosophy directly counters the \"brevity bias\" prevalent in many early prompt optimization techniques, which prioritize concise instructions over comprehensive knowledge accumulation.22 The ACE approach is enabled by and designed for modern long-context LLMs, which have demonstrated the ability to effectively"
  },
  {
    "id": "report_source",
    "chunk": "tions over comprehensive knowledge accumulation.22 The ACE approach is enabled by and designed for modern long-context LLMs, which have demonstrated the ability to effectively process long, detailed inputs and distill relevance autonomously.22 The context, therefore, should function as a detailed repository of insights, allowing the model to decide what is relevant at inference time rather than having a human or another model pre-emptively discard potentially useful information.\n\n#### **4.2 The Modular ACE Architecture: Generate, Reflect, Curate**\n\nTo manage these evolving playbooks, ACE employs a structured, modular workflow built around three cooperative agentic roles, which together form a feedback loop for continuous improvement.25 This architecture can be seen as an implementation of the scientific method for context optimization.\n\n1. **The Generator:** This agent's role is to perform the primary task (the *experiment*). It uses the current version of the context playbook to attempt a solution. As it executes, it records an execution trace and, crucially, flags which specific elements of the context (e.g., which bullet points in the playbook) were helpful or harmful to its process.24 The outcome of its task (e.g., success or failure from an API call) serves as the experimental *data*.  \n2. **The Reflector:** This agent acts as the analyst. It takes the execution trace and performance data from the Generator and performs a critical analysis to distill concrete, actionable lessons (*conclusions*).23 It specializes in identifying the root causes of failures or the key drivers of success, moving beyond simple self-correction to extract generalizable insights.  \n3. **The Curator:** This agent is responsible for updating "
  },
  {
    "id": "report_source",
    "chunk": "f failures or the key drivers of success, moving beyond simple self-correction to extract generalizable insights.  \n3. **The Curator:** This agent is responsible for updating the knowledge base. It takes the insights from the Reflector and incorporates them into the context playbook. Critically, it does so through structured, incremental \"delta updates\"such as appending new bullet points, updating counters on existing ones, or performing semantic deduplicationrather than rewriting the entire context.24 This *refines* the original hypothesis (the context) for the next experimental loop.\n\n#### **4.3 Overcoming the Core Limitations of Prior Approaches**\n\nThe ACE framework is specifically designed to solve two key problems that plague simpler context adaptation methods: context collapse and the need for supervised data.  \n**Context collapse** is a phenomenon where methods that rely on an LLM to iteratively rewrite or summarize its own context often degrade over time. The model tends to produce shorter, less informative summaries with each iteration, causing a gradual erosion of valuable, detailed knowledge and leading to sharp performance declines.21 ACE's use of structured, incremental updates directly prevents this. By only adding or modifying small, discrete pieces of information, the Curator ensures that hard-won knowledge from past experiences is preserved and accumulated, rather than being compressed away.24  \nPerhaps most importantly, ACE enables **self-improvement without labeled supervision**. Many machine learning approaches require large datasets of \"correct\" examples to learn from. ACE, however, is designed to learn from natural execution feedbacksimple success or failure signals from the environment, such as "
  },
  {
    "id": "report_source",
    "chunk": "tasets of \"correct\" examples to learn from. ACE, however, is designed to learn from natural execution feedbacksimple success or failure signals from the environment, such as the output of a code execution or an API call.21 This capability is the key to creating truly autonomous, self-improving AI systems that can learn and adapt from their operational experience in dynamic environments.\n\n#### **4.4 Implications for the V2V Pathway**\n\nThe ACE framework provides powerful, quantitative evidence for the value of a sophisticated, self-improving approach to context management, aligning perfectly with the \"Virtuosity\" stage of the Vibecoding to Virtuosity pathway. A virtuoso practitioner does not merely use a tool with a fixed technique; they reflect on their performance, learn from their mistakes, and continuously refine their process and knowledge. ACE is the programmatic implementation of this exact principle.  \nThe empirical results are compelling. Across agent and domain-specific benchmarks, ACE consistently outperformed strong baselines, showing performance gains of \\+10.6% on agent tasks.21 Notably, the research demonstrated that by using ACE to build a superior context playbook, a smaller, open-source model was able to match the performance of a top-ranked, much larger proprietary agent on the AppWorld leaderboard.21 This shows that superior context can be a more efficient path to high performance than simply scaling up model size. For the Citizen Architect, this is a profound lesson: mastery lies not just in accessing the biggest model, but in architecting the most intelligent context for any model.\n\n## **Part III: Pedagogical Frameworks for AI Mastery**\n\nHaving established the technical evolution from prompt engineer"
  },
  {
    "id": "report_source",
    "chunk": "chitecting the most intelligent context for any model.\n\n## **Part III: Pedagogical Frameworks for AI Mastery**\n\nHaving established the technical evolution from prompt engineering to advanced, agentic context management, the focus now shifts to pedagogy: how can these complex cognitive skills be taught effectively? This section bridges the technical methodologies with established educational theory, proposing a robust pedagogical foundation for the Citizen Architect Academy. The analysis suggests that the Cognitive Apprenticeship model provides an ideal overarching structure for the learning journey, while a mindset of \"collaborative intelligence\" defines the ultimate goal of mastery.\n\n### **Section 5: Cognitive Apprenticeship in the Age of AI**\n\nThe process of becoming a proficient Context Engineer is not one of simple knowledge acquisition but of developing a complex set of cognitive skills, including systems thinking, information architecture, and strategic problem-solving. The Cognitive Apprenticeship model, a well-established pedagogical framework, is perfectly suited for this challenge because it is specifically designed to teach such abstract, expert-level thinking processes. The model provides a research-backed structure for moving learners systematically from novice observation to independent, expert-level practice.\n\n#### **5.1 The Cognitive Apprenticeship Model Explained**\n\nDeveloped by Allan Collins, John Seely Brown, and Susan Newman, the Cognitive Apprenticeship model adapts the principles of traditional, hands-on apprenticeships (like those for chefs or artisans) to the domain of cognitive and metacognitive skills.27 Its central aim is to make the \"invisible\" thinking processes of an expert visible to the le"
  },
  {
    "id": "report_source",
    "chunk": "hose for chefs or artisans) to the domain of cognitive and metacognitive skills.27 Its central aim is to make the \"invisible\" thinking processes of an expert visible to the learner. Instead of just observing the final product of an expert's work, the apprentice is guided through *how* the expert approaches problems, analyzes information, and makes decisions.27  \nThe model is composed of six core teaching components that guide the learner's journey:\n\n1. **Modeling:** An expert performs a task while verbalizing their thought process (\"thinking out loud\"). This externalizes the internal dialogue, strategies, and reasoning that underpin expert performance, making them observable to the learner.27  \n2. **Coaching:** The learner attempts the task, and the expert observes, providing guidance, hints, and targeted feedback to help them refine their approach and correct misconceptions.27  \n3. **Scaffolding:** The learner is provided with structural supports that allow them to complete tasks they could not manage on their own. These scaffolds can be tools, templates, checklists, or simplified versions of the problem. As the learner's competence grows, these supports are gradually removed or \"faded\".27  \n4. **Articulation:** The learner is prompted to verbalize their own reasoning and problem-solving strategies. This act of explaining forces them to clarify their understanding and makes their thought processes visible to the coach for feedback.27  \n5. **Reflection:** The learner compares their performance and processes against those of the expert or other peers. This encourages metacognition, helping them identify areas for improvement and internalize expert standards.27  \n6. **Exploration:** Finally, the learner is encouraged to ap"
  },
  {
    "id": "report_source",
    "chunk": ". This encourages metacognition, helping them identify areas for improvement and internalize expert standards.27  \n6. **Exploration:** Finally, the learner is encouraged to apply their acquired skills independently to new, unfamiliar, and open-ended problems, fostering autonomy and the ability to generalize their knowledge.27\n\n#### **5.2 Mapping the V2V Pathway to Cognitive Apprenticeship**\n\nThe Cognitive Apprenticeship model provides a powerful and logical \"wrapper\" for the entire Vibecoding to Virtuosity (V2V) curriculum. The journey of a Citizen Architect naturally mirrors the stages of the model, providing a clear blueprint for structuring lesson plans, activities, and projects.\n\n| Apprenticeship Stage | Description | V2V Curriculum Application (Example Activity) |\n| :---- | :---- | :---- |\n| **Modeling** | Expert demonstrates and verbalizes their thought process. | An instructor live-codes the development of a RAG system, explaining *why* they are choosing a specific chunking strategy or how they are formulating the prompt template to handle retrieved context. |\n| **Coaching** | Learner practices with expert guidance and feedback. | Learners submit their prompt chains or RAG pipeline configurations for code review. The instructor provides specific feedback on their design choices and suggests improvements. |\n| **Scaffolding** | Learner uses supports (tools, templates) that are gradually faded. | Learners are given a pre-built project template for a RAG application with a basic prompt and are asked to fill in the retrieval logic. In a later module, they must build the entire application from scratch. |\n| **Articulation** | Learner explains their reasoning and process. | In a group setting, a learner presents their so"
  },
  {
    "id": "report_source",
    "chunk": "le, they must build the entire application from scratch. |\n| **Articulation** | Learner explains their reasoning and process. | In a group setting, a learner presents their solution to a context management problem and must defend their architectural choices to their peers and the instructor. |\n| **Reflection** | Learner compares their work to an expert's or a standard. | After completing a project, learners are shown an expert-level implementation of the same project and are asked to write a short analysis comparing their approach and identifying key differences. |\n| **Exploration** | Learner applies skills to new, open-ended problems. | A capstone project where learners are given a broad business problem (e.g., \"Improve customer onboarding for a new SaaS product\") and must independently design and build an AI-powered solution. |\n\nThis mapping demonstrates how the curriculum can be explicitly structured to ensure learners are not just passively consuming information but are actively and systematically developing expert-level cognitive skills. AI tools themselves can also serve as powerful scaffolds within this process, providing services like grammar correction, idea organization, vocabulary expansion, or stylistic refinement, which can reduce the cognitive load on learners and allow them to focus on higher-order thinking.29\n\n#### **5.3 AI as the Ultimate \"Cognitive Tool\" and Practice Environment**\n\nWithin the Cognitive Apprenticeship framework, AI is not just the subject of study but also a powerful pedagogical tool. It can be conceptualized as a \"cognitive tool\" that augments human intellectual capabilities rather than replacing them.30 While there are valid concerns that over-reliance on AI could foster cognitive shor"
  },
  {
    "id": "report_source",
    "chunk": "cognitive tool\" that augments human intellectual capabilities rather than replacing them.30 While there are valid concerns that over-reliance on AI could foster cognitive shortcuts and passive learning habits, thoughtful integration can enhance scaffolded learning and support deep conceptual growth.30  \nOne of the most powerful applications of AI in this context is to facilitate **AI-assisted deliberate practice**. Deliberate practicerepeated, goal-oriented practice with immediate feedbackis a cornerstone of developing expertise. AI chatbots and agents can create dynamic, simulated environments for learners to engage in this type of practice at scale.32 For example, a learner can prompt an AI to act as a difficult client, an anxious student, or a Socratic debate partner, allowing them to practice communication, teaching, or argumentation skills in a safe, repeatable setting.33 A framework for a generative AI-powered platform could even feature virtual student agents with varied learning styles and mentor agents that provide real-time feedback, allowing teachers-in-training to refine their methods through iterative practice.32 This use of AI as a simulator for deliberate practice is a powerful way to help learners develop the flexible, transferable skills needed for real-world application.\n\n### **Section 6: Fostering Collaborative Intelligence: Human-AI Partnership Frameworks**\n\nMastery in the age of AI extends beyond individual skill acquisition to a fundamental shift in mindset: viewing AI not as a tool to be commanded, but as a partner in a collaborative system. The most effective practitioners are those who have learned how to \"think with\" AI, strategically allocating cognitive labor between the human and the machin"
  },
  {
    "id": "report_source",
    "chunk": "aborative system. The most effective practitioners are those who have learned how to \"think with\" AI, strategically allocating cognitive labor between the human and the machine to create a whole that is greater than the sum of its parts. This concept of \"collaborative intelligence\" requires specific mental models and a core set of competencies that must be explicitly taught.\n\n#### **6.1 Mental Models for Human-AI Collaboration**\n\nTo move beyond a simple tool-user relationship, learners need powerful mental models to conceptualize their partnership with AI. **Distributed Cognition** provides such a framework. Pioneered by cognitive scientist Edwin Hutchins, this theory posits that cognitive processes are not confined to an individual's mind but are distributed across people, tools, and the environment.34 In a human-AI partnership, the cognitive task is shared: the human provides strategic intent, domain expertise, ethical judgment, and creative synthesis, while the AI contributes speed, scale, pattern matching across vast datasets, and the tireless execution of well-defined tasks.34 A successful collaboration depends on understanding each partner's unique strengths and weaknesses and dividing the cognitive labor accordingly.  \nThis partnership can take different forms depending on the task and context. Research in Human-AI Collaboration (HAIC) identifies several modes of interaction, such as **AI-Centric** (where the AI takes the lead, and the human supervises), **Human-Centric** (where the human directs, and the AI assists), and **Symbiotic** (a true, deeply integrated partnership).35 Teaching learners to recognize and design for the appropriate mode of collaboration is a critical component of architectural thinking. For"
  },
  {
    "id": "report_source",
    "chunk": "e, deeply integrated partnership).35 Teaching learners to recognize and design for the appropriate mode of collaboration is a critical component of architectural thinking. For instance, a task requiring high creativity and novel problem-solving might call for a Human-Centric approach, while a task involving the rapid analysis of thousands of documents would be better suited to an AI-Centric mode.\n\n#### **6.2 Core Competencies for the Citizen Architect**\n\nBuilding on these mental models, a Citizen Architect must cultivate a specific set of competencies to operate effectively.\n\n* **AI Literacy:** This is the foundational layer. A comprehensive AI literacy curriculum should be staged according to learner development. It begins with basic awareness, curiosity, and pattern recognition. It then progresses to a deeper understanding of how AI is used in daily life, an introduction to programming and building simple models, and an awareness of the ethical challenges and risks, such as inherent bias, the potential for dependency, and inequitable access. At the most advanced level, it includes skills for building complex systems and the critical ability to differentiate authentic content from AI-generated fakes and misinformation.36  \n* **Computational Thinking in the AI Era:** The core skills of computational thinkingdecomposition, pattern recognition, abstraction, and algorithmic thinkingare not made obsolete by AI; they are re-contextualized and amplified.37 Effective prompt engineering and, more broadly, context engineering are modern manifestations of computational thinking. The ability to decompose a complex business problem into a series of logical steps that an AI can execute, to abstract a task into a reusable prompt tem"
  },
  {
    "id": "report_source",
    "chunk": "omputational thinking. The ability to decompose a complex business problem into a series of logical steps that an AI can execute, to abstract a task into a reusable prompt template, or to recognize patterns in AI failures to debug a system are all applications of computational thinking in this new era.38 Efficient prompting, in this view, can be seen as a form of writing pseudocode for the LLM.38  \n* **The 4D Framework for AI Fluency:** As a practical, memorable framework for guiding interaction, Anthropic's AI Fluency Framework offers four interconnected competencies for effective, efficient, and ethical collaboration:  \n  1. **Delegation:** Strategically identifying which tasks are suitable for AI and planning the project accordingly.  \n  2. **Description:** Clearly and effectively communicating the task, context, and constraints to the AI.  \n  3. **Discernment:** Critically evaluating the AI's output for accuracy, bias, and relevance.  \n  4. **Diligence:** Iteratively refining prompts and outputs through a feedback loop, and understanding the ethical responsibilities involved.39\n\nThe ultimate meta-skill for a Citizen Architect is mastering this \"cognitive allocation.\" The virtuoso practitioner is one who, when faced with a problem, instinctively determines which parts of the cognitive work are best suited for human thought and which are best delegated to the machine's processing power. They do not ask the AI for strategic vision; they delegate the task of generating ten possible strategies based on a well-defined goal and a curated dataset. This ability to orchestrate a distributed cognitive system is the essence of moving from a mere user to a true architect.\n\n## **Part IV: Application in Practice: Structured AI Deve"
  },
  {
    "id": "report_source",
    "chunk": "ability to orchestrate a distributed cognitive system is the essence of moving from a mere user to a true architect.\n\n## **Part IV: Application in Practice: Structured AI Development Workflows**\n\nThis final part synthesizes the principles of Context Engineering and the pedagogical frameworks of AI collaboration, applying them directly to the practical domain of software development. The goal is to move practitioners beyond ad-hoc, conversational \"chat with your code\" interactions and toward formal, repeatable, and professional engineering workflows. The most successful of these workflows share a common pattern: they use human-authored artifacts like tests and specifications as a form of high-fidelity, non-linguistic context to constrain the AI's behavior and rigorously verify its output. This represents the ultimate application of Context Engineering in a coding context.\n\n### **Section 7: From Ad-Hoc Interaction to Repeatable Process**\n\nThe integration of AI into software development necessitates a formalization of process. Just as the industry moved from unstructured coding to methodologies like Agile and DevOps to manage complexity, so too must it adopt structured workflows to effectively manage human-AI collaboration. This shift is driven by a fundamental evolution in the nature of the developer's role itself.\n\n#### **7.1 The Evolving Role of the Developer: From Coder to Orchestrator**\n\nIndustry analysis and research project a significant transformation in the developer's role. As AI code assistants become increasingly capable of generating boilerplate code, implementing functions, and automating repetitive tasks, the developer's primary value shifts away from the manual act of typing code. The future of programming i"
  },
  {
    "id": "report_source",
    "chunk": "rplate code, implementing functions, and automating repetitive tasks, the developer's primary value shifts away from the manual act of typing code. The future of programming is less about writing lines of code and more about defining intent, guiding AI systems, and integrating their outputs into coherent, robust solutions.40  \nIn this new paradigm, the developer becomes an **orchestrator of an AI-driven development ecosystem**. Their core responsibilities evolve to include higher-order skills that machines are ill-suited for: strategic planning, architectural design, creative problem-solving, and critical judgment. This provides the fundamental \"why\" for teaching structured workflows: these workflows are the instruments through which the orchestrator conducts the AI.\n\n#### **7.2 Best Practices for AI Pair Programming**\n\nTo function effectively as an orchestrator, developers must adhere to a set of best practices for AI pair programming that ensure a productive and reliable collaboration.  \nA foundational practice is the **clear definition of roles**. In this model, the human developer acts as the **\"Navigator,\"** responsible for the overall strategy, making architectural decisions, defining the problem, and critically reviewing all AI-generated code. The AI assistant acts as the **\"Driver,\"** responsible for the tactical implementation, generating code, suggesting refactoring opportunities, and explaining complex algorithms.41  \nThis collaboration is only effective if the Navigator provides **high-quality, curated context**. AI coding agents lack the full project context that a human developer possesses. To maximize their effectiveness, the developer must explicitly share relevant parts of the codebase, explain architect"
  },
  {
    "id": "report_source",
    "chunk": "e full project context that a human developer possesses. To maximize their effectiveness, the developer must explicitly share relevant parts of the codebase, explain architectural patterns and design decisions, specify coding standards, and clearly define constraints and requirements.41  \nFinally, a core tenet of responsible AI pair programming is **iterative refinement and critical human oversight**. AI-generated code should always be treated as a suggestion or a first draft, not a final solution.43 The developer must remain actively involved, reviewing all outputs for correctness, security vulnerabilities, performance characteristics, and adherence to project requirements. This iterative loopwhere the AI generates, the human reviews and provides feedback, and the AI refinesis essential for producing high-quality software.41\n\n#### **7.3 Quality Assurance in AI-Driven Development**\n\nTo formalize the review and validation process, developers are adapting established software engineering quality assurance methodologies for the AI era. Two such approaches stand out as particularly effective for guiding AI code generation: Test-Driven Development and Spec-Driven Development.  \n**AI-Assisted Test-Driven Development (TDD)** provides a powerful framework for ensuring the quality and correctness of AI-generated code. In the traditional TDD cycle, a developer writes a failing test that defines a desired behavior, writes the minimal code to make the test pass, and then refactors. When adapted for AI, this workflow provides concrete \"guardrails\" for the AI assistant.44 The workflow becomes an \"edit-test loop\":\n\n1. The human developer writes a failing test that precisely captures a requirement.  \n2. The test suite is provided as c"
  },
  {
    "id": "report_source",
    "chunk": "ssistant.44 The workflow becomes an \"edit-test loop\":\n\n1. The human developer writes a failing test that precisely captures a requirement.  \n2. The test suite is provided as context to the AI.  \n3. The AI is prompted with the simple instruction: \"Make this test pass\".42  \n4. The AI generates code, which is then automatically run against the test suite.  \n5. The results (pass or fail) are fed back to the AI, which iterates until the test passes.45\n\nThis process is powerful because the test suite serves as an unambiguous, executable specification of the desired outcome. It is a perfect form of context that leaves little room for the AI to hallucinate or misinterpret the requirements.44  \nA related and slightly broader approach is **Spec-Driven Development**. In this methodology, the central artifact is a formal, detailed specification document that acts as a contract for how the code should behave. This spec becomes the single source of truth that AI agents use to generate not only the implementation code but also the tests and validation checks.47 The process typically involves the human and AI collaborating on the spec first, then a technical plan, then the tests, and finally the code. This ensures that every step of the AI-driven process is anchored to a clear, human-vetted definition of intent, reducing guesswork and improving the quality of the final product.47 These methodologies are not just \"good coding practices\" to be used alongside AI; they are the optimal interface for guiding and controlling AI code generation. The tests and specifications *are* the prompt, in its most powerful and verifiable form.\n\n### **Section 8: Case Study: The 3-File System and Other Formalized Workflows**\n\nThe principles of structured AI"
  },
  {
    "id": "report_source",
    "chunk": "s *are* the prompt, in its most powerful and verifiable form.\n\n### **Section 8: Case Study: The 3-File System and Other Formalized Workflows**\n\nThe principles of structured AI development are best understood through concrete, teachable workflows that embody them. Ryan Carson's \"3-File System\" has emerged as a prominent example of a practical, repeatable workflow that formalizes the expert cognitive process of software development into a set of machine-readable artifacts. This system serves as an excellent pedagogical tool, providing a capstone workflow that integrates Context Engineering, AI pedagogy, and structured development into a single, coherent process.\n\n#### **8.1 Deep Dive: Ryan Carson's 3-File AI Development System**\n\nThe 3-File System is designed to bring structure, clarity, and control to the process of building complex features with AI, moving beyond frustrating \"vibe coding\".48 It externalizes the key phases of software developmentdefining scope, detailed planning, and iterative implementationinto three distinct files that guide an AI coding agent. This approach scaffolds the entire development process for both the human and the AI, decomposing a single, complex request into a series of simple, verifiable steps.50  \nThe workflow revolves around three core markdown files, which serve as the primary context for the AI agent 48:\n\n1. **The Product Requirement Document (PRD):** This is the blueprint and the starting point. The developer collaborates with the AI, often using a template prompt (e.g., create-prd.md), to generate a clear and comprehensive specification for the feature. The PRD defines the *what* and the *why*what is being built, for whom, and what the goals are. This initial step ensures that bot"
  },
  {
    "id": "report_source",
    "chunk": "comprehensive specification for the feature. The PRD defines the *what* and the *why*what is being built, for whom, and what the goals are. This initial step ensures that both the human and the AI have a shared understanding of the feature's scope before any code is written.49  \n2. **The Atomic Task List:** Once the PRD is finalized, it is fed to the AI along with another template prompt (e.g., generate-tasks.md). The AI's job is to break down the high-level requirements from the PRD into a granular, sequential, and actionable checklist of development tasks. This file defines the *how*the step-by-step implementation plan. This is a critical step, as it forces the AI to construct a logical plan of attack, which the human can review and amend before implementation begins.49  \n3. **Iterative Implementation and Verification:** With the task list in hand, the developer then guides an AI coding agent (such as Cursor or Claude Code) to execute the plan. Using a final prompt (e.g., process-task-list.md), the developer instructs the AI to tackle the tasks one at a time. After the AI completes a task, the developer reviews the changes. If the code is correct, they give a simple affirmative command (e.g., \"yes\") to instruct the AI to mark the task as complete and move to the next one. If corrections are needed, the developer provides feedback to refine the current task before proceeding. This human-in-the-loop process ensures continuous verification and control.49\n\nThis system is a practical implementation of Cognitive Apprenticeship for AI development. It formalizes the expert process (Define \\-\\> Plan \\-\\> Execute \\-\\> Verify) into concrete artifacts that perfectly scaffold the interaction for both the human developer and the A"
  },
  {
    "id": "report_source",
    "chunk": "ormalizes the expert process (Define \\-\\> Plan \\-\\> Execute \\-\\> Verify) into concrete artifacts that perfectly scaffold the interaction for both the human developer and the AI agent.\n\n#### **8.2 Synthesis of Other Structured Workflows**\n\nRyan Carson's system is a powerful specific implementation of the broader principles discussed throughout this report. The PRD is a form of **spec-driven development**, creating a human-vetted source of truth. The iterative, one-task-at-a-time implementation is a form of the **edit-test loop**, where the \"test\" is the human developer's review against the task description. The entire system is an exercise in meticulous **Context Engineering**, where curated files, rather than a long conversational history, provide the stable context for the AI.  \nCase studies of context engineering in practice reveal similar patterns across the industry. The company Manus, in building its agent framework, learned the importance of keeping the prompt prefix stable and making the context append-only to improve performance, principles that align with the 3-File System's use of static, referenced files.53 Vellum's platform for building AI workflows emphasizes orchestrating multi-step chains (retrieve, search, call tool, summarize) and systematically experimenting with different context strategies, echoing the structured, multi-artifact approach.54 These real-world examples show that organizations building robust AI systems are independently converging on the same core principles: externalizing state, structuring workflows, and curating context, moving far beyond simple prompting.11\n\n| Workflow | Core Principle | Key Artifacts | Primary Use Case |\n| :---- | :---- | :---- | :---- |\n| **AI-Assisted TDD** | Veri"
  },
  {
    "id": "report_source",
    "chunk": "context, moving far beyond simple prompting.11\n\n| Workflow | Core Principle | Key Artifacts | Primary Use Case |\n| :---- | :---- | :---- | :---- |\n| **AI-Assisted TDD** | Verification-first development; tests as executable specifications. | Unit/Integration Tests, Code Implementation. | Ensuring correctness and robustness of AI-generated code for well-defined functions or modules. |\n| **Spec-Driven Development** | Intent-first development; formal specification as the source of truth. | Specification Document, Technical Plan, Test Cases, Code. | Greenfield projects or adding large, complex features where upfront clarity of intent is critical. |\n| **Ryan Carson's 3-File System** | Decompose, plan, then execute with human-in-the-loop verification. | Product Requirement Document (PRD), Atomic Task List, Codebase. | A practical, streamlined workflow for solo developers or small teams building features iteratively. |\n| **Agentic Context Engineering (ACE)** | Self-improvement through empirical feedback. | Evolving Context \"Playbook,\" Execution Traces. | Creating autonomous agents that can learn and adapt over time in dynamic environments without supervision. |\n\nThis comparative overview shows that while the specific artifacts and level of automation may differ, all advanced workflows are united by the principle of using structured, externalized information to guide and constrain AI behavior. This empowers practitioners to choose or design the right workflow for their specific project needs.\n\n## **Part V: Synthesis and Recommendations for the Citizen Architect Academy**\n\nThis report has synthesized a broad range of academic research and industry best practices to build a coherent intellectual framework for the \"Vibecoding to Vir"
  },
  {
    "id": "report_source",
    "chunk": "ect Academy**\n\nThis report has synthesized a broad range of academic research and industry best practices to build a coherent intellectual framework for the \"Vibecoding to Virtuosity\" (V2V) pathway. The analysis confirms a clear and accelerating paradigm shift from the craft of prompt engineering to the discipline of Context Engineering, supported by robust pedagogical models and structured development workflows. This final section distills this synthesis into the direct, actionable outputs requested in the original research proposal: a refined lexicon for the V2V pathway and a set of strategic recommendations for curriculum development.\n\n### **Section 9: A Refined Lexicon for the V2V Pathway**\n\nA clear, consistent, and defensible vocabulary is the foundation of any rigorous curriculum. The following definitions are proposed to anchor the core concepts of the Citizen Architect Academy, grounding its internal language in the findings of this research.\n\n#### **9.1 Core Terminology**\n\n* **Context Engineering:** Formally defined as \"The engineering discipline of designing, building, and managing the dynamic information environment (context) provided to an AI model to ensure reliable, accurate, and efficient performance on complex, multi-step tasks.\" This definition positions it as a systems-level discipline distinct from prompting.1  \n* **Vibecoding:** Defined as \"An early, intuitive, and ad-hoc stage of human-AI interaction characterized by conversational prompting without a structured workflow or systematic context management. Effective for simple, exploratory tasks but brittle and unreliable for building robust applications.\" This term captures the essence of the novice stage, which the V2V pathway is designed to move lea"
  },
  {
    "id": "report_source",
    "chunk": "ploratory tasks but brittle and unreliable for building robust applications.\" This term captures the essence of the novice stage, which the V2V pathway is designed to move learners beyond.  \n* **Virtuosity:** Defined as \"A state of mastery in human-AI collaboration characterized by the ability to design and orchestrate robust, self-improving, and repeatable workflows that effectively combine human strategic intent with AI operational capability.\" This definition aligns mastery with architectural skill and connects directly to advanced concepts like Agentic Context Engineering.23  \n* **Citizen Architect:** Defined as \"A practitioner who possesses the multidisciplinary skills of Context Engineering, AI literacy, and structured workflow design to build and manage sophisticated human-AI collaborative systems.\" This title emphasizes the user's role as a designer and orchestrator, not just a coder or prompter.\n\n#### **9.2 Supporting Concepts**\n\nA curriculum knowledge base should include a glossary of key technical and pedagogical terms identified in this report. Each term should be accompanied by a concise definition and a citation to a key source.\n\n* **Agentic Context Engineering (ACE):** A framework that treats contexts as evolving playbooks that are refined through a modular process of generation, reflection, and curation, enabling self-improvement from execution feedback.21  \n* **Brevity Bias:** The tendency of some prompt optimization methods to prioritize concise instructions over comprehensive, domain-rich information, which can lead to the omission of critical details.22  \n* **Cognitive Apprenticeship:** A pedagogical model focused on making expert thinking processes visible and learnable through modeling, coaching, sc"
  },
  {
    "id": "report_source",
    "chunk": "n of critical details.22  \n* **Cognitive Apprenticeship:** A pedagogical model focused on making expert thinking processes visible and learnable through modeling, coaching, scaffolding, articulation, reflection, and exploration.27  \n* **Cognitive Scaffolding:** Temporary supports (e.g., tools, templates, simplified tasks) provided to a learner to help them complete a task that would otherwise be beyond their current capabilities.29  \n* **Context Collapse:** The degradation of information in an iterative context-rewriting process, where an LLM's summarization tendency erodes valuable details over time.21  \n* **Context Window Management:** The set of strategies used to efficiently and effectively utilize an LLM's limited context window, analogous to RAM management in an operating system.10  \n* **Distributed Cognition:** A theoretical framework that views cognitive processes as being distributed across individuals, tools, and the environment, providing a model for human-AI partnership.34  \n* **Retrieval-Augmented Generation (RAG):** A core Context Engineering technique that enhances LLM outputs by dynamically retrieving relevant information from an external knowledge base and adding it to the prompt.9  \n* **Structured Prompting:** A family of prompt engineering techniques that decompose complex tasks into modular, explicit steps to improve the reliability and interpretability of LLM outputs.5\n\n### **Section 10: Strategic Recommendations for Curriculum Artifacts**\n\nBased on the comprehensive analysis, the following strategic recommendations are provided to guide the development of the Citizen Architect Academy's curriculum, lesson plans, and supporting materials.\n\n#### **10.1 Foundational Course Structure**\n\nIt is recommende"
  },
  {
    "id": "report_source",
    "chunk": " to guide the development of the Citizen Architect Academy's curriculum, lesson plans, and supporting materials.\n\n#### **10.1 Foundational Course Structure**\n\nIt is recommended that the core curriculum be structured to mirror the logical flow of this report, guiding learners along the V2V pathway from foundational skills to architectural mastery. A potential five-module structure would be:\n\n1. **Module 1: The Foundations and Limits of Prompting:** This module would cover the full spectrum of prompt engineering, from few-shot learning and Chain-of-Thought to advanced structured prompting and mega-prompts. The goal is to give learners a solid foundation while clearly establishing the limitations of a prompt-centric approach, creating the motivation for Context Engineering.  \n2. **Module 2: Principles of Context Engineering:** This module introduces the paradigm shift to systems thinking. It should teach the core architectural components (retrieval, summarization, tools, memory) and the critical skill of proactive context window management, using the powerful metaphor of designing an operating system for an AI.  \n3. **Module 3: The RAG Toolkit:** This should be a practical, hands-on module focused on building a production-grade RAG system. It must go beyond a simple demo and cover the essential best practices: data cleaning, hybrid search and re-ranking, and systematic evaluation.  \n4. **Module 4: The Collaborative Mindset:** This module focuses on the \"human\" side of human-AI collaboration. It should teach pedagogical frameworks like Cognitive Apprenticeship and mental models like Distributed Cognition. It would also cover the core competencies of AI Literacy and Computational Thinking in the AI era.  \n5. **Module 5: The A"
  },
  {
    "id": "report_source",
    "chunk": "iceship and mental models like Distributed Cognition. It would also cover the core competencies of AI Literacy and Computational Thinking in the AI era.  \n5. **Module 5: The Architect's Workflow:** This capstone module brings everything together, focusing on the application of all preceding principles in the context of software development. It should provide in-depth, hands-on training in structured workflows like AI-Assisted Test-Driven Development and, as a culminating project, Ryan Carson's 3-File System.\n\n#### **10.2 Key Learning Activities and Projects**\n\nThe curriculum should be project-based, emphasizing the development of practical skills through activities that directly reflect the principles of Cognitive Apprenticeship.\n\n* **Activity: \"Deconstruct a Mega-Prompt\":** In Module 1 or 2, provide students with a complex, brittle mega-prompt and have them refactor it into a more robust, context-engineered system with externalized knowledge files and a simpler, dynamic prompt. This directly demonstrates the value of the paradigm shift.  \n* **Project: \"Build Your Own RAG\":** A multi-week project in Module 3 where students must select a domain, curate a knowledge base, and build a RAG chatbot. This will force them to confront the real-world challenges of data cleaning, chunking strategy, and evaluating retrieval quality.  \n* **Activity: \"Cognitive Apprenticeship Role-Play\":** In Module 4, pair students to practice the roles of \"expert\" and \"apprentice.\" One student must \"model\" their process for solving a complex AI interaction task, verbalizing their thoughts, while the other \"coaches\" them, providing feedback.  \n* **Capstone Project: \"The 3-File Feature Build\":** The final project for Module 5\\. Students are given an e"
  },
  {
    "id": "report_source",
    "chunk": "ir thoughts, while the other \"coaches\" them, providing feedback.  \n* **Capstone Project: \"The 3-File Feature Build\":** The final project for Module 5\\. Students are given an existing open-source codebase and tasked with adding a non-trivial new feature using the 3-File System. They must produce the PRD, the atomic task list, and the final, working code with a pull request as their deliverables.\n\n#### **10.3 Curated Knowledge Base**\n\nTo support both instructor training and learner supplementation, a curated knowledge base is essential. This directly fulfills a primary objective of the initial research proposal.\n\n* It is recommended that this research report serve as the foundational document for the instructor training knowledge base, providing the core intellectual framework and pedagogical rationale for the curriculum.  \n* A supplementary, learner-facing library should be created. This library should be organized by the five curriculum modules recommended above. For each module, it should contain links to the most salient and high-quality external resources identified in this research. This includes the key arXiv papers (e.g., on ACE), seminal technical blog posts (e.g., on RAG best practices and context window management), and influential GitHub repositories (e.g., Ryan Carson's ai-dev-tasks). This curated repository will accelerate curriculum development by leveraging existing high-quality materials and provide learners with pathways for deeper exploration.\n\n#### **Works cited**\n\n1. Understanding Prompt Engineering and Context Engineering, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineer"
  },
  {
    "id": "report_source",
    "chunk": "ed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  \n2. In Context Learning Guide \\- PromptHub, accessed October 15, 2025, [https://www.prompthub.us/blog/in-context-learning-guide](https://www.prompthub.us/blog/in-context-learning-guide)  \n3. What is In-Context Learning (ICL)? | IBM, accessed October 15, 2025, [https://www.ibm.com/think/topics/in-context-learning](https://www.ibm.com/think/topics/in-context-learning)  \n4. Precision In Practice: Structured Prompting Strategies to Enhance ..., accessed October 15, 2025, [https://my.tesol.org/news/1166339](https://my.tesol.org/news/1166339)  \n5. Structured Prompting Approaches \\- Emergent Mind, accessed October 15, 2025, [https://www.emergentmind.com/topics/structured-prompting](https://www.emergentmind.com/topics/structured-prompting)  \n6. Manuel\\_PROMPTING\\_engl.docx, accessed October 15, 2025, [https://www.unileoben.ac.at/fileadmin/shares/ctl/Word\\_Dateien/Manuel\\_PROMPTING\\_engl.docx](https://www.unileoben.ac.at/fileadmin/shares/ctl/Word_Dateien/Manuel_PROMPTING_engl.docx)  \n7. Mega prompts \\- do they work? : r/ChatGPTPro \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/ChatGPTPro/comments/1ley49z/mega\\_prompts\\_do\\_they\\_work/](https://www.reddit.com/r/ChatGPTPro/comments/1ley49z/mega_prompts_do_they_work/)  \n8. Context Engineering vs Prompt Engineering : r/PromptEngineering \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1lmnftf/context\\_engineering\\_vs\\_prompt\\_engineering/](https://www.reddit.com/r/PromptEngineering/comments/1lmnftf/context_engineering_vs_pro"
  },
  {
    "id": "report_source",
    "chunk": "com/r/PromptEngineering/comments/1lmnftf/context\\_engineering\\_vs\\_prompt\\_engineering/](https://www.reddit.com/r/PromptEngineering/comments/1lmnftf/context_engineering_vs_prompt_engineering/)  \n9. Practical tips for retrieval-augmented generation (RAG) \\- Stack ..., accessed October 15, 2025, [https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/](https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/)  \n10. LLM Context Engineering. Introduction | by Kumar Nishant | Medium, accessed October 15, 2025, [https://medium.com/@knish5790/llm-context-engineering-66097070161b](https://medium.com/@knish5790/llm-context-engineering-66097070161b)  \n11. Context Engineering \\- LangChain Blog, accessed October 15, 2025, [https://blog.langchain.com/context-engineering-for-agents/](https://blog.langchain.com/context-engineering-for-agents/)  \n12. Context Window Management: Maximizing AI Memory for Complex ..., accessed October 15, 2025, [https://blog.qolaba.ai/ai-tools-by-qolaba/context-window-management-maximizing-ai-memory-for-complex-tasks/](https://blog.qolaba.ai/ai-tools-by-qolaba/context-window-management-maximizing-ai-memory-for-complex-tasks/)  \n13. What Is Retrieval-Augmented Generation aka RAG \\- NVIDIA Blog, accessed October 15, 2025, [https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)  \n14. Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2508.19357v1](https://arxiv.org/html/2508.19357v1)  \n15. What is RAG? \\- Retrieval-Augmented Generation AI "
  },
  {
    "id": "report_source",
    "chunk": "omains \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2508.19357v1](https://arxiv.org/html/2508.19357v1)  \n15. What is RAG? \\- Retrieval-Augmented Generation AI Explained \\- AWS \\- Updated 2025, accessed October 15, 2025, [https://aws.amazon.com/what-is/retrieval-augmented-generation/](https://aws.amazon.com/what-is/retrieval-augmented-generation/)  \n16. 10 Real-World Examples of Retrieval Augmented Generation, accessed October 15, 2025, [https://www.signitysolutions.com/blog/real-world-examples-of-retrieval-augmented-generation](https://www.signitysolutions.com/blog/real-world-examples-of-retrieval-augmented-generation)  \n17. Top 7 examples of retrieval-augmented generation \\- Glean, accessed October 15, 2025, [https://www.glean.com/blog/rag-examples](https://www.glean.com/blog/rag-examples)  \n18. What is retrieval augmented generation (RAG) \\[examples included\\] \\- SuperAnnotate, accessed October 15, 2025, [https://www.superannotate.com/blog/rag-explained](https://www.superannotate.com/blog/rag-explained)  \n19. 9 powerful examples of retrieval-augmented generation (RAG) \\- Merge.dev, accessed October 15, 2025, [https://www.merge.dev/blog/rag-examples](https://www.merge.dev/blog/rag-examples)  \n20. 7 Practical Applications of RAG Models and Their Impact on Society \\- Hyperight, accessed October 15, 2025, [https://hyperight.com/7-practical-applications-of-rag-models-and-their-impact-on-society/](https://hyperight.com/7-practical-applications-of-rag-models-and-their-impact-on-society/)  \n21. \\[2510.04618\\] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models \\- arXiv, accessed October 15, 2025, [https://www.arxiv.org/abs/2510.04618](https://www.arxiv.org/abs/2510.04618)  \n22. Age"
  },
  {
    "id": "report_source",
    "chunk": "volving Contexts for Self-Improving Language Models \\- arXiv, accessed October 15, 2025, [https://www.arxiv.org/abs/2510.04618](https://www.arxiv.org/abs/2510.04618)  \n22. Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2510.04618v1](https://arxiv.org/html/2510.04618v1)  \n23. Agentic Context Engineering: Evolving Contexts for Self-Improving ..., accessed October 15, 2025, [https://www.alphaxiv.org/overview/2510.04618v1](https://www.alphaxiv.org/overview/2510.04618v1)  \n24. Agentic Context Engineering \\- unwind ai, accessed October 15, 2025, [https://www.theunwindai.com/p/agentic-context-engineering](https://www.theunwindai.com/p/agentic-context-engineering)  \n25. Agentic Context Engineering: Prompting Strikes Back | by Shashi Jagtap | Superagentic AI, accessed October 15, 2025, [https://medium.com/superagentic-ai/agentic-context-engineering-prompting-strikes-back-c5beade49acc](https://medium.com/superagentic-ai/agentic-context-engineering-prompting-strikes-back-c5beade49acc)  \n26. sci-m-wang/ACE-open: An open-sourced implementation for \"Agentic Context Engineering (ACE)\" methon from \\*Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models\\* (arXiv:2510.04618). \\- GitHub, accessed October 15, 2025, [https://github.com/sci-m-wang/ACE-open](https://github.com/sci-m-wang/ACE-open)  \n27. What Is the Cognitive Apprenticeship Model of Teaching and Its Use ..., accessed October 15, 2025, [https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning](https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning)  \n28. Understanding the Co"
  },
  {
    "id": "report_source",
    "chunk": "eship-model-of-teaching-and-its-use-in-elearning](https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning)  \n28. Understanding the Cognitive Apprenticeship Framework for Smarter Learning \\- Pooks.ai, accessed October 15, 2025, [https://www.pooks.ai/posts/understanding-the-cognitive-apprenticeship-framework-for-smarter-learning.html](https://www.pooks.ai/posts/understanding-the-cognitive-apprenticeship-framework-for-smarter-learning.html)  \n29. AI-Integrated Scaffolding to Enhance Agency and Creativity in K-12 English Language Learners: A Systematic Review \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2078-2489/16/7/519](https://www.mdpi.com/2078-2489/16/7/519)  \n30. Beyond Problem-Solving: The Future of Learning in an AI-Driven ..., accessed October 15, 2025, [https://www.scirp.org/journal/paperinformation?paperid=142115](https://www.scirp.org/journal/paperinformation?paperid=142115)  \n31. Exploring the Impact of AI Tools on Cognitive Skills: A Comparative Analysis \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/1999-4893/18/10/631](https://www.mdpi.com/1999-4893/18/10/631)  \n32. Generative AI-Based Platform for Deliberate Teaching Practice: A Review and a Suggested Framework \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/390139014\\_Generative\\_AI-Based\\_Platform\\_for\\_Deliberate\\_Teaching\\_Practice\\_A\\_Review\\_and\\_a\\_Suggested\\_Framework](https://www.researchgate.net/publication/390139014_Generative_AI-Based_Platform_for_Deliberate_Teaching_Practice_A_Review_and_a_Suggested_Framework)  \n33. Exploring the pedagogical uses of AI chatbots | Teaching Commons, accessed October 15, 2025, [https://teachingcommons.stanford.ed"
  },
  {
    "id": "report_source",
    "chunk": "ice_A_Review_and_a_Suggested_Framework)  \n33. Exploring the pedagogical uses of AI chatbots | Teaching Commons, accessed October 15, 2025, [https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide/exploring-pedagogical-uses-ai-chatbots](https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide/exploring-pedagogical-uses-ai-chatbots)  \n34. Human-AI Partnerships In Education: Entering The Age Of ..., accessed October 15, 2025, [https://the-learning-agency.com/the-cutting-ed/article/human-ai-partnerships-in-education-entering-the-age-of-collaborative-intelligence/](https://the-learning-agency.com/the-cutting-ed/article/human-ai-partnerships-in-education-entering-the-age-of-collaborative-intelligence/)  \n35. Evaluating Human-AI Collaboration: A Review and Methodological Framework \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2407.19098v2](https://arxiv.org/html/2407.19098v2)  \n36. Pros and cons of AI in learning \\- Technology News | The Financial ..., accessed October 15, 2025, [https://www.financialexpress.com/life/technology/pros-and-cons-of-ai-in-learning/4008272/](https://www.financialexpress.com/life/technology/pros-and-cons-of-ai-in-learning/4008272/)  \n37. AI in Computational Thinking Education in Higher Education: A Systematic Literature Review1footnote 11footnote 1A poster based on this paper was accepted and published in the Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education (ITiCSE 2025), DOI: https://doi.org/10.1145/3724389.3730775. \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2510.09677v1](https://arxiv.org/html/2510.09677v1)  \n38. Leveraging Computational Thinking in"
  },
  {
    "id": "report_source",
    "chunk": "145/3724389.3730775. \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2510.09677v1](https://arxiv.org/html/2510.09677v1)  \n38. Leveraging Computational Thinking in the Era of Generative AI ..., accessed October 15, 2025, [https://cacm.acm.org/blogcacm/leveraging-computational-thinking-in-the-era-of-generative-ai/](https://cacm.acm.org/blogcacm/leveraging-computational-thinking-in-the-era-of-generative-ai/)  \n39. AI Fluency: Framework & Foundations \\- Anthropic Courses \\- Skilljar, accessed October 15, 2025, [https://anthropic.skilljar.com/ai-fluency-framework-foundations](https://anthropic.skilljar.com/ai-fluency-framework-foundations)  \n40. The developer role is evolving. Here's how to stay ahead. \\- The ..., accessed October 15, 2025, [https://github.blog/ai-and-ml/the-developer-role-is-evolving-heres-how-to-stay-ahead/](https://github.blog/ai-and-ml/the-developer-role-is-evolving-heres-how-to-stay-ahead/)  \n41. Best practices for pair programming with AI assistants \\- Graphite, accessed October 15, 2025, [https://graphite.dev/guides/ai-pair-programming-best-practices](https://graphite.dev/guides/ai-pair-programming-best-practices)  \n42. AI Agent Best Practices: 12 Lessons from AI Pair Programming for ..., accessed October 15, 2025, [https://forgecode.dev/blog/ai-agent-best-practices/](https://forgecode.dev/blog/ai-agent-best-practices/)  \n43. Pair Programming with AI Coding Agents: Is It Beneficial? \\- Zencoder, accessed October 15, 2025, [https://zencoder.ai/blog/best-practices-for-pair-programming-with-ai-coding-agents](https://zencoder.ai/blog/best-practices-for-pair-programming-with-ai-coding-agents)  \n44. AI Code Assistants Are Revolutionizing Test-Driven Development, accessed October 15, 2025, [https"
  },
  {
    "id": "report_source",
    "chunk": "oder.ai/blog/best-practices-for-pair-programming-with-ai-coding-agents)  \n44. AI Code Assistants Are Revolutionizing Test-Driven Development, accessed October 15, 2025, [https://www.qodo.ai/blog/ai-code-assistants-test-driven-development/](https://www.qodo.ai/blog/ai-code-assistants-test-driven-development/)  \n45. Test-Driven Development with AI: The Right Way to Code Using Generative AI, accessed October 15, 2025, [https://www.readysetcloud.io/blog/allen.helton/tdd-with-ai/](https://www.readysetcloud.io/blog/allen.helton/tdd-with-ai/)  \n46. Test Driven Development Meets Generative AI, accessed October 15, 2025, [https://www.btc-embedded.com/test-driven-development-meets-generative-ai/](https://www.btc-embedded.com/test-driven-development-meets-generative-ai/)  \n47. Spec-driven development with AI: Get started with a new open ..., accessed October 15, 2025, [https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/)  \n48. Full Tutorial: A Proven 3-File  Behind the Craft  Apple Podcasts, accessed October 15, 2025, [https://podcasts.apple.com/ch/podcast/full-tutorial-a-proven-3-file-system-to-vibe-code/id1736359687?i=1000730220313\\&l=fr-FR](https://podcasts.apple.com/ch/podcast/full-tutorial-a-proven-3-file-system-to-vibe-code/id1736359687?i=1000730220313&l=fr-FR)  \n49. snarktank/ai-dev-tasks: A simple task management system ... \\- GitHub, accessed October 15, 2025, [https://github.com/snarktank/ai-dev-tasks](https://github.com/snarktank/ai-dev-tasks)  \n50. Use this 3-file system for structured vibecoding \\- YouTube, accessed October 15, 20"
  },
  {
    "id": "report_source",
    "chunk": "ttps://github.com/snarktank/ai-dev-tasks](https://github.com/snarktank/ai-dev-tasks)  \n50. Use this 3-file system for structured vibecoding \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/shorts/5Pib\\_Llas28](https://www.youtube.com/shorts/5Pib_Llas28)  \n51. A 3-step AI coding workflow for solo founders | Ryan Carson (5x ..., accessed October 15, 2025, [https://pod.wave.co/podcast/how-i-ai/a-3-step-ai-coding-workflow-for-solo-founders-ryan-carson-5x-founder-4fd6a2d4](https://pod.wave.co/podcast/how-i-ai/a-3-step-ai-coding-workflow-for-solo-founders-ryan-carson-5x-founder-4fd6a2d4)  \n52. He's Building a Startup With AI (ft Ryan Carson) \\- Ep 49 \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=Ps3-1c2YrA0](https://www.youtube.com/watch?v=Ps3-1c2YrA0)  \n53. Context Engineering for AI Agents: Lessons from Building Manus, accessed October 15, 2025, [https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)  \n54. Why 'Context Engineering' is the New Frontier for AI Agents, accessed October 15, 2025, [https://www.vellum.ai/blog/context-is-king-why-context-engineering-is-the-new-frontier-for-ai-agents](https://www.vellum.ai/blog/context-is-king-why-context-engineering-is-the-new-frontier-for-ai-agents)  \n55. Case Studies: Real-World Applications of Context Engineering ..., accessed October 15, 2025, [https://www.marktechpost.com/2025/08/12/case-studies-real-world-applications-of-context-engineering/](https://www.marktechpost.com/2025/08/12/case-studies-real-world-applications-of-context-engineering/)  \n56. Context Engineering \\- What it is, and techniques to consider \\- LlamaIndex"
  },
  {
    "id": "report_source",
    "chunk": "www.marktechpost.com/2025/08/12/case-studies-real-world-applications-of-context-engineering/)  \n56. Context Engineering \\- What it is, and techniques to consider \\- LlamaIndex, accessed October 15, 2025, [https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider](https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider)  \n57. Understanding Context Engineering: Principles, Practices, and Its Distinction from Prompt Engineering \\- Architecture & Governance Magazine, accessed October 15, 2025, [https://www.architectureandgovernance.com/applications-technology/understanding-context-engineering-principles-practices-and-its-distinction-from-prompt-engineering/](https://www.architectureandgovernance.com/applications-technology/understanding-context-engineering-principles-practices-and-its-distinction-from-prompt-engineering/)  \n58. The effects of artificial intelligence-based interactive scaffolding on ..., accessed October 15, 2025, [https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2470319](https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2470319)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/05-V2V Pathway Research Proposal Execution.md\">\n\n\n# **The V2V Pathway: A Framework for Achieving Virtuosity in AI-Driven Development through Context Engineering**\n\n## **Executive Summary**\n\nThis report presents a comprehensive framework for advancing software engineering practices in the era of generative artificial intelligence (AI), conceptualized as the \"Vibecoding to Virtuosity\" (V2V) pathway. This developmental model addresses the critical need for a structured transition from the intuitive, often ad-hoc methods of early-stage AI interac"
  },
  {
    "id": "report_source",
    "chunk": " Virtuosity\" (V2V) pathway. This developmental model addresses the critical need for a structured transition from the intuitive, often ad-hoc methods of early-stage AI interaction (\"Vibecoding\") to a disciplined, systematic, and mastery-level approach (\"Virtuosity\"). The central thesis of this analysis is that **Context Engineering** serves as the foundational discipline enabling this transformation. It represents a paradigm shift from the tactical craft of \"prompt engineering\" to the strategic and architectural design of an AI's complete cognitive environment, encompassing its memory, tools, and operational history.  \nThe V2V pathway is structured by a set of integrated methodologies designed to cultivate expert-level performance in human-AI collaboration. The pedagogical underpinning of this journey is the **Cognitive Apprenticeship** model, a framework that makes expert thought processes visible and learnable through stages of modeling, coaching, scaffolding, articulation, reflection, and exploration. This educational model provides a structured approach for training developers to adopt more rigorous and effective interaction patterns with AI systems.  \nAt the core of Virtuosity lies the principle of verifiable correctness, which is operationalized through the disciplined practice of **AI-assisted Test-Driven Development (TDD)**. By writing tests before generating code, developers provide AI models with unambiguous, executable specifications, thereby transforming the development process from a probabilistic exercise into a deterministic engineering practice. This methodology is complemented by a sophisticated model of **Human-AI Teaming**, where developers and AI agents operate in clearly defined, symbiotic roles. Fra"
  },
  {
    "id": "report_source",
    "chunk": "ng practice. This methodology is complemented by a sophisticated model of **Human-AI Teaming**, where developers and AI agents operate in clearly defined, symbiotic roles. Frameworks such as Anthropic's 4D model (Delegation, Description, Discernment, Diligence) provide the operational grammar for this new mode of collaboration, ensuring that interactions are effective, ethical, and accountable.  \nThis report concludes with a series of strategic recommendations for the aiascent.dev project. It proposes a phased implementation roadmap that guides developers through the V2V pathway, from foundational skills in structured prompting to mastery of full-lifecycle automation. The recommendations emphasize the necessity of a holistic approach that integrates advanced tooling, a curriculum based on cognitive apprenticeship, and the redesign of team workflows. The ultimate goal is not merely to enhance developer productivity but to foster a new, more resilient engineering discipline capable of building the next generation of reliable, maintainable, and sophisticated AI-powered systems.  \n---\n\n## **Part I: The Foundational Discipline of Context Engineering**\n\nThe maturation of AI-driven software development is contingent upon a fundamental evolution in how engineers interact with Large Language Models (LLMs). This evolution represents a shift from the tactical manipulation of instructions to the strategic architecture of information environments. The discipline that governs this new paradigm is Context Engineering, a holistic approach that supersedes the narrower focus of prompt engineering and establishes the bedrock for building reliable, scalable, and truly intelligent agentic systems.\n\n### **1.1. Defining the New Paradigm: From "
  },
  {
    "id": "report_source",
    "chunk": "er focus of prompt engineering and establishes the bedrock for building reliable, scalable, and truly intelligent agentic systems.\n\n### **1.1. Defining the New Paradigm: From Prompt to System**\n\nContext Engineering is the delicate art and science of curating and managing the entire set of tokens provided to an LLM to consistently achieve a desired outcome.1 While often conflated with prompt engineering, the two concepts differ fundamentally in scope and ambition. Prompt engineering typically focuses on the immediate, \"for the moment\" task of crafting a specific instruction to elicit a particular response.3 It is a critical skill, but it represents only one component of a much larger system. Context Engineering, in contrast, is the architectural practice of designing the LLM's entire working memoryits \"RAM,\" to use a common analogyat each step of its operation.5 It is not about writing a single perfect letter but about successfully running the entire household.6  \nThis broader perspective acknowledges that an LLM's behavior is conditioned by a rich and dynamic information environment, not just a static instruction. The components of this environment, which must be meticulously engineered, form the context window. These components include:\n\n* **System Prompts / Instructions:** The foundational guidance that sets the agent's role, goals, and constraints.1  \n* **User Input:** The immediate query or task request from the user.1  \n* **Short-Term Memory (Chat History):** The record of recent interactions, providing conversational context.1  \n* **Long-Term Memory:** Persistent information retrieved from external stores, such as vector databases, giving the LLM knowledge beyond the immediate session.1  \n* **Tool Definitions:** "
  },
  {
    "id": "report_source",
    "chunk": "m Memory:** Persistent information retrieved from external stores, such as vector databases, giving the LLM knowledge beyond the immediate session.1  \n* **Tool Definitions:** Descriptions of the functions, APIs, or other external resources the LLM can access.1  \n* **Tool Responses:** The data returned from executing those tools, which is fed back into the context as new information.1  \n* **Structured Data and Schemas:** Formalized data structures (e.g., JSON schemas) that can either constrain the LLM's output for reliability or provide dense, token-efficient information as input.1\n\nThe critical distinction is that Context Engineering treats these elements not as a static collection but as a *dynamic system*.8 In any industrial-strength LLM application, especially agentic systems that operate over multiple turns, the context is constantly evolving. New information arrives from tool calls, the chat history grows, and the relevance of past information shifts. Context Engineering is therefore the practice of building the systems and logic that dynamically select, format, and present the right information and tools at each step, transforming the interaction from a simple, one-shot query into a sophisticated, stateful workflow.8\n\n### **1.2. The Anatomy of Effective Context: Managing Cognitive Load**\n\nThe necessity of Context Engineering stems directly from the architectural limitations of LLMs. Despite their rapidly expanding capabilities, models operate under significant cognitive constraints, analogous to the limits of human working memory.2 The primary constraint is the finite size of the context window, which serves as the model's \"attention budget\".2 Every token introduced into this window, whether an instruction, a piece"
  },
  {
    "id": "report_source",
    "chunk": " constraint is the finite size of the context window, which serves as the model's \"attention budget\".2 Every token introduced into this window, whether an instruction, a piece of data, or a tool definition, depletes this budget. This scarcity is rooted in the transformer architecture itself, where the computational cost of attention scales quadratically with the number of tokens (O(n2)), meaning every token must attend to every other token.2  \nExceeding this attention budget or poorly managing the information within it leads to a range of predictable failure modes that degrade performance and reliability. These failures underscore why simply expanding the context window is not a panacea; the quality and organization of the context are paramount. Common failure modes include:\n\n* **Context Poisoning:** Occurs when a hallucination or incorrect piece of information enters the context and is subsequently referenced by the model, leading to a cascade of errors.5  \n* **Context Distraction:** An overly long or cluttered context window can cause the model to focus on irrelevant details, overwhelming its ability to identify the primary task or the most salient information.5 This is often referred to as the \"lost in the middle\" problem, where information presented in the middle of a long context is more likely to be ignored.11  \n* **Context Confusion:** Superfluous or poorly structured context can improperly influence the model's response, leading it to generate outputs that are technically plausible but misaligned with the user's intent.5  \n* **Context Clash:** Arises when different parts of the context contain contradictory information, forcing the model to either choose one over the other arbitrarily or generate a confused, non-"
  },
  {
    "id": "report_source",
    "chunk": "** Arises when different parts of the context contain contradictory information, forcing the model to either choose one over the other arbitrarily or generate a confused, non-committal response.5\n\nGiven these constraints, the central task of Context Engineering is an optimization problem: to find the \"smallest possible set of high-signal tokens that maximize the likelihood of some desired outcome\".2 This principle of cognitive load management dictates that every token must justify its presence in the context window. The discipline moves beyond simply providing information to strategically curating it, ensuring that the model's limited attention is always focused on the most relevant, accurate, and non-contradictory data needed to perform the next step in its task.\n\n### **1.3. A Taxonomy of Context Management Strategies**\n\nTo address the challenges of managing an LLM's cognitive load, practitioners have developed a set of canonical strategies. These techniques can be grouped into four high-level categories: **Write, Select, Compress, and Isolate**.5 Together, they form a comprehensive toolkit for architecting an efficient and effective context window.\n\n* **Write (Externalizing Information):** \"Write\" strategies involve storing information outside the immediate LLM prompt for later use, analogous to a human taking notes to offload working memory.9 This prevents the context window from being burdened with information that is not immediately relevant but may be needed later.  \n  * **Scratchpads:** These are temporary storage areas for an agent to record intermediate thoughts, plans, or data during a multi-step task. They can be implemented as a tool that writes to a file or as a field in a runtime state object that persists "
  },
  {
    "id": "report_source",
    "chunk": " intermediate thoughts, plans, or data during a multi-step task. They can be implemented as a tool that writes to a file or as a field in a runtime state object that persists for the duration of a session.5  \n  * **Long-Term Memory:** This involves persisting information across sessions. Techniques range from simple file storage to sophisticated systems. Research concepts like **Reflection**, where an agent summarizes what it learned after each turn, and **Generative Memory**, where an agent periodically synthesizes important facts, provide frameworks for creating and updating these persistent memories.5  \n* **Select (Retrieving Relevant Information):** \"Select\" strategies focus on pulling the right information into the context window at the right time.9 If writing creates an external library of knowledge, selecting is the process of checking out the correct book.  \n  * **Retrieval-Augmented Generation (RAG):** RAG is the cornerstone \"select\" technique, allowing an LLM to access vast external knowledge bases by retrieving only the most relevant chunks of information for a given query.9 This is the primary mechanism for grounding models in up-to-date, domain-specific facts without fine-tuning.  \n  * **Memory and Tool Selection:** For agents with access to extensive long-term memories or a large suite of tools, selection mechanisms are crucial. This can involve using RAG to retrieve the most relevant memories or tool descriptions based on the current task, preventing tool overload and ensuring the agent has the right capabilities at its disposal.5  \n* **Compress (Reducing Token Footprint):** \"Compress\" strategies aim to retain the semantic essence of information while reducing its token count, making the most of the limite"
  },
  {
    "id": "report_source",
    "chunk": " **Compress (Reducing Token Footprint):** \"Compress\" strategies aim to retain the semantic essence of information while reducing its token count, making the most of the limited space in the context window.5  \n  * **Summarization:** This involves using an LLM to generate a concise summary of lengthy content, such as a long conversation history or a verbose tool output. Summaries can be hierarchical (summarizing chunks and then summarizing the summaries) or continual (maintaining a running summary that is updated each turn).9  \n  * **Trimming / Pruning:** Instead of rephrasing content, this strategy simply drops the least relevant pieces. This can be done with simple heuristics, like removing the oldest messages from a chat history, or with more sophisticated models trained specifically to prune context intelligently.5  \n  * **Persona Injection:** A novel compression technique where the semantic essence of a very long interaction (e.g., 900K tokens) is distilled into a set of stable personas or roles. By instructing a new session to \"become the heroes\" of the previous one, the core dynamics and knowledge are preserved in a highly compressed, narrative form, reducing the token count by over 95% while maintaining the spirit of the interaction.13  \n* **Isolate (Compartmentalizing Context):** \"Isolate\" strategies involve splitting a task or its required information into separate, focused contexts to prevent interference and maintain clarity.9  \n  * **Multi-Agent Architectures:** This is a powerful isolation technique where a complex problem is decomposed and distributed among specialized sub-agents.2 Each sub-agent has its own isolated context window, instructions, and tools, allowing it to focus deeply on its sub-task. A lead"
  },
  {
    "id": "report_source",
    "chunk": "and distributed among specialized sub-agents.2 Each sub-agent has its own isolated context window, instructions, and tools, allowing it to focus deeply on its sub-task. A lead or supervisor agent coordinates their work, receiving only condensed summaries, which keeps its own context clean and high-level.5  \n  * **Sandboxed Environments:** For tasks involving code execution or interaction with complex data (like images or audio), the process can be run in a sandboxed environment. The LLM's context is kept clean, only receiving selected, relevant outputs (e.g., return values, error messages) from the sandbox rather than the entire execution trace.5\n\nThe application of these strategies is not merely a technical exercise; it is a form of cognitive ergonomics for AI. The limitations of the LLM's context windowits finite size and susceptibility to distractionare analogous to the cognitive limits of human attention and working memory. The strategies of writing (offloading to external memory), selecting (just-in-time retrieval), compressing (summarization), and isolating (avoiding multitasking) are direct parallels to the techniques humans use to manage complex cognitive tasks. This parallel suggests that the most effective AI engineers will be those who think like cognitive scientists, designing the \"mental\" environment of their agents to optimize for focus, clarity, and performance.  \nFurthermore, as these strategies become more sophisticated, they point toward the next layer of abstraction in AI system design: **Workflow Engineering**.1 While Context Engineering focuses on optimizing the information for a single LLM call, Workflow Engineering addresses the optimal *sequence* of LLM calls and non-LLM steps required to reliab"
  },
  {
    "id": "report_source",
    "chunk": "ngineering focuses on optimizing the information for a single LLM call, Workflow Engineering addresses the optimal *sequence* of LLM calls and non-LLM steps required to reliably complete a complex task. Multi-agent systems are a prime example of this, where the overall task is broken down into a workflow of specialized agent calls. This hierarchical structurefrom prompt to context to workflowindicates a clear path of maturation for the field. True mastery in AI development extends beyond perfecting a single agent's context; it lies in architecting and orchestrating meta-level workflows of multiple agents, tools, and human validation steps.  \nThe following table provides a clear, multi-faceted comparison of the paradigm shift from prompt engineering to the more comprehensive discipline of Context Engineering, serving as a foundational reference for understanding this critical evolution.\n\n| Dimension | Prompt Engineering | Context Engineering |\n| :---- | :---- | :---- |\n| **Scope** | Operates within a single input-output pair; focused on the immediate turn.4 | Handles the entire agentic workflow, including memory, history, and tools across multiple turns.4 |\n| **Mindset** | Creative writing or copy-tweaking; relies on wordsmithing and intuition.4 | Systems design or software architecture for LLMs; focuses on designing the entire flow and thought process.1 |\n| **Goal** | Elicit a specific, often one-off, response from the model.4 | Ensure consistent, reliable, and predictable system performance across sessions, users, and tasks.4 |\n| **Key Artifacts** | The prompt text itself.3 | The full context window: system prompts, memory, tool outputs, chat history, and schemas.1 |\n| **Tools & Techniques** | Text editors, prompt lib"
  },
  {
    "id": "report_source",
    "chunk": "* | The prompt text itself.3 | The full context window: system prompts, memory, tool outputs, chat history, and schemas.1 |\n| **Tools & Techniques** | Text editors, prompt libraries, and direct interaction with chat interfaces.4 | RAG systems, vector stores, multi-agent frameworks (e.g., LangGraph), state management, and API chaining.4 |\n| **Scalability** | Brittle and difficult to maintain at scale; prone to failure with more users and edge cases.4 | Designed with scale, consistency, and reusability in mind from the beginning.4 |\n| **Debugging** | Rewording prompts and guessing what went wrong in the model's \"black box\".4 | Inspecting the full context window, token flow, memory slots, and tool call outputs to diagnose system behavior.4 |\n\n---\n\n## **Part II: Charting the Course from Vibecoding to Virtuosity**\n\nThe journey of a developer or a team in the age of generative AI can be understood as a progression along a spectrum, from an informal, intuition-driven style of interaction to a highly disciplined, expert-level practice. This progression, termed the \"Vibecoding to Virtuosity\" (V2V) pathway, requires more than just the adoption of new tools; it demands a new pedagogy for skill acquisition and a deeper understanding of the cognitive principles that underpin mastery.\n\n### **2.1. The Developer's Journey: Defining the V2V Spectrum**\n\nThe V2V spectrum describes the maturation of a developer's approach to building with AI. The two ends of this spectrum, Vibecoding and Virtuosity, represent distinct philosophies, workflows, and outcomes.\n\n* **Vibecoding:** This is the entry point for many developers interacting with AI. It is an intuitive, exploratory, and often unstructured mode of development characterized by \"free-form"
  },
  {
    "id": "report_source",
    "chunk": "ing:** This is the entry point for many developers interacting with AI. It is an intuitive, exploratory, and often unstructured mode of development characterized by \"free-form,\" chat-based interactions.14 The developer relies on a \"vibe\" or a general sense of what is needed, engaging in a conversational back-and-forth with the AI to shape the outcome. This approach is powerful for rapid prototyping, brainstorming, and solving small, self-contained problems.14 It is analogous to the \"one time success, vibe coding\" mentality, where the goal is a quick, functional result for a proof-of-concept.3 However, this method is inherently brittle. It lacks reproducibility, is difficult to debug when it fails, and does not scale to complex, production-grade systems where reliability and maintainability are paramount.14  \n* **Virtuosity:** This represents the mastery end of the spectrum. It is a disciplined, systematic, and reliable approach to AI-driven development. Virtuosity is characterized by \"spec-driven workflows,\" where interactions are guided by explicit plans, requirements, and verifiable constraints.14 This approach embodies the \"industrialization of prompting,\" moving from casual conversation to the design of standardized, repeatable, and robust processes.16 A developer operating at the level of Virtuosity does not simply \"chat\" with the AI; they architect the interaction, applying sound software engineering principles to ensure the final product is predictable, maintainable, and scalable. This involves deliberate planning, rigorous validation (e.g., through Test-Driven Development), and a deep understanding of the principles of Context Engineering.\n\n### **2.2. Cognitive Apprenticeship: A Pedagogical Model for the V2V Path"
  },
  {
    "id": "report_source",
    "chunk": "through Test-Driven Development), and a deep understanding of the principles of Context Engineering.\n\n### **2.2. Cognitive Apprenticeship: A Pedagogical Model for the V2V Pathway**\n\nThe transition from Vibecoding to Virtuosity is not automatic; it requires a deliberate and structured learning process. The **Cognitive Apprenticeship** model, originally developed to teach complex cognitive skills, provides an ideal pedagogical framework for guiding this journey.17 The central tenet of this model is to make the invisible thinking processes of an expert visible to the novice, allowing them to learn not just *what* to do, but *how* to think.17 This is particularly relevant for the V2V pathway, where the goal is to internalize the expert mindset of a Virtuoso developer.  \nThe six stages of Cognitive Apprenticeship can be directly mapped to the process of training a developer in advanced AI interaction techniques:\n\n1. **Modeling:** An expert (e.g., a senior developer or team lead) demonstrates a Virtuoso workflow. They don't just execute the steps; they \"think aloud,\" verbalizing their thought process.17 For example, they might explain *why* they are structuring a task context file in a particular way, *why* they are choosing a specific RAG strategy, or *why* they are writing a failing test before prompting the AI for code. This makes the expert's strategic reasoning explicit and accessible.  \n2. **Coaching:** The novice developer attempts a task, such as implementing a feature using an AI agent. The expert observes and provides real-time guidance, feedback, and hints.18 This coaching can focus on improving the developer's context curation, their prompt structuring, or their critical evaluation of the AI's output. Increasingly,"
  },
  {
    "id": "report_source",
    "chunk": "k, and hints.18 This coaching can focus on improving the developer's context curation, their prompt structuring, or their critical evaluation of the AI's output. Increasingly, AI itself can serve as a tireless coach, providing 24/7 guidance through chatbots or intelligent tutoring systems that reinforce learning.19  \n3. **Scaffolding:** This involves providing structured supports that help the novice perform the task, which are gradually faded as their competence grows.18 In the V2V context, scaffolding can take many forms: providing pre-written prompt templates, offering a library of effective RAG pipeline configurations, or supplying structured planning documents (e.g., planning.md or PRD templates) that guide the developer's interaction with the AI.22 The developer initially uses these scaffolds to produce reliable results and eventually learns to build these structures themselves.  \n4. **Articulation:** The novice is required to explain their own reasoning and thought processes.18 For instance, a coach might ask the developer to justify their choice of chunking strategy for a RAG system or to articulate the step-by-step plan they intend to give the AI agent. This crucial step forces the developer to move from an intuitive, \"vibe-based\" approach to one based on explicit, defensible engineering decisions.  \n5. **Reflection:** The developer compares their own performanceboth the process and the final productagainst an expert's or a pre-defined standard.18 This could involve comparing their AI-generated code to a reference implementation or analyzing why their workflow was less efficient than the one demonstrated during the modeling phase. This reflective practice is key to identifying weaknesses and internalizing best"
  },
  {
    "id": "report_source",
    "chunk": "ing why their workflow was less efficient than the one demonstrated during the modeling phase. This reflective practice is key to identifying weaknesses and internalizing best practices.  \n6. **Exploration:** Finally, the developer is given a complex, open-ended problem and is challenged to solve it independently, applying the full suite of Virtuoso techniques.18 This stage encourages them to move beyond simply executing learned procedures and to adapt, innovate, and push the boundaries of their skills in new contexts.\n\n### **2.3. The Engine of Progress: Deliberate Practice and Computational Thinking**\n\nUnderpinning the Cognitive Apprenticeship model are two fundamental concepts that drive the acquisition of expertise: Deliberate Practice and Computational Thinking.\n\n* **Deliberate Practice:** Mastery is not achieved through simple repetition but through **Deliberate Practice**a highly structured form of practice that involves intentionally repeating an activity to improve performance, with careful attention to feedback.24 It is this purposeful, goal-oriented effort that distinguishes experts from novices.26 In the V2V pathway, this means not just using AI tools frequently, but engaging in focused exercises designed to improve specific skills. For example, a developer might deliberately practice the skill of context pruning by repeatedly attempting to reduce the token count of a complex prompt without degrading the AI's performance, or practice prompt refinement by iteratively improving a prompt to handle a known set of edge cases.  \n* **Computational Thinking:** This is the essential mental toolkit required to operate at the Virtuosity level. Computational Thinking is a problem-solving approach that involves a set of c"
  },
  {
    "id": "report_source",
    "chunk": "nal Thinking:** This is the essential mental toolkit required to operate at the Virtuosity level. Computational Thinking is a problem-solving approach that involves a set of core skills, including **decomposition** (breaking complex problems into smaller, manageable parts), **pattern recognition**, **abstraction** (focusing on essential details while ignoring irrelevant ones), and **algorithmic thinking** (developing step-by-step solutions).27 This skillset is not just for computer scientists; it is a universally applicable framework for systematic problem-solving.29 In the age of AI, effective prompt engineering is, in essence, an application of computational thinking.27 The ability to decompose a complex feature request into a logical sequence of AI prompts and tool calls, to abstract the core requirements into a clear specification, and to design an algorithmic workflow for the AI to follow is the very definition of moving beyond Vibecoding.31\n\nThe transition from Vibecoding to Virtuosity is therefore not merely a technical upgrade but a profound cultural and organizational shift. Vibecoding represents an individualistic, \"hero\" model of development, where success is dependent on the opaque intuition of a single developer. Virtuosity, in contrast, is a team-oriented, systemic model built on shared processes, explicit reasoning, and verifiable artifacts. The Cognitive Apprenticeship model is inherently social, focused on the transmission of expertise through guided interaction. Consequently, any platform or initiative aiming to facilitate the V2V pathway must provide tools that support collaborative workflows, such as sharing prompt templates, version-controlling context strategies, and enabling peer review of AI-gener"
  },
  {
    "id": "report_source",
    "chunk": "pathway must provide tools that support collaborative workflows, such as sharing prompt templates, version-controlling context strategies, and enabling peer review of AI-generated plans and code. The product becomes not just a developer tool, but a tool for organizational change management.  \nThis new paradigm also necessitates a shift in pedagogy. The very availability of powerful AI tools, which can provide answers without the preceding cognitive struggle, disrupts traditional learning models and creates a risk of fostering passive learning and cognitive shortcuts.32 The educational focus must therefore shift from *problem-solving*, which AI can increasingly automate, to *problem-framing*, critical evaluation, and the interrogation of AI-generated outputs.32 This aligns perfectly with the V2V pathway. A Vibecoder might passively accept an AI's output. A Virtuoso developer, however, actively interrogates itfor instance, by writing a failing test *first* to rigorously validate the AI's code, or by forcing the AI to critique its own plan to expose hidden assumptions. The most valuable human skill in the AI era is not the generation of solutions, but their validation. A platform designed to foster Virtuosity must be a \"gym\" for Deliberate Practice in this critical skill, actively equipping users to challenge, verify, and refine AI-generated artifacts.  \n---\n\n## **Part III: Core Methodologies for Structured AI Interaction**\n\nAchieving Virtuosity in AI-driven development requires a move away from the ambiguity of natural language conversation towards the precision of formal specification. This transition is enabled by a set of core methodologies that structure the dialogue between human and AI, making interactions more pred"
  },
  {
    "id": "report_source",
    "chunk": "he precision of formal specification. This transition is enabled by a set of core methodologies that structure the dialogue between human and AI, making interactions more predictable, reliable, and repeatable. These techniques form the foundational building blocks for any disciplined, spec-driven workflow.\n\n### **3.1. Structuring the Dialogue: From Conversation to Specification**\n\nThe first and most crucial step away from Vibecoding is the adoption of **Structured Prompting**. This methodology decomposes complex tasks into a series of modular, explicit steps and leverages formalized structures to constrain the model's input, reasoning process, or output.35 Unlike conversational prompting, which is accessible but often yields inconsistent results, structured prompting is an engineering discipline aimed at producing reliable and reusable \"recipes\" for AI interaction.36  \nA key technique within this approach is the use of structured data formats, most notably JSON. By providing the LLM with a JSON template for its output, developers can enforce a strict schema, ensuring adherence to required data types and constraints with over 99% fidelity.16 This transforms the LLM's output from an unpredictable string of text into a dependable, machine-readable data object, dramatically reducing parsing errors and simplifying integration with downstream systems.16 This method also promotes a clean prompt architecture, allowing different components of the promptsuch as system instructions, context, few-shot examples, and the user queryto be neatly organized into labeled keys and objects, enhancing clarity and maintainability.16  \nBuilding on this foundation, **Mega-Prompting** offers a framework for creating complex, reusable AI assista"
  },
  {
    "id": "report_source",
    "chunk": "beled keys and objects, enhancing clarity and maintainability.16  \nBuilding on this foundation, **Mega-Prompting** offers a framework for creating complex, reusable AI assistants. A mega-prompt is not a single instruction but a comprehensive configuration that includes several key elements to guide the AI's behavior across a range of scenarios.38 The P.R.E.P. framework provides a useful mnemonic for its construction: **P**rompt it, give it a **R**ole, provide **E**xplicit instructions, and set **P**recise parameters.38 A well-crafted mega-prompt goes further by incorporating conditional logic (e.g., \"if-then\" statements) and scenario-based instructions, allowing the AI to adapt its response based on the specific context of a business challenge.38 This turns the AI from a simple tool into a configurable, multi-purpose assistant.  \nA powerful and often overlooked aspect of structuring the dialogue is the optimization of **In-Context Learning (ICL)**. While providing few-shot examples is a well-known best practice for guiding model behavior 2, Virtuosity demands a more sophisticated approach. Research has shown that the performance of ICL is highly sensitive to the *order* in which examples are presented in the prompt.39 Techniques like **OptiSeq** use the log probabilities of LLM outputs to systematically prune the vast search space of possible orderings and identify the optimal sequence on-the-fly, improving accuracy by a significant margin.39 An even more advanced strategy is **Context Tuning**, a method that directly optimizes the context itself without altering the model's weights.41 This technique initializes a trainable prompt or prefix with task-specific demonstration examples and then uses gradient descent to refin"
  },
  {
    "id": "report_source",
    "chunk": "thout altering the model's weights.41 This technique initializes a trainable prompt or prefix with task-specific demonstration examples and then uses gradient descent to refine this \"soft prompt,\" effectively fine-tuning the context to better steer the model's behavior for a specific task.41 This represents a move from manually crafting examples to algorithmically optimizing the context for maximum performance.\n\n### **3.2. Engineering the Knowledge Flow: Advanced RAG Strategies**\n\nFor any AI system that must operate on information beyond its training data, **Retrieval-Augmented Generation (RAG)** is an indispensable component of Context Engineering.43 RAG systems ground the LLM in external, up-to-date, and domain-specific knowledge, significantly enhancing factual accuracy and reducing hallucinations.44 However, moving from a basic RAG implementation to a Virtuoso-level system requires engineering a sophisticated, multi-stage pipeline where each component is carefully optimized.  \nThe modern RAG pipeline can be broken down into a sequence of distinct stages, each with its own set of best practices:\n\n1. **Pre-processing and Chunking:** The initial step involves breaking down source documents into smaller, retrievable chunks. The choice of chunk size and strategy is critical; smaller chunks can improve retrieval accuracy but may lack sufficient context, while larger chunks provide more context but can introduce noise and increase processing overhead.11 Advanced strategies like \"Small2Big\" chunking, where smaller chunks are retrieved but the surrounding larger \"parent\" chunk is passed to the LLM, aim to balance these trade-offs.44  \n2. **Retrieval:** This stage involves finding the most relevant chunks for a given query. Wh"
  },
  {
    "id": "report_source",
    "chunk": "g larger \"parent\" chunk is passed to the LLM, aim to balance these trade-offs.44  \n2. **Retrieval:** This stage involves finding the most relevant chunks for a given query. While vector similarity search is the standard approach, advanced techniques can significantly improve relevance. These include **query transformations**, where the initial query is rewritten or expanded by an LLM to better match the documents. Examples include **HyDE (Hypothetical Document Embeddings)**, which generates a hypothetical answer and uses its embedding to find similar real documents, and **query decomposition**, which breaks a complex question into sub-questions for individual retrieval.11 Other advanced methods include **multi-source retrieval**, which can pull from different knowledge bases (e.g., structured and unstructured data), and **hybrid search**, which combines keyword-based search with vector search to capture both semantic relevance and lexical matches.46  \n3. **Re-ranking and Filtering:** The initial retrieval step often returns more documents than can fit in the context window. A crucial subsequent step is **re-ranking**, where a more powerful (but slower) model evaluates the relevance of the initially retrieved chunks and reorders them to place the most important information first.11 This directly addresses the \"lost in the middle\" problem, where LLMs tend to pay more attention to information at the beginning and end of the context.11 Filtering can also be applied here to remove redundant or low-quality documents.  \n4. **Synthesis and Post-processing:** Before passing the retrieved information to the final generator LLM, a synthesis step can create a more condensed and structured context. This might involve using another LL"
  },
  {
    "id": "report_source",
    "chunk": "* Before passing the retrieved information to the final generator LLM, a synthesis step can create a more condensed and structured context. This might involve using another LLM to summarize the key points from the retrieved documents or to extract and organize the most relevant facts into a structured format.11 Methods like **RECOMP** and **LLMLingua** are designed specifically for this purpose, compressing the retrieved context to reduce the token count and cognitive load on the final generator model.11\n\nRecent research has also introduced novel, end-to-end RAG architectures that push performance further. **Contrastive In-Context Learning RAG** enhances response accuracy by including demonstration examples of similar query structures within the prompt, allowing the model to learn effective response patterns.47 **Focus Mode RAG** improves signal-to-noise ratio by extracting only the most essential sentences from the retrieved documents, rather than passing the entire chunk, ensuring the context is dense with relevant information.  \nThese structured interaction methodologies are not merely isolated techniques for improving AI outputs; they are practical implementations of the \"Scaffolding\" stage of Cognitive Apprenticeship. For a novice developer operating in the Vibecoding realm, the challenge of getting a reliable, production-quality output from an AI can be insurmountable. Structured Prompting, with its rigid JSON schemas and templates, provides a critical scaffold that guides the AI's output into a predictable form.16 Similarly, a pre-configured, advanced RAG pipeline acts as a knowledge scaffold, relieving the developer of the burden of manually finding and injecting the necessary context. A platform designed to fost"
  },
  {
    "id": "report_source",
    "chunk": " advanced RAG pipeline acts as a knowledge scaffold, relieving the developer of the burden of manually finding and injecting the necessary context. A platform designed to foster Virtuosity can therefore be conceptualized as a \"scaffolding engine,\" offering a library of progressively more complex templates and RAG configurations. Users can adopt these scaffolds initially and then learn to customize and build their own as they advance along the V2V pathway, perfectly embodying the pedagogical principle of the \"gradual release of responsibility\".22  \nThe evolution of RAG itself serves as a microcosm of the entire V2V journey. Early, basic RAG implementations are akin to Vibecoding: one simply throws a query at a vector store and hopes for a relevant result. In contrast, the advanced, multi-stage RAG pipelines described in recent literature are spec-driven workflows.11 They consist of distinct, deliberately engineered stagesquery classification, expansion, multi-modal retrieval, re-ranking, and compressive synthesis. This complex process requires systematic design and optimization at each step, embodying the principles of decomposition and algorithmic thinking that are central to Virtuosity. Therefore, guiding a developer through the process of building and optimizing a sophisticated RAG system is, in itself, a powerful exercise in moving them from Vibecoding to Virtuosity.  \n---\n\n## **Part IV: Collaborative Frameworks and Disciplined Workflows**\n\nAs AI systems move from simple tools to active collaborators in the software development process, the focus shifts from individual interaction techniques to the overarching frameworks and workflows that govern the human-AI partnership. Achieving Virtuosity is not just about what a"
  },
  {
    "id": "report_source",
    "chunk": "s shifts from individual interaction techniques to the overarching frameworks and workflows that govern the human-AI partnership. Achieving Virtuosity is not just about what a single developer does, but about how teams structure their collaboration with AI to ensure reliability, accountability, and quality at scale. This requires adopting formal collaboration models and transitioning from ad-hoc, conversational coding to disciplined, spec-driven development lifecycles.\n\n### **4.1. Structuring the Partnership: Human-AI Collaboration Frameworks**\n\nEffective and responsible AI integration demands a deliberate model for collaboration. The nature of the partnership can be categorized along a spectrum of control and agency, typically falling into one of three modes: **AI-Centric**, where the AI drives the process with human oversight; **Human-Centric**, where humans play the critical leadership and decision-making roles; and **Symbiotic**, where humans and AI engage in a deeply integrated, mutually beneficial partnership, leveraging the unique strengths of each.48 The V2V pathway strongly advocates for a Human-Centric or Symbiotic model, emphasizing that while AI can handle implementation and analysis, humans must retain strategic direction, ethical judgment, and critical oversight.50  \nTo operationalize this partnership, a structured framework is needed to guide the interaction. **Anthropic's 4D AI Fluency Framework** provides a robust and comprehensive model for this purpose, defining four interconnected competencies necessary for effective, efficient, ethical, and safe human-AI collaboration.54 These four \"Ds\" are directly applicable to the software development context:\n\n1. **Delegation:** This is the strategic act of decid"
  },
  {
    "id": "report_source",
    "chunk": "ethical, and safe human-AI collaboration.54 These four \"Ds\" are directly applicable to the software development context:\n\n1. **Delegation:** This is the strategic act of deciding whether, when, and how to engage with AI.55 It involves making thoughtful decisions about which tasks are appropriate to offload to an AI agent and which require human expertise. In software development, this means delegating tasks like writing boilerplate code, generating unit tests, or drafting documentation, while retaining human control over architectural design, complex logic, and final code review.56  \n2. **Description:** This competency involves effectively communicating goals to the AI to prompt useful behaviors and outputs.55 It is the practical application of the structured prompting and context engineering techniques discussed previously. A virtuous developer excels at providing clear, context-rich, and unambiguous descriptions of the desired outcome, scope, and constraints, maximizing the efficiency and effectiveness of the AI's contribution.57  \n3. **Discernment:** This is the critical skill of accurately assessing the usefulness of AI outputs and behaviors.55 It requires the developer to thoughtfully and critically evaluate what the AI produces for accuracy, quality, security vulnerabilities, performance implications, and alignment with project requirements.57 Discernment is the core of the \"human-in-the-loop\" role and involves a continuous feedback loop of evaluation and refinement.56  \n4. **Diligence:** This competency encompasses taking responsibility for the interactions with AI and the final work product.55 It involves being thoughtful about which AI systems to use, being transparent about the AI's role in the work, and ultima"
  },
  {
    "id": "report_source",
    "chunk": "he interactions with AI and the final work product.55 It involves being thoughtful about which AI systems to use, being transparent about the AI's role in the work, and ultimately taking ownership and accountability for verifying and vouching for the outputs that are used or shared.56\n\nThis model finds a parallel in educational settings with the **Human-Centric AI-First (HCAIF)** teaching framework, which similarly emphasizes the importance of **attribution** (clearly showing how and where AI was used) and **reflection** (analyzing the effectiveness and limitations of the AI's contribution) as essential factors for responsible and ethical AI engagement.59  \nThese competencies directly map to the V2V pathway at the individual developer level. A Vibecoder may delegate poorly, asking the AI to \"build the entire feature,\" leading to failure. A Virtuoso developer, in contrast, makes granular, strategic delegation decisions that align with their role as an architect. Description is the practical application of the structured interaction techniques that define progress along the path. Discernment is the critical thinking skill that separates a passive user from an active engineering partner, serving as the antidote to the cognitive shortcuts that unstructured AI use can encourage. Finally, Diligence represents the professional accountability that is the hallmark of Virtuosity; where a Vibecoder might \"copy-paste and pray,\" a Virtuoso takes full ownership of the final, verified artifact.\n\n### **4.2. From Free-Form to Spec-Driven: AI Development Workflows**\n\nThe choice of collaboration framework must be reflected in the team's day-to-day development workflow. The V2V pathway marks a clear transition between two primary AI coding "
  },
  {
    "id": "report_source",
    "chunk": "\nThe choice of collaboration framework must be reflected in the team's day-to-day development workflow. The V2V pathway marks a clear transition between two primary AI coding styles:\n\n* **Free-form / Vibe Coding:** This is a chat-based, flexible workflow that is fast for prototyping and exploration. However, it is often messy, difficult to reproduce, and unreliable for complex, production-grade tasks as the AI can easily lose context or deviate from the intended plan.14  \n* **Spec-driven Workflows:** This is a more structured and reliable approach that forces a more disciplined, step-by-step process. It takes more time upfront but results in more predictable and higher-quality outcomes.14\n\nA virtuous, spec-driven workflow incorporates several key practices that transform the development process:\n\n1. **Planning First:** Before any code is written, a detailed, written plan is created. This can be a formal Product Requirements Document (PRD) or a simpler planning.md file within the repository.60 A powerful technique is to have the AI draft an initial plan and then prompt it to critique its own plan, forcing it to identify gaps, edge cases, and potential issues before implementation begins. This single step can eliminate the vast majority of instances where an AI \"gets confused\" halfway through a task.61  \n2. **Structured AI Coding with Task Context:** Instead of relying on the ephemeral history of a chat window, this approach uses dedicated, persistent files (e.g., .task files) to define the AI's task.63 These \"Task Context\" files contain the prompt, the plan, and references to relevant parts of the codebase. This makes the AI interaction a first-class citizen of the workspaceit becomes shareable, reproducible, and version"
  },
  {
    "id": "report_source",
    "chunk": "he plan, and references to relevant parts of the codebase. This makes the AI interaction a first-class citizen of the workspaceit becomes shareable, reproducible, and version-controlled, just like any other piece of code.63  \n3. **AI Pair Programming Best Practices:** The interaction is structured like a traditional pair programming session, with clearly defined roles. The **human acts as the Navigator**, directing the overall strategy, making architectural decisions, and reviewing the AI's work. The **AI acts as the Driver**, generating code, suggesting refactors, and implementing the human's plan.65 This model requires the human to provide curated context, engage in iterative refinement loops, and perform critical code review on all AI-generated output.65\n\nThis evolution from a chat-based interaction to a spec-driven workflow using persistent, version-controlled task files represents a fundamental change in the programming model. A chat is a largely stateless and ephemeral interaction, characteristic of the Vibecoding model. A spec-driven workflow, however, externalizes the state and instructions into a formal, machine-readable artifact. This artifact can be versioned in Git, reviewed by peers, and fed to an AI agent in a completely reproducible manner. The AI's role shifts from a conversational partner to an executor of a formal plan. This transforms the interaction from a conversation into a more deterministic computation. The developer is no longer just \"chatting\" with the AI; they are, in effect, programming the AI by creating and manipulating a state document that defines the task. This is a far more robust, scalable, and professional model for software engineering in the age of AI.  \n---\n\n## **Part V: Engineerin"
  },
  {
    "id": "report_source",
    "chunk": "g a state document that defines the task. This is a far more robust, scalable, and professional model for software engineering in the age of AI.  \n---\n\n## **Part V: Engineering for Verifiable Virtuosity**\n\nThe culmination of the V2V pathway is the application of rigorous engineering disciplines to the development process, ensuring that software built with AI assistance is not just functional but also correct, robust, and maintainable. This level of Virtuosity is achieved by integrating methods that provide objective, verifiable evidence of quality, moving beyond subjective evaluation to automated validation.\n\n### **5.1. Test-Driven Development (TDD) with AI: The Ultimate Validation**\n\n**Test-Driven Development (TDD)** is a software development process that inverts the traditional code-then-test cycle. It follows a simple, rhythmic loop: **Red** (write a test for a new feature that initially fails), **Green** (write the minimum amount of code necessary to make the test pass), and **Refactor** (clean up the code while ensuring all tests continue to pass).67  \nThis methodology is exceptionally well-suited for AI-driven development because it addresses the core challenge of working with generative models: their inherent ambiguity and potential for hallucination. A failing test serves as the clearest, most unambiguous specification or goal that can be given to an AI.67 Instead of a vague natural language prompt like \"create a login function,\" the AI is given an executable contract that defines exactly how the function must behave, including its handling of valid inputs, invalid inputs, and edge cases. This provides a concrete target for the AI, allowing it to iterate and self-correct with precision until the job is done corre"
  },
  {
    "id": "report_source",
    "chunk": "of valid inputs, invalid inputs, and edge cases. This provides a concrete target for the AI, allowing it to iterate and self-correct with precision until the job is done correctly.67  \nThe most effective workflow for AI-assisted TDD is the **\"Edit-Test Loop,\"** a practice that perfectly embodies the Human-Navigator/AI-Driver collaboration model.61 In this loop:\n\n1. The human developer (the Navigator) writes a new test that captures the desired behavior and confirms that it fails. This step requires human expertise to define the requirements and edge cases.  \n2. The developer then instructs the AI (the Driver) with a simple prompt: \"Make this test pass\".61  \n3. The AI generates the implementation code, runs the test, and refines the code until the test passes.  \n4. The human developer reviews the now-passing code and the test, and then refactors for quality and clarity.\n\nThis process leverages the strengths of both partners: the human provides the strategic direction and quality control by defining the test, while the AI handles the tactical, often repetitive, work of implementation. AI can further accelerate this process by generating the boilerplate for test files, suggesting potential test cases based on the code's structure, and even automatically updating tests when the underlying code is refactored.67 This turns TDD's perceived weaknessthe time required to write tests upfrontinto a massive accelerator, as the manual labor of test writing is significantly reduced.67  \nThe adoption of AI-assisted TDD is the ultimate expression of the \"Discernment\" competency from the 4D framework. It elevates the evaluation of AI-generated code from a subjective, manual code reviewwhich is prone to human error and oversightto an o"
  },
  {
    "id": "report_source",
    "chunk": "nt\" competency from the 4D framework. It elevates the evaluation of AI-generated code from a subjective, manual code reviewwhich is prone to human error and oversightto an objective, automated, and rigorous validation process. The test is not just a check on the code; it is an executable specification of correctness. By compelling the AI's output to be validated against a pre-written, human-authored test, the developer is performing the most robust form of discernment possible. In this model, the test *is* the discernment. This positions TDD not merely as a software engineering best practice, but as the most mature and effective methodology for managing the inherent unreliability of generative models and ensuring a high standard of quality.\n\n### **5.2. Automating the Blueprint: AI-Generated PRDs**\n\nJust as TDD provides a verifiable blueprint for a single feature, a formal **Product Requirements Document (PRD)** serves as the strategic blueprint for an entire product or major initiative. The creation of a PRD is a capstone activity that demonstrates a mature, structured, and plan-first development culturea hallmark of Virtuosity.71  \nAI can significantly accelerate and enhance the creation of PRDs by synthesizing unstructured information into a formal, comprehensive document. Modern AI PRD generators can process a wide range of inputs, such as stakeholder meeting notes, customer feedback, user research, and technical documentation, and automatically structure them into a coherent blueprint.71  \nThe workflow for creating an AI-generated PRD mirrors the high-level principles of symbiotic human-AI collaboration:\n\n1. **Input Context:** The human product manager or team lead provides the high-level strategic inputs. This in"
  },
  {
    "id": "report_source",
    "chunk": " the high-level principles of symbiotic human-AI collaboration:\n\n1. **Input Context:** The human product manager or team lead provides the high-level strategic inputs. This includes defining the user problem to be solved, the business goals, the target audience, key success metrics, and any known technical constraints.72 The quality of this initial context is the primary determinant of the quality of the final document.  \n2. **AI Generates Structure and Content:** The AI tool processes these inputs and generates a complete PRD structure. It synthesizes the provided information into standard sections, such as an executive summary, user stories with acceptance criteria, technical specifications, a measurement plan, and a risk assessment.72 Advanced tools can also suggest relevant requirements or edge cases based on the product type and industry context.72  \n3. **Human Refines and Finalizes:** The AI-generated document serves as a high-quality first draft, not the final product. The human expert's role is to perform the crucial final step of refinement and validation. This involves ensuring the requirements align with the broader business strategy, validating the technical feasibility with the engineering team, prioritizing features based on impact and effort, and fine-tuning the language for clarity and completeness.72\n\nThis process exemplifies a high-level, Virtuoso-level collaboration. It produces a verifiable artifact that aligns the entire team and guides the development lifecycle, ensuring that what is built is what was intended. The creation of this document reveals that the principles of Virtuosity are recursive and apply at all levels of abstraction. The same structured, context-driven workflow used to generate a s"
  },
  {
    "id": "report_source",
    "chunk": "this document reveals that the principles of Virtuosity are recursive and apply at all levels of abstraction. The same structured, context-driven workflow used to generate a single functionPlan \\-\\> AI Implements \\-\\> Human Reviewsis structurally identical to the workflow used to generate the project's entire strategic blueprint. This suggests a fractal pattern: the core principles of planning, context curation, and human validation are universal to effective human-AI collaboration, whether applied to a line of code or a multi-year product strategy. This understanding dramatically expands the potential scope of the V2V pathway, positioning it as a universal methodology for any complex knowledge work in the age of AI.  \n---\n\n## **Part VI: Strategic Synthesis and Recommendations for aiascent.dev**\n\nThe analysis presented in this report culminates in a clear imperative: the transition from the improvisational nature of \"Vibecoding\" to the disciplined mastery of \"Virtuosity\" is essential for the future of AI-driven software development. The aiascent.dev project is uniquely positioned to guide and accelerate this transition. The following strategic recommendations provide an actionable roadmap for implementing the V2V pathway through a combination of targeted tooling, a structured training curriculum, and the promotion of new team workflows.\n\n### **6.1. Implementing the V2V Pathway: A Phased Roadmap**\n\nThe journey from novice to expert is a gradual one. The aiascent.dev platform and its associated curriculum should be structured as a phased roadmap that guides developers through progressively more sophisticated stages of skill acquisition, mirroring the principles of Cognitive Apprenticeship.\n\n* **Phase 1: Foundational Skil"
  },
  {
    "id": "report_source",
    "chunk": "t guides developers through progressively more sophisticated stages of skill acquisition, mirroring the principles of Cognitive Apprenticeship.\n\n* **Phase 1: Foundational Skills (The Apprentice):** The initial focus should be on moving developers away from unstructured, free-form prompting. This phase introduces the core concepts of deliberate, structured interaction.  \n  * **Curriculum:** Teach the fundamentals of **Structured Prompting**, including the use of roles, explicit instructions, and JSON schemas to achieve predictable outputs. Introduce **Basic Context Management** techniques, such as providing relevant code snippets and chat history. The **4D Framework (Delegation, Description, Discernment, Diligence)** should be presented as the foundational mental model for all AI interactions.  \n  * **Tooling:** Provide simple prompt templates and a \"context scratchpad\" where developers can easily gather and edit the information they will provide to the AI.  \n  * **Goal:** Transition developers from pure Vibecoding to a state of deliberate, reliable, single-turn interactions with AI.  \n* **Phase 2: Building Systems (The Journeyman):** This phase focuses on scaling the foundational skills to build complex, multi-turn agentic systems. The developer learns to architect not just a single prompt, but an entire workflow.  \n  * **Curriculum:** Introduce **Advanced RAG** as a core competency, teaching developers how to build and optimize multi-stage retrieval pipelines. The curriculum should cover **Multi-turn Agentic Workflows** and the principles of **Spec-Driven Development**, emphasizing the importance of creating a written plan before coding. The Cognitive Apprenticeship model should be explicitly used as the teaching method"
  },
  {
    "id": "report_source",
    "chunk": "-Driven Development**, emphasizing the importance of creating a written plan before coding. The Cognitive Apprenticeship model should be explicitly used as the teaching methodology, with expert demonstrations and guided practice.  \n  * **Tooling:** Offer advanced features such as a RAG pipeline builder, support for version-controlled planning.md or .task files, and integrations for multi-agent orchestration.  \n  * **Goal:** Enable developers to reliably build and debug multi-step AI systems that are grounded in external knowledge and guided by explicit plans.  \n* **Phase 3: Verifiable Mastery (The Virtuoso):** The final phase is dedicated to achieving production-grade quality and reliability through rigorous engineering discipline.  \n  * **Curriculum:** Focus on **AI-assisted Test-Driven Development (TDD)** as the primary methodology for ensuring code correctness. Teach the \"Edit-Test Loop\" workflow. Introduce **Full-Lifecycle Automation**, demonstrating how the same structured principles can be used for high-level tasks like generating PRDs from stakeholder requirements.  \n  * **Tooling:** Provide first-class IDE support for TDD workflows, allowing developers to easily write a failing test and then task an AI agent to make it pass. Integrate AI-powered PRD generation tools that connect strategic planning directly to the development environment.  \n  * **Goal:** Empower teams to ship production-grade, verifiable, and maintainable software with AI assistance, establishing a culture of engineering excellence.\n\n### **6.2. Recommendations for Tooling, Training, and Workflow Design**\n\nTo support the phased roadmap, aiascent.dev should focus its development on three interconnected pillars:\n\n* **Tooling:** The platform's tools m"
  },
  {
    "id": "report_source",
    "chunk": "aining, and Workflow Design**\n\nTo support the phased roadmap, aiascent.dev should focus its development on three interconnected pillars:\n\n* **Tooling:** The platform's tools must be designed to facilitate Virtuoso practices. This means moving beyond simple chat interfaces to a more structured development environment. A key recommendation is to develop or integrate tools that treat **plans and context as first-class citizens**. This could involve native support for .task files or planning.md documents within the IDE, allowing these artifacts to be versioned, shared, and used to initiate reproducible AI sessions. The tooling should also provide built-in scaffolding for constructing and debugging advanced RAG pipelines and should deeply integrate TDD workflows, making the \"test-first\" approach the path of least resistance.  \n* **Training:** The aiascent.dev curriculum should be explicitly designed around the **Cognitive Apprenticeship** model. Each module should follow the sequence of modeling, coaching, and scaffolding. The content should be structured around the competencies of the **4D Framework**, with specific lessons and exercises designed to build skills in Delegation, Description, Discernment, and Diligence. The training should incorporate **Deliberate Practice**, providing targeted exercises with immediate feedback loops that allow developers to hone specific skills, such as context compression or prompt refinement, in a controlled environment.  \n* **Workflow Design:** Beyond individual skills, aiascent.dev should champion a cultural shift in how teams collaborate with AI. The platform should provide best-practice templates for **spec-driven development**, encouraging the adoption of a \"plan-first\" and \"test-first\""
  },
  {
    "id": "report_source",
    "chunk": "how teams collaborate with AI. The platform should provide best-practice templates for **spec-driven development**, encouraging the adoption of a \"plan-first\" and \"test-first\" mindset. It should include features that support the collaborative review not only of AI-generated code but also of the plans and prompts that produced it. By making the entire human-AI interaction transparent and reviewable, the platform can foster a culture of shared ownership and continuous improvement.\n\n### **6.3. Conclusion: Beyond Productivity \\- Towards a New Engineering Discipline**\n\nThe V2V pathway outlined in this report is fundamentally about more than just making individual developers faster. While productivity gains are a significant benefit, the true purpose of this framework is to foster a new, more rigorous and resilient engineering discipline that is adapted to the unique challenges and opportunities of the generative AI era. The inherent non-determinism and potential for error in LLMs demand a shift away from the informal, trust-based methods of the past towards a system based on explicit specification and objective verification.  \nBy embracing the architectural mindset of Context Engineering, the collaborative frameworks of Human-AI Teaming, and the verifiable correctness of Test-Driven Development, the software engineering community can move beyond the brittle and unpredictable nature of Vibecoding. The aiascent.dev project has the opportunity to lead this transformation. By building a platform and a community dedicated to the principles of Virtuosity, it can help define what it means to be an expert developer in the age of AI and, in doing so, shape the future of how we build intelligent systems.\n\n#### **Works cited**\n\n1. Conte"
  },
  {
    "id": "report_source",
    "chunk": "an help define what it means to be an expert developer in the age of AI and, in doing so, shape the future of how we build intelligent systems.\n\n#### **Works cited**\n\n1. Context Engineering \\- What it is, and techniques to consider ..., accessed October 15, 2025, [https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider](https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider)  \n2. Effective context engineering for AI agents \\\\ Anthropic, accessed October 15, 2025, [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)  \n3. Context Engineering vs Prompt Engineering : r/PromptEngineering \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1lmnftf/context\\_engineering\\_vs\\_prompt\\_engineering/](https://www.reddit.com/r/PromptEngineering/comments/1lmnftf/context_engineering_vs_prompt_engineering/)  \n4. Context Engineering vs Prompt Engineering | by Mehul Gupta | Data Science in Your Pocket, accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d)  \n5. Context Engineering \\- LangChain Blog, accessed October 15, 2025, [https://blog.langchain.com/context-engineering-for-agents/](https://blog.langchain.com/context-engineering-for-agents/)  \n6. Prompt Engineer vs Context Engineer: Why Design Leadership Needs to See the Bigger Picture | by Elizabeth Eagle-Simbeye | Bootcamp | Medium, accessed October 15, 2025, [https://medium.com/design-bootcam"
  },
  {
    "id": "report_source",
    "chunk": "Engineer: Why Design Leadership Needs to See the Bigger Picture | by Elizabeth Eagle-Simbeye | Bootcamp | Medium, accessed October 15, 2025, [https://medium.com/design-bootcamp/prompt-engineer-vs-context-engineer-why-design-leadership-needs-to-see-the-bigger-picture-24eec7ea9a91](https://medium.com/design-bootcamp/prompt-engineer-vs-context-engineer-why-design-leadership-needs-to-see-the-bigger-picture-24eec7ea9a91)  \n7. Context Engineering Guide, accessed October 15, 2025, [https://www.promptingguide.ai/guides/context-engineering-guide](https://www.promptingguide.ai/guides/context-engineering-guide)  \n8. The rise of \"context engineering\" \\- LangChain Blog, accessed October 15, 2025, [https://blog.langchain.com/the-rise-of-context-engineering/](https://blog.langchain.com/the-rise-of-context-engineering/)  \n9. LLM Context Engineering. Introduction | by Kumar Nishant \\- Medium, accessed October 15, 2025, [https://medium.com/@knish5790/llm-context-engineering-66097070161b](https://medium.com/@knish5790/llm-context-engineering-66097070161b)  \n10. Boost AI Accuracy with Context Engineering Pruning and Retrieval ..., accessed October 15, 2025, [https://www.geeky-gadgets.com/context-engineering-techniques-for-ai/](https://www.geeky-gadgets.com/context-engineering-techniques-for-ai/)  \n11. Best Practices for RAG Pipelines \\- Mastering LLM (Large Language Model), accessed October 15, 2025, [https://masteringllm.medium.com/best-practices-for-rag-pipeline-8c12a8096453](https://masteringllm.medium.com/best-practices-for-rag-pipeline-8c12a8096453)  \n12. medium.com, accessed October 15, 2025, [https://medium.com/@knish5790/llm-context-engineering-66097070161b\\#:\\~:text=Key%20Strategies%20for%20LLM%20Context,Select%2C%20Compress%2C%20a"
  },
  {
    "id": "report_source",
    "chunk": "om, accessed October 15, 2025, [https://medium.com/@knish5790/llm-context-engineering-66097070161b\\#:\\~:text=Key%20Strategies%20for%20LLM%20Context,Select%2C%20Compress%2C%20and%20Isolate.](https://medium.com/@knish5790/llm-context-engineering-66097070161b#:~:text=Key%20Strategies%20for%20LLM%20Context,Select%2C%20Compress%2C%20and%20Isolate.)  \n13. Persona Injection: LLM context management experiment and model's self-analysis, accessed October 15, 2025, [https://news.ycombinator.com/item?id=45453317](https://news.ycombinator.com/item?id=45453317)  \n14. Free-form AI coding vs spec-driven AI workflows : r/ExperiencedDevs \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/ExperiencedDevs/comments/1mugowu/freeform\\_ai\\_coding\\_vs\\_specdriven\\_ai\\_workflows/](https://www.reddit.com/r/ExperiencedDevs/comments/1mugowu/freeform_ai_coding_vs_specdriven_ai_workflows/)  \n15. The Art of LLM Context Management: Optimizing AI Agents for App Development \\- Medium, accessed October 15, 2025, [https://medium.com/@ravikhurana\\_38440/the-art-of-llm-context-management-optimizing-ai-agents-for-app-development-e5ef9fcf8f75](https://medium.com/@ravikhurana_38440/the-art-of-llm-context-management-optimizing-ai-agents-for-app-development-e5ef9fcf8f75)  \n16. Structured Prompting with JSON: The Engineering Path to Reliable LLMs | by vishal dutt | Sep, 2025 | Medium, accessed October 15, 2025, [https://medium.com/@vdutt1203/structured-prompting-with-json-the-engineering-path-to-reliable-llms-2c0cb1b767cf](https://medium.com/@vdutt1203/structured-prompting-with-json-the-engineering-path-to-reliable-llms-2c0cb1b767cf)  \n17. What Is the Cognitive Apprenticeship Model of Teaching and Its Use ..., accessed October 15, 2025, [https://www.co"
  },
  {
    "id": "report_source",
    "chunk": "on-the-engineering-path-to-reliable-llms-2c0cb1b767cf)  \n17. What Is the Cognitive Apprenticeship Model of Teaching and Its Use ..., accessed October 15, 2025, [https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning](https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning)  \n18. Understanding the Cognitive Apprenticeship Framework for Smarter Learning \\- Pooks.ai, accessed October 15, 2025, [https://www.pooks.ai/posts/understanding-the-cognitive-apprenticeship-framework-for-smarter-learning.html](https://www.pooks.ai/posts/understanding-the-cognitive-apprenticeship-framework-for-smarter-learning.html)  \n19. AI & Cognitive Apprenticeships \\- AI and Learning Mini-S... \\- AI Innovation Lounge, accessed October 15, 2025, [https://www.aiinnovationlounge.com/blog/ai-cognitive-apprenticeships](https://www.aiinnovationlounge.com/blog/ai-cognitive-apprenticeships)  \n20. Using Artificial Intelligence to Transform Manager Development \\- Valence, accessed October 15, 2025, [https://www.valence.co/charter-report](https://www.valence.co/charter-report)  \n21. Generative AI Meets Cognitive Apprenticeship \\- The EvoLLLution, accessed October 15, 2025, [https://evolllution.com/technology/tech-tools-and-resources/using-ai-and-cognitive-apprenticeships-to-upskill-and-retool-adult-learners](https://evolllution.com/technology/tech-tools-and-resources/using-ai-and-cognitive-apprenticeships-to-upskill-and-retool-adult-learners)  \n22. Scaffolding for AI: Building Competence, One Prompt at a Time \\- TxDLA, accessed October 15, 2025, [https://www.txdla.org/scaffolding-for-ai/](https://www.txdla.org/scaffolding-for-ai/)  \n23. Scaffolding Creativity: Integrating Gen"
  },
  {
    "id": "report_source",
    "chunk": " Time \\- TxDLA, accessed October 15, 2025, [https://www.txdla.org/scaffolding-for-ai/](https://www.txdla.org/scaffolding-for-ai/)  \n23. Scaffolding Creativity: Integrating Generative AI Tools and Real-world Experiences in Business Education \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.06527v2](https://arxiv.org/html/2501.06527v2)  \n24. Practice for knowledge acquisition (not drill and kill) \\- American Psychological Association, accessed October 15, 2025, [https://www.apa.org/education-career/k12/practice-acquisition](https://www.apa.org/education-career/k12/practice-acquisition)  \n25. C'mon guys, Deliberate Practice is Real \\- LessWrong, accessed October 15, 2025, [https://www.lesswrong.com/posts/Y4bKhhZyZ7ru7zqsh/c-mon-guys-deliberate-practice-is-real](https://www.lesswrong.com/posts/Y4bKhhZyZ7ru7zqsh/c-mon-guys-deliberate-practice-is-real)  \n26. Full article: Why deliberate practice is not a basis for teacher expertise, accessed October 15, 2025, [https://www.tandfonline.com/doi/full/10.1080/0305764X.2025.2516524?src=](https://www.tandfonline.com/doi/full/10.1080/0305764X.2025.2516524?src)  \n27. Leveraging Computational Thinking in the Era of Generative AI ..., accessed October 15, 2025, [https://cacm.acm.org/blogcacm/leveraging-computational-thinking-in-the-era-of-generative-ai/](https://cacm.acm.org/blogcacm/leveraging-computational-thinking-in-the-era-of-generative-ai/)  \n28. Computational Thinking is Key to Effective Human-AI Interaction | Codility, accessed October 15, 2025, [https://www.codility.com/blog/computational-thinking-the-key-to-effective-human-ai-interaction/](https://www.codility.com/blog/computational-thinking-the-key-to-effective-human-ai-interaction/)  \n29. Computational Thinki"
  },
  {
    "id": "report_source",
    "chunk": "hinking-the-key-to-effective-human-ai-interaction/](https://www.codility.com/blog/computational-thinking-the-key-to-effective-human-ai-interaction/)  \n29. Computational Thinking Is A Key Problem-Solving Skill In The AI Era \\- Forbes, accessed October 15, 2025, [https://www.forbes.com/councils/forbeshumanresourcescouncil/2024/07/23/computational-thinking-is-a-key-problem-solving-skill-in-the-ai-era/](https://www.forbes.com/councils/forbeshumanresourcescouncil/2024/07/23/computational-thinking-is-a-key-problem-solving-skill-in-the-ai-era/)  \n30. Computational thinking in the age of AI with Susan Stocker \\- Sprout Labs, accessed October 15, 2025, [https://sproutlabs.com.au/blog/computational-thinking-in-the-age-of-ai-with-susan-stocker/](https://sproutlabs.com.au/blog/computational-thinking-in-the-age-of-ai-with-susan-stocker/)  \n31. Computational Thinking: The Idea That Lived \\- Communications of the ACM, accessed October 15, 2025, [https://cacm.acm.org/blogcacm/computational-thinking-the-idea-that-lived/](https://cacm.acm.org/blogcacm/computational-thinking-the-idea-that-lived/)  \n32. Beyond Problem-Solving: The Future of Learning in an AI-Driven ..., accessed October 15, 2025, [https://www.scirp.org/journal/paperinformation?paperid=142115](https://www.scirp.org/journal/paperinformation?paperid=142115)  \n33. Beyond Problem-Solving: The Future of Learning in an AI-Driven World \\- Scirp.org, accessed October 15, 2025, [https://www.scirp.org/pdf/ce2025164\\_46308703.pdf](https://www.scirp.org/pdf/ce2025164_46308703.pdf)  \n34. Learning to learn with AI \\- Thot Cursus, accessed October 15, 2025, [https://cursus.edu/en/32384/learning-to-learn-with-ai](https://cursus.edu/en/32384/learning-to-learn-with-ai)  \n35. Structured Prompt"
  },
  {
    "id": "report_source",
    "chunk": " Thot Cursus, accessed October 15, 2025, [https://cursus.edu/en/32384/learning-to-learn-with-ai](https://cursus.edu/en/32384/learning-to-learn-with-ai)  \n35. Structured Prompting Approaches \\- Emergent Mind, accessed October 15, 2025, [https://www.emergentmind.com/topics/structured-prompting](https://www.emergentmind.com/topics/structured-prompting)  \n36. Conversational vs Structured Prompting \\- The Prompt Engineering Institute, accessed October 15, 2025, [https://promptengineering.org/a-guide-to-conversational-and-structured-prompting/](https://promptengineering.org/a-guide-to-conversational-and-structured-prompting/)  \n37. Why Structure Your Prompts \\- Hardik Pandya, accessed October 15, 2025, [https://hvpandya.com/structured-prompts](https://hvpandya.com/structured-prompts)  \n38. 100 Days of AI: Reflections on Phase 1 | by Steven Luengo | Medium, accessed October 15, 2025, [https://medium.com/@stevenluengo/100-days-of-ai-reflections-on-phase-1-f13687df11c4](https://medium.com/@stevenluengo/100-days-of-ai-reflections-on-phase-1-f13687df11c4)  \n39. \\\\ours: Optimizing Example Ordering for In-Context Learning \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.15030v1](https://arxiv.org/html/2501.15030v1)  \n40. \\[2501.15030\\] OptiSeq: Ordering Examples On-The-Fly for In-Context Learning \\- arXiv, accessed October 15, 2025, [https://arxiv.org/abs/2501.15030](https://arxiv.org/abs/2501.15030)  \n41. Context Tuning for In-Context Optimization \\- arXiv, accessed October 15, 2025, [https://arxiv.org/abs/2507.04221](https://arxiv.org/abs/2507.04221)  \n42. Context Tuning for In-Context Optimization \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2507.04221v1](https://arxiv.org/html/2507.04221v1)  \n43. "
  },
  {
    "id": "report_source",
    "chunk": ".04221)  \n42. Context Tuning for In-Context Optimization \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2507.04221v1](https://arxiv.org/html/2507.04221v1)  \n43. Enhancing Retrieval-Augmented Generation: A Study of Best Practices \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.07391v1](https://arxiv.org/html/2501.07391v1)  \n44. Searching for Best Practices in Retrieval-Augmented Generation \\- ACL Anthology, accessed October 15, 2025, [https://aclanthology.org/2024.emnlp-main.981.pdf](https://aclanthology.org/2024.emnlp-main.981.pdf)  \n45. Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2508.19357v1](https://arxiv.org/html/2508.19357v1)  \n46. Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2506.00054v1](https://arxiv.org/html/2506.00054v1)  \n47. Enhancing Retrieval-Augmented Generation: A Study of Best Practices, accessed October 15, 2025, [https://arxiv.org/abs/2501.07391](https://arxiv.org/abs/2501.07391)  \n48. Evaluating Human-AI Collaboration: A Review and Methodological Framework \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2407.19098v2](https://arxiv.org/html/2407.19098v2)  \n49. Human-AI Symbiotic Theory (HAIST): Development, Multi-Framework Assessment, and AI-Assisted Validation in Academic Research \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2227-9709/12/3/85](https://www.mdpi.com/2227-9709/12/3/85)  \n50. (PDF) Human-Centered Human-AI Collaboration (HCHAC) \\- ResearchGate, accessed October 15, 2025, [https://www.re"
  },
  {
    "id": "report_source",
    "chunk": "227-9709/12/3/85](https://www.mdpi.com/2227-9709/12/3/85)  \n50. (PDF) Human-Centered Human-AI Collaboration (HCHAC) \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/392167944\\_Human-Centered\\_Human-AI\\_Collaboration\\_HCHAC](https://www.researchgate.net/publication/392167944_Human-Centered_Human-AI_Collaboration_HCHAC)  \n51. Human-Centered AI: What Is Human-Centric Artificial Intelligence?, accessed October 15, 2025, [https://online.lindenwood.edu/blog/human-centered-ai-what-is-human-centric-artificial-intelligence/](https://online.lindenwood.edu/blog/human-centered-ai-what-is-human-centric-artificial-intelligence/)  \n52. Empowering Humanity: The Rise of Human-Centered AI (HCAI) \\- XB Software, accessed October 15, 2025, [https://xbsoftware.com/blog/human-centered-ai/](https://xbsoftware.com/blog/human-centered-ai/)  \n53. Symbiotic AI: The Future of Human-AI Collaboration \\- AI Asia Pacific Institute, accessed October 15, 2025, [https://aiasiapacific.org/2025/05/28/symbiotic-ai-the-future-of-human-ai-collaboration/](https://aiasiapacific.org/2025/05/28/symbiotic-ai-the-future-of-human-ai-collaboration/)  \n54. AI Fluency: Framework & Foundations \\- Anthropic Courses, accessed October 15, 2025, [https://anthropic.skilljar.com/ai-fluency-framework-foundations](https://anthropic.skilljar.com/ai-fluency-framework-foundations)  \n55. The Al Fluency Framework \\- Anthropic, accessed October 15, 2025, [https://www-cdn.anthropic.com/334975cdec18f744b4fa511dc8518bd8d119d29d.pdf](https://www-cdn.anthropic.com/334975cdec18f744b4fa511dc8518bd8d119d29d.pdf)  \n56. The 4D Framework for AI Fluency \\- ucf stars, accessed October 15, 2025, [https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1500\\&co"
  },
  {
    "id": "report_source",
    "chunk": "4fa511dc8518bd8d119d29d.pdf)  \n56. The 4D Framework for AI Fluency \\- ucf stars, accessed October 15, 2025, [https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1500\\&context=teachwithai](https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1500&context=teachwithai)  \n57. 3 things I learned about AI Fluency from Anthropic | by Ameet Ranadive | Medium, accessed October 15, 2025, [https://medium.com/@ameet/3-things-i-learned-about-ai-fluency-from-anthropic-12ae781b9b8c](https://medium.com/@ameet/3-things-i-learned-about-ai-fluency-from-anthropic-12ae781b9b8c)  \n58. Lesson 2B: The 4D Framework | AI Fluency: Framework & Foundations Course \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=W4Ua6XFfX9w](https://www.youtube.com/watch?v=W4Ua6XFfX9w)  \n59. A Framework for Human-Centric AI-First Teaching | AACSB, accessed October 15, 2025, [https://www.aacsb.edu/insights/articles/2025/02/a-framework-for-human-centric-ai-first-teaching](https://www.aacsb.edu/insights/articles/2025/02/a-framework-for-human-centric-ai-first-teaching)  \n60. A 3-step AI coding workflow for solo founders | Ryan Carson (5x ..., accessed October 15, 2025, [https://pod.wave.co/podcast/how-i-ai/a-3-step-ai-coding-workflow-for-solo-founders-ryan-carson-5x-founder-4fd6a2d4](https://pod.wave.co/podcast/how-i-ai/a-3-step-ai-coding-workflow-for-solo-founders-ryan-carson-5x-founder-4fd6a2d4)  \n61. AI Agent Best Practices: 12 Lessons from AI Pair Programming for Developers | Forge Code, accessed October 15, 2025, [https://forgecode.dev/blog/ai-agent-best-practices/](https://forgecode.dev/blog/ai-agent-best-practices/)  \n62. After 6 months of daily AI pair programming, here's what actually ..., accessed October 15, 2025, [https://www"
  },
  {
    "id": "report_source",
    "chunk": "ces/](https://forgecode.dev/blog/ai-agent-best-practices/)  \n62. After 6 months of daily AI pair programming, here's what actually ..., accessed October 15, 2025, [https://www.reddit.com/r/ClaudeAI/comments/1l1uea1/after\\_6\\_months\\_of\\_daily\\_ai\\_pair\\_programming\\_heres/](https://www.reddit.com/r/ClaudeAI/comments/1l1uea1/after_6_months_of_daily_ai_pair_programming_heres/)  \n63. Structured AI Coding with Task Context: A Better Way to Work with AI ..., accessed October 15, 2025, [https://eclipsesource.com/blogs/2025/07/01/structure-ai-coding-with-task-context/](https://eclipsesource.com/blogs/2025/07/01/structure-ai-coding-with-task-context/)  \n64. Can GitHub Copilot Follow a Structured Development Workflow? A Real-World Experiment, accessed October 15, 2025, [https://dev.to/vigneshiyergithub/can-github-copilot-follow-a-structured-development-workflow-a-real-world-experiment-3el7](https://dev.to/vigneshiyergithub/can-github-copilot-follow-a-structured-development-workflow-a-real-world-experiment-3el7)  \n65. Best practices for pair programming with AI assistants \\- Graphite, accessed October 15, 2025, [https://graphite.dev/guides/ai-pair-programming-best-practices](https://graphite.dev/guides/ai-pair-programming-best-practices)  \n66. Beginner's Guide to Pair Programming | Zero To Mastery, accessed October 15, 2025, [https://zerotomastery.io/blog/pair-programming/](https://zerotomastery.io/blog/pair-programming/)  \n67. Test-Driven Development with AI \\- Builder.io, accessed October 15, 2025, [https://www.builder.io/blog/test-driven-development-ai](https://www.builder.io/blog/test-driven-development-ai)  \n68. Using Test-Driven Development to Get Better AI-Generated Code | by Andr Gardi, accessed October 15, 2025, [https:/"
  },
  {
    "id": "report_source",
    "chunk": "tps://www.builder.io/blog/test-driven-development-ai)  \n68. Using Test-Driven Development to Get Better AI-Generated Code | by Andr Gardi, accessed October 15, 2025, [https://javascript.plainenglish.io/using-test-driven-development-to-get-better-ai-generated-code-ebcc7f7fd107](https://javascript.plainenglish.io/using-test-driven-development-to-get-better-ai-generated-code-ebcc7f7fd107)  \n69. AI Code Assistants Are Revolutionizing Test-Driven Development \\- Qodo, accessed October 15, 2025, [https://www.qodo.ai/blog/ai-code-assistants-test-driven-development/](https://www.qodo.ai/blog/ai-code-assistants-test-driven-development/)  \n70. Test-Driven Generation (TDG): Adopting TDD again this time with ..., accessed October 15, 2025, [https://chanwit.medium.com/test-driven-generation-tdg-adopting-tdd-again-this-time-with-gen-ai-27f986bed6f8](https://chanwit.medium.com/test-driven-generation-tdg-adopting-tdd-again-this-time-with-gen-ai-27f986bed6f8)  \n71. 20 Best AI PRD Generators for Startups in 2025(Free & Paid, accessed October 15, 2025, [https://www.oreateai.com/blog/ai-prd-generator/](https://www.oreateai.com/blog/ai-prd-generator/)  \n72. AI PRD Tool: Write PRDs Fast (Free Template) | Revo.pm, accessed October 15, 2025, [https://revo.pm/blog/ai-prd-tool-write-prds-fast-free-template](https://revo.pm/blog/ai-prd-tool-write-prds-fast-free-template)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/06-V2V Academy Context Engineering Research.md\">\n\n\n# **The Context Revolution: A Strategic Blueprint for V2V Academy on the Transition from Prompting to Systems Engineering in AI**\n\n## **The Paradigm Shift: From Linguistic Tuning to Systems Architecture**\n\nThe field of generative artificial intelligence (AI) is undergoin"
  },
  {
    "id": "report_source",
    "chunk": "ng to Systems Engineering in AI**\n\n## **The Paradigm Shift: From Linguistic Tuning to Systems Architecture**\n\nThe field of generative artificial intelligence (AI) is undergoing a profound and rapid maturation. The initial focus on mastering the \"art of the prompt\" is giving way to a more rigorous, scalable, and defensible engineering discipline. This transition, from prompt engineering to context engineering, represents a fundamental shift in how developers interact with, control, and build upon Large Language Models (LLMs). It marks the evolution of AI development from an artisanal craft, reliant on linguistic nuance and trial-and-error, to a structured practice of systems architecture. For educational institutions, recognizing and codifying this paradigm shift is not merely an academic exercise; it is a strategic imperative to equip the next generation of AI professionals with the skills necessary to build the robust, reliable, and complex AI systems of the future. This report provides a comprehensive analysis of this transition, deconstructs the core principles of context engineering, and presents a strategic blueprint for V2V Academy to establish a market-leading curriculum in this critical new domain.\n\n### **The Limits of Prompting: From \"Magic Words\" to Brittle Systems**\n\nPrompt engineering is the practice of designing and refining textual inputsor promptsto guide the output of generative AI models. It can be understood as a form of \"linguistic tuning,\" where practitioners use carefully crafted language, specific phrasing, illustrative examples (few-shot prompting), and structured reasoning patterns (chain-of-thought) to influence a model's behavior.1 The accessibility of this approach has been a primary driver o"
  },
  {
    "id": "report_source",
    "chunk": "les (few-shot prompting), and structured reasoning patterns (chain-of-thought) to influence a model's behavior.1 The accessibility of this approach has been a primary driver of the widespread adoption of LLMs, allowing individuals with minimal technical background to achieve remarkable results through natural language interaction. For rapid prototyping and simple, single-turn tasks like creative writing or basic code generation, prompt engineering is a fast and powerful tool.1  \nHowever, the very accessibility of prompt engineering belies its fundamental limitations in professional and enterprise settings. The primary drawback is its inherent **brittleness**.1 Systems built solely on prompt engineering are highly sensitive to minor variations in wording, formatting, or the placement of examples. A slight change in a prompt that works perfectly in one scenario can cause a notable and unpredictable degradation in output quality or reliability in another.1 This fragility is a significant barrier to building scalable, production-grade applications. Furthermore, prompt-based interactions are stateless; they lack persistence and the ability to generalize across complex, multi-step workflows that require memory and consistent state management.1  \nThis brittleness has led to a perception within the technical community that prompt engineering, while a useful introductory skill, is not a sustainable engineering discipline. Discussions often frame it as a superficial practice, with some dismissing it as a \"cash grab\" by non-technical individuals selling \"magic words\".2 While this view can be an oversimplification, it reflects a broader consensus that as AI applications grow in complexity, a more robust methodology is required.3 The"
  },
  {
    "id": "report_source",
    "chunk": " words\".2 While this view can be an oversimplification, it reflects a broader consensus that as AI applications grow in complexity, a more robust methodology is required.3 The search for \"magic prompts\" is being replaced by the need for predictable, repeatable, and reliable systems.2\n\n### **The Rise of Context Engineering: A New Discipline for a New Era**\n\nIn response to the limitations of prompting, context engineering has emerged as a distinct and more comprehensive discipline. It represents a paradigm shift from \"linguistic tuning\" to **\"systems thinking\"**.1 This evolution is championed by influential figures in the AI community. Andrej Karpathy, a prominent AI researcher, has been a key proponent of this terminological and conceptual shift, defining context engineering as \"the delicate art and science of filling the context window with just the right information for the next step\".6 This definition moves beyond the singular prompt to encompass the entire information payload provided to the model at inference time. Similarly, Shopify CEO Tobi Ltke has endorsed the term, emphasizing that the core skill is not crafting clever prompts but \"providing all the necessary context for the LLM\".6  \nContext engineering, at its core, is the systematic process of designing, structuring, and optimizing the entire informational ecosystem surrounding an AI interaction to enhance its understanding, accuracy, and relevance.5 It reframes the developer's role from that of a \"prompt writer\" to an \"information architect\" or \"AI systems designer\".9 This discipline is not concerned with the single instruction but with the holistic assembly of a dynamic context that may include 1:\n\n* System prompts and role definitions.  \n* User dialogue hi"
  },
  {
    "id": "report_source",
    "chunk": " not concerned with the single instruction but with the holistic assembly of a dynamic context that may include 1:\n\n* System prompts and role definitions.  \n* User dialogue history.  \n* Real-time data fetched from APIs.  \n* Relevant documents retrieved from knowledge bases.  \n* Definitions of external tools the model can use.  \n* Structured memory representations.\n\nThis shift is a direct and necessary response to the increasing sophistication of AI applications. As these systems are tasked with performing complex, multi-turn, and stateful operations, the simple, static prompt is no longer sufficient. Context engineering provides the architectural framework to build applications that can maintain session continuity, handle failures in external tool calls, and deliver a consistent, reliable user experience over time.1\n\n### **The Industrial Imperative: Why This Shift Matters for Enterprise AI**\n\nThe transition from prompt to context engineering is not merely an academic distinction; it is driven by the rigorous demands of building and deploying AI in enterprise environments. \"Industrial-strength LLM apps\" cannot be built on the fragile foundation of prompt-tuning alone.10 Businesses require AI systems that are predictable, repeatable, secure, and scalablequalities that context engineering is specifically designed to provide.5  \nConsider the example of an enterprise customer service chatbot. A simple prompt-based bot might answer a generic question based on its training data. However, an effective enterprise agent must operate with a complete and dynamic understanding of the customer's context. It needs to synthesize information from a fragmented landscape of business systems: CRM data about the customer's purchase history,"
  },
  {
    "id": "report_source",
    "chunk": "ic understanding of the customer's context. It needs to synthesize information from a fragmented landscape of business systems: CRM data about the customer's purchase history, support tickets detailing previous issues, and internal documentation about product specifications.6 A customer who has already returned a product should not be asked generic troubleshooting questions about it. This level of stateful, personalized interaction is impossible to achieve with simple prompting. It requires a context-engineered system that can dynamically retrieve, filter, and assemble information from multiple sources to construct a comprehensive view of the situation before generating a response.6  \nThis evolution in AI development mirrors the historical maturation of software engineering itself. In the early days of computing, development was often an ad-hoc process of individual programmers writing unstructured code, analogous to today's prompt engineering. As the complexity of software systems grew, the industry was forced to develop more structured disciplines: structured programming, object-oriented design, architectural patterns, and the formal role of the software architect. These disciplines were created to manage complexity and enable the construction of large-scale, reliable systems.  \nContext engineering represents the same evolutionary leap for the generative AI field. It signals that the domain is moving out of its initial, experimental \"stone age\" and into an era of professionalized, industrial-scale engineering.14 The principles of information architecture, memory management, and modular systems design are the AI-native equivalents of the foundational practices that enabled the modern software industry. Therefore, a curr"
  },
  {
    "id": "report_source",
    "chunk": "tecture, memory management, and modular systems design are the AI-native equivalents of the foundational practices that enabled the modern software industry. Therefore, a curriculum designed for the future of AI must treat context engineering as a formal engineering discipline, grounded in the principles of systems design and information theory, rather than as a collection of clever \"tips and tricks.\"\n\n## **Deconstructing Context: Core Principles and Architectural Components**\n\nTo build a robust curriculum around context engineering, it is essential to move beyond high-level definitions and establish a first-principles understanding of its components and operational frameworks. Context is not a monolithic block of text; it is a structured, multi-layered information ecosystem that must be architected with the same rigor as a software system.\n\n### **The Anatomy of Context: A Multi-Layered Information Ecosystem**\n\nContext engineering encompasses the entire informational environment provided to an LLM during an interaction.9 This environment can be deconstructed into several distinct layers, each serving a specific purpose in guiding the model's reasoning and response generation:\n\n* **Explicit Context:** This layer contains the most direct and overt information provided to the model. It includes clearly defined parameters, direct instructions (the prompt itself), specified constraints, and any data explicitly passed to the model for the immediate task.9  \n* **Implicit Context:** This is the underlying, often unstated, information that influences interpretation. It includes domain-specific knowledge, cultural references, and shared assumptions that the model is expected to leverage. Engineering this layer involves ensuring th"
  },
  {
    "id": "report_source",
    "chunk": "etation. It includes domain-specific knowledge, cultural references, and shared assumptions that the model is expected to leverage. Engineering this layer involves ensuring the model has access to the necessary background knowledge, often through retrieval from external sources.9  \n* **Dynamic Context:** This layer is composed of evolving information that changes throughout the lifecycle of an interaction. It includes the conversation history, user preferences learned over time, session data, and real-time inputs from external tools or APIs.9 Managing this layer is critical for building stateful and adaptive AI agents.\n\nTo effectively manage these layers, a key principle is the establishment of a **Context Hierarchy**. This involves organizing information based on its relevance and importance to the immediate task, ensuring that the model's limited attention is focused on the most critical data.9 A typical hierarchy includes:\n\n1. **Primary Context:** Mission-critical information directly required to complete the current task.  \n2. **Secondary Context:** Supporting details that enhance the model's understanding and provide nuance.  \n3. **Tertiary Context:** Broader background information that provides a wider perspective but is not essential for the immediate step.\n\nBy structuring information in this hierarchical manner, engineers can more effectively manage the model's focus and prevent it from being distracted by less relevant data.\n\n| Feature | Prompt Engineering | Context Engineering |\n| :---- | :---- | :---- |\n| **Primary Focus** | Designing and refining textual instructions (*prompts*) to guide generative AI models. | Strategic assembly and management of all relevant information and resources an AI model requires. |"
  },
  {
    "id": "report_source",
    "chunk": " refining textual instructions (*prompts*) to guide generative AI models. | Strategic assembly and management of all relevant information and resources an AI model requires. |\n| **Core Metaphor** | Linguistic Tuning / \"Linguistic Programmer\" | Systems Thinking / \"AI Systems Architect\" |\n| **Scope** | A single interaction or turn. | The entire application lifecycle and informational ecosystem. |\n| **Complexity** | Low and accessible, but brittle. | High and systemic, requiring architectural design. |\n| **Key Skills** | Natural language finesse, creative phrasing, example curation. | Information architecture, API design, memory management, systems thinking. |\n| **Typical Application** | Creative generation, simple Q\\&A, rapid prototyping. | Enterprise agents, complex multi-step workflows, stateful applications. |\n| **Failure Mode** | Brittle and unpredictable responses to small prompt variations. | Systemic architectural flaws, context poisoning, or information overload. |\n\n### **The Four Pillars of Context Engineering: A Foundational Framework**\n\nA powerful and pedagogically effective way to conceptualize the core operations of context engineering is through a framework often referred to as the \"Four Pillars.\" These pillars represent the fundamental actions an engineer takes to manage the flow of information into, out of, and around the LLM's context window. This framework is a cornerstone of modern agent design, heavily utilized in libraries like LangChain's LangGraph and for complex tasks like video understanding.15\n\n1. **Write Context:** This pillar concerns the strategy of saving important information *outside* the immediate context window for later use. Since the context window is a finite and expensive resource, per"
  },
  {
    "id": "report_source",
    "chunk": "ar concerns the strategy of saving important information *outside* the immediate context window for later use. Since the context window is a finite and expensive resource, persistent knowledge, long conversation histories, or user preferences are often written to an external store, such as a \"scratchpad,\" a file, or a dedicated memory system. This prevents critical information from being lost as the conversation progresses.8  \n2. **Select Context:** This is the process of intelligently retrieving and injecting only the most relevant information into the context window at the precise moment it is needed. Rather than overwhelming the model with an entire document or conversation history, a selection mechanismoften powered by semantic searchpulls in the specific instructions, knowledge chunks, or tool feedback required for the current step. This maximizes the signal-to-noise ratio within the context window.8  \n3. **Compress Context:** When selected information is still too verbose to fit efficiently within the token budget, this pillar involves condensing it while preserving its essential meaning. Common techniques include using an LLM to generate summaries of long documents or conversation turns, or creating abstract representations of complex tool outputs. This is a critical strategy for managing long-running agentic tasks.8  \n4. **Isolate Context:** This strategy involves separating concerns by splitting context across different, specialized agents or running processes in sandboxed environments. For example, in a multi-agent system, a \"research agent\" might have its own context window focused on web search results, while a \"writing agent\" has a separate context focused on drafting a report. This prevents different stre"
  },
  {
    "id": "report_source",
    "chunk": "ent\" might have its own context window focused on web search results, while a \"writing agent\" has a separate context focused on drafting a report. This prevents different streams of information from conflicting or confusing the model and allows for greater specialization.8\n\nThis \"Four Pillars\" framework provides a powerful mental model that can be analogized to the core functions of a computer's operating system. If, as Andrej Karpathy suggests, the LLM is the \"CPU\" and its context window is the \"RAM,\" then context engineering is the \"OS\" that manages this hardware.8 The **Select** and **Compress** pillars function like the OS's memory manager, deciding which data is loaded into the finite RAM. The **Write** pillar is analogous to using virtual memory or a swap file, moving less-used data from RAM to the hard drive (an external memory store) to be retrieved later. Finally, the **Isolate** pillar mirrors how an OS uses processes and memory sandboxing to prevent different applications from interfering with one another's memory space. This analogy is not merely illustrative; it reveals that context engineering is borrowing and adapting fundamental computer science principles to manage a new kind of computational resource. A curriculum built around this concept would provide students with a deep, transferable understanding of the discipline.\n\n### **Core Component Deep Dive: Memory, Tools, and Knowledge**\n\nThe \"Four Pillars\" framework operates on a set of core architectural components that form the building blocks of any sophisticated context-engineered system. Mastering the design and integration of these components is the primary practical task of the context engineer.\n\n* **Memory Management:** Creating the illusion of a co"
  },
  {
    "id": "report_source",
    "chunk": "system. Mastering the design and integration of these components is the primary practical task of the context engineer.\n\n* **Memory Management:** Creating the illusion of a continuous, coherent conversation requires explicit memory management. This is typically divided into two categories 9:  \n  * **Short-Term Memory:** This refers to the information maintained within a single session, such as the recent conversation history or the current state of a multi-step task.18 It is often managed directly within the context window, using compression and summarization techniques as the conversation grows.  \n  * **Long-Term Memory:** This involves persisting information across multiple conversations or sessions. Examples include storing a user's profile information, their stated preferences, or key facts from past interactions.6 This information is typically held in an external database and selectively retrieved into the context when relevant.  \n* **Tool Integration:** To move beyond simple text generation and perform actions in the world, LLMs must be given access to tools. A tool is a function that the model can invoke to perform an external task, such as querying a database, calling a scheduling API, or searching the web.1 Effective tool integration requires the engineer to provide the LLM with a clear, structured description of each tool, including its name, its purpose, and the parameters it expects.10 This allows the LLM to reason about which tool is appropriate for a given task and how to call it correctly.  \n* **Knowledge Retrieval:** One of the most significant limitations of LLMs is their reliance on the static knowledge contained in their training data, which can be outdated or lack domain-specific detail. Knowledge ret"
  },
  {
    "id": "report_source",
    "chunk": "significant limitations of LLMs is their reliance on the static knowledge contained in their training data, which can be outdated or lack domain-specific detail. Knowledge retrieval is the process of grounding the LLM in external, factual information to combat hallucinations and provide up-to-date, specialized expertise.18 This is the foundational principle of **Retrieval-Augmented Generation (RAG)**, where a user's query is first used to search a knowledge base (e.g., a company's internal wiki), and the most relevant documents are retrieved and injected into the context along with the original query.19\n\n### **Multi-Modal Context: Beyond Text**\n\nA final, critical principle is that modern context engineering is an inherently multi-modal discipline. While early interactions with LLMs were text-based, today's advanced models can process and reason about a wide variety of data types. A comprehensive context must therefore integrate information from multiple modalities to provide a richer and more complete understanding of the task environment. This includes 9:\n\n* **Visual Context:** Images, diagrams, charts, and user interface layouts.  \n* **Structured Data:** Information from databases, spreadsheets, and APIs.  \n* **Temporal Context:** Time-series data, schedules, and event logs.  \n* **Spatial Context:** Geographical information, maps, and physical layouts.\n\nBuilding systems that can seamlessly fuse these different types of context is a frontier of the field and is essential for creating the next generation of AI applications that can understand and interact with the world in a more holistic and human-like way.\n\n## **Mastering the Context Window: Foundational Management Strategies**\n\nThe primary technical constraint driving"
  },
  {
    "id": "report_source",
    "chunk": "teract with the world in a more holistic and human-like way.\n\n## **Mastering the Context Window: Foundational Management Strategies**\n\nThe primary technical constraint driving the entire discipline of context engineering is the nature of the LLM's **context window**. This finite and computationally expensive resource is the \"working memory\" of the AI, representing the total amount of informationinstructions, history, retrieved documents, and tool outputsthat the model can \"see\" and consider at any given moment.20 Effectively managing this bottleneck is the foundational skill of the context engineer.\n\n### **Understanding the Bottleneck: The Physics of the Context Window**\n\nWhile model providers are continuously expanding the size of context windows, with some now capable of processing millions of tokens, a larger window does not eliminate the core challenges. In fact, it can often exacerbate them.21 The fundamental physics of the context window introduce several critical problems:\n\n* **Cost and Latency:** The computational complexity of the attention mechanism in Transformer architectures, the foundation of most LLMs, scales quadratically with the length of the input sequence. This means that doubling the context length can quadruple the processing time and associated API costs. Overly long contexts can lead to slow response times and prohibitive operational expenses, making them impractical for many real-time applications.10  \n* **The \"Lost in the Middle\" Problem:** Research and empirical evidence have shown that LLMs do not pay equal attention to all parts of the context window. They tend to have a strong recall of information presented at the very beginning and the very end of the context, but their performance degra"
  },
  {
    "id": "report_source",
    "chunk": " all parts of the context window. They tend to have a strong recall of information presented at the very beginning and the very end of the context, but their performance degrades significantly when trying to retrieve information buried in the middle of a long input sequence.22 This \"lost in the middle\" effect means that simply adding more information does not guarantee the model will use it effectively.  \n* **Context Dilution and \"Rot\":** As the context window grows with more turns of conversation, retrieved documents, and tool outputs, any single piece of information becomes a smaller and smaller percentage of the whole. This phenomenon, sometimes called \"context rot\" or dilution, can cause the model's focus to drift. The model is not \"forgetting\" in the human sense; rather, its attention is being diluted by an increasing amount of potentially irrelevant information, or \"noise\".24\n\nThese challenges are not just technical hurdles; they represent a fundamental economic driver for context engineering. Every token sent to an LLM API has a direct monetary cost, and every millisecond of latency impacts user experience.22 Therefore, the practice of context engineering is, in essence, a discipline of resource optimization. The engineer's goal is to maximize the \"signal-to-noise\" ratio within a given token budget, achieving the desired outcome with the minimum possible cost and latency. This requires a toolkit of strategies for curating, compressing, and structuring the information that enters the context window. A curriculum focused on this discipline must therefore include training on the economics of AI, teaching students to measure token costs, analyze latency, and evaluate the return on investment of different context manag"
  },
  {
    "id": "report_source",
    "chunk": " therefore include training on the economics of AI, teaching students to measure token costs, analyze latency, and evaluate the return on investment of different context management techniques.\n\n### **Foundational Strategy 1: Progressive Context Building and Priming**\n\nOne of the most effective and intuitive strategies for managing context is to build it progressively rather than attempting to load all possible information at the outset. This approach involves starting a conversation or task with only the most essential context and then gradually adding layers of detail as the interaction develops.9  \nThis technique is also known as **priming**. Much like setting the stage before a play, priming systematically prepares the AI's understanding on a step-by-step basis. For example, when teaching the AI a complex topic, one would first prime it with the basic definitions, then use that established knowledge as the foundation for the next concept, and so on.20 This creates a coherent and logical learning path for the model, reducing the chances of misunderstanding and ensuring that new information is correctly integrated with what has already been discussed. It avoids overwhelming the model with excessive initial context, which can lead to distraction and the \"lost in the middle\" problem.\n\n### **Foundational Strategy 2: Summarization and Compression**\n\nAs conversations or tasks proceed, the amount of dynamic context (e.g., chat history) can quickly exceed the optimal size of the context window. Summarization and compression techniques are essential for managing this growth. These methods aim to condense large amounts of information into a more compact form while retaining the most critical details.9  \nThere are several approac"
  },
  {
    "id": "report_source",
    "chunk": "naging this growth. These methods aim to condense large amounts of information into a more compact form while retaining the most critical details.9  \nThere are several approaches to summarization:\n\n* **Extractive Summarization:** This involves identifying and selecting the most important key sentences or phrases from a larger text. It is a simple and fast method for reducing verbosity.9  \n* **Abstractive Summarization:** This more sophisticated technique involves using an LLM to generate a new, concise summary that captures the essential meaning of the original text. This can often produce more coherent and natural-sounding summaries than extractive methods.9  \n* **Hierarchical Compression:** For very large documents or long histories, a single summary may not be sufficient. Hierarchical compression involves creating layered summaries at different levels of detail. For example, one might have a one-sentence summary, a one-paragraph summary, and a one-page summary of a book, allowing the system to select the appropriate level of detail based on the current task's needs.9\n\n### **Foundational Strategy 3: Strategic Truncation and Context Refreshing**\n\nTruncation is the simplest, albeit most blunt, strategy for managing context length: simply cutting off the oldest messages or information once a certain limit is reached.22 While fast and easy to implement, this method is risky as it can inadvertently discard essential information that may be needed later in the conversation.  \nA more sophisticated and safer approach is the **Context Refresh** strategy. This technique functions like the \"Previously on...\" segment of a television series, designed to help the AI maintain context continuity and realign its focus.20 There are two "
  },
  {
    "id": "report_source",
    "chunk": "his technique functions like the \"Previously on...\" segment of a television series, designed to help the AI maintain context continuity and realign its focus.20 There are two common ways to perform a context refresh:\n\n1. **Ask the AI to Summarize:** The user or system can periodically prompt the AI to summarize the current state of the conversation, including what has been discussed, what key decisions have been made, and what the current focus is. This summary then becomes the new, compressed context for the next turn.  \n2. **Ask the AI to Check Understanding:** The user can explicitly ask the AI to confirm its understanding of the current context (e.g., \"Please confirm we are working on \\[topic\\] and the last point we discussed was \\[point\\]. Is this correct?\"). This helps to catch any misunderstandings or context drift early before they derail the task.20\n\n### **Foundational Strategy 4: Structured and Token-Aware Prompting**\n\nThis is the point where the discipline of prompt engineering is subsumed as a crucial *component* of the broader context engineering framework. Instead of focusing on finding \"magic words,\" this strategy emphasizes the efficient encoding of information within the prompt itself. It involves using structured formats and being deliberate about token usage to maximize clarity and minimize waste.20\n\n* **Structured Formats:** Using formats like Markdown (with headers and lists) or JSON to organize information within the prompt helps the model parse and understand the relationships between different pieces of context. This provides a clear, logical pathway for the model's reasoning process.25  \n* **Token-Awareness:** This involves being mindful of the token count of each piece of information being added"
  },
  {
    "id": "report_source",
    "chunk": " a clear, logical pathway for the model's reasoning process.25  \n* **Token-Awareness:** This involves being mindful of the token count of each piece of information being added to the context. By understanding that every token has a cost, an engineer can make strategic decisions about what to include, what to summarize, and what to omit. This practice prioritizes essential information, sets a clear scope for the task, and leads to more efficient and reliable responses.20\n\n## **The Modern Context Stack: Advanced Techniques and Frameworks**\n\nWhile foundational context management strategies are essential for controlling the context window, building state-of-the-art AI agents requires a more sophisticated stack of techniques and frameworks. These modern approaches move beyond passive management to actively augment the model's capabilities, ground it in factual reality, and even enable it to participate in the curation of its own context.\n\n### **Retrieval-Augmented Generation (RAG): Grounding Models in Reality**\n\nRetrieval-Augmented Generation (RAG) has become the de facto standard for building reliable, knowledge-intensive LLM applications. It is a technique that enhances a model's responses by dynamically injecting relevant, external context into the prompt at runtime.19 RAG directly addresses two of the most significant weaknesses of standalone LLMs: their lack of access to real-time or domain-specific information, and their propensity to \"hallucinate\" or generate factually incorrect content.18  \nThe RAG process typically involves a multi-stage pipeline 19:\n\n1. **Indexing (Offline Process):** A corpus of documents (e.g., a company's internal documentation, product manuals, or a set of research papers) is processed. Each doc"
  },
  {
    "id": "report_source",
    "chunk": "e 19:\n\n1. **Indexing (Offline Process):** A corpus of documents (e.g., a company's internal documentation, product manuals, or a set of research papers) is processed. Each document is broken down into smaller, manageable sections or \"chunks.\"  \n2. **Embedding:** Each chunk is passed through an embedding model, which converts the text into a numerical vector representation that captures its semantic meaning.  \n3. **Storage:** These embeddings are stored in a specialized vector database, which is optimized for fast similarity searches.  \n4. **Retrieval (Runtime Process):** When a user submits a query, the query itself is converted into an embedding vector. This vector is then used to search the vector database to find the text chunks with the most semantically similar embeddings.  \n5. **Augmentation and Generation:** The top-ranked, most relevant text chunks are retrieved and \"augmented\" into the LLM's context, typically placed alongside the original user query. The LLM then generates a response that is grounded in the provided information, allowing it to answer questions about content that was not part of its original training data.19\n\n### **Advanced RAG: Beyond Simple Retrieval**\n\nWhile basic RAG is powerful, it can struggle with ambiguous or complex queries that require more than a simple semantic search. The field has rapidly evolved to include a suite of advanced RAG techniques designed to improve the precision and recall of the retrieval step and enable more complex reasoning.28\n\n* **Hybrid Search:** This technique combines the strengths of traditional keyword-based search (sparse retrieval, like BM25) with modern semantic search (dense retrieval). Sparse retrieval excels at matching specific terms and acronyms, whil"
  },
  {
    "id": "report_source",
    "chunk": "itional keyword-based search (sparse retrieval, like BM25) with modern semantic search (dense retrieval). Sparse retrieval excels at matching specific terms and acronyms, while dense retrieval is better at understanding broader intent and meaning. A hybrid approach uses both methods and combines their results to produce a more robust and relevant set of documents.28  \n* **Re-ranking:** The initial retrieval step is often optimized for speed and may return a large set of potentially relevant documents. A re-ranking stage can be added to the pipeline, where a second, more powerful (and often slower) model is used to re-evaluate and re-order this initial set. This ensures that the most relevant documents are placed at the top of the list before being passed to the final generation model, improving its focus.28  \n* **Multi-hop Reasoning:** Many complex questions cannot be answered from a single piece of information. Multi-hop reasoning enables a system to answer such questions by breaking them down into sub-questions and performing a sequence of retrieval and synthesis steps. For example, to answer \"Which film by the director of *Jaws* won the Oscar for Best Picture?\", a multi-hop system would first retrieve the director of *Jaws* (Steven Spielberg), then perform a second retrieval to find which of his films won Best Picture (*Schindler's List*).26\n\n### **Self-Reflective and Agentic Frameworks**\n\nThe frontier of context engineering involves creating systems where the AI model itself becomes an active participant in managing its own context. These frameworks move from a passive, one-way flow of information to a dynamic, reflective loop, enabling a form of artificial metacognitionthe system learns to \"think about its own thin"
  },
  {
    "id": "report_source",
    "chunk": "orks move from a passive, one-way flow of information to a dynamic, reflective loop, enabling a form of artificial metacognitionthe system learns to \"think about its own thinking process.\"\n\n* **SELF-RAG:** This framework introduces a layer of self-reflection into the RAG process. Before generating a response, the model first uses \"reflection tokens\" to decide whether retrieval is necessary at all for the given query. If it decides to retrieve, it then generates a response and reflects on both the retrieved passages and its own output to assess quality and factual accuracy. This allows the model to operate on-demand, retrieving information only when needed and iteratively improving its own output.26  \n* **Agentic Context Engineering (ACE):** Developed by researchers at Stanford and other institutions, ACE is a state-of-the-art framework that treats an agent's context not as a temporary input but as an evolving **\"playbook\"** of strategies and knowledge.29 The ACE framework employs a modular, multi-agent architecture:  \n  1. The **Generator** is responsible for attempting to solve a given task using the current playbook.  \n  2. The **Reflector** analyzes the Generator's output (its \"execution feedback\"), identifying both successes and failures. It then distills specific, actionable insights from this analysis.  \n  3. The Curator takes these insights and integrates them back into the playbook, refining existing strategies or adding new ones.  \n     This \"generate-reflect-curate\" loop allows the agent to learn and self-improve its own context over time, purely from experience, without requiring any ground-truth labels or supervised training.29 ACE uses efficient mechanisms like \"Incremental Delta Updates\" and a \"Grow-and-Re"
  },
  {
    "id": "report_source",
    "chunk": ", purely from experience, without requiring any ground-truth labels or supervised training.29 ACE uses efficient mechanisms like \"Incremental Delta Updates\" and a \"Grow-and-Refine\" principle to ensure the playbook remains compact and relevant as it expands.29\n\nThe emergence of these self-reflective systems represents a significant leap in AI development. They parallel the human learning process of cognitive apprenticeship, where a novice learns not just facts, but effective strategies and heuristics by observing an expert, practicing, and reflecting on their own performance.31 In essence, frameworks like ACE are designed to create an AI that can be its own cognitive apprentice, continuously refining its internal \"playbook\" for solving problems. An advanced curriculum must therefore prepare students to build these self-improving, reflective systems, as they represent the future of autonomous agent design.\n\n### **Information-Theoretic Approaches**\n\nUnderscoring the maturation of context engineering into a formal discipline is the application of rigorous mathematical principles. Frameworks like **Directed Information -covering** demonstrate this trend. This approach uses concepts from information theory, specifically Directed Information (a causal analogue of mutual information), to measure the predictive relationship between different chunks of context.33 By formulating context selection as a mathematical optimization problem (a -cover problem), this framework allows for the selection of a diverse and non-redundant set of context chunks. A key advantage is that this selection process can be computed offline in a query-agnostic manner, incurring no latency during online inference. While highly theoretical, the existence o"
  },
  {
    "id": "report_source",
    "chunk": "tage is that this selection process can be computed offline in a query-agnostic manner, incurring no latency during online inference. While highly theoretical, the existence of such frameworks signals a move away from purely empirical heuristics and towards a more principled, scientific foundation for context engineering.33\n\n## **The Implementation Layer: The Protocol and Tooling Ecosystem**\n\nThe principles and advanced techniques of context engineering are brought to life through a rapidly growing ecosystem of protocols, frameworks, and tools. For aspiring context engineers, mastering this implementation layer is just as crucial as understanding the underlying theory. This section provides a survey of the key technologies that form the modern developer's toolkit for building context-aware AI systems.\n\n### **The Need for Standardization: The Model Context Protocol (MCP)**\n\nAs AI agents became more capable, a significant bottleneck emerged: the \"M x N integration problem.\" Every one of the *M* available LLMs required a custom, bespoke integration to connect with each of the *N* external tools and data sources an application might need. This led to a fragmented, inefficient, and difficult-to-maintain development landscape.35  \nTo address this, Anthropic introduced the **Model Context Protocol (MCP)**, an open-source standard designed to create a universal interface between AI applications and external systems.36 MCP acts as a \"universal remote\" or a \"USB-C port for AI,\" defining a common language that any model can use to communicate with any tool, provided both support the protocol.36 By standardizing this communication layer, MCP reduces the integration complexity from a multiplicative M x N problem to an additive M \\+ N"
  },
  {
    "id": "report_source",
    "chunk": "ovided both support the protocol.36 By standardizing this communication layer, MCP reduces the integration complexity from a multiplicative M x N problem to an additive M \\+ N problem, drastically simplifying the process of building and extending capable AI agents.35\n\n### **MCP Architecture and Core Primitives**\n\nMCP is built on a robust client-server architecture inspired by the Language Server Protocol (LSP) used in software development environments.36 The key components are:\n\n* **MCP Host:** The AI-powered application that the end-user interacts with, such as Claude Desktop or an AI-integrated IDE. The host manages and coordinates connections to various servers.  \n* **MCP Client:** An intermediary component that lives within the host. The host creates a separate client instance for each server it connects to, managing the secure, isolated communication session.  \n* **MCP Server:** A lightweight, standalone program that exposes the capabilities of a specific external system. For example, a github-mcp-server would expose functions for interacting with the GitHub API.\n\nThis architecture allows for a decoupling of intelligence and capability. The core reasoning is handled by the LLM within the host application, while the ability to act upon the world is provided by a distributed network of specialized, composable MCP servers. This is analogous to a microservices architecture in traditional software, where complex applications are built from small, independent, and reusable services. This model allows teams to develop and deploy new capabilities (as MCP servers) without needing to modify the core AI agent's logic.  \nMCP defines three core primitives that servers can expose 35:\n\n1. **Tools:** Executable functions that the L"
  },
  {
    "id": "report_source",
    "chunk": "CP servers) without needing to modify the core AI agent's logic.  \nMCP defines three core primitives that servers can expose 35:\n\n1. **Tools:** Executable functions that the LLM can decide to call to perform an action (e.g., send\\_email, query\\_database).  \n2. **Resources:** Read-only data sources that provide context to the model (e.g., the content of a file, a list of calendar events).  \n3. **Prompts:** Pre-defined, reusable templates for standardized interactions, often combining specific tools and resources for a common workflow.\n\nFurthermore, MCP includes advanced features that enable more dynamic and agentic interactions 35:\n\n* **Sampling:** This powerful feature reverses the typical flow of control, allowing a server to *request* an LLM completion from the client. For example, a code review server could analyze a file and then ask the client's LLM to generate a summary of potential issues. This enables servers to leverage AI without needing their own API keys, while the client retains full control over model access and permissions.  \n* **Elicitation:** This allows a server to pause its operation and request additional information from the end-user. For instance, if a GitHub server is asked to commit code but the branch is not specified, it can use elicitation to prompt the user for the correct branch name before proceeding.\n\n### **The MCP Ecosystem in Practice**\n\nMCP is rapidly moving from a theoretical standard to a practical and growing ecosystem. A wide range of open-source MCP servers are now available for popular tools and platforms, including 40:\n\n* **github-mcp-server:** For interacting with code repositories, issues, and pull requests.  \n* **drawio-mcp-server:** For programmatically creating and editing ar"
  },
  {
    "id": "report_source",
    "chunk": "luding 40:\n\n* **github-mcp-server:** For interacting with code repositories, issues, and pull requests.  \n* **drawio-mcp-server:** For programmatically creating and editing architectural diagrams.  \n* **slack-mcp-server:** For sending messages and interacting with team communications.  \n* **postgres-mcp-pro:** For querying and managing PostgreSQL databases.\n\nCommunity-driven marketplaces and GitHub repositories have emerged as central hubs for discovering, sharing, and contributing new MCP servers, accelerating the adoption of the protocol.38 Numerous tutorials and courses are also available to guide developers in building their own custom MCP servers, further lowering the barrier to entry.42\n\n### **Orchestration Frameworks and Libraries**\n\nWhile MCP provides the standardized \"plumbing\" for tool communication, higher-level orchestration frameworks provide the building blocks for designing the agent's logic and managing its internal state.\n\n* **LangChain and LangGraph:** LangChain is a popular framework that offers a wide array of components for building LLM applications. A key component for advanced agent design is **LangGraph**, a library for building stateful, multi-agent applications by representing them as graphs.15 The cyclical nature of graphs makes LangGraph particularly well-suited for implementing the complex, iterative reasoning loops found in advanced agents, such as the \"generate-reflect-curate\" cycle of the ACE framework.26 LangGraph provides a low-level, explicit way to manage the flow of context and state between different nodes in an agent's thought process.  \n* **The Open-Source Landscape:** The broader open-source community on platforms like GitHub is a vibrant source of tools and libraries for context "
  },
  {
    "id": "report_source",
    "chunk": " agent's thought process.  \n* **The Open-Source Landscape:** The broader open-source community on platforms like GitHub is a vibrant source of tools and libraries for context engineering. A survey of available repositories reveals a rich landscape of specialized tools, including 7:  \n  * Frameworks for managing and versioning prompts as software artifacts.  \n  * Libraries for advanced memory systems (e.g., LangMem, Zep).  \n  * Complete agentic development kits and frameworks (e.g., from GitHub and Google).  \n  * Tools for automatically extracting and structuring context from codebases.\n\nA curriculum for AI systems architecture must therefore focus on this service-oriented paradigm. Students need to learn not only how to build a single, monolithic agent but also how to design, build, and deploy composable, reusable MCP servers. This skill is becoming essential for anyone looking to build enterprise-grade AI systems that are scalable, maintainable, and extensible.\n\n| Category | Tool/Protocol Name | Description | Primary Use Case | Key References |\n| :---- | :---- | :---- | :---- | :---- |\n| **Standardization Protocol** | Model Context Protocol (MCP) | An open-source standard that acts as a \"universal connector\" for AI models and external tools. | Achieving interoperability and solving the M x N integration problem. | 36 |\n| **Orchestration Frameworks** | LangGraph | A library for building stateful, multi-agent applications by representing them as cyclical graphs. | Implementing complex agentic reasoning loops and managing state. | 15 |\n| **Agentic Development Kits** | GitHub's AI Workflow Framework | A layered framework of Markdown prompts, agentic primitives, and context engineering for reliable AI workflows. | AI-assiste"
  },
  {
    "id": "report_source",
    "chunk": "elopment Kits** | GitHub's AI Workflow Framework | A layered framework of Markdown prompts, agentic primitives, and context engineering for reliable AI workflows. | AI-assisted software development and CI/CD automation. | 25 |\n| **Memory Systems** | Zep, LangMem | Specialized libraries and services for managing both short-term conversational memory and long-term persistent knowledge. | Building stateful chatbots and personalized agents. | 6 |\n| **RAG / Vector DB Tools** | OpenAI Retrieval API, Pinecone, Weaviate | Platforms and APIs for creating vector embeddings and performing semantic search on large document corpora. | Grounding LLM responses in factual data and reducing hallucinations. | 11 |\n\n## **Context in Action: Agentic Workflows and Collaborative Development**\n\nThe theoretical principles and tooling ecosystem of context engineering converge in a set of practical, high-value applications that are actively transforming professional workflows. By grounding the curriculum in these real-world use cases, students can understand not just *how* to build context-aware systems, but *why* they are so impactful. These examples demonstrate a shift from AI as a simple automation tool to AI as a cognitive partner that reshapes and enhances human thought processes.\n\n### **AI-Assisted Software Architecture and Design**\n\nContext engineering is enabling AI to move beyond simple code generation and become an active participant in the creative and strategic process of software architecture. By providing an AI agent with the right contextsuch as design principles, existing system diagrams, and real-time conversational inputit can function as a powerful assistant for architects and engineers.  \nA prime example of this is the use of"
  },
  {
    "id": "report_source",
    "chunk": "les, existing system diagrams, and real-time conversational inputit can function as a powerful assistant for architects and engineers.  \nA prime example of this is the use of the drawio-mcp-server.40 An architect can engage in a natural language conversation with an AI agent about a desired system design. The agent, connected to the Draw.io diagramming tool via MCP, can listen to the discussion and generate or modify architectural diagrams in real time. If the architect says, \"Let's add a caching layer between the API gateway and the microservices,\" the agent can immediately update the diagram to reflect this change. This creates a fluid, iterative design loop where ideas are instantly visualized, helping teams to identify ambiguities, explore alternatives, and create tangible design artifacts that can be version-controlled alongside the code.40  \nBeyond real-time diagramming, context-aware AI can perform sophisticated architectural analysis. By ingesting an entire codebase as context, an AI can identify architectural weak points, suggest performance optimizations, detect potential security vulnerabilities, and even automate the generation of comprehensive system documentation based on the code's structure and dependencies.12\n\n### **The Human-AI Pair Programming Workflow**\n\nThe traditional practice of pair programming, where two developers work together at one workstation, has been reimagined in the age of AI. In the modern human-AI pair programming paradigm, the roles are clearly delineated to leverage the complementary strengths of human and machine.50\n\n* **The Human as \"Navigator\":** The human developer takes on the strategic role. They set the overall direction, make high-level architectural decisions, define the re"
  },
  {
    "id": "report_source",
    "chunk": "ine.50\n\n* **The Human as \"Navigator\":** The human developer takes on the strategic role. They set the overall direction, make high-level architectural decisions, define the requirements for a feature, and critically review the code generated by the AI.  \n* **The AI as \"Driver\":** The AI assistant acts as the tireless coder. It generates code implementations based on the human's instructions, suggests refactoring opportunities, identifies syntax errors in real time, and automates repetitive tasks like writing unit tests or boilerplate code.\n\nThe success of this collaborative workflow is entirely dependent on the human's ability to practice effective context engineering. The AI's output is only as good as the context it is given. An effective \"Navigator\" must provide the AI with clear and curated context, including the project's architecture, established coding standards, examples of existing patterns, and specific requirements and edge cases for the task at hand.51 Best practices have emerged for this workflow, such as starting with a detailed written plan, using a test-driven \"edit-test loop\" (where the AI is tasked with making a failing test pass), and demanding the AI to explain its reasoning step-by-step before writing code.52 This process forces the human developer to structure their own thinking more rigorously, leading to better-defined requirements and higher-quality outcomes.\n\n### **Building Reliable Agentic Workflows with GitHub**\n\nGitHub, as a central platform for software development, has developed a comprehensive framework for building reliable, enterprise-grade AI workflows that serves as an excellent real-world case study.25 Their approach demonstrates how the various layers of context engineering can be in"
  },
  {
    "id": "report_source",
    "chunk": "eliable, enterprise-grade AI workflows that serves as an excellent real-world case study.25 Their approach demonstrates how the various layers of context engineering can be integrated into a cohesive system. The framework consists of three layers:\n\n1. **Strategic Prompt Engineering with Markdown:** At the base layer, Markdown is used to structure prompts. Its hierarchical nature (headers, lists) provides a natural way to guide the AI's reasoning pathways.  \n2. **Agentic Primitives:** These are reusable, configurable building blocks written in natural language that formalize an agent's capabilities and constraints. They include:  \n   * .instructions.md files to define global rules and behaviors.  \n   * .chatmode.md files to create domain-specific personas with bounded tool access, preventing cross-domain interference.  \n   * .prompt.md files to create templates for common, repeatable tasks.  \n3. **Context Engineering:** This top layer focuses on optimizing the information provided to the agent. It involves techniques like **session splitting** (using fresh context windows for distinct tasks), applying **modular rules** that activate only for specific file types, and using memory files to maintain project knowledge across sessions.\n\nThis layered approach provides a concrete example of how to move from ad-hoc prompting to a systematic, engineered process for creating robust and repeatable AI systems for developers, integrating them directly into the CI/CD pipeline.\n\n### **Cognitive Apprenticeship with AI**\n\nBeyond software development, context engineering has profound implications for education and skill acquisition. The pedagogical model of **Cognitive Apprenticeship** posits that learners acquire complex skills most effec"
  },
  {
    "id": "report_source",
    "chunk": "ing has profound implications for education and skill acquisition. The pedagogical model of **Cognitive Apprenticeship** posits that learners acquire complex skills most effectively when an expert makes their implicit thought processes visible and provides scaffolding to guide the learner's practice.31  \nA well-engineered AI agent can serve as a powerful and scalable \"expert\" in this model. Within a community of practice or a learning environment, an AI can act as a tireless tutor, available 24/7 to assist novices. By being provided with the context of a student's current task and knowledge level, the AI can 32:\n\n* **Provide Cognitive Scaffolding:** Offer hints, break down complex problems into smaller steps, and provide just-in-time feedback.  \n* **Offer Data-Driven Insights:** Analyze a student's code or writing and offer suggestions based on best practices learned from vast datasets.  \n* **Present Personalized Learning Opportunities:** Recommend relevant exercises or reading material tailored to the individual learner's needs.\n\nThis application highlights a future where context engineering is used not just to build products, but to build more effective learning environments, fundamentally changing how skills are taught and acquired. A curriculum on context engineering should therefore include a module on \"Human-AI Collaboration,\" teaching not only the technical skills to build these systems but also the new workflows and cognitive skills required to partner effectively with them.\n\n## **Navigating the Pitfalls: Common Challenges and Mitigation Strategies**\n\nWhile context engineering enables the creation of powerful and reliable AI systems, it is not without its challenges. Building robust agentic systems requires a pra"
  },
  {
    "id": "report_source",
    "chunk": "ategies**\n\nWhile context engineering enables the creation of powerful and reliable AI systems, it is not without its challenges. Building robust agentic systems requires a pragmatic understanding of their common failure modes and a toolkit of strategies to mitigate them. This requires a shift in mindset towards a form of adversarial thinking, where the engineer must constantly anticipate how the system can fail and proactively design defenses. The failure modes of context engineering are the LLM-native equivalent of traditional software vulnerabilities, and the mitigation strategies are analogous to security best practices like input validation and sandboxing.\n\n### **Common Failure Modes: When Context Goes Wrong**\n\nAs the context window fills with information from various sourcesconversation history, retrieved documents, tool outputsseveral distinct failure patterns can emerge. These have been identified and named by experts and the developer community.8\n\n* **Context Poisoning:** This occurs when a piece of factually incorrect information, either from a hallucination by the model or from an unreliable external source, is introduced into the context. If this \"poisoned\" data is then saved to a memory or repeatedly referenced in a long conversation, it can corrupt all subsequent outputs. The model will treat the incorrect statement as true, leading to a cascade of errors.  \n* **Context Distraction:** This is a signal-to-noise problem. If the context window is filled with too much irrelevant or noisy information, it can overwhelm the model's attention mechanism. The model may lose focus on the primary task or the most critical instructions, leading to off-topic or low-quality responses. This is a direct consequence of cont"
  },
  {
    "id": "report_source",
    "chunk": " mechanism. The model may lose focus on the primary task or the most critical instructions, leading to off-topic or low-quality responses. This is a direct consequence of context dilution.  \n* **Context Confusion:** This failure mode arises when superfluous but potentially relevant-sounding information influences the model's output in undesirable ways. A common example is providing the model with descriptions for too many tools, some of which have overlapping functionalities. The model may become confused about which tool is the correct one to use for a specific task, leading to incorrect actions.  \n* **Context Clash:** This happens when the context contains conflicting information from two or more sources. For example, two retrieved documents might offer contradictory facts about a topic. Without a mechanism to resolve this conflict, the model may produce an inconsistent answer, express uncertainty, or simply choose one source at random.\n\n### **A Toolkit of Mitigation Strategies**\n\nFor each of these failure modes, a corresponding set of defensive design patterns and mitigation strategies has been developed. A robust curriculum should equip students with this practical toolkit.10\n\n* **Mitigating Context Poisoning:**  \n  * **Validation and Feedback Loops:** Before writing information to a long-term memory or a persistent knowledge base, implement a validation step. This could involve cross-referencing with a trusted data source or, for critical information, requiring human verification.  \n  * **Source Attribution:** Tag information with its source. This allows the model (or a human reviewer) to assess the reliability of the context and potentially down-weight or ignore information from less trusted sources.  \n* **Mitigati"
  },
  {
    "id": "report_source",
    "chunk": "his allows the model (or a human reviewer) to assess the reliability of the context and potentially down-weight or ignore information from less trusted sources.  \n* **Mitigating Context Distraction:**  \n  * **Aggressive Pruning and Summarization:** Regularly apply compression techniques to the conversation history and other verbose context elements.  \n  * **Relevance Scoring and Filtering:** When using RAG, implement a re-ranking step or apply strict relevance filters to ensure that only the most pertinent chunks of information are injected into the context. The goal is to maximize the signal-to-noise ratio.  \n* **Mitigating Context Confusion:**  \n  * **Context Isolation:** Employ multi-agent architectures where each agent has a small, specialized set of tools and a focused context window. This prevents tool descriptions from overlapping and competing for the model's attention.  \n  * **Structured Schemas:** Use clear and unambiguous schemas (e.g., JSON Schema) for tool definitions and data structures. This reduces the chance that the model will misinterpret the purpose or format of a piece of information.  \n* **Mitigating Context Clash:**  \n  * **Meta-Tags and Source Labeling:** As with poisoning, explicitly labeling the source of each piece of information can help. An instruction can be given to the model on how to handle conflicts, such as \"If sources disagree, state the conflict and cite both sources.\"  \n  * **Let the Model Express Uncertainty:** In cases of unresolvable conflict, it is often better for the model to state that it has found conflicting information rather than confidently asserting a potentially incorrect fact.\n\n### **The Human-in-the-Loop: The Ultimate Failsafe**\n\nFinally, it is critical to recognize t"
  },
  {
    "id": "report_source",
    "chunk": "cting information rather than confidently asserting a potentially incorrect fact.\n\n### **The Human-in-the-Loop: The Ultimate Failsafe**\n\nFinally, it is critical to recognize that no amount of engineering can completely eliminate the risk of failure in complex, stochastic systems. The ultimate failsafe in any robust agentic system is meaningful **human oversight**.12 For critical or irreversible actionssuch as sending an email to a customer, modifying a production database, or deploying codea mandatory human review and approval step should be built into the workflow. The goal of context engineering is to create a highly capable and reliable AI partner that augments human intelligence, not to replace it entirely. A responsible AI systems architect understands the limits of the technology and designs systems that keep the human in control.\n\n## **V2V Academy Curriculum Blueprint: Recommendations for Course Development**\n\nThe analysis presented in this report demonstrates a clear and urgent need for a new educational paradigm focused on the principles and practices of context engineering. The transition from simple prompting to complex systems architecture is the defining characteristic of the maturation of the AI development field. By developing and launching a comprehensive, rigorous certification program based on this shift, V2V Academy has a strategic opportunity to define the industry standard for this critical new role and establish itself as the premier institution for training the next generation of AI leaders. This final section provides a concrete, actionable blueprint for such a curriculum.\n\n### **Proposed Program Title: Certified AI Systems Architect**\n\nIt is recommended that the program move beyond narrow and i"
  },
  {
    "id": "report_source",
    "chunk": "crete, actionable blueprint for such a curriculum.\n\n### **Proposed Program Title: Certified AI Systems Architect**\n\nIt is recommended that the program move beyond narrow and increasingly commoditized titles like \"Prompt Engineer.\" A title such as **Certified AI Systems Architect** or **Certified Context Engineer** more accurately reflects the systems-level thinking, architectural skills, and engineering rigor required for the role. This positioning aligns with the professionalization of the field and will command higher value and recognition in the job market, attracting serious professionals looking to build defensible, high-impact careers in AI.\n\n### **Modular Curriculum Structure**\n\nA modular curriculum is proposed, designed to guide students logically from foundational principles to advanced, specialized topics. Each module should combine theoretical instruction with hands-on labs and projects, culminating in a capstone project that requires students to synthesize all their learned skills.\n\n* **Module 1: Foundations of AI Systems** (Corresponds to Sections I & II)  \n  * **Topics:** The paradigm shift from prompting to context engineering. The limits of linguistic tuning. The principles of systems thinking in AI. The anatomy of context (explicit, implicit, dynamic). The \"Four Pillars\" framework (Write, Select, Compress, Isolate). Core components: memory, tools, and knowledge.  \n  * **Objective:** Students will be able to articulate the strategic importance of context engineering and deconstruct any AI interaction into its core contextual components.  \n* **Module 2: Context Window Resource Management** (Corresponds to Section III)  \n  * **Topics:** The \"physics\" of the context window (cost, latency, \"lost in the middle"
  },
  {
    "id": "report_source",
    "chunk": "ts.  \n* **Module 2: Context Window Resource Management** (Corresponds to Section III)  \n  * **Topics:** The \"physics\" of the context window (cost, latency, \"lost in the middle\"). The economics of token management. Foundational strategies: progressive building (priming), summarization and compression techniques, context refreshing, and structured, token-aware prompting.  \n  * **Objective:** Students will be able to apply a variety of techniques to manage the context window efficiently, balancing performance, cost, and accuracy.  \n* **Module 3: Advanced Retrieval and Knowledge Systems** (Corresponds to Section IV)  \n  * **Topics:** Deep dive into Retrieval-Augmented Generation (RAG). Indexing, embedding, and vector databases. Advanced RAG techniques: hybrid search, re-ranking, and multi-hop reasoning. Introduction to self-reflective frameworks like SELF-RAG and Agentic Context Engineering (ACE).  \n  * **Objective:** Students will be able to build, evaluate, and optimize a production-grade RAG pipeline from scratch.  \n* **Module 4: The Agentic Tooling and Protocol Ecosystem** (Corresponds to Section V)  \n  * **Topics:** The M x N integration problem. The Model Context Protocol (MCP) architecture and primitives (Tools, Resources, Prompts). Advanced MCP features: Sampling and Elicitation. Survey of the MCP server ecosystem. Deep dive into orchestration frameworks like LangGraph.  \n  * **Objective:** Students will be able to design, build, and deploy a custom MCP server for a common business tool (e.g., Google Calendar, Slack) and integrate it into an agent built with LangGraph.  \n* **Module 5: Human-AI Collaborative Development Patterns** (Corresponds to Section VI)  \n  * **Topics:** AI-assisted software architecture and desi"
  },
  {
    "id": "report_source",
    "chunk": "nt built with LangGraph.  \n* **Module 5: Human-AI Collaborative Development Patterns** (Corresponds to Section VI)  \n  * **Topics:** AI-assisted software architecture and design patterns. The Human-AI pair programming workflow (Navigator/Driver roles). Best practices for collaborative development (e.g., planning, test-driven loops). Case study: building reliable workflows with GitHub's agentic framework. Cognitive Apprenticeship with AI.  \n  * **Objective:** Students will be able to structure and manage a complex software development task using an AI partner, applying best practices for context curation and workflow management.  \n* **Module 6: AI System Resilience and Safety** (Corresponds to Section VII)  \n  * **Topics:** Common context failure modes (Poisoning, Distraction, Confusion, Clash). A toolkit of mitigation strategies and defensive design patterns. The critical role of the human-in-the-loop. Principles of AI trust and safety in agentic systems.  \n  * **Objective:** Students will be able to identify potential context vulnerabilities in an AI system and implement appropriate mitigation strategies to improve its robustness and reliability.  \n* **Module 7: Capstone Project: Building a Multi-Agent System**  \n  * **Project:** Students will work in teams to design and build a complex, multi-agent system that solves a real-world business problem. The project will require them to integrate all skills learned throughout the program: designing a system architecture, implementing multiple specialized agents with isolated contexts, building or integrating custom tools via MCP, developing a RAG-based knowledge system, and implementing robust error handling and human-in-the-loop checkpoints.  \n  * **Objective:** Students wil"
  },
  {
    "id": "report_source",
    "chunk": "ting custom tools via MCP, developing a RAG-based knowledge system, and implementing robust error handling and human-in-the-loop checkpoints.  \n  * **Objective:** Students will deliver a fully functional, production-quality AI system and a comprehensive architectural design document, demonstrating mastery of the principles of AI systems architecture.\n\n### **Key Learning Objectives and Hands-On Projects**\n\nThe curriculum must be heavily project-based to ensure students develop practical, job-ready skills. In addition to the capstone, each module should feature hands-on labs. Examples include:\n\n* **Lab 1:** Building a memory-enabled chatbot that can recall user preferences across sessions.  \n* **Lab 2:** Comparing the cost and latency of different context compression strategies for a long document Q\\&A task.  \n* **Lab 3:** Implementing a simple version of the Generator-Reflector-Curator loop from the ACE framework to create a self-improving agent for a simple game.  \n* **Lab 4:** Developing a pair programming agent with custom .instructions.md and .chatmode.md files to enforce specific coding standards.\n\n### **Final Recommendation: A Call for Leadership**\n\nThe shift from prompt engineering to context engineering is not an incremental change; it is a fundamental re-platforming of how advanced AI applications are built. This transition is creating a new, high-skill professional role: the AI Systems Architect. Currently, the educational market lacks a comprehensive, rigorous program dedicated to training for this role. This presents a unique and timely opportunity for V2V Academy. By launching a world-class certification program based on the blueprint outlined in this report, the Academy can move ahead of the curve, define th"
  },
  {
    "id": "report_source",
    "chunk": "pportunity for V2V Academy. By launching a world-class certification program based on the blueprint outlined in this report, the Academy can move ahead of the curve, define the industry standard for this critical new discipline, and solidify its reputation as the premier institution for training the architects and engineers who will build the future of artificial intelligence.\n\n#### **Works cited**\n\n1. Understanding Prompt Engineering and Context Engineering, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  \n2. Prompt engineering, Context Engineering, Protocol Whatever... It's all Linguistics Programming... : r/OpenAI \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt\\_engineering\\_context\\_engineering\\_protocol/](https://www.reddit.com/r/OpenAI/comments/1lru3jy/prompt_engineering_context_engineering_protocol/)  \n3. I find the word \"engineering\" used in this context extremely annoying ..., accessed October 15, 2025, [https://news.ycombinator.com/item?id=45556685](https://news.ycombinator.com/item?id=45556685)  \n4. Context Engineering Guide | Hacker News, accessed October 15, 2025, [https://news.ycombinator.com/item?id=44508068](https://news.ycombinator.com/item?id=44508068)  \n5. Context Engineering (1/2)Getting the best out of Agentic AI ..., accessed October 15, 2025, [https://abvijaykumar.medium.com/context-engineering-1-2-getting-the-best-out-of-agentic-ai-systems-90e4fe036faf](https://abvijaykumar.medium.com/context-engineering-1-2-getting-the-best-out-of-agentic-ai-systems-90e4fe036faf)  \n6. What is Context Engin"
  },
  {
    "id": "report_source",
    "chunk": "of-agentic-ai-systems-90e4fe036faf](https://abvijaykumar.medium.com/context-engineering-1-2-getting-the-best-out-of-agentic-ai-systems-90e4fe036faf)  \n6. What is Context Engineering, Anyway? \\- Zep, accessed October 15, 2025, [https://blog.getzep.com/what-is-context-engineering/](https://blog.getzep.com/what-is-context-engineering/)  \n7. davidkimai/Context-Engineering: \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\"  Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration \\- GitHub, accessed October 15, 2025, [https://github.com/davidkimai/Context-Engineering](https://github.com/davidkimai/Context-Engineering)  \n8. Context Engineering \\- LangChain Blog, accessed October 15, 2025, [https://blog.langchain.com/context-engineering-for-agents/](https://blog.langchain.com/context-engineering-for-agents/)  \n9. Context Engineering in AI: Principles, Methods, and Uses \\- Code B, accessed October 15, 2025, [https://code-b.dev/blog/context-engineering](https://code-b.dev/blog/context-engineering)  \n10. A Gentle Introduction to Context Engineering in LLMs \\- KDnuggets, accessed October 15, 2025, [https://www.kdnuggets.com/a-gentle-introduction-to-context-engineering-in-llms](https://www.kdnuggets.com/a-gentle-introduction-to-context-engineering-in-llms)  \n11. Context Engineering: Moving Beyond Prompting in AI \\- DigitalOcean, accessed October 15, 2025, [https://www.digitalocean.com/community/tutorials/context-engineering-moving-beyond-prompting-ai](https://www.digitalocean.com/community/tutorials/context-engineering-mo"
  },
  {
    "id": "report_source",
    "chunk": "2025, [https://www.digitalocean.com/community/tutorials/context-engineering-moving-beyond-prompting-ai](https://www.digitalocean.com/community/tutorials/context-engineering-moving-beyond-prompting-ai)  \n12. The Role of AI in Software Architecture: Trends and Innovations, accessed October 15, 2025, [https://www.imaginarycloud.com/blog/ai-in-software-architecture](https://www.imaginarycloud.com/blog/ai-in-software-architecture)  \n13. Operation AI: Your New Guide for AI Solutions \\- Rubico, accessed October 15, 2025, [https://rubicotech.com/blog/operation-ai-reinvention-of-rubico/](https://rubicotech.com/blog/operation-ai-reinvention-of-rubico/)  \n14. We're in the context engineering stone age. You the engineer ..., accessed October 15, 2025, [https://news.ycombinator.com/item?id=45097424](https://news.ycombinator.com/item?id=45097424)  \n15. langchain-ai/context\\_engineering \\- GitHub, accessed October 15, 2025, [https://github.com/langchain-ai/context\\_engineering](https://github.com/langchain-ai/context_engineering)  \n16. Context Engineering for Video Understanding \\- Twelve Labs, accessed October 15, 2025, [https://www.twelvelabs.io/blog/context-engineering-for-video-understanding](https://www.twelvelabs.io/blog/context-engineering-for-video-understanding)  \n17. Everybody is talking about how context engineering is replacing prompt engineering nowadays. But what really is this new buzzword? : r/AI\\_Agents \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/AI\\_Agents/comments/1mq935t/everybody\\_is\\_talking\\_about\\_how\\_context/](https://www.reddit.com/r/AI_Agents/comments/1mq935t/everybody_is_talking_about_how_context/)  \n18. What is Context Engineering? \\- Elasticsearch Labs, accessed October 15, 2025, [http"
  },
  {
    "id": "report_source",
    "chunk": "www.reddit.com/r/AI_Agents/comments/1mq935t/everybody_is_talking_about_how_context/)  \n18. What is Context Engineering? \\- Elasticsearch Labs, accessed October 15, 2025, [https://www.elastic.co/search-labs/blog/context-engineering-overview](https://www.elastic.co/search-labs/blog/context-engineering-overview)  \n19. Retrieval Augmented Generation (RAG) and Semantic Search for GPTs, accessed October 15, 2025, [https://help.openai.com/en/articles/8868588-retrieval-augmented-generation-rag-and-semantic-search-for-gpts](https://help.openai.com/en/articles/8868588-retrieval-augmented-generation-rag-and-semantic-search-for-gpts)  \n20. AI Prompting (3/10): Context Windows ExplainedTechniques ..., accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1iftklk/ai\\_prompting\\_310\\_context\\_windows/](https://www.reddit.com/r/PromptEngineering/comments/1iftklk/ai_prompting_310_context_windows/)  \n21. What Is an AI Context Window? \\- Coursera, accessed October 15, 2025, [https://www.coursera.org/articles/context-window](https://www.coursera.org/articles/context-window)  \n22. 6 Techniques You Should Know to Manage Context Lengths in LLM Apps \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LLMDevs/comments/1mviv2a/6\\_techniques\\_you\\_should\\_know\\_to\\_manage\\_context/](https://www.reddit.com/r/LLMDevs/comments/1mviv2a/6_techniques_you_should_know_to_manage_context/)  \n23. Tool-space interference in the MCP era: Designing for agent compatibility at scale, accessed October 15, 2025, [https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/](https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-de"
  },
  {
    "id": "report_source",
    "chunk": "log/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/](https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/)  \n24. Effective context engineering for AI agents | Hacker News, accessed October 15, 2025, [https://news.ycombinator.com/item?id=45418251](https://news.ycombinator.com/item?id=45418251)  \n25. How to build reliable AI workflows with agentic primitives and ..., accessed October 15, 2025, [https://github.blog/ai-and-ml/github-copilot/how-to-build-reliable-ai-workflows-with-agentic-primitives-and-context-engineering/?utm\\_source=blog-release-oct-2025\\&utm\\_campaign=agentic-copilot-cli-launch-2025](https://github.blog/ai-and-ml/github-copilot/how-to-build-reliable-ai-workflows-with-agentic-primitives-and-context-engineering/?utm_source=blog-release-oct-2025&utm_campaign=agentic-copilot-cli-launch-2025)  \n26. Advanced Retrieval Augmented Generation (RAG) Techniques | by Sepehr (Sep) Keykhaie, accessed October 15, 2025, [https://blog.gopenai.com/advanced-retrieval-augmented-generation-rag-techniques-5abad385ac66](https://blog.gopenai.com/advanced-retrieval-augmented-generation-rag-techniques-5abad385ac66)  \n27. OpenAI and it's Retrieval-Augmented Generation (RAG) Systems \\- slidefactory, accessed October 15, 2025, [https://www.theslidefactory.com/post/exploring-retrieval-augmented-generation-rag-systems-with-openai](https://www.theslidefactory.com/post/exploring-retrieval-augmented-generation-rag-systems-with-openai)  \n28. Advanced RAG Techniques | DataCamp, accessed October 15, 2025, [https://www.datacamp.com/blog/rag-advanced](https://www.datacamp.com/blog/rag-advanced)  \n29. arxiv.org, accessed October 15, 202"
  },
  {
    "id": "report_source",
    "chunk": "iques | DataCamp, accessed October 15, 2025, [https://www.datacamp.com/blog/rag-advanced](https://www.datacamp.com/blog/rag-advanced)  \n29. arxiv.org, accessed October 15, 2025, [https://arxiv.org/html/2510.04618v1](https://arxiv.org/html/2510.04618v1)  \n30. Is Fine-Tuning Dead? Discover Agentic Context Engineering for Model Evolution Without Fine-Tuning \\- 36, accessed October 15, 2025, [https://eu.36kr.com/en/p/3504237709859976](https://eu.36kr.com/en/p/3504237709859976)  \n31. A Review of Cognitive Apprenticeship Methods in Computing Education Research, accessed October 15, 2025, [https://par.nsf.gov/servlets/purl/10491208](https://par.nsf.gov/servlets/purl/10491208)  \n32. (PDF) Cowboys and Aliens in the Digital Frontier: The Emergence of ..., accessed October 15, 2025, [https://www.researchgate.net/publication/380098993\\_Cowboys\\_and\\_Aliens\\_in\\_the\\_Digital\\_Frontier\\_The\\_Emergence\\_of\\_Techno-Social\\_Learning\\_in\\_AI-Enhanced\\_Communities\\_of\\_Practice](https://www.researchgate.net/publication/380098993_Cowboys_and_Aliens_in_the_Digital_Frontier_The_Emergence_of_Techno-Social_Learning_in_AI-Enhanced_Communities_of_Practice)  \n33. Directed Information -covering: An Information-Theoretic Framework for Context Engineering \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2510.00079v1](https://arxiv.org/html/2510.00079v1)  \n34. Directed Information $\\\\gamma $-covering: An Information-Theoretic ..., accessed October 15, 2025, [https://www.arxiv.org/abs/2510.00079](https://www.arxiv.org/abs/2510.00079)  \n35. MCP 101: An Introduction to Model Context Protocol | DigitalOcean, accessed October 15, 2025, [https://www.digitalocean.com/community/tutorials/model-context-protocol](https://www.digitalocean.com/comm"
  },
  {
    "id": "report_source",
    "chunk": "o Model Context Protocol | DigitalOcean, accessed October 15, 2025, [https://www.digitalocean.com/community/tutorials/model-context-protocol](https://www.digitalocean.com/community/tutorials/model-context-protocol)  \n36. What Is the Model Context Protocol (MCP) and How It Works, accessed October 15, 2025, [https://www.descope.com/learn/post/mcp](https://www.descope.com/learn/post/mcp)  \n37. Model Context Protocol, accessed October 15, 2025, [https://modelcontextprotocol.io/](https://modelcontextprotocol.io/)  \n38. MCP Explained: The New Standard Connecting AI to Everything | by Edwin Lisowski, accessed October 15, 2025, [https://medium.com/@elisowski/mcp-explained-the-new-standard-connecting-ai-to-everything-79c5a1c98288](https://medium.com/@elisowski/mcp-explained-the-new-standard-connecting-ai-to-everything-79c5a1c98288)  \n39. The Model Context Protocol (MCP)  A Complete Tutorial | by Dr. Nimrita Koul \\- Medium, accessed October 15, 2025, [https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef](https://medium.com/@nimritakoul01/the-model-context-protocol-mcp-a-complete-tutorial-a3abe8a7f4ef)  \n40. Model Context Protocol (MCP) Server: A Comprehensive Guide for ..., accessed October 15, 2025, [https://skywork.ai/skypage/en/Model%20Context%20Protocol%20(MCP)%20Server%3A%20A%20Comprehensive%20Guide%20for%20AI%20Engineers%20on%20drawio-mcp-server/1971041320309944320](https://skywork.ai/skypage/en/Model%20Context%20Protocol%20\\(MCP\\)%20Server%3A%20A%20Comprehensive%20Guide%20for%20AI%20Engineers%20on%20drawio-mcp-server/1971041320309944320)  \n41. 13+ Popular MCP servers for developers to unlock AI actions \\- DronaHQ, accessed October 15, 2025, [https://www.dronahq.com/popular-mcp-s"
  },
  {
    "id": "report_source",
    "chunk": "mcp-server/1971041320309944320)  \n41. 13+ Popular MCP servers for developers to unlock AI actions \\- DronaHQ, accessed October 15, 2025, [https://www.dronahq.com/popular-mcp-servers/](https://www.dronahq.com/popular-mcp-servers/)  \n42. Model Context Protocol Tutorial \\- AI Hero, accessed October 15, 2025, [https://www.aihero.dev/model-context-protocol-tutorial](https://www.aihero.dev/model-context-protocol-tutorial)  \n43. Model Context Protocol (MCP): A Guide With Demo Project \\- DataCamp, accessed October 15, 2025, [https://www.datacamp.com/tutorial/mcp-model-context-protocol](https://www.datacamp.com/tutorial/mcp-model-context-protocol)  \n44. Welcome to the Model Context Protocol (MCP) Course \\- Hugging Face, accessed October 15, 2025, [https://huggingface.co/learn/mcp-course/unit0/introduction](https://huggingface.co/learn/mcp-course/unit0/introduction)  \n45. yzfly/awesome-context-engineering: A curated collection of resources, papers, tools, and best practices for Context Engineering in AI agents and Large Language Models (LLMs). \\- GitHub, accessed October 15, 2025, [https://github.com/yzfly/awesome-context-engineering](https://github.com/yzfly/awesome-context-engineering)  \n46. context-engineering  GitHub Topics, accessed October 15, 2025, [https://github.com/topics/context-engineering?l=shell](https://github.com/topics/context-engineering?l=shell)  \n47. context-engineering  GitHub Topics, accessed October 15, 2025, [https://github.com/topics/context-engineering?l=typescript\\&o=desc\\&s=updated](https://github.com/topics/context-engineering?l=typescript&o=desc&s=updated)  \n48. context-engineering  GitHub Topics, accessed October 15, 2025, [https://github.com/topics/context-engineering](https://github.com/topics/c"
  },
  {
    "id": "report_source",
    "chunk": "=typescript&o=desc&s=updated)  \n48. context-engineering  GitHub Topics, accessed October 15, 2025, [https://github.com/topics/context-engineering](https://github.com/topics/context-engineering)  \n49. From Code to Collaboration: The Future of AI-Powered Pair Programming in Enterprise Environments \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/390280664\\_From\\_Code\\_to\\_Collaboration\\_The\\_Future\\_of\\_AI-Powered\\_Pair\\_Programming\\_in\\_Enterprise\\_Environments](https://www.researchgate.net/publication/390280664_From_Code_to_Collaboration_The_Future_of_AI-Powered_Pair_Programming_in_Enterprise_Environments)  \n50. AI Pair Programming: How to Improve Coding Efficiency with AI ..., accessed October 15, 2025, [https://www.corexta.com/ai-pair-programming/](https://www.corexta.com/ai-pair-programming/)  \n51. Best practices for pair programming with AI assistants \\- Graphite, accessed October 15, 2025, [https://graphite.dev/guides/ai-pair-programming-best-practices](https://graphite.dev/guides/ai-pair-programming-best-practices)  \n52. AI Agent Best Practices: 12 Lessons from AI Pair Programming for ..., accessed October 15, 2025, [https://forgecode.dev/blog/ai-agent-best-practices/](https://forgecode.dev/blog/ai-agent-best-practices/)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/07-V2V Pathway Research Proposal.md\">\n\n\n# **Context as the Curriculum: A Foundational Report for the Vibecoding to Virtuosity Pathway**\n\n## **Executive Summary**\n\nThe field of artificial intelligence in software development is undergoing a critical and rapid evolution. The initial excitement surrounding the tactical craft of \"Prompt Engineering\"the art of phrasing inputs to elicit specific outputs fr"
  },
  {
    "id": "report_source",
    "chunk": "dergoing a critical and rapid evolution. The initial excitement surrounding the tactical craft of \"Prompt Engineering\"the art of phrasing inputs to elicit specific outputs from Large Language Models (LLMs)is giving way to the recognition of a more profound and demanding discipline: \"Context Engineering.\" This emerging field is not concerned with the linguistic finesse of a single request but with the systematic design and architecture of the entire information environment in which an AI model operates. It encompasses the dynamic assembly of instructions, memory, retrieved data, and tool definitions to create reliable, scalable, and stateful AI systems.  \nThis report provides a comprehensive analysis of this paradigm shift, grounding the concept of Context Engineering in a broad survey of academic literature, technical articles, and industry discourse. The analysis confirms that the distinction between prompt and context engineering is not merely semantic; it represents a fundamental maturation of the industry, moving from crafting clever demonstrations to engineering production-grade, AI-native applications. A detailed blueprint of Context Engineering is presented, organized into three core phases: Context Retrieval and Generation, Context Processing and Optimization, and Context Management for Agentic Systems. This framework provides a technical foundation for a new generation of AI development curricula.  \nA competitive analysis of the current pedagogical landscape reveals a significant market gap. Existing courses on platforms such as Coursera and DeepLearning.AI, while valuable, overwhelmingly focus on teaching developers how to *use* AI tools as assistants within the traditional Software Development Lifecycle (SDL"
  },
  {
    "id": "report_source",
    "chunk": "and DeepLearning.AI, while valuable, overwhelmingly focus on teaching developers how to *use* AI tools as assistants within the traditional Software Development Lifecycle (SDLC). They operate within the older paradigm of prompt engineering, treating AI as an add-on rather than a foundational component of a new architectural approach. This leaves a strategic opening for a curriculum that teaches the more advanced, systems-level discipline of architecting AI-native applications from the ground up.  \nFurthermore, this report explores the application of the Cognitive Apprenticeship model as a pedagogical framework for this new discipline. By mapping the model's core methodsModeling, Coaching, Scaffolding, Articulation, Reflection, and Explorationto the capabilities of modern AI assistants, a powerful new teaching paradigm emerges. However, this approach is not without its perils. The report identifies the critical risk of \"pseudo-apprenticeship,\" where learners become passive consumers of AI-generated solutions, bypassing the productive struggle necessary for deep learning. Mitigating this risk requires a curriculum designed to foster metacognitive skills and use AI as a Socratic partner rather than an answer engine.  \nBased on these findings, this report puts forth a set of strategic recommendations for the \"Vibecoding to Virtuosity\" (V2V) pathway. The central recommendation is to position V2V not as another course on using AI tools, but as a premier program for mastering **AI-Native Systems Architecture**. The proposed curriculum is structured around the core principles of Context Engineering and Cognitive Apprenticeship, designed to guide learners from the foundational \"vibecoding\" of AI interaction to the \"virtuosity\" "
  },
  {
    "id": "report_source",
    "chunk": "d the core principles of Context Engineering and Cognitive Apprenticeship, designed to guide learners from the foundational \"vibecoding\" of AI interaction to the \"virtuosity\" of architecting robust, autonomous agents. By embracing this forward-looking position, the V2V pathway has a significant opportunity to define the next generation of AI development education and produce graduates with a durable, high-value, and market-differentiating skillset.\n\n## **The Paradigm Shift: From Prompt Crafting to Context Architecture**\n\nThe lexicon of AI development is evolving, reflecting a deeper understanding of what it takes to build meaningful applications with Large Language Models (LLMs). The initial term that captured the public imagination, \"Prompt Engineering,\" is proving insufficient to describe the complex, systemic work required for production-grade AI systems. A new term, \"Context Engineering,\" is emerging from both academic and industry circles to more accurately represent this discipline. This section will deconstruct the limitations of the former and build a comprehensive, evidence-based case for the strategic adoption of the latter, thereby validating the foundational premise of the Vibecoding to Virtuosity (V2V) pathway.\n\n### **Deconstructing \"Prompt Engineering\": The Art of the One-Shot Request**\n\nPrompt Engineering is best understood as the practice of designing and structuring text-based instructions to guide an AI model toward a specific, desired output for a single interaction.1 Its focus is squarely on the immediate input-output pair, treating the LLM as a function to be called with carefully crafted arguments. The \"engineering\" in this context is primarily linguistic and tactical, involving the meticulous selec"
  },
  {
    "id": "report_source",
    "chunk": "ing the LLM as a function to be called with carefully crafted arguments. The \"engineering\" in this context is primarily linguistic and tactical, involving the meticulous selection of words, phrases, and structures to influence the probabilistic path the model takes in generating its response.1  \nThe core techniques of prompt engineering are well-established and represent a form of linguistic tuning. These methods include:\n\n* **Role Assignment:** Providing the model with a persona to adopt, such as \"You are a professional translator\" or \"You are an expert research planner,\" to constrain its tone and knowledge domain.1  \n* **Few-Shot Examples:** Including several input-output pairs within the prompt to demonstrate the desired format or reasoning pattern, guiding the model by example rather than by explicit instruction alone.1  \n* **Chain-of-Thought (CoT) Reasoning:** Instructing the model to \"think step-by-step\" or providing examples of such reasoning to encourage a more deliberative and transparent thought process, which often leads to more accurate results in complex tasks.1  \n* **Output Constraints:** Specifying formatting requirements, such as requesting responses in JSON, bullet points, or a particular sentence structure, to make the output more predictable and machine-readable.1\n\nWhile powerful for experimentation, demonstrations, and simple, one-off tasks, this prompt-centric approach suffers from a fundamental flaw: it is inherently brittle.1 The performance of a meticulously crafted prompt can be highly sensitive to minor variations in wording, the order of instructions, or even subtle shifts in the underlying model's behavior between versions.1 This fragility makes it an unstable foundation for building reliable,"
  },
  {
    "id": "report_source",
    "chunk": "g, the order of instructions, or even subtle shifts in the underlying model's behavior between versions.1 This fragility makes it an unstable foundation for building reliable, scalable, and maintainable software systems. As applications grow in complexity, managing an ever-expanding set of prompt variations for different edge cases becomes untenable.6 This sentiment is echoed in community forums, where some practitioners now argue that for building serious applications, \"Prompt Engineering is long dead,\" relegated to casual conversations and brainstorming sessions rather than the systematic construction of AI products.7\n\n### **The Emergence of \"Context Engineering\": A Systems-Level Discipline**\n\nIn response to the limitations of prompt-centric thinking, the field is coalescing around a more comprehensive and robust discipline: Context Engineering. This paradigm shift re-frames the challenge from \"How do I phrase my question?\" to \"How do I design the entire information environment the AI needs to succeed?\".8 Context Engineering is defined as the \"delicate art and science\" of strategically managing the full information payload that fills an LLM's context window at the moment of inference.9 It is a systems-level discipline focused on the dynamic and programmatic assembly of all relevant informationincluding but not limited to the user's immediate promptto guide the model's behavior reliably over time.1  \nThis evolution is not merely an industry trend; it is being formalized in academic research. A recent, comprehensive survey introduces Context Engineering as a formal discipline that \"transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs\".5 This work, analyzing over 1,40"
  },
  {
    "id": "report_source",
    "chunk": "eering as a formal discipline that \"transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs\".5 This work, analyzing over 1,400 research papers, provides a taxonomy that decomposes the field into its foundational components, establishing a technical roadmap for building context-aware AI.5 Crucially, this academic framing positions prompt engineering as a *subset* of the broader field of context engineering, a component responsible for generating one type of information that feeds into the larger system.5  \nThis academic formalization is mirrored by a growing consensus among industry leaders. Figures such as OpenAI's Andrej Karpathy and Shopify's Tobi Ltke have championed the shift in terminology, arguing that \"Context Engineering\" more accurately describes the core skill required to build serious LLM applications.8 Their perspective is that the term \"prompt\" implies a short, singular instruction, whereas real-world applications involve constructing a rich information state from multiple sources, including memory, knowledge bases, tool definitions, and conversation history. The true craft lies in deciding what to load into the model's \"RAM\"its context windowat each step of a complex task.16 This alignment between cutting-edge research and top-tier industry practice provides a powerful validation for the V2V curriculum's focus on this concept.\n\n### **A Comparative Framework: Why the Distinction Matters**\n\nThe distinction between prompt engineering and context engineering is foundational for developing a meaningful curriculum, as it reflects a move from tactical craft to strategic architecture. Prompt engineering is a necessary skill, but it is insufficient for building th"
  },
  {
    "id": "report_source",
    "chunk": "g a meaningful curriculum, as it reflects a move from tactical craft to strategic architecture. Prompt engineering is a necessary skill, but it is insufficient for building the next generation of AI applications. The true value and complexity lie in the engineering of the context that surrounds the prompt.  \nFraming this difference clearly is essential. Prompt engineering can be seen as a *tactic*: the skill of what to say to the model at a specific moment in time. In contrast, context engineering is a *strategy*: the skill of designing the entire flow and architecture of a model's thought process, including what it knows, what it remembers, and what it can do.3 This strategic mindset is what separates a developer who can use an AI from an architect who can build with AI.  \nThis strategic difference is reflected in the scope of work and the tools required. Prompt engineering can be practiced with nothing more than a text editor or a chatbot interface. It operates within a single input-output pair.3 Context engineering, however, operates at the system level. It requires a backend infrastructure of memory modules, Retrieval-Augmented Generation (RAG) systems, vector databases, API orchestration frameworks, and logic for dynamically assembling these components into a coherent whole before every model call.3 The effort shifts from creative writing to systems design.  \nThe following table provides a clear, comparative analysis of these two disciplines, synthesizing the key differences across multiple dimensions. This framework serves not only as an analytical tool for this report but also as a potential cornerstone for the V2V curriculum itself, establishing the core philosophy of the pathway from the outset.  \n**Table 1: Pro"
  },
  {
    "id": "report_source",
    "chunk": "tical tool for this report but also as a potential cornerstone for the V2V curriculum itself, establishing the core philosophy of the pathway from the outset.  \n**Table 1: Prompt Engineering vs. Context Engineering: A Comparative Analysis**\n\n| Dimension | Prompt Engineering | Context Engineering |\n| :---- | :---- | :---- |\n| **Mindset** | Creative writing or copy-tweaking; crafting clear, static instructions to elicit a specific response.2 | Systems design or software architecture for LLMs; designing the entire information flow of the model's thought process.3 |\n| **Scope** | Operates within a single input-output pair; focuses on the immediate instruction or question.3 | Handles the entire information ecosystem the model sees: memory, history, tools, retrieved documents, and system prompts.3 |\n| **Primary Goal** | Elicit a specific, high-quality response for a one-off task or demonstration.3 | Ensure consistent, reliable, and scalable performance across multiple users, sessions, and complex, multi-step tasks.3 |\n| **Tools Involved** | Text editors, chatbot interfaces (e.g., ChatGPT), or a simple prompt box.3 | RAG systems, vector databases, memory modules, API chaining frameworks (e.g., LangChain), and backend orchestration logic.3 |\n| **Scalability** | Brittle and difficult to scale; tends to fail as complexity and the number of edge cases increase.1 | Built with scale in mind from the beginning; designed for consistency, reuse, and programmatic management.3 |\n| **Debugging Process** | Primarily involves rewording the prompt, tweaking phrasing, and guessing what went wrong in the model's interpretation.3 | Involves inspecting the full context window, memory state, token flow, and retrieval logs to diagnose systemic fail"
  },
  {
    "id": "report_source",
    "chunk": " guessing what went wrong in the model's interpretation.3 | Involves inspecting the full context window, memory state, token flow, and retrieval logs to diagnose systemic failures.3 |\n| **Risk of Failure** | When it fails, the output is typically off-topic, poorly toned, or factually incorrect for a single turn.3 | When it fails, the entire system can behave unpredictably, forget its goals, misuse tools, or propagate errors across a long-running task.3 |\n| **Effort Type** | Focused on wordsmithing and crafting the perfect phrasing to guide the model's generation.3 | Focused on information logistics: delivering the right data at the right time, thereby reducing the cognitive burden on the prompt itself.3 |\n\nThe evolution from prompt engineering to context engineering is a leading indicator of the AI industry's maturation. The initial phase of any transformative technology is often characterized by experimentation and \"magic tricks\" that produce impressive but unreliable results. The subsequent phase is about taming that technology with engineering discipline to build predictable, valuable systems. The shift in terminology reflects this journeyfrom the \"AI whisperer\" crafting magic spells to the \"AI systems architect\" designing robust information pipelines. By explicitly teaching \"Context Engineering,\" the V2V curriculum positions itself at the forefront of this mature, professional phase of AI development, offering a powerful differentiator in a market saturated with introductory prompt-crafting courses.\n\n## **A Blueprint for Context Engineering: Components, Processes, and Practices**\n\nTransitioning from the conceptual distinction between prompt and context engineering to its practical implementation requires a structure"
  },
  {
    "id": "report_source",
    "chunk": "mponents, Processes, and Practices**\n\nTransitioning from the conceptual distinction between prompt and context engineering to its practical implementation requires a structured, architectural blueprint. The academic formalization of Context Engineering provides such a framework, decomposing the discipline into a systematic pipeline of distinct but interconnected phases: Context Retrieval and Generation, Context Processing and Optimization, and Context Management for Agentic Systems.5 This section details the components, processes, and best practices within each phase, providing the technical core that should form the backbone of the V2V curriculum.\n\n### **Phase 1: Context Retrieval and Generation**\n\nThis initial phase is concerned with acquiring the raw informational assets that will be used to construct the final context window. It is the foundation of the entire process, as the quality and relevance of the information gathered here directly determine the potential of the system. This phase involves two primary activities: generating context from the model's own capabilities and retrieving it from external, authoritative sources.5  \n**Prompt-Based Generation:** This is the domain of traditional prompt engineering, now understood as one of several methods for generating context. It leverages the LLM's vast internal knowledge to produce useful information. Foundational techniques include:\n\n* **Zero-Shot and Few-Shot Learning:** Using direct instructions or a small number of examples to prompt the model to generate baseline information, code snippets, or plans.1  \n* **Chain-of-Thought (CoT) and other Reasoning Techniques:** Prompting the model to generate a step-by-step reasoning process before providing a final answer. Th"
  },
  {
    "id": "report_source",
    "chunk": " or plans.1  \n* **Chain-of-Thought (CoT) and other Reasoning Techniques:** Prompting the model to generate a step-by-step reasoning process before providing a final answer. This generated \"thought process\" becomes part of the context for subsequent steps, improving coherence and accuracy.5\n\n**External Knowledge Retrieval:** This is the critical process of grounding the LLM in external reality, mitigating hallucinations and providing it with up-to-date or proprietary information.\n\n* **Retrieval-Augmented Generation (RAG):** RAG is the fundamental pattern for this process. At its core, it involves taking a user query, using it to search an external knowledge base (typically a vector database), retrieving the most relevant chunks of information, and prepending them to the prompt before sending it to the LLM.5 This ensures the model's response is based on specific, verifiable data.  \n* **RAG as a Component, Not the Whole:** It is crucial to understand that while RAG is a cornerstone of context engineering, it is not the entirety of it. A simple RAG pipeline augments a user's query with retrieved documents. A fully context-engineered system goes further, programmatically incorporating not just retrieved text, but also system instructions, conversation history, long-term memory, and the outputs of tools into the LLM's context.22 The V2V curriculum must emphasize this distinction, teaching RAG as a foundational retrieval pattern within a much broader architectural framework.  \n* **Advanced Retrieval Strategies:** The field is moving beyond simple vector search. Advanced techniques include leveraging knowledge graphs to retrieve structured entities and their relationships, which allows for more complex, multi-hop reasoning. Furt"
  },
  {
    "id": "report_source",
    "chunk": "arch. Advanced techniques include leveraging knowledge graphs to retrieve structured entities and their relationships, which allows for more complex, multi-hop reasoning. Furthermore, modular and agentic retrieval systems are emerging, where an LLM agent might decide which of several different knowledge bases to query based on the user's request.5\n\n**Dynamic Context Assembly:** The culmination of this phase is the programmatic assembly of the context. In a well-engineered system, the final prompt the LLM sees is not a static template but is constructed on-the-fly for each request. This process involves writing code that orchestrates the various components, weaving together a system instruction, the current user query, data fetched from a RAG pipeline, the output from a previous tool call, and a summary of the conversation history into a single, coherent payload for the model.1 This assembly logic is the heart of a context-engineered application.\n\n### **Phase 2: Context Processing and Optimization**\n\nOnce the raw contextual assets are gathered, they must be processed and optimized to fit within the primary constraint of any LLM system: the finite context window. This phase is governed by the principle of information logisticsthe science of managing a scarce resource to maximize its utility. The context window is not just a technical limit; it is a cognitive focusing mechanism for the AI. Overloading it with irrelevant or redundant information leads to performance degradation, a phenomenon known as \"context rot\" or the \"lost-in-the-middle\" problem, where the model struggles to recall information buried deep within a large context.23 Even with modern models boasting massive context windows of up to 2 million tokens, effect"
  },
  {
    "id": "report_source",
    "chunk": "here the model struggles to recall information buried deep within a large context.23 Even with modern models boasting massive context windows of up to 2 million tokens, effective curation remains critical for performance, latency, and cost.24  \nThe key techniques for managing this scarce resource include:\n\n* **Intelligent Selection and Pruning:** Not all context is created equal. This involves implementing algorithms that score the relevance of different pieces of information based on the current task.26 Irrelevant, outdated, or low-signal information should be actively pruned to maintain a high signal-to-noise ratio in the context window.26  \n* **Summarization and Compression:** To fit more relevant information into the limited space, various compression strategies are employed. This can range from simple conversation trimming (keeping only the last N turns) to more sophisticated methods like using a secondary LLM call to generate a concise summary of a long document or conversation history.1 Advanced techniques like hierarchical summarization, which creates layered summaries of varying detail, can also be used to provide the model with both high-level overviews and the option to \"zoom in\" on details if needed.20  \n* **Long-Context Architectural Considerations:** While hardware and model architecture innovations like Position Interpolation are expanding the technical size of context windows, they do not eliminate the need for engineering discipline.5 Larger windows increase processing time and computational cost.25 Therefore, the principles of selection and compression remain paramount. The goal is not to use the entire window but to use the smallest, most potent portion of it required for the task. The curriculum shoul"
  },
  {
    "id": "report_source",
    "chunk": "ction and compression remain paramount. The goal is not to use the entire window but to use the smallest, most potent portion of it required for the task. The curriculum should frame context window management not as a frustrating limitation but as a core design principle for building efficient and focused AI systems.\n\n### **Phase 3: Context Management for Agentic Systems**\n\nThis final phase extends context engineering into the temporal dimension, orchestrating the flow of information over multiple turns to create stateful, tool-using, autonomous agents. This is where the system moves from being a reactive question-answerer to a proactive problem-solver. It is the most complex and powerful application of context engineering.  \n**Memory Systems:** To act coherently over time, an agent needs memory. Context engineering provides the mechanisms for this memory.\n\n* **Short-Term vs. Long-Term Memory:** A critical distinction is made between short-term memory, which typically consists of the recent conversation history within the context window, and long-term memory, which involves persisting information outside the context window in a database or file system.1 This could include user profiles, project-specific knowledge, or summaries of past conversations.  \n* **Practical Implementation:** Techniques like \"memory slotting\" can be used to maintain different channels of context (e.g., a \"scratchpad\" slot for intermediate thoughts, a \"user profile\" slot).1 For performance, strategies like context caching (to avoid re-processing stable prefixes of the context, like the system prompt) and designing the context to be append-only are crucial.23\n\n**Tool Integration and Reasoning:** Tools are what give an agent the ability to act upon t"
  },
  {
    "id": "report_source",
    "chunk": " like the system prompt) and designing the context to be append-only are crucial.23\n\n**Tool Integration and Reasoning:** Tools are what give an agent the ability to act upon the world. They are external functions, such as API calls, database queries, or file system operations, that the agent can decide to invoke.\n\n* **Defining Tools in Context:** The agent doesn't magically know about these tools. They must be described within the context, including the tool's name, a natural language description of what it does, and the parameters it accepts.1 The quality of these descriptions is paramount; the model uses them to decide which tool to call and with what arguments.  \n* **Designing for Efficiency:** Tool design is a key aspect of context engineering. Tool names should be descriptive and consistently prefixed (e.g., browser\\_navigate, browser\\_read\\_content) to help the model make better choices.23 The output of tools must also be managed; a tool that returns a massive, unstructured blob of text can easily overwhelm the context window. Therefore, tool outputs should be concise, structured, and token-efficient.24\n\n**Isolation and Control Flow:** For complex tasks, a single monolithic agent can become confused as its context window fills with conflicting information from different sub-tasks.\n\n* **Sub-Agent Architectures:** A powerful pattern is to use a main \"orchestrator\" agent that delegates specific tasks to specialized sub-agents. Each sub-agent operates with its own clean, isolated context window focused on its specific task (e.g., a \"researcher\" agent, a \"coder\" agent). It performs its work and then returns a concise summary or result to the main agent, keeping the orchestrator's context clean and focused.24  \n* **Ownin"
  },
  {
    "id": "report_source",
    "chunk": "ent, a \"coder\" agent). It performs its work and then returns a concise summary or result to the main agent, keeping the orchestrator's context clean and focused.24  \n* **Owning the Control Loop:** A robust agentic system is not just a series of LLM calls. The developer must \"own the control loop\"the code that sits between the user and the LLM. This code is responsible for executing the tool calls chosen by the LLM, handling errors, managing the agent's state, and deciding when to pause for human intervention or clarification. This separation of concernsthe LLM decides *what* to do, the system code determines *how* to do itis essential for building predictable, debuggable, and reliable agents.9\n\nBy structuring the curriculum around these three phases, the V2V pathway can provide a comprehensive and systematic education in the engineering principles required to build sophisticated, modern AI applications.\n\n## **The State of the Art in AI Development Pedagogy**\n\nTo position the Vibecoding to Virtuosity (V2V) curriculum for maximum impact, a thorough analysis of the existing educational landscape is essential. A survey of current offerings from major online platforms, technology companies, and professional training providers reveals a consistent set of pedagogical themes and, more importantly, a significant strategic gap. The market is saturated with courses that teach developers how to *use* AI as an assistive tool, but it largely fails to teach them how to *architect* the AI-native systems of the future.\n\n### **Survey of Existing Curricula**\n\nAn examination of courses and specializations across prominent platforms provides a clear picture of the current state of AI development education.  \n**Platform and Course Analysis"
  },
  {
    "id": "report_source",
    "chunk": "amination of courses and specializations across prominent platforms provides a clear picture of the current state of AI development education.  \n**Platform and Course Analysis:**\n\n* **DeepLearning.AI & Coursera:** The \"Generative AI for Software Development\" specialization is a prime example of the current paradigm.30 Its syllabus is structured around applying LLMs to discrete phases of the traditional Software Development Lifecycle (SDLC). Modules cover \"Pair-coding with an LLM,\" \"Team Software Engineering with AI\" (including testing, debugging, and documentation), and \"AI-Powered Software and System Design\" (covering databases and design patterns).30 The learning objectives consistently use verbs like \"prompt an LLM to assist,\" \"work with an LLM to iteratively modify,\" and \"use an LLM to explore\".30  \n* **Microsoft:** Microsoft offers a suite of \"AI for Beginners\" curricula, including a general AI course, a Generative AI course, and a new \"AI Agents for Beginners\" course.33 These are excellent resources for practical application, focusing on using tools like TensorFlow, PyTorch, and Azure AI services. The \"Mastering GitHub Copilot\" pathway similarly focuses on best practices for using the tool effectively, covering prompt crafting, responsible use, and integrating it into various environments.37  \n* **Other Providers:** Training materials from providers like Great Learning and Certstaffix for tools like GitHub Copilot follow a similar pattern, focusing on installation, basic usage in Python, and leveraging the tool for productivity gains.40\n\nCommon Pedagogical Themes:  \nAcross these diverse offerings, a clear set of recurring topics emerges:\n\n1. **Foundations of LLMs:** Most curricula begin with an introduction to how "
  },
  {
    "id": "report_source",
    "chunk": "n Pedagogical Themes:  \nAcross these diverse offerings, a clear set of recurring topics emerges:\n\n1. **Foundations of LLMs:** Most curricula begin with an introduction to how LLMs and transformer architectures work at a high level.32  \n2. **AI as a Pair Programmer:** A central theme is teaching the interactive loop of writing code alongside an AI assistant, a practice explicitly taught in courses from DeepLearning.AI and Microsoft.31  \n3. **Task-Specific Application:** A significant portion of these courses is dedicated to applying AI to specific SDLC tasks, such as generating unit tests, debugging code, improving performance, writing documentation, and managing dependencies.30  \n4. **Prompt Engineering Fundamentals:** The core interaction skill taught is prompt engineering, focusing on techniques like iterative prompting, providing feedback to the LLM, and assigning roles to get better outputs.30\n\n### **Identifying the Curricular Gap**\n\nWhile the existing courses provide a valuable introduction to the productivity benefits of AI, their collective focus reveals a profound curricular gap. This gap represents the primary strategic opportunity for the V2V pathway.  \n**The Focus on \"Using\" vs. \"Architecting\":** The overwhelming pedagogical approach in the current market is to treat the developer as a *user* of an AI tool. The curriculum is designed to make them a more effective consumer of AI assistance within their existing workflow. There is a conspicuous absence of content that treats the developer as an *architect* of an AI system. The fundamental questions of Context EngineeringHow do you design a memory system? What is the optimal strategy for dynamic context assembly? How do you orchestrate a multi-agent workflow? Ho"
  },
  {
    "id": "report_source",
    "chunk": "uestions of Context EngineeringHow do you design a memory system? What is the optimal strategy for dynamic context assembly? How do you orchestrate a multi-agent workflow? How do you manage a token budget across a long-running task?are largely unaddressed.  \n**The \"Vibecoding\" Trap:** The current educational landscape excels at teaching what could be termed the \"Vibecoding\" stage of AI development. It helps developers get a feel for the conversational, iterative nature of working with an LLM. It builds intuition for what makes a good prompt and how to coax a useful response from the model. However, it does not provide a structured, engineering-driven pathway to \"Virtuosity.\" Virtuosity in this new paradigm is not just about being a skilled AI user; it is about having the discipline and architectural knowledge to build predictable, reliable, and scalable systems that have AI at their core. The current market teaches the craft of the conversation, not the science of the system.  \nThis analysis suggests the current educational market is a \"Red Ocean,\" saturated with similar offerings focused on \"Prompt Engineering for X.\" They are all competing to teach the same set of valuable but ultimately tactical skills. The opportunity for V2V is to create a \"Blue Ocean\" by targeting a different, more advanced need: the need for systems architecture in an AI-native world.\n\n### **Opportunity for V2V Differentiation**\n\nThe V2V curriculum is uniquely positioned to fill this gap by fundamentally shifting the pedagogical focus from using AI to building with it.  \n**Beyond the Chatbot in the IDE:** The V2V pathway's core differentiator should be its promise to teach developers what happens *behind* the chat interface. It should be positio"
  },
  {
    "id": "report_source",
    "chunk": "Beyond the Chatbot in the IDE:** The V2V pathway's core differentiator should be its promise to teach developers what happens *behind* the chat interface. It should be positioned as the curriculum that explains how to build the backend systems, the information pipelines, and the agentic control loops that power truly intelligent applications. While other courses teach you how to talk to GitHub Copilot, V2V will teach you how to build a system *like* GitHub Copilot.  \n**The \"AI-Native SDLC\":** Existing curricula tend to map AI assistance onto the traditional SDLC. This is a logical but limited approach that treats AI as an enhancement to the old way of developing software. V2V has the opportunity to teach a new, \"AI-Native SDLC.\" Instead of structuring modules around \"Testing\" and \"Documentation,\" the curriculum could be structured around the phases of building an agentic system: \"Context Architecture Design,\" \"Memory and Retrieval Systems,\" \"Tool Definition and Integration,\" and \"Agent Orchestration and Control.\" This forward-looking approach prepares developers for the future of software, not just for optimizing the present.  \nThe following table provides a high-level overview of the competitive landscape, highlighting the common focus and the resulting strategic gap that V2V can exploit.  \n**Table 2: Competitive Landscape of AI-Assisted Software Development Curricula**\n\n| Dimension | DeepLearning.AI \"GenAI for SW Dev\" | Microsoft \"AI for Beginners\" / Copilot | V2V Pathway (Proposed) |\n| :---- | :---- | :---- | :---- |\n| **Target Audience** | Software developers looking to enhance productivity with AI tools.31 | Beginners and developers seeking practical skills with Microsoft's AI stack and tools.35 | Ambitious develope"
  },
  {
    "id": "report_source",
    "chunk": "are developers looking to enhance productivity with AI tools.31 | Beginners and developers seeking practical skills with Microsoft's AI stack and tools.35 | Ambitious developers and engineers aiming to become architects of AI-native systems. |\n| **Core Topics** | Pair-coding, AI for testing/debugging/docs, prompt engineering, AI-assisted design patterns.30 | Fundamentals of AI/ML, practical use of tools like PyTorch, Azure AI, and GitHub Copilot.34 | **Context Engineering Architecture**, Memory Systems, RAG at scale, Multi-Agent Orchestration, AI-Native SDLC. |\n| **Key Projects** | Implementing algorithms with LLM help, refactoring code, building database schemas with AI assistance.30 | Building simple AI models (e.g., image classifiers), using Copilot to complete coding exercises.38 | **Designing a context pipeline**, building a stateful, tool-using agent, debugging context-related system failures. |\n| **Pedagogical Focus** | **Using AI as a tool** to assist in the traditional SDLC. The developer is the user.32 | **Applying AI tools** to solve specific problems. The developer is the implementer. | **Architecting AI systems**. The developer is the systems designer and engineer. |\n\nBy consciously adopting the positioning outlined in the final column, the V2V curriculum can establish itself as the clear next step for developers who have completed the introductory courses offered by competitors and are ready to move from simply using AI to truly mastering it.\n\n## **Reimagining Cognitive Apprenticeship in the AI Co-Pilot Era**\n\nThe \"Vibecoding to Virtuosity\" pathway is explicitly based on the Cognitive Apprenticeship model, a robust pedagogical framework with a long history of success in teaching complex cognitive skills. In"
  },
  {
    "id": "report_source",
    "chunk": "sity\" pathway is explicitly based on the Cognitive Apprenticeship model, a robust pedagogical framework with a long history of success in teaching complex cognitive skills. In the age of AI, this model does not become obsolete; rather, it becomes more relevant than ever. AI coding assistants can be powerful new mediums for implementing the core methods of cognitive apprenticeship. However, their misuse can also lead to superficial learning. This section explores how to structure the V2V learning experience to leverage AI as a cognitive mentor while actively mitigating the pedagogical risks it introduces.\n\n### **The Cognitive Apprenticeship Model: A Refresher**\n\nCognitive Apprenticeship is an instructional model designed to help students acquire cognitive and metacognitive skills by making the tacit thinking processes of experts visible and accessible.46 Unlike traditional apprenticeships that focus on physical tasks, cognitive apprenticeship focuses on the internal processes of problem-solving, reasoning, and strategic thinking.48 The model was developed by Collins, Brown, and Newman and is built upon six core teaching methods that guide a learner from observation to independent practice.47  \nThe six methods are:\n\n1. **Modeling:** The expert performs a task while externalizing their thought process, making their internal monologue and decision-making criteria explicit to the learner.  \n2. **Coaching:** The expert observes the learner attempting the task and offers real-time hints, feedback, and guidance.  \n3. **Scaffolding:** The expert provides structural support to the learner, which can take the form of suggestions, boilerplate code, or breaking down a complex problem into simpler parts. This support is gradually remo"
  },
  {
    "id": "report_source",
    "chunk": "uctural support to the learner, which can take the form of suggestions, boilerplate code, or breaking down a complex problem into simpler parts. This support is gradually removed as the learner's competence grows (a process known as fading).  \n4. **Articulation:** The learner is prompted to articulate their own knowledge, reasoning, and problem-solving processes, making their own thinking visible to the expert and to themselves.  \n5. **Reflection:** The learner is encouraged to compare their own problem-solving processes with those of the expert or other learners, fostering a deeper understanding of their performance.  \n6. Exploration: The learner is pushed to solve new, related problems on their own, applying their acquired skills in novel contexts and moving toward true expertise.\n\n   46\n\n### **AI as a Cognitive Mentor: Mapping Methods to Tools**\n\nModern AI coding assistants are uniquely suited to facilitate several of these methods, acting as a scalable, always-available cognitive mentor.\n\n* **Modeling:** An AI assistant excels at making expert processes visible. A student can prompt the AI to not only generate a solution but to \"explain your reasoning step-by-step.\" This use of Chain-of-Thought prompting is a direct implementation of modeling, where the AI's \"thought process\" is externalized in text.48 The V2V curriculum can design exercises where students are required to analyze these AI-generated models of expert performance, deconstructing how a complex problem was broken down and solved.  \n* **Coaching and Scaffolding:** AI tools provide powerful mechanisms for coaching and scaffolding. When a student is stuck, the AI can offer a contextual hint rather than a full solution. It can identify and explain errors in r"
  },
  {
    "id": "report_source",
    "chunk": "erful mechanisms for coaching and scaffolding. When a student is stuck, the AI can offer a contextual hint rather than a full solution. It can identify and explain errors in real-time, acting as a tireless coach.50 Scaffolding can be implemented through AI-powered features that generate boilerplate code for a new component, suggest function signatures, or provide personalized support to help learners overcome the initial hurdles of a complex task.51 A recent study on a scaffolded AI interface named Giuseppe found that novice programmers welcomed these additional supports at the outset of their learning journey.53  \n* **Articulation and Reflection:** This is the most critical and pedagogically challenging stage to implement with AI, yet it holds the most promise. The goal is to shift the learner from a passive recipient of information to an active participant in their own learning. Instead of simply asking the AI for an answer, the curriculum must structure interactions that force articulation and reflection. For example, an assignment could require a student to:  \n  1. First, write out their own plan to solve a problem and submit it to the AI for critique (Articulation).  \n  2. Second, implement their solution.  \n  3. Third, ask the AI to generate an alternative solution.  \n  4. Finally, write a reflection comparing their approach to the AI's, analyzing the trade-offs in terms of efficiency, readability, and design (Reflection).46\n\nThis process uses the AI not as an answer key, but as a dialogic partner that makes the student's own thinking the central object of study.\n\n### **The \"Pseudo-Apprenticeship\" Pitfall: A Critical Challenge**\n\nThe greatest pedagogical risk of integrating powerful AI assistants into education is "
  },
  {
    "id": "report_source",
    "chunk": "ral object of study.\n\n### **The \"Pseudo-Apprenticeship\" Pitfall: A Critical Challenge**\n\nThe greatest pedagogical risk of integrating powerful AI assistants into education is the phenomenon of \"pseudo-apprenticeship\".54 Recent research has identified this pattern where students use LLMs to obtain expert-level solutions but fail to engage in the active, effortful stages of cognitive apprenticeship that are necessary for building robust, independent problem-solving skills.54 They become adept at observing the output of the expert (the AI) but do not \"do\" the difficult cognitive work themselves.  \nThis is not a theoretical concern. One study of introductory computer science students using ChatGPT found that a significant portion prompted for complete solutions before making any effort on their own, and they often failed to verify the correctness of the AI-generated code.54 This behavior bypasses the essential learning processes of trial, error, debugging, and synthesis. The student receives a correct answer but builds no lasting mental model of how to arrive at that answer. The primary challenge for the V2V curriculum is to design a learning environment that actively counteracts this tendency.\n\n### **Designing for Productive Struggle**\n\nThe key to mitigating pseudo-apprenticeship is to design for \"productive struggle.\" The goal of an AI-powered pedagogy should not be to make coding easier by eliminating challenges, but to make the student's thinking more visible by structuring those challenges in a scaffolded way.  \nThe V2V curriculum must teach students to interact with AI not as an answer engine, but as a Socratic partner. This involves a fundamental shift in how prompts are formulated and how interactions are structured."
  },
  {
    "id": "report_source",
    "chunk": "ts to interact with AI not as an answer engine, but as a Socratic partner. This involves a fundamental shift in how prompts are formulated and how interactions are structured. The curriculum should provide explicit instruction and practice in using the AI to ask questions, explore alternatives, critique ideas, and simulate scenarios, rather than simply generating final code.  \nUltimately, the role of the AI in a V2V cognitive apprenticeship should be to scaffold the student's *metacognitive skills*their ability to plan their work, monitor their understanding, evaluate their progress, and reflect on their learning process. In the AI era, \"learning to code\" is becoming inseparable from \"learning to learn with AI.\" The most valuable and durable skill a developer can possess is the ability to effectively and critically use these powerful, fallible tools to augment their own intelligence. Therefore, the V2V curriculum must include explicit modules on \"Metacognition and AI Collaboration.\" These modules would teach frameworks for formulating effective learning questions, strategies for verifying AI-generated outputs, techniques for using AI to explore a problem space without premature solution-seeking, and structured methods for reflecting on the co-creation process. This elevates the curriculum from a course that teaches coding *with* AI to a program that teaches the essential cognitive skills for thriving as a developer *in an age of* AI.\n\n## **Strategic Recommendations for the V2V Curriculum**\n\nThe preceding analysis provides a clear and compelling case for a new approach to AI development education. The industry is shifting from the tactical craft of prompt engineering to the strategic discipline of context engineering; th"
  },
  {
    "id": "report_source",
    "chunk": "e for a new approach to AI development education. The industry is shifting from the tactical craft of prompt engineering to the strategic discipline of context engineering; the educational market has not yet caught up to this shift; and the pedagogical framework of cognitive apprenticeship offers a powerful, albeit challenging, model for teaching these new skills. This final section synthesizes these findings into a concrete set of strategic recommendations for the design, positioning, and implementation of the Vibecoding to Virtuosity (V2V) pathway.\n\n### **Core Value Proposition and Positioning**\n\n**Recommendation:** Position the Vibecoding to Virtuosity (V2V) pathway as an **\"AI-Native Systems Architecture\"** program.  \n**Rationale:** This positioning is a direct response to the analysis in Section 3\\. It immediately and decisively moves V2V out of the crowded, commoditized \"Red Ocean\" of \"Prompt Engineering for Developers\" courses. It establishes the program as a premier, advanced curriculum focused on the durable and high-value skills of building reliable, scalable, and agentic AI systems. This language and focus will attract a more senior, ambitious, and motivated learner who is looking to future-proof their career by moving beyond using AI tools to architecting AI-powered products. It signals a focus on engineering discipline over clever hacks, and on systems over single prompts.\n\n### **Proposed Curriculum Structure: The Virtuosity Pathway**\n\nThe curriculum should be structured to guide the learner along a logical path from foundational concepts to advanced application, mirroring the structure of this report. The four proposed modules represent a journey from understanding the new paradigm to mastering its implemen"
  },
  {
    "id": "report_source",
    "chunk": "ts to advanced application, mirroring the structure of this report. The four proposed modules represent a journey from understanding the new paradigm to mastering its implementation.  \n**Module 1: The Context Engineering Paradigm**\n\n* **Content:** This module will be based on the analysis in Section 1\\. It will formally introduce and define Context Engineering, using the comparative framework (Table 1\\) to definitively establish its distinction from and superiority to prompt engineering as a discipline for building systems. It will ground the V2V philosophy in the latest academic and industry discourse, giving learners a robust mental model for the rest of the course.\n\n**Module 2: The Architecture of Context**\n\n* **Content:** This module forms the technical core of the curriculum, based on the blueprint in Section 2\\. It will provide a deep, hands-on dive into the three phases of the context engineering pipeline:  \n  * **Unit 2.1: Retrieval and Generation:** Covers prompt-based generation, RAG patterns, and dynamic context assembly.  \n  * **Unit 2.2: Processing and Optimization:** Focuses on context window management, including selection, summarization, and compression techniques to combat \"context rot.\"  \n  * **Unit 2.3: Management for Agents:** Teaches the principles of building stateful systems, including memory architectures, tool integration, and agentic control loops.\n\n**Module 3: Metacognitive Apprenticeship with AI**\n\n* **Content:** This module will operationalize the pedagogical framework from Section 4\\. It is not just about theory; it is about practice. Learners will be explicitly taught how to use AI assistants to facilitate their own learning through Modeling, Coaching, and Scaffolding. Crucially, they will "
  },
  {
    "id": "report_source",
    "chunk": "about practice. Learners will be explicitly taught how to use AI assistants to facilitate their own learning through Modeling, Coaching, and Scaffolding. Crucially, they will engage in structured exercises that require them to practice Articulation and Reflection, forcing them to make their own thinking visible and to critically engage with AI-generated content. This module's primary goal is to inoculate learners against the \"pseudo-apprenticeship\" trap.\n\n**Module 4: Capstone Project \\- Building an Autonomous Agent**\n\n* **Content:** This is the culminating project where all skills are integrated. Learners will be tasked with designing and building a stateful, tool-using autonomous agent from the ground up to solve a complex problem. The project will require them to architect a full context pipeline, including retrieval, memory, and tool use. The final deliverable will not just be the functional agent, but also a comprehensive design document justifying their architectural choices and a \"Cognitive Apprenticeship Log\" detailing their AI-mediated development process.\n\n### **Key Learning Activities and Projects**\n\nTo bring the curriculum to life and reinforce its core principles, the following innovative learning activities are recommended:\n\n* **The \"Context Debugger\" Lab:** In this lab, students are given a failing multi-turn AI agent and a log of its interactions. Their task is to act as a \"context debugger,\" inspecting the context window at each step to diagnose the root cause of the failure. Potential failure modes to diagnose would include context poisoning (a hallucination from a previous step derails future steps), context distraction (irrelevant retrieved information causes the model to lose focus), or memory loss (a"
  },
  {
    "id": "report_source",
    "chunk": "poisoning (a hallucination from a previous step derails future steps), context distraction (irrelevant retrieved information causes the model to lose focus), or memory loss (a critical piece of information was pruned from the context window too early). This lab directly teaches the systems-level debugging skills that are absent from other curricula.  \n* **The \"Cognitive Apprenticeship Dialogue\" Project:** For a mid-course project, the final submission should not be a piece of code, but a transcript of the student's development dialogue with an AI assistant. The student would be required to annotate this transcript with reflections at key decision points. Grading would be based on the quality of the student's prompts (e.g., are they asking for critiques or just answers?), their critical evaluation of AI suggestions (e.g., do they catch and correct AI errors?), and their articulation of their own design choices. This project makes the metacognitive learning process the explicit object of assessment.  \n* **The \"RAG is Not Enough\" Challenge:** This project would be structured in two parts. First, students build a simple RAG-based question-answering bot for a given knowledge base. Then, in part two, the requirements are expanded: the bot must now handle multi-turn, task-oriented requests that require it to remember previous interactions and potentially call external tools (e.g., \"Based on the document you found, book a meeting for me using the calendar API\"). This forces students to confront the limitations of simple RAG and build the more complex context management and agentic systems required for stateful tasks.\n\n### **Final Recommendation: Grounding the Brand**\n\n**Recommendation:** The marketing and branding for the V2V pa"
  },
  {
    "id": "report_source",
    "chunk": "t management and agentic systems required for stateful tasks.\n\n### **Final Recommendation: Grounding the Brand**\n\n**Recommendation:** The marketing and branding for the V2V pathway should consistently and aggressively use the language of **\"engineering discipline,\" \"systems architecture,\" \"information logistics,\"** and **\"cognitive mentorship.\"**  \n**Rationale:** This vocabulary will resonate with the target audience of serious, career-focused developers who understand the difference between a fleeting trend and a foundational shift in their profession. It clearly communicates that V2V is not a collection of \"tips and tricks\" for talking to a chatbot, but a structured, rigorous, and comprehensive program for mastering the core principles of the next era of software development. This branding will attract the right students, set clear expectations, and firmly establish V2V as a leader in advanced AI education.\n\n#### **Works cited**\n\n1. Understanding Prompt Engineering and Context Engineering, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  \n2. Difference Between Prompt Engineering and Context Engineering \\- C\\# Corner, accessed October 15, 2025, [https://www.c-sharpcorner.com/article/difference-between-prompt-engineering-and-context-engineering/](https://www.c-sharpcorner.com/article/difference-between-prompt-engineering-and-context-engineering/)  \n3. Context Engineering vs Prompt Engineering | by Mehul Gupta | Data ..., accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medi"
  },
  {
    "id": "report_source",
    "chunk": "ng | by Mehul Gupta | Data ..., accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d)  \n4. Context Engineering Guide, accessed October 15, 2025, [https://www.promptingguide.ai/guides/context-engineering-guide](https://www.promptingguide.ai/guides/context-engineering-guide)  \n5. A Survey of Context Engineering for Large Language Models \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2507.13334v1](https://arxiv.org/html/2507.13334v1)  \n6. Prompt Engineering Is Dead, and Context Engineering Is Already Obsolete: Why the Future Is Automated Workflow Architecture with LLMs \\- OpenAI Developer Community, accessed October 15, 2025, [https://community.openai.com/t/prompt-engineering-is-dead-and-context-engineering-is-already-obsolete-why-the-future-is-automated-workflow-architecture-with-llms/1314011](https://community.openai.com/t/prompt-engineering-is-dead-and-context-engineering-is-already-obsolete-why-the-future-is-automated-workflow-architecture-with-llms/1314011)  \n7. Prompt Engineering is overrated. AIs just need context now \\-- try speaking to it \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1ic8c43/prompt\\_engineering\\_is\\_overrated\\_ais\\_just\\_need/](https://www.reddit.com/r/PromptEngineering/comments/1ic8c43/prompt_engineering_is_overrated_ais_just_need/)  \n8. Context Engineering: Bringing Engineering Discipline to Prompts ..., accessed October 15, 2025, [https://www.oreilly.com/radar/context-engineering-bringing-engineering-discipline-to-prompts-part-1/](https://www.oreilly.com/radar/"
  },
  {
    "id": "report_source",
    "chunk": "o Prompts ..., accessed October 15, 2025, [https://www.oreilly.com/radar/context-engineering-bringing-engineering-discipline-to-prompts-part-1/](https://www.oreilly.com/radar/context-engineering-bringing-engineering-discipline-to-prompts-part-1/)  \n9. Context Engineering for Reliable AI Agents | 2025 Guide \\- Kubiya, accessed October 15, 2025, [https://www.kubiya.ai/blog/context-engineering-ai-agents](https://www.kubiya.ai/blog/context-engineering-ai-agents)  \n10. What Is Context Engineering And Why Should You Care? | In The Loop Episode 23, accessed October 15, 2025, [https://www.mindset.ai/blogs/in-the-loop-ep23-what-is-context-engineering](https://www.mindset.ai/blogs/in-the-loop-ep23-what-is-context-engineering)  \n11. Context Engineering Guide in 2025 \\- Turing College, accessed October 15, 2025, [https://www.turingcollege.com/blog/context-engineering-guide](https://www.turingcollege.com/blog/context-engineering-guide)  \n12. \\[2507.13334\\] A Survey of Context Engineering for Large Language Models \\- arXiv, accessed October 15, 2025, [https://arxiv.org/abs/2507.13334](https://arxiv.org/abs/2507.13334)  \n13. A Survey of Context Engineering for Large Language Models \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/393783866\\_A\\_Survey\\_of\\_Context\\_Engineering\\_for\\_Large\\_Language\\_Models](https://www.researchgate.net/publication/393783866_A_Survey_of_Context_Engineering_for_Large_Language_Models)  \n14. Directed Information -covering: An Information-Theoretic Framework for Context Engineering \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2510.00079v1](https://arxiv.org/html/2510.00079v1)  \n15. Karpathy: \"context engineering\" over \"prompt engineering\" \\- Hacker News,"
  },
  {
    "id": "report_source",
    "chunk": "ed October 15, 2025, [https://arxiv.org/html/2510.00079v1](https://arxiv.org/html/2510.00079v1)  \n15. Karpathy: \"context engineering\" over \"prompt engineering\" \\- Hacker News, accessed October 15, 2025, [https://news.ycombinator.com/item?id=44379538](https://news.ycombinator.com/item?id=44379538)  \n16. Everybody is talking about how context engineering is replacing prompt engineering nowadays. But what really is this new buzzword? : r/AI\\_Agents \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/AI\\_Agents/comments/1mq935t/everybody\\_is\\_talking\\_about\\_how\\_context/](https://www.reddit.com/r/AI_Agents/comments/1mq935t/everybody_is_talking_about_how_context/)  \n17. Context Engineering: The AI Skill You Should Master in 2025 \\- Charter Global, accessed October 15, 2025, [https://www.charterglobal.com/context-engineering/](https://www.charterglobal.com/context-engineering/)  \n18. Context Engineering in AI: Principles, Methods, and Uses \\- Code B, accessed October 15, 2025, [https://code-b.dev/blog/context-engineering](https://code-b.dev/blog/context-engineering)  \n19. Context Engineering \\- What it is, and techniques to consider \\- LlamaIndex, accessed October 15, 2025, [https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider](https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider)  \n20. Context Engineering: Techniques, Tools, and Implementation \\- iKala, accessed October 15, 2025, [https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/](https://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/)  \n21. A Survey of Context Engineering for Large Language Models \\- 2507.13334v2.pdf | "
  },
  {
    "id": "report_source",
    "chunk": "ttps://ikala.ai/blog/ai-trends/context-engineering-techniques-tools-and-implementation/)  \n21. A Survey of Context Engineering for Large Language Models \\- 2507.13334v2.pdf | Community Highlights & Summary | Glasp, accessed October 15, 2025, [https://glasp.co/discover?url=arxiv.org%2Fpdf%2F2507.13334](https://glasp.co/discover?url=arxiv.org/pdf/2507.13334)  \n22. Context Engineering ( RAG 2.0 ) : The Next Chapter in GenAI \\- Medium, accessed October 15, 2025, [https://medium.com/@ramakrishna.sanikommu/context-engineering-rag-2-0-the-next-chapter-in-genai-4e53c0382bf4](https://medium.com/@ramakrishna.sanikommu/context-engineering-rag-2-0-the-next-chapter-in-genai-4e53c0382bf4)  \n23. Context Engineering for AI Agents: Lessons from Building Manus, accessed October 15, 2025, [https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)  \n24. Effective context engineering for AI agents \\- Anthropic, accessed October 15, 2025, [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)  \n25. What is long context and why does it matter for AI? | Google Cloud Blog, accessed October 15, 2025, [https://cloud.google.com/transform/the-prompt-what-are-long-context-windows-and-why-do-they-matter](https://cloud.google.com/transform/the-prompt-what-are-long-context-windows-and-why-do-they-matter)  \n26. MCP Context Window Management \\- Tetrate, accessed October 15, 2025, [https://tetrate.io/learn/ai/mcp/context-window-management](https://tetrate.io/learn/ai/mcp/context-window-management)  \n27. Context Engineering for AI Agents"
  },
  {
    "id": "report_source",
    "chunk": "ctober 15, 2025, [https://tetrate.io/learn/ai/mcp/context-window-management](https://tetrate.io/learn/ai/mcp/context-window-management)  \n27. Context Engineering for AI Agents: The Complete Guide | by IRFAN KHAN \\- Medium, accessed October 15, 2025, [https://medium.com/@khanzzirfan/context-engineering-for-ai-agents-the-complete-guide-5047f84595c7](https://medium.com/@khanzzirfan/context-engineering-for-ai-agents-the-complete-guide-5047f84595c7)  \n28. Context Engineering \\- Short-Term Memory Management with Sessions from OpenAI Agents SDK, accessed October 15, 2025, [https://cookbook.openai.com/examples/agents\\_sdk/session\\_memory](https://cookbook.openai.com/examples/agents_sdk/session_memory)  \n29. How to Perform Effective Agentic Context Engineering | Towards Data Science, accessed October 15, 2025, [https://towardsdatascience.com/how-to-perform-effective-agentic-context-engineering/](https://towardsdatascience.com/how-to-perform-effective-agentic-context-engineering/)  \n30. Generative AI for Software Development \\- DeepLearning.AI, accessed October 15, 2025, [https://www.deeplearning.ai/courses/generative-ai-for-software-development/](https://www.deeplearning.ai/courses/generative-ai-for-software-development/)  \n31. Generative AI for Software Development Skill Certificate \\- Coursera, accessed October 15, 2025, [https://www.coursera.org/professional-certificates/generative-ai-for-software-development](https://www.coursera.org/professional-certificates/generative-ai-for-software-development)  \n32. Introduction to Generative AI for Software Development \\- Coursera, accessed October 15, 2025, [https://www.coursera.org/learn/introduction-to-generative-ai-for-software-development](https://www.coursera.org/learn/introductio"
  },
  {
    "id": "report_source",
    "chunk": "ment \\- Coursera, accessed October 15, 2025, [https://www.coursera.org/learn/introduction-to-generative-ai-for-software-development](https://www.coursera.org/learn/introduction-to-generative-ai-for-software-development)  \n33. Student Hub Overview \\- Microsoft Learn, accessed October 15, 2025, [https://learn.microsoft.com/en-us/training/student-hub/](https://learn.microsoft.com/en-us/training/student-hub/)  \n34. AI for Beginners, accessed October 15, 2025, [https://microsoft.github.io/AI-For-Beginners/](https://microsoft.github.io/AI-For-Beginners/)  \n35. microsoft/generative-ai-for-beginners: 21 Lessons, Get Started Building with Generative AI, accessed October 15, 2025, [https://github.com/microsoft/generative-ai-for-beginners](https://github.com/microsoft/generative-ai-for-beginners)  \n36. microsoft/ai-agents-for-beginners: 12 Lessons to Get Started Building AI Agents \\- GitHub, accessed October 15, 2025, [https://github.com/microsoft/ai-agents-for-beginners](https://github.com/microsoft/ai-agents-for-beginners)  \n37. GitHub Learning Pathways, accessed October 15, 2025, [https://resources.github.com/learn/pathways/](https://resources.github.com/learn/pathways/)  \n38. GitHub Copilot Fundamentals Part 1 of 2 \\- Training \\- Microsoft Learn, accessed October 15, 2025, [https://learn.microsoft.com/en-us/training/paths/copilot/](https://learn.microsoft.com/en-us/training/paths/copilot/)  \n39. How to write better prompts for GitHub Copilot, accessed October 15, 2025, [https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/](https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/)  \n40. GitHub Copilot using Python Free Course with Certificate \\- Great Lear"
  },
  {
    "id": "report_source",
    "chunk": "lot/](https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/)  \n40. GitHub Copilot using Python Free Course with Certificate \\- Great Learning, accessed October 15, 2025, [https://www.mygreatlearning.com/academy/learn-for-free/courses/github-copilot-using-python](https://www.mygreatlearning.com/academy/learn-for-free/courses/github-copilot-using-python)  \n41. AI Software Development with GitHub Copilot \\- eLearning Bundle Course, accessed October 15, 2025, [https://training.certstaff.com/assnfe/CourseView.asp?MODE=VIEW\\&clCourseID=473](https://training.certstaff.com/assnfe/CourseView.asp?MODE=VIEW&clCourseID=473)  \n42. Generative AI for Software Development is open for enrollment\\! \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=baYKwwZx-CQ](https://www.youtube.com/watch?v=baYKwwZx-CQ)  \n43. Online Course: Introduction to Generative AI for Software Development from DeepLearning.AI | Class Central, accessed October 15, 2025, [https://www.classcentral.com/course/coursera-introduction-to-generative-ai-for-software-development-299764](https://www.classcentral.com/course/coursera-introduction-to-generative-ai-for-software-development-299764)  \n44. microsoft/AI-For-Beginners: 12 Weeks, 24 Lessons, AI for All\\! \\- GitHub, accessed October 15, 2025, [https://github.com/microsoft/AI-For-Beginners](https://github.com/microsoft/AI-For-Beginners)  \n45. Generative AI for Developers: Deep Learning Online Program | Edubex, accessed October 15, 2025, [https://www.edubex.com/program-details/generative-ai-for-software-development-deep-learning-26-10-2025-aed](https://www.edubex.com/program-details/generative-ai-for-software-development-deep-learning-26-10-2025-aed)  \n46. Transla"
  },
  {
    "id": "report_source",
    "chunk": "or-software-development-deep-learning-26-10-2025-aed](https://www.edubex.com/program-details/generative-ai-for-software-development-deep-learning-26-10-2025-aed)  \n46. Translating knowledge to practice: application of the public health apprenticeship \\- PMC, accessed October 15, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12230075/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12230075/)  \n47. Investigating the Impact of the Stratified Cognitive Apprenticeship Model on High School Students' Math Performance \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2227-7102/14/8/898](https://www.mdpi.com/2227-7102/14/8/898)  \n48. What Is the Cognitive Apprenticeship Model of Teaching and Its Use in eLearning, accessed October 15, 2025, [https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning](https://www.coursebox.ai/blog/cognitive-apprenticeship-model-of-teaching-and-its-use-in-elearning)  \n49. Cognitive Apprenticeship and Artificial Intelligence Coding Assistants | Request PDF, accessed October 15, 2025, [https://www.researchgate.net/publication/378823978\\_Cognitive\\_Apprenticeship\\_and\\_Artificial\\_Intelligence\\_Coding\\_Assistants](https://www.researchgate.net/publication/378823978_Cognitive_Apprenticeship_and_Artificial_Intelligence_Coding_Assistants)  \n50. The Impact of AI Feedback in Applied Learning \\- Multiverse, accessed October 15, 2025, [https://www.multiverse.io/en-GB/blog/the-impact-of-ai-feedback-in-applied-learning](https://www.multiverse.io/en-GB/blog/the-impact-of-ai-feedback-in-applied-learning)  \n51. Scaffolding Creativity: Integrating Generative AI Tools and Real-world Experiences in Business Education \\- arXiv, accessed October 15, 2025, [https://arxiv.org/ht"
  },
  {
    "id": "report_source",
    "chunk": "ning)  \n51. Scaffolding Creativity: Integrating Generative AI Tools and Real-world Experiences in Business Education \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.06527v1](https://arxiv.org/html/2501.06527v1)  \n52. AI-Integrated Scaffolding to Enhance Agency and Creativity in K-12 English Language Learners: A Systematic Review \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2078-2489/16/7/519](https://www.mdpi.com/2078-2489/16/7/519)  \n53. (PDF) Supporting Novice Programmers with Scaffolded and Open-Ended Generative AI Interfaces: Insights from a Design-Based Research Study \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/392472771\\_Supporting\\_Novice\\_Programmers\\_with\\_Scaffolded\\_and\\_Open-Ended\\_Generative\\_AI\\_Interfaces\\_Insights\\_from\\_a\\_Design-Based\\_Research\\_Study](https://www.researchgate.net/publication/392472771_Supporting_Novice_Programmers_with_Scaffolded_and_Open-Ended_Generative_AI_Interfaces_Insights_from_a_Design-Based_Research_Study)  \n54. Observing Without Doing: Pseudo-Apprenticeship Patterns in Student LLM Use \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2510.04986v1](https://arxiv.org/html/2510.04986v1)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/08-V2V Pathway Research Proposal.md\">\n\n\n# **From Context to Cognition: A Foundational Report for the Vibecoding to Virtuosity (V2V) Curriculum**\n\n## **Executive Summary**\n\nThe proliferation of Large Language Models (LLMs) has initiated a paradigm shift in software development, moving beyond simple tool adoption to a fundamental re-architecting of the developer's role and workflow. This report presents a foundational analysis intended to serve as the intell"
  },
  {
    "id": "report_source",
    "chunk": "beyond simple tool adoption to a fundamental re-architecting of the developer's role and workflow. This report presents a foundational analysis intended to serve as the intellectual and structural blueprint for the \"Vibecoding to Virtuosity\" (V2V) curriculum. The core thesis of this analysis is that the future of elite AI-assisted software development lies at the intersection of two powerful frameworks: **Context Engineering** as a technical discipline and **Cognitive Apprenticeship** as a pedagogical model.  \nThe current landscape of AI interaction is rapidly maturing from the tactical craft of \"prompt engineering\"the art of phrasing instructionsto the strategic discipline of **Context Engineering**. This evolution involves designing the entire informational environment in which an AI operates, managing its memory, tools, and access to data to ensure reliable, scalable, and stateful performance. This shift is not merely semantic; it is a direct response to the demands of building production-grade, agentic AI systems that are deeply embedded in enterprise workflows.  \nTo effectively teach this new paradigm, a corresponding pedagogical evolution is required. This report posits that the **Cognitive Apprenticeship** model, with its emphasis on making the tacit thought processes of experts visible, provides the ideal framework. Its core methodsmodeling, coaching, scaffolding, articulation, reflection, and explorationare uniquely suited to teaching the complex, often invisible skills of designing and interacting with intelligent systems. Furthermore, modern AI tools are not only the subject of this pedagogy but also powerful instruments for its implementation, capable of acting as tireless mentors that can model expert be"
  },
  {
    "id": "report_source",
    "chunk": " modern AI tools are not only the subject of this pedagogy but also powerful instruments for its implementation, capable of acting as tireless mentors that can model expert behavior, provide real-time coaching, and offer adaptive scaffolding.  \nThe proposed V2V pathway is a structured curriculum designed to guide developers from intuitive, tactical use of AI (\"Vibecoding\") to principled, strategic design (\"Virtuosity\"). It progresses through three distinct stages: The AI-Augmented Developer, The Context-Aware Architect, and The Agentic Systems Designer. This journey is designed to cultivate not just technical proficiency but advanced **metacognitive abilities**, or \"Meta AI Skills,\" transforming the developer from a mere user of AI tools into a strategic architect and critical validator of complex human-AI collaborative systems. This report provides a detailed analysis of these domains and concludes with a concrete curriculum blueprint, including signature pedagogies and capstone projects, to realize this transformative educational vision.  \n---\n\n## **Part I: The Foundational Paradigm \\- Engineering the Context**\n\nThis initial part of the report establishes the core technical and conceptual shift that underpins the entire V2V curriculum. To construct a meaningful pedagogy for AI-assisted development, it is imperative to first define the nature of the work itself. This requires moving beyond the popular but limited notion of prompt crafting and embracing the more robust, systemic discipline of engineering the AI's context.\n\n### **The Evolution from Prompt Crafting to Context Architecture**\n\nThe discourse surrounding human-AI interaction has been dominated by the term \"prompt engineering.\" While a crucial entry point, this"
  },
  {
    "id": "report_source",
    "chunk": "rompt Crafting to Context Architecture**\n\nThe discourse surrounding human-AI interaction has been dominated by the term \"prompt engineering.\" While a crucial entry point, this term is rapidly becoming insufficient to describe the sophisticated work required to build reliable, production-grade AI applications. A more comprehensive and strategically vital discipline, Context Engineering, has emerged as its natural successor, marking a critical evolution from a tactical craft to a formal engineering practice.  \nThe fundamental distinction lies in scope, mindset, and objective. Prompt Engineering is the tactical art of crafting the immediate instructions for an LLM.1 It is the practice of \"massaging words\" 2 and structuring clear, explicit instructions to elicit a specific, often one-off, response from a model.3 Its focus is narrow, operating within a single input-output pair, and its methods include role assignment, formatting constraints, and few-shot examples.4 In contrast, Context Engineering is the strategic science of designing the \"entire mental world the model operates in\".3 It is a form of \"systems thinking\" 4 that involves managing the \"broader pool of information that surrounds and informs the AI's decision-making process\".6 This includes constructing automated pipelines that assemble and filter diverse information sources such as user dialogue history, real-time data, retrieved documents, and external tools, all of which must be formatted and ordered within the model's finite context window.4 The mindset shifts from that of a creative writer or copy-tweaker to that of a \"systems design or software architecture for LLMs\".3  \nThis distinction clarifies the relationship between the two disciplines: Prompt Engineerin"
  },
  {
    "id": "report_source",
    "chunk": " or copy-tweaker to that of a \"systems design or software architecture for LLMs\".3  \nThis distinction clarifies the relationship between the two disciplines: Prompt Engineering is a subset of Context Engineering.3 A well-crafted prompt is a vital component of an AI system, but its efficacy is entirely dependent on the engineered context that surrounds it. As one analysis notes, even the best instruction is rendered useless if it is \"lost at token 12,000 behind three FAQs and a JSON blob\".3 A robustly engineered context protects, structures, and empowers the prompt, ensuring its clarity and priority.3  \nThis evolution from prompt crafting to context architecture represents the maturation of the field. Prompt engineering is often described as a \"scrappy startup's idea\" 2 or a \"quick-and-dirty hack\" 3, valuable for prototyping and experimentation but ultimately \"brittle\" and difficult to scale.4 Context Engineering, conversely, is the application of formal engineering principles to build reliable, repeatable, and scalable LLM-powered systems.2 This view is strongly supported by industry analysis from firms like Gartner, which states that prompt engineering is \"fading into tooling and templates,\" while context engineering is becoming a \"core enterprise capability\" and a strategic priority.9  \nThe emergence of Context Engineering is not an arbitrary semantic shift but a necessary adaptation driven by the changing application of LLMs in the enterprise. Early use cases were often stateless and conversational, such as generating creative text or answering one-off questions, for which prompt engineering was sufficient.3 However, as organizations began integrating LLMs into critical business workflowsbuilding stateful customer su"
  },
  {
    "id": "report_source",
    "chunk": "ne-off questions, for which prompt engineering was sufficient.3 However, as organizations began integrating LLMs into critical business workflowsbuilding stateful customer support bots, personalized CRM assistants, or complex multi-turn agentsthe inherent limitations of a prompt-only approach became prohibitive.3 The fragility of prompts, where minor wording changes could yield drastically different results, and their inability to manage state or incorporate real-time data, made them an unstable foundation for reliable systems.4 This demand for consistency, personalization at scale, and deep integration with backend systems necessitated the development of a more robust, architectural approach. Thus, the rise of Context Engineering is a direct consequence of the enterprise adoption of LLMs, reflecting the need for systems that can reliably manage a dynamic informational environment. Teaching this discipline is therefore not just about imparting a new technique; it is about teaching the architectural patterns essential for modern, production-grade AI software.  \nA foundational element within this new paradigm is Retrieval-Augmented Generation (RAG), a pattern where an LLM's knowledge is supplemented at runtime with relevant information retrieved from external data sources.11 While RAG is a cornerstone of Context Engineering, it is important to recognize that it is a component, not the entirety of the discipline. A comprehensive context-engineered system integrates not only retrieved text via RAG but also a rich tapestry of other elements, including explicit instructions (prompts), conversational memory, user profile information, and the schemas and outputs of external tools.12\n\n| Aspect | Prompt Engineering | Context Eng"
  },
  {
    "id": "report_source",
    "chunk": "explicit instructions (prompts), conversational memory, user profile information, and the schemas and outputs of external tools.12\n\n| Aspect | Prompt Engineering | Context Engineering |\n| :---- | :---- | :---- |\n| **Definition** | Crafting specific input text (prompts) to elicit a desired, immediate output from an LLM.6 | Designing and managing the entire informational environment provided to an AI system to guide its behavior over time.6 |\n| **Primary Goal** | Obtain a specific, high-quality response for a single task.3 | Ensure consistent, reliable, and scalable AI performance across multiple users, sessions, and tasks.2 |\n| **Scope** | Narrow: Operates within a single input-output pair.3 | Broad: Manages the entire context window, including memory, retrieval, tools, and dialogue history.4 |\n| **Mindset** | Tactical, akin to creative writing or copy-tweaking.3 | Strategic, akin to systems design or software architecture for LLMs.3 |\n| **Core Practices** | Role assignment, few-shot examples, chain-of-thought, meticulous wording and formatting.4 | Context retrieval (RAG), summarization, tool integration, memory management, dynamic prompt assembly.2 |\n| **Tools** | Text editors, chat interfaces (e.g., ChatGPT).3 | Vector databases, RAG systems, orchestration frameworks (e.g., LangGraph), API chaining.1 |\n| **Scalability** | Brittle and hard to scale; requires manual tweaks for new edge cases.4 | Designed for consistency and reuse; built with scale in mind from the beginning.3 |\n| **Failure Mode** | The output is weird, off-topic, or factually incorrect.3 | The entire system may behave unpredictably, forget goals, or misuse tools.3 |\n| **Strategic Importance** | A foundational but increasingly commoditized skill; a \"quick-"
  },
  {
    "id": "report_source",
    "chunk": "ct.3 | The entire system may behave unpredictably, forget goals, or misuse tools.3 |\n| **Strategic Importance** | A foundational but increasingly commoditized skill; a \"quick-and-dirty hack\".3 | A core enterprise capability for building production-grade, agentic AI systems; the \"real design work\".3 |\n\n### **The Mechanics of the Context Window: Managing AI's Cognitive Load**\n\nTransitioning from the conceptual framework of Context Engineering to its practical implementation requires a deep understanding of the LLM's primary operational constraint: the context window. This is the finite set of tokensunits of text that can be characters, words, or parts of wordsthat a model can process at any given time.14 Effectively, the context window functions as the AI's working memory or cognitive workspace.15 The engineering challenge is to optimize the utility of these tokens to consistently achieve a desired outcome.14 This perspective is powerfully captured in Andrej Karpathy's analogy: \"the LLM is the CPU and the context window is the RAM. The craft is deciding what to load into that RAM at each step\".17  \nSimply having a large context window is not a panacea. Research has identified a significant \"lost in the middle\" problem, where models exhibit a performance degradation when critical information is placed in the middle of a long input context, showing a clear bias towards information at the beginning and end.15 This demonstrates that the *structure* and *prioritization* of information within the window are as crucial as its size. Therefore, effective context window management is a core competency of the Context Engineer.  \nA taxonomy of management strategies can be established, progressing from simple, brute-force methods to "
  },
  {
    "id": "report_source",
    "chunk": "text window management is a core competency of the Context Engineer.  \nA taxonomy of management strategies can be established, progressing from simple, brute-force methods to sophisticated architectural patterns:\n\n1. **Reductionist Techniques:** These are the most direct approaches to fitting information into a constrained window.  \n   * **Truncation:** The simplest method, which involves cutting off excess tokens from the input until it fits. While easy to implement, it is a \"dumb\" approach that lacks semantic awareness and risks excising critical information, leading to unreliable responses.19  \n   * **Compression & Summarization:** These techniques aim to reduce token count while preserving meaning. This can involve condensing long documents or conversation histories into compact summaries.4  \n2. **Routing and Selection:** These methods involve making intelligent choices about what information to process and which model to use.  \n   * **Dynamic Routing:** Instead of trimming the input, a system can route requests that exceed the context window of a smaller, cheaper model to a larger, more capable one.19  \n   * **Intelligent Selection:** This involves using algorithms or relevance scoring to identify and select only the most pertinent information for the current task, pruning irrelevant or outdated context.20  \n3. **Architectural Patterns for Long Documents:** For tasks involving documents that far exceed any single context window, more complex processing patterns are required.  \n   * **Chunking:** The foundational approach of splitting a large document into smaller, manageable chunks that can be processed individually.21  \n   * **Map-Reduce:** Each chunk is processed in parallel (the \"map\" step), and the individual re"
  },
  {
    "id": "report_source",
    "chunk": "cument into smaller, manageable chunks that can be processed individually.21  \n   * **Map-Reduce:** Each chunk is processed in parallel (the \"map\" step), and the individual results (e.g., summaries) are then combined and synthesized in a final step (the \"reduce\" step).21  \n   * **Refine:** This is an iterative approach where the first chunk is processed, and its output is then passed along with the second chunk to the model, allowing the model to refine its understanding and build upon its previous analysis. This continues sequentially through all chunks.21  \n   * **Map-Rerank:** Each chunk is processed to generate an output, and these outputs are then ranked based on their relevance to a specific user query. Only the highest-ranked outputs are used for the final response.21  \n4. **Conversational Memory Patterns:** To maintain coherence in long-running dialogues, specific strategies are needed.  \n   * **Rolling Window:** This approach prioritizes recent messages while gradually phasing out the oldest ones to keep the conversation flowing without exceeding the token limit.18  \n   * **Explicit Summarization:** The system can periodically generate a summary of the conversation so far, replacing the detailed history with a condensed version to free up tokens while retaining key information.16\n\nThe technical practices of context window management are more than just an optimization exercise; they represent the externalization and programming of a cognitive skill that human experts perform tacitly. When a human expert tackles a complex problem, they do not hold every single piece of data in their conscious working memory. Instead, they engage in a dynamic process of managing their cognitive load: they retrieve relevant knowledg"
  },
  {
    "id": "report_source",
    "chunk": " hold every single piece of data in their conscious working memory. Instead, they engage in a dynamic process of managing their cognitive load: they retrieve relevant knowledge from long-term memory as needed, focus their attention on the immediate sub-problem, and periodically summarize their progress and conclusions before moving to the next step. This is an internal, metacognitive process of information management. An LLM, constrained by its context window, cannot perform this internal process. It can only \"reason\" about the information it can currently \"see\".15 The techniques of context engineeringsuch as RAG, chunking, and summarizationare explicit, programmable systems that mimic this expert cognitive process. RAG is analogous to an expert recalling a specific fact from memory. Summarization is equivalent to an expert recapping their progress. Therefore, teaching context window management is a core element of a Cognitive Apprenticeship in the AI era. It is a method for making an expert's invisible process of information management visible, tangible, and transferable to both the AI system and the human learner.\n\n### **The Frontier \\- Agentic Context Engineering (ACE) and Self-Improving Systems**\n\nThe principles of Context Engineering culminate in a cutting-edge framework known as Agentic Context Engineering (ACE). This framework represents a fundamental shift from designing static context pipelines to architecting dynamic, learning systems. ACE treats an AI's context not as a fixed set of instructions but as an \"evolving playbook\" that accumulates, refines, and organizes strategies over time based on experience.22 The central innovation of ACE is its ability to enable an LLM to improve its own performance without "
  },
  {
    "id": "report_source",
    "chunk": "lates, refines, and organizes strategies over time based on experience.22 The central innovation of ACE is its ability to enable an LLM to improve its own performance without any changes to its underlying weights, relying solely on the sophisticated manipulation of its context.24  \nThe ACE framework operates on a continuous, three-part cycle that facilitates learning from experience 24:\n\n1. **Generator:** This is the LLM agent that attempts to perform a given task. It executes a plan, takes actions (e.g., calling an API, writing code), and critically, records a detailed trace of its actions and the environment's response.  \n2. **Reflector:** This is a specialized, secondary LLM agent that acts as a critical analyst. It takes the trace from the Generator and the final outcome (success or failure) as input. Its sole purpose is to perform a structured introspection, identifying the root cause of any failures and distilling the experience into a concise, actionable \"key insight.\" For example, it might conclude, \"For monetary values, use regex pattern \\\\d+(\\\\.\\\\d+)? instead of \\\\d+ to handle decimals\".24  \n3. **Curator:** This component takes the structured insight from the Reflector and updates the \"playbook\" or memory store. This is not a simple rewriting process but a structured, incremental update that adds the new strategy or insight to the context that will be provided to the Generator in future attempts at similar tasks.\n\nThis cyclical process is specifically designed to overcome two critical failure modes of simpler context adaptation methods: \"brevity bias,\" where iterative summarization loses important domain-specific details, and \"context collapse,\" where continuous rewriting gradually erodes the original knowledge"
  },
  {
    "id": "report_source",
    "chunk": "revity bias,\" where iterative summarization loses important domain-specific details, and \"context collapse,\" where continuous rewriting gradually erodes the original knowledge over time.22 Perhaps the most powerful feature of the ACE framework is its ability to learn from natural **execution feedback** without requiring expensive, human-labeled supervision.23 The success or failure signal can come directly from the environment: Did the generated code pass its unit tests? Did the API call return a 200 OK or a 404 Not Found? This capability allows for the creation of genuinely self-improving systems that can adapt and optimize their behavior in real-world operational environments.24 The performance gains demonstrated by this approach are significant, with studies showing that ACE can substantially boost agent accuracy and enable smaller, open-source models to match or even surpass the performance of larger, proprietary models on complex benchmarks.22  \nThe Generator-Reflector-Curator loop is not merely an clever technical architecture; it is the direct, programmatic embodiment of a complete human learning cycle: Action  Reflection  Consolidation. This maps perfectly onto the most advanced stages of the Cognitive Apprenticeship model, which are designed to transition a learner into an independent expert. The final stages of apprenticeshipArticulation, Reflection, and Explorationare operationalized within the ACE system itself.28 The **Generator's** detailed trace of its actions is a literal form of *Articulation*it is making its \"thought\" process explicit. The **Reflector** is a pure implementation of *Reflection*, as it critically analyzes performance against a desired outcome to identify errors in its own process. Fi"
  },
  {
    "id": "report_source",
    "chunk": "explicit. The **Reflector** is a pure implementation of *Reflection*, as it critically analyzes performance against a desired outcome to identify errors in its own process. Finally, the **Curator's** role in updating the playbook enables future **Generators** to engage in *Exploration* by attempting the task again with new, improved strategies derived from past failures.  \nThis profound alignment provides a clear, aspirational technical goal for the V2V curriculum. By teaching developers to build ACE-like systems, the curriculum moves beyond simply apprenticing the developer *with* an AI. It teaches them how to build AI systems that can perform the apprenticeship learning cycle *on their own*. This represents the ultimate transition from being a consumer of AI-driven pedagogy to becoming a creator of itthe very definition of virtuosity.  \n---\n\n## **Part II: The Pedagogical Framework \\- Cognitive Apprenticeship in the AI Era**\n\nHaving established Context Engineering as the core technical paradigm, this part of the report details the educational theory that will structure the V2V curriculum. The Cognitive Apprenticeship model is proposed as the ideal framework for teaching the complex, often tacit, skills required for this new form of software development. It provides a structured, evidence-based approach that is uniquely well-suited to the challenges and opportunities presented by AI.\n\n### **Core Principles of the Cognitive Apprenticeship Model**\n\nThe Cognitive Apprenticeship model, as articulated by Collins, Brown, and Newman, extends the principles of traditional apprenticeship to the learning of cognitive and metacognitive skills.28 Unlike traditional apprenticeships that focus on physical crafts, cognitive apprentice"
  },
  {
    "id": "report_source",
    "chunk": "s of traditional apprenticeship to the learning of cognitive and metacognitive skills.28 Unlike traditional apprenticeships that focus on physical crafts, cognitive apprenticeship is designed for domains where the expert's processes are largely internal and invisible. The primary goal of the model is to make these \"subtle, tacit elements of expert practice\" explicit and observable to the learner, thereby creating a guided path to mastery.28  \nThe model is built upon a foundation of six core instructional methods, which are designed to be sequenced and interwoven to support the learner's development from novice to expert 28:\n\n1. **Modeling:** The expert (or teacher) performs a task while explicitly externalizing their internal thought processes. This involves \"thinking aloud\" to demonstrate not just *what* to do, but *how* and *why* decisions are made, making the expert's strategic and heuristic knowledge visible.  \n2. **Coaching:** The expert observes the learner as they attempt the task and provides real-time, context-specific feedback, hints, and encouragement. This guidance is tailored to the learner's immediate needs and helps them navigate challenges as they arise.  \n3. **Scaffolding:** The expert provides the learner with structural supports that allow them to accomplish tasks that are just beyond their current unassisted capabilities. This can take the form of tools, templates, checklists, or breaking a complex problem down into more manageable sub-tasks.  \n4. **Articulation:** Learners are prompted to verbalize their own knowledge, reasoning, and problem-solving processes. This can involve explaining their approach to a problem or answering diagnostic questions from the expert, forcing them to make their own taci"
  },
  {
    "id": "report_source",
    "chunk": ", and problem-solving processes. This can involve explaining their approach to a problem or answering diagnostic questions from the expert, forcing them to make their own tacit understanding explicit.  \n5. **Reflection:** Learners are encouraged to compare their own problem-solving processes and outcomes with those of the expert or an idealized model. This critical self-analysis helps them identify strengths, weaknesses, and areas for improvement.  \n6. **Fading and Exploration:** As the learner's proficiency increases, the expert gradually withdraws the coaching and scaffolding (fading). This reduction in support encourages the learner to function more independently and to test their skills in new and varied situations (exploration), solidifying their ability to solve problems autonomously.\n\n### **The AI as Cognitive Mentor: Implementing the Model with Technology**\n\nThe Cognitive Apprenticeship model provides a powerful theoretical lens, and modern AI tools offer an unprecedented medium for its practical implementation. An AI coding assistant or agent can be framed not just as a tool, but as a \"cognitive mentor\" capable of executing the core methods of the model tirelessly and at scale. This section systematically maps each of the six methods to the specific capabilities of AI technology.\n\n* **AI as Modeler:** AI coding assistants excel at modeling expert performance. When a developer provides a problem description and the AI generates a complete, idiomatic solution, it is demonstrating *how* an expert might approach that problem, making an effective implementation visible.30 The process goes beyond just code; a developer can prompt the AI to explain its reasoning, justify its architectural choices, or compare alternativ"
  },
  {
    "id": "report_source",
    "chunk": "e implementation visible.30 The process goes beyond just code; a developer can prompt the AI to explain its reasoning, justify its architectural choices, or compare alternative approaches, thereby modeling the critical *articulation* of thought that accompanies expert action.  \n* **AI as Coach:** The interactive, back-and-forth nature of working with an AI directly simulates the coaching process.30 A developer writes a piece of code, and the AI can be prompted to review it, suggest a refactoring, and explain the benefits of the change. When a bug occurs, the developer can paste the stack trace into the AI and receive not just a fix, but an explanation of the root cause.32 This immediate, task-specific, and iterative feedback loop is the essence of effective coaching.  \n* **AI as Scaffolding:** AI provides a rich and dynamic source of scaffolding, reducing the learner's extraneous cognitive load so they can focus on the core conceptual challenges of a problem.34 This support manifests in several forms identified in educational research 36:  \n  * **Procedural Scaffolding:** Generating boilerplate code, configuration files, or the syntax for a complex API call.  \n  * **Conceptual Scaffolding:** Explaining a new design pattern, summarizing the documentation for an unfamiliar library, or clarifying a complex algorithm.  \n  * **Strategic Scaffolding:** Suggesting a high-level plan for implementing a new feature or breaking a large problem down into smaller, more manageable steps.  \n* **AI as a Catalyst for Articulation and Reflection:** While AI can model and coach, its most profound pedagogical impact may lie in how it forces the human user to engage in higher-order thinking.  \n  * **Articulation through Prompting:** To get a"
  },
  {
    "id": "report_source",
    "chunk": " and coach, its most profound pedagogical impact may lie in how it forces the human user to engage in higher-order thinking.  \n  * **Articulation through Prompting:** To get a high-quality response from an AI, a developer cannot be vague. They are forced to *articulate* their mental model of the problem with extreme clarity and precision in the form of a detailed prompt.37 A poor output from the AI is often a direct reflection of a poorly articulated request, creating a powerful feedback loop that hones the developer's ability to structure and communicate their thoughts.  \n  * **Reflection through Evaluation:** An AI is not an infallible oracle; it is a probabilistic system prone to errors.39 Consequently, every line of AI-generated code must be met with a critical, reflective act from the developer: \"Is this code correct? Is it secure? Does it follow our project's conventions? Is there a simpler way to do this?\".33 This constant cycle of evaluation and validation is a potent form of reflection, forcing the developer to compare the AI's output against their own internal model of quality. The ACE framework's \"Reflector\" module represents the ultimate codification of this process, turning reflection into a programmable system component.24  \n* **AI for Fading and Exploration:** The AI acts as a persistent safety net that facilitates the final stages of apprenticeship. As a learner gains competence, they can naturally reduce their reliance on the AI (fading), shifting from asking for entire functions to asking only for specific API signatures or conceptual clarifications. This safety net lowers the cost of failure and encourages *exploration*. A developer is more likely to experiment with a new library or architectural patte"
  },
  {
    "id": "report_source",
    "chunk": "ual clarifications. This safety net lowers the cost of failure and encourages *exploration*. A developer is more likely to experiment with a new library or architectural pattern if they know an AI mentor is available to help them get \"unstuck\" should they encounter difficulties.32\n\n| Cognitive Apprenticeship Method | Description | AI-Enabled Implementation |\n| :---- | :---- | :---- |\n| **Modeling** | The expert demonstrates a task, making their internal thought processes visible.28 | AI generates a complete, idiomatic code solution for a problem and, when prompted, explains its architectural choices, trade-offs, and reasoning.30 |\n| **Coaching** | The expert observes the learner and provides real-time, task-specific feedback and hints.29 | A developer submits their code to an AI chat, which provides immediate feedback, bug fixes with explanations, and suggestions for refactoring and optimization.32 |\n| **Scaffolding** | The expert provides structural support (tools, templates) to help the learner manage tasks beyond their current ability.29 | AI generates boilerplate code, configuration files, unit test skeletons, and documentation, reducing cognitive load and allowing the learner to focus on core logic.36 |\n| **Articulation** | The learner is prompted to explain their reasoning and thought processes, making their understanding explicit.28 | The process of writing a precise, detailed prompt forces the developer to articulate their mental model of the problem. A poor AI response often signals a need for clearer articulation.37 |\n| **Reflection** | The learner compares their performance and processes to those of an expert or an ideal model.29 | The developer must critically evaluate every AI code suggestion for correctness"
  },
  {
    "id": "report_source",
    "chunk": "e learner compares their performance and processes to those of an expert or an ideal model.29 | The developer must critically evaluate every AI code suggestion for correctness, security, and quality, constantly comparing the AI's output against their own internal standards.33 |\n| **Fading & Exploration** | The expert gradually withdraws support, encouraging the learner to work independently and test new skills.30 | As proficiency grows, the developer naturally reduces reliance on the AI, using it as a safety net that lowers the risk of exploring new libraries, languages, or design patterns.32 |\n\n### **Cultivating Metacognition and \"Meta AI\" Skills**\n\nThe ultimate objective of the V2V curriculum, and indeed any effective implementation of Cognitive Apprenticeship, is not to create dependence on the mentor but to foster independent, expert practitioners. In the context of AI-assisted development, this translates to cultivating developers with advanced metacognitive skills who can strategically and critically manage their collaboration with AI. This capability can be termed \"Meta AI Skill.\"  \nThe importance of this focus is underscored by research indicating that the productivity benefits of generative AI are not uniform; they disproportionately accrue to individuals with high metacognitive abilitythe capacity to think about one's own thinking.42 As one analysis puts it, a \"weak cognitive strategy plus AI yields faster mediocrity\".42 Therefore, the V2V curriculum must explicitly aim to enhance these metacognitive faculties.  \n\"Meta AI Skill\" can be defined as the ability to consciously monitor, manage, and critically evaluate one's use of AI tools in a professional software development context.43 This is a multi-faceted co"
  },
  {
    "id": "report_source",
    "chunk": "fined as the ability to consciously monitor, manage, and critically evaluate one's use of AI tools in a professional software development context.43 This is a multi-faceted competency that includes:\n\n* **Strategic Delegation:** Knowing which tasks are suitable for AI (e.g., boilerplate, repetitive code, initial drafts) and which require deep human oversight (e.g., core business logic, security-critical sections, final architectural decisions).39  \n* **Critical Validation:** Resisting \"automation bias\" and treating every AI suggestion as a hypothesis to be verified, rather than a fact to be accepted.33 This involves a deep-seated practice of reviewing, testing, and understanding all AI-generated code before integration.  \n* **Workflow Design:** Structuring personal and team workflows to maximize the benefits of AI while mitigating its risks. This includes practices like breaking problems into smaller, AI-manageable chunks and committing code frequently to avoid getting lost in AI-generated rabbit holes.33  \n* **Ethical and Responsible Use:** Understanding the limitations of AI, including its potential for bias, security vulnerabilities, and intellectual property complications, and navigating these challenges responsibly.43\n\nAI tools themselves can be leveraged to develop these very skills. For instance, an instructor can design an assignment where students use an AI to generate feedback on their work, and then the students' primary task is to write a critique of the AI's feedback, identifying its strengths and weaknesses.43 This forces a meta-level analysis of the AI's capabilities. Similarly, using AI to generate summaries or mind maps of complex topics can help students \"visualize their comprehension gaps and refine the"
  },
  {
    "id": "report_source",
    "chunk": "nalysis of the AI's capabilities. Similarly, using AI to generate summaries or mind maps of complex topics can help students \"visualize their comprehension gaps and refine their reflection processes,\" a core metacognitive activity.45  \nThe integration of powerful AI assistants into the development workflow fundamentally reframes the role of the senior developer. As AI takes on an increasing share of the direct implementation or \"driver\" taskswriting functions, completing lines of code, generating teststhe human's primary value shifts decisively toward higher-order cognitive and metacognitive functions. The human becomes the system's indispensable \"Chief Validation Officer.\" This role is defined by strategic planning, architectural oversight, and, most importantly, the critical validation of all system components, whether human- or AI-generated. The AI provides speed and breadth of knowledge; the human provides judgment, context, and accountability. The V2V curriculum must be explicitly designed to train developers for this elevated role. Its success should be measured not by how much faster its graduates can code, but by how much more effectively they can think, validate, and architect within a human-AI collaborative system.  \n---\n\n## **Part III: Synthesis and Curriculum Blueprint \\- The Vibecoding to Virtuosity Pathway**\n\nThis final part of the report synthesizes the technical paradigm of Context Engineering and the pedagogical framework of Cognitive Apprenticeship into a concrete, multi-stage curriculum blueprint. It begins with an analysis of the existing educational market to identify a strategic niche for the V2V program, then details the proposed V2V pathway, and concludes with recommendations for signature learn"
  },
  {
    "id": "report_source",
    "chunk": "he existing educational market to identify a strategic niche for the V2V program, then details the proposed V2V pathway, and concludes with recommendations for signature learning activities and capstone projects.\n\n### **Analysis of the Existing Educational Landscape**\n\nA critical review of the current educational offerings for AI-assisted software development reveals a consistent but limited focus. Courses available on major platforms like Coursera, DeepLearning.AI, and Microsoft Learn provide a solid foundation in using AI as a productivity tool but leave a significant gap in teaching the more advanced architectural and systems-thinking principles that define true expertise in the field. This gap represents the primary strategic opportunity for the V2V curriculum.  \nExisting courses from these providers tend to coalesce around a common set of topics.44 A typical curriculum includes:\n\n* **LLM Fundamentals:** An introduction to how large language models work.  \n* **Pair Programming with AI:** Practical guidance on using tools like GitHub Copilot and ChatGPT as a day-to-day coding partner to write, refactor, and complete code.44  \n* **AI for Discrete SDLC Tasks:** Modules focused on leveraging AI for specific, well-defined tasks within the software development lifecycle, such as generating unit tests, debugging code, writing documentation, and managing dependencies.46  \n* **Prompt Engineering for Developers:** Best practices for crafting effective prompts to guide AI tools in a development context, including techniques for summarizing, transforming, and expanding text.49\n\nWhile this content is valuable and necessary, it is heavily weighted towards teaching the developer how to *use* an AI as an assistant within a largely t"
  },
  {
    "id": "report_source",
    "chunk": "nd expanding text.49\n\nWhile this content is valuable and necessary, it is heavily weighted towards teaching the developer how to *use* an AI as an assistant within a largely traditional workflow. The identified gap is the lack of curricula focused on teaching the developer how to *architect* the intelligent systems within which these assistants operate. There is a dearth of structured education on the principles of Context Engineeringhow to build the RAG pipelines, memory systems, and tool integrations that enable reliable agentic behavior. Furthermore, there is almost no pedagogical content available on the frontier of Agentic Engineeringhow to design systems that can learn and improve from their own operational feedback.  \nThis gap is validated by an analysis of practitioner discussions in community forums like Hacker News and Reddit.33 While developers are actively discovering and sharing best practices for *using* AI tools (e.g., the importance of breaking down problems, the necessity of validating all output), they are largely teaching themselves the more advanced architectural concepts through trial and error. This signals a clear and unmet market need for expert-led, structured education that goes beyond tool usage and delves into the systems-level design of context-aware AI applications. The V2V curriculum is perfectly positioned to fill this niche.\n\n### **The V2V Curriculum Framework \\- A Staged Approach**\n\nTo address the identified gap and guide learners along a deliberate path from tactical proficiency to strategic mastery, a three-stage curriculum framework is proposed. This framework is designed to mirror the progression from \"Vibecoding\"the intuitive, often ad-hoc use of AI toolsto \"Virtuosity\"the prin"
  },
  {
    "id": "report_source",
    "chunk": "e curriculum framework is proposed. This framework is designed to mirror the progression from \"Vibecoding\"the intuitive, often ad-hoc use of AI toolsto \"Virtuosity\"the principled, systematic design of intelligent, self-improving systems. Each stage builds upon the last, progressively deepening both the technical skills and the corresponding focus within the Cognitive Apprenticeship model.  \n**Stage 1: The AI-Augmented Developer (Foundations \\- \"Vibecoding\")**\n\n* **Core Competency:** Proficiently using AI as a high-leverage tool to accelerate the traditional software development lifecycle. This stage masters the current state-of-the-art in AI-assisted development as taught by existing programs.  \n* **Skills & Concepts:** Advanced pair programming techniques with AI 32; effective prompting patterns for developers (e.g., persona, few-shot, chain-of-thought) 49; AI-assisted testing, debugging, and documentation generation 46; and a strong foundation in responsible AI use, including awareness of limitations, biases, and ethical considerations.40  \n* **Cognitive Apprenticeship Focus:** This stage heavily emphasizes **Modeling** and **Coaching**. The AI serves primarily as an expert model, demonstrating how to solve problems, and as a real-time coach, providing immediate feedback on the learner's code.\n\n**Stage 2: The Context-Aware Architect (Intermediate)**\n\n* **Core Competency:** Designing and building the context pipelines and information systems that enable reliable, scalable, and stateful AI agent performance. This stage moves beyond using AI as a tool to architecting the environment in which the tool operates.  \n* **Skills & Concepts:** The full Context Engineering paradigm 4; advanced context window management strateg"
  },
  {
    "id": "report_source",
    "chunk": "tool to architecting the environment in which the tool operates.  \n* **Skills & Concepts:** The full Context Engineering paradigm 4; advanced context window management strategies (chunking, map-reduce, refine) 20; practical implementation of Retrieval-Augmented Generation (RAG) pipelines using vector databases; tool integration and API calling; and designing short-term and long-term memory systems for agents.2  \n* **Cognitive Apprenticeship Focus:** The emphasis shifts to **Scaffolding** and **Articulation**. The learner is now building the scaffolding (the context systems) that supports the AI's performance. This process requires a high degree of *articulation*, as designing an effective information architecture forces the developer to explicitly define and structure the entire problem space.\n\n**Stage 3: The Agentic Systems Designer (Advanced \\- \"Virtuosity\")**\n\n* **Core Competency:** Architecting and implementing self-improving AI systems that can learn and adapt from execution feedback. This stage represents the frontier of AI application development.  \n* **Skills & Concepts:** The principles of Agentic Context Engineering (ACE) 22; designing and implementing Generator-Reflector-Curator loops; leveraging environmental success/failure signals for automated learning 23; and principles of multi-agent orchestration and communication.1  \n* **Cognitive Apprenticeship Focus:** The final stage focuses on **Reflection** and **Exploration**. The learner is tasked with building systems that codify the reflective process itself (the Reflector agent). This enables the creation of agents that can engage in autonomous *exploration*, testing new strategies and evolving their own \"playbooks\" without direct human intervention.\n\n| Stage"
  },
  {
    "id": "report_source",
    "chunk": "ables the creation of agents that can engage in autonomous *exploration*, testing new strategies and evolving their own \"playbooks\" without direct human intervention.\n\n| Stage Title | Core Competency | Key Concepts & Skills | Primary Tools & Frameworks | Cognitive Apprenticeship Focus |\n| :---- | :---- | :---- | :---- | :---- |\n| **Stage 1: The AI-Augmented Developer** | Proficiently using AI as a high-leverage tool to accelerate the traditional SDLC. | AI Pair Programming, Advanced Prompt Engineering, AI-Assisted Testing & Debugging, Responsible AI Use.32 | GitHub Copilot, ChatGPT, Cursor, IDE-integrated Chat. | **Modeling** & **Coaching** |\n| **Stage 2: The Context-Aware Architect** | Designing and building context pipelines and information systems for reliable AI agents. | Context Engineering Principles, Context Window Management, RAG, Tool Integration, Memory Systems.4 | LangChain/LlamaIndex, Vector Databases (e.g., Pinecone, Chroma), API Orchestration. | **Scaffolding** & **Articulation** |\n| **Stage 3: The Agentic Systems Designer** | Architecting and implementing self-improving AI systems that learn from execution feedback. | Agentic Context Engineering (ACE), Generator-Reflector-Curator Loops, Learning from Execution Feedback, Multi-Agent Orchestration.22 | LangGraph, CrewAI, Custom Agentic Frameworks, Automated Testing Environments. | **Reflection** & **Exploration** |\n\n### **Signature Pedagogies and Capstone Projects**\n\nTo translate this framework into a compelling and effective learning experience, the curriculum should be anchored by hands-on, project-based \"signature pedagogies\" that are deeply aligned with the principles of Cognitive Apprenticeship.  \n**Stage 1 Pedagogies:**\n\n* **Signature Activity: \"Refact"
  },
  {
    "id": "report_source",
    "chunk": "ds-on, project-based \"signature pedagogies\" that are deeply aligned with the principles of Cognitive Apprenticeship.  \n**Stage 1 Pedagogies:**\n\n* **Signature Activity: \"Refactor and Reflect.\"** Learners are provided with a piece of poorly written or outdated legacy code. Their task is to use an AI assistant to refactor the code to modern standards of readability, performance, and security. The deliverable is not just the refactored code but also a \"Reflection Log\" where they document the AI's key suggestions, justify which suggestions they accepted or rejected, and explain their reasoning. This activity directly trains the core Meta AI Skills of critical validation and *Reflection*.37  \n* **Signature Activity: \"The Prompt Gauntlet.\"** Learners are given a single, well-defined coding problem (e.g., \"implement a REST API endpoint for user authentication\"). They must solve this problem multiple times, each time using a different, prescribed prompting strategy (e.g., zero-shot, few-shot with examples, persona pattern, chain-of-thought prompting).4 This builds a deep, practical intuition for how different prompting techniques shape AI behavior and output quality.\n\n**Stage 2 Pedagogies:**\n\n* **Capstone Project: \"The Knowledgeable Assistant.\"** Learners are tasked with building a question-answering chatbot for a specific, complex domain, such as a company's internal technical documentation or a set of legal policies. To succeed, they must implement a full RAG pipeline from scratch: chunking the source documents, generating embeddings, storing them in a vector database, and implementing a retrieval mechanism that injects the relevant context into the LLM's prompt at query time. This project forces a hands-on application of all c"
  },
  {
    "id": "report_source",
    "chunk": "tor database, and implementing a retrieval mechanism that injects the relevant context into the LLM's prompt at query time. This project forces a hands-on application of all core **Context Engineering** principles in a real-world scenario.11\n\n**Stage 3 Pedagogies:**\n\n* **Capstone Project: \"The Self-Correcting Coder.\"** This advanced project requires learners to build a system that uses an AI to autonomously generate code that passes a series of challenging unit tests. The system must implement a simplified ACE loop: a **Generator** agent writes the code, an automated testing environment executes it and provides a binary success/failure signal, and a **Reflector** agent analyzes the test failure output (e.g., the stack trace) to generate a specific hint or insight. This insight is then added to the context for the Generator's next attempt. This project serves as a direct, hands-on implementation of the state-of-the-art principles of self-improving systems, embodying the \"virtuosity\" goal of the V2V pathway.23\n\n## **Conclusion and Recommendations**\n\nThis report has established a comprehensive foundation for the \"Vibecoding to Virtuosity\" (V2V) curriculum, grounded in the technical paradigm of Context Engineering and the pedagogical model of Cognitive Apprenticeship. The analysis reveals a clear and significant opportunity to create a best-in-class educational program that moves beyond the current market's focus on basic tool usage and instead teaches the architectural and systems-thinking skills required to build the next generation of intelligent applications.  \nThe evolution from Prompt Engineering to Context Engineering is not a fleeting trend but a fundamental maturation of the field, driven by the demands of creating "
  },
  {
    "id": "report_source",
    "chunk": "lications.  \nThe evolution from Prompt Engineering to Context Engineering is not a fleeting trend but a fundamental maturation of the field, driven by the demands of creating reliable, scalable, and stateful AI systems for the enterprise. The V2V curriculum must be built upon this modern understanding of the discipline. Simultaneously, the Cognitive Apprenticeship model provides a robust, evidence-based framework for teaching these complex skills, with AI tools themselves serving as powerful new mediums for implementing its core methods of making expert thinking visible.  \nThe ultimate goal is to cultivate \"Meta AI Skills\"the advanced metacognitive ability to strategically manage and critically validate human-AI collaboration. This reframes the developer's role, elevating them from a simple coder to an architect and \"Chief Validation Officer\" of intelligent systems.  \nBased on this analysis, the following recommendations are put forth for the V2V curriculum development team:\n\n1. **Adopt the Three-Stage Framework:** Structure the curriculum around the proposed three stagesThe AI-Augmented Developer, The Context-Aware Architect, and The Agentic Systems Designer. This provides a clear and logical progression from foundational skills to state-of-the-art expertise.  \n2. **Center the Curriculum on Signature Projects:** Implement the proposed signature pedagogies and capstone projects for each stage. These hands-on activities are essential for translating theoretical knowledge into practical skill and are designed to directly embody the principles of Cognitive Apprenticeship.  \n3. **Explicitly Teach Metacognition:** Integrate the concept of \"Meta AI Skills\" as a core learning objective throughout the curriculum. Activities sh"
  },
  {
    "id": "report_source",
    "chunk": "nitive Apprenticeship.  \n3. **Explicitly Teach Metacognition:** Integrate the concept of \"Meta AI Skills\" as a core learning objective throughout the curriculum. Activities should consistently require learners to not only use AI but also to reflect on, critique, and justify their use of AI.  \n4. **Emphasize Systems Thinking:** From Stage 2 onwards, the focus should shift decisively from individual prompts and code snippets to the design of the overall system. The curriculum should teach learners to think about information flow, state management, and the orchestration of multiple components as first-order concerns.  \n5. **Stay Aligned with the Frontier:** The field of agentic AI is evolving at an extraordinary pace. The curriculum, particularly Stage 3, must be designed for continuous updating to incorporate new research, frameworks, and best practices as they emerge, ensuring that V2V remains a leading-edge educational program.\n\nBy implementing these recommendations, the V2V pathway can provide a transformative learning experience that prepares developers not just for the software industry of today, but for the intelligent, collaborative, and agentic future of tomorrow.\n\n#### **Works cited**\n\n1. Context Engineering vs Prompt Engineering: The 2025 Guide to Building Reliable LLM Products \\- Vatsal Shah, accessed October 15, 2025, [https://vatsalshah.in/blog/context-engineering-vs-prompt-engineering-2025-guide](https://vatsalshah.in/blog/context-engineering-vs-prompt-engineering-2025-guide)  \n2. Beyond prompt engineering: the shift to context engineering | Nearform, accessed October 15, 2025, [https://nearform.com/digital-community/beyond-prompt-engineering-the-shift-to-context-engineering/](https://nearform.com/digital-com"
  },
  {
    "id": "report_source",
    "chunk": "ng | Nearform, accessed October 15, 2025, [https://nearform.com/digital-community/beyond-prompt-engineering-the-shift-to-context-engineering/](https://nearform.com/digital-community/beyond-prompt-engineering-the-shift-to-context-engineering/)  \n3. Context Engineering vs Prompt Engineering | by Mehul Gupta | Data Science in Your Pocket, accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d)  \n4. Understanding Prompt Engineering and Context Engineering, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  \n5. Context Engineering \\- The Evolution Beyond Prompt Engineering | Vinci Rufus, accessed October 15, 2025, [https://www.vincirufus.com/posts/context-engineering/](https://www.vincirufus.com/posts/context-engineering/)  \n6. Understanding Context Engineering: Principles, Practices, and Its Distinction from Prompt Engineering \\- Architecture & Governance Magazine, accessed October 15, 2025, [https://www.architectureandgovernance.com/applications-technology/understanding-context-engineering-principles-practices-and-its-distinction-from-prompt-engineering/](https://www.architectureandgovernance.com/applications-technology/understanding-context-engineering-principles-practices-and-its-distinction-from-prompt-engineering/)  \n7. Context Engineering vs Prompt Engineering \\- AI at work for all \\- secure AI agents, search, workflows \\- Shieldbase AI, accessed October 15, 2025, [https://shieldbase.ai/"
  },
  {
    "id": "report_source",
    "chunk": "\n7. Context Engineering vs Prompt Engineering \\- AI at work for all \\- secure AI agents, search, workflows \\- Shieldbase AI, accessed October 15, 2025, [https://shieldbase.ai/blog/context-engineering-vs-prompt-engineering](https://shieldbase.ai/blog/context-engineering-vs-prompt-engineering)  \n8. Context engineering is just software engineering for LLMs \\- Inngest Blog, accessed October 15, 2025, [https://www.inngest.com/blog/context-engineering-is-software-engineering-for-llms](https://www.inngest.com/blog/context-engineering-is-software-engineering-for-llms)  \n9. Context engineering: Why it's Replacing Prompt Engineering for ..., accessed October 15, 2025, [https://www.gartner.com/en/articles/context-engineering](https://www.gartner.com/en/articles/context-engineering)  \n10. Prompt Engineering Is Dead, and Context Engineering Is Already Obsolete: Why the Future Is Automated Workflow Architecture with LLMs \\- OpenAI Developer Community, accessed October 15, 2025, [https://community.openai.com/t/prompt-engineering-is-dead-and-context-engineering-is-already-obsolete-why-the-future-is-automated-workflow-architecture-with-llms/1314011](https://community.openai.com/t/prompt-engineering-is-dead-and-context-engineering-is-already-obsolete-why-the-future-is-automated-workflow-architecture-with-llms/1314011)  \n11. Context Engineering: The Evolution Beyond Prompt Engineering \\- Hugging Face, accessed October 15, 2025, [https://huggingface.co/blog/Svngoku/context-engineering-the-evolution-beyond-prompt-en](https://huggingface.co/blog/Svngoku/context-engineering-the-evolution-beyond-prompt-en)  \n12. Context Engineering ( RAG 2.0 ) : The Next Chapter in GenAI \\- Medium, accessed October 15, 2025, [https://medium.com/@ramakrishna.san"
  },
  {
    "id": "report_source",
    "chunk": "ng-the-evolution-beyond-prompt-en)  \n12. Context Engineering ( RAG 2.0 ) : The Next Chapter in GenAI \\- Medium, accessed October 15, 2025, [https://medium.com/@ramakrishna.sanikommu/context-engineering-rag-2-0-the-next-chapter-in-genai-4e53c0382bf4](https://medium.com/@ramakrishna.sanikommu/context-engineering-rag-2-0-the-next-chapter-in-genai-4e53c0382bf4)  \n13. Context Engineering vs. Prompt Engineering: Smarter AI with RAG & Agents \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=vD0E3EUb8-8](https://www.youtube.com/watch?v=vD0E3EUb8-8)  \n14. Effective context engineering for AI agents \\- Anthropic, accessed October 15, 2025, [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)  \n15. What is a context window? \\- IBM, accessed October 15, 2025, [https://www.ibm.com/think/topics/context-window](https://www.ibm.com/think/topics/context-window)  \n16. AI Prompting (3/10): Context Windows ExplainedTechniques Everyone Should Know : r/PromptEngineering \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1iftklk/ai\\_prompting\\_310\\_context\\_windows/](https://www.reddit.com/r/PromptEngineering/comments/1iftklk/ai_prompting_310_context_windows/)  \n17. Everybody is talking about how context engineering is replacing prompt engineering nowadays. But what really is this new buzzword? : r/AI\\_Agents \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/AI\\_Agents/comments/1mq935t/everybody\\_is\\_talking\\_about\\_how\\_context/](https://www.reddit.com/r/AI_Agents/comments/1mq935t/everybody_is_talking_about_how_context/)  \n18. Quality over Quantity: 3 Tip"
  },
  {
    "id": "report_source",
    "chunk": "t/everybody\\_is\\_talking\\_about\\_how\\_context/](https://www.reddit.com/r/AI_Agents/comments/1mq935t/everybody_is_talking_about_how_context/)  \n18. Quality over Quantity: 3 Tips for Context Window Management \\- Tilburg.ai, accessed October 15, 2025, [https://tilburg.ai/2025/03/context-window-management/](https://tilburg.ai/2025/03/context-window-management/)  \n19. Top techniques to Manage Context Lengths in LLMs \\- Agenta, accessed October 15, 2025, [https://agenta.ai/blog/top-6-techniques-to-manage-context-length-in-llms](https://agenta.ai/blog/top-6-techniques-to-manage-context-length-in-llms)  \n20. MCP Context Window Management \\- Tetrate, accessed October 15, 2025, [https://tetrate.io/learn/ai/mcp/context-window-management](https://tetrate.io/learn/ai/mcp/context-window-management)  \n21. Context Window Optimizing Strategies in Gen AI Applications \\- Cloudkitect, accessed October 15, 2025, [https://cloudkitect.com/context-window-optimizing-strategies-in-gen-ai-applications/](https://cloudkitect.com/context-window-optimizing-strategies-in-gen-ai-applications/)  \n22. \\[2510.04618\\] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models \\- arXiv, accessed October 15, 2025, [https://www.arxiv.org/abs/2510.04618](https://www.arxiv.org/abs/2510.04618)  \n23. Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2510.04618v1](https://arxiv.org/html/2510.04618v1)  \n24. Agentic Context Engineering: Teaching Language Models to Learn from Experience | by Bing \\- Medium, accessed October 15, 2025, [https://medium.com/@bingqian/agentic-context-engineering-teaching-language-models-to-learn-from-experience-706c31a872ca]("
  },
  {
    "id": "report_source",
    "chunk": "ence | by Bing \\- Medium, accessed October 15, 2025, [https://medium.com/@bingqian/agentic-context-engineering-teaching-language-models-to-learn-from-experience-706c31a872ca](https://medium.com/@bingqian/agentic-context-engineering-teaching-language-models-to-learn-from-experience-706c31a872ca)  \n25. Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models, accessed October 15, 2025, [https://www.alphaxiv.org/overview/2510.04618v1](https://www.alphaxiv.org/overview/2510.04618v1)  \n26. accessed December 31, 1969, [https://arxiv.org/abs/2510.04618](https://arxiv.org/abs/2510.04618)  \n27. Paper page \\- Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models \\- Hugging Face, accessed October 15, 2025, [https://huggingface.co/papers/2510.04618](https://huggingface.co/papers/2510.04618)  \n28. (PDF) The cognitive apprenticeship model in educational practice \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/312341574\\_The\\_cognitive\\_apprenticeship\\_model\\_in\\_educational\\_practice](https://www.researchgate.net/publication/312341574_The_cognitive_apprenticeship_model_in_educational_practice)  \n29. Investigating the Impact of the Stratified Cognitive Apprenticeship Model on High School Students' Math Performance \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2227-7102/14/8/898](https://www.mdpi.com/2227-7102/14/8/898)  \n30. Navigating New Frontier: AI's Transformation of Dissertation ..., accessed October 15, 2025, [https://files.eric.ed.gov/fulltext/EJ1462199.pdf](https://files.eric.ed.gov/fulltext/EJ1462199.pdf)  \n31. Cognitive Apprenticeship and Artificial Intelligence Coding ..., accessed October 15, 2025, [https://www.researchga"
  },
  {
    "id": "report_source",
    "chunk": "df](https://files.eric.ed.gov/fulltext/EJ1462199.pdf)  \n31. Cognitive Apprenticeship and Artificial Intelligence Coding ..., accessed October 15, 2025, [https://www.researchgate.net/publication/378823978\\_Cognitive\\_Apprenticeship\\_and\\_Artificial\\_Intelligence\\_Coding\\_Assistants](https://www.researchgate.net/publication/378823978_Cognitive_Apprenticeship_and_Artificial_Intelligence_Coding_Assistants)  \n32. Pair Programming & TDD in 2025: Evolving or Obsolete in an AIFirst Era | by Pravir Raghu, accessed October 15, 2025, [https://medium.com/@pravir.raghu/pair-programming-tdd-in-2025-evolving-or-obsolete-in-an-ai-first-era-00680ce93695](https://medium.com/@pravir.raghu/pair-programming-tdd-in-2025-evolving-or-obsolete-in-an-ai-first-era-00680ce93695)  \n33. After 7 years, I'm finally coding again, thanks to Cursor ... \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/webdev/comments/1n2a1nu/after\\_7\\_years\\_im\\_finally\\_coding\\_again\\_thanks\\_to/](https://www.reddit.com/r/webdev/comments/1n2a1nu/after_7_years_im_finally_coding_again_thanks_to/)  \n34. The Effect of AI Based Scaffolding on Problem Solving and Metacognitive Awareness in Learners \\- ResearchGate, accessed October 15, 2025, [https://www.researchgate.net/publication/394235327\\_The\\_Effect\\_of\\_AI\\_Based\\_Scaffolding\\_on\\_Problem\\_Solving\\_and\\_Metacognitive\\_Awareness\\_in\\_Learners](https://www.researchgate.net/publication/394235327_The_Effect_of_AI_Based_Scaffolding_on_Problem_Solving_and_Metacognitive_Awareness_in_Learners)  \n35. AI-Integrated Scaffolding to Enhance Agency and Creativity in K-12 English Language Learners: A Systematic Review \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2078-2489/16/7/519](https://www.mdpi.com/2078"
  },
  {
    "id": "report_source",
    "chunk": "y and Creativity in K-12 English Language Learners: A Systematic Review \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2078-2489/16/7/519](https://www.mdpi.com/2078-2489/16/7/519)  \n36. The effects of artificial intelligence-based interactive scaffolding on ..., accessed October 15, 2025, [https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2470319](https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2470319)  \n37. I Spent 30 Days Pair Programming with AIHere's What It Taught ..., accessed October 15, 2025, [https://dev.to/arpitstack/i-spent-30-days-pair-programming-with-ai-heres-what-it-taught-me-4dal](https://dev.to/arpitstack/i-spent-30-days-pair-programming-with-ai-heres-what-it-taught-me-4dal)  \n38. This Simple Prompt Saved Me Hours of Debugging AI-Generated Code : r/cursor \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/cursor/comments/1hwt5nx/this\\_simple\\_prompt\\_saved\\_me\\_hours\\_of\\_debugging/](https://www.reddit.com/r/cursor/comments/1hwt5nx/this_simple_prompt_saved_me_hours_of_debugging/)  \n39. Pair Programming with AI: Tips to Get the Most from Your Coding ..., accessed October 15, 2025, [https://www.gocodeo.com/post/pair-programming-with-ai-tips-to-get-the-most-from-your-coding-assistant](https://www.gocodeo.com/post/pair-programming-with-ai-tips-to-get-the-most-from-your-coding-assistant)  \n40. What I've Learned from AI-Assisted Programming \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/programming/comments/1hovxjb/what\\_ive\\_learned\\_from\\_aiassisted\\_programming/](https://www.reddit.com/r/programming/comments/1hovxjb/what_ive_learned_from_aiassisted_programming/)  \n41. AI helps math teachers build better \"scaffolds\" \\- Stanford Accelerator for Le"
  },
  {
    "id": "report_source",
    "chunk": "w.reddit.com/r/programming/comments/1hovxjb/what_ive_learned_from_aiassisted_programming/)  \n41. AI helps math teachers build better \"scaffolds\" \\- Stanford Accelerator for Learning, accessed October 15, 2025, [https://acceleratelearning.stanford.edu/story/ai-helps-math-teachers-build-better-scaffolds/](https://acceleratelearning.stanford.edu/story/ai-helps-math-teachers-build-better-scaffolds/)  \n42. Metacognition Is the Key to Unlocking AI Productivity at Work \\- Reworked, accessed October 15, 2025, [https://www.reworked.co/learning-development/metacognition-your-ai-productivity-edge/](https://www.reworked.co/learning-development/metacognition-your-ai-productivity-edge/)  \n43. Beyond Digital Literacy: Cultivating Meta AI Skills in Students and ..., accessed October 15, 2025, [https://www.facultyfocus.com/articles/teaching-with-technology-articles/beyond-digital-literacy-cultivating-meta-ai-skills-in-students-and-faculty/](https://www.facultyfocus.com/articles/teaching-with-technology-articles/beyond-digital-literacy-cultivating-meta-ai-skills-in-students-and-faculty/)  \n44. GitHub Copilot Fundamentals Part 1 of 2 \\- Training | Microsoft Learn, accessed October 15, 2025, [https://learn.microsoft.com/en-us/training/paths/copilot/](https://learn.microsoft.com/en-us/training/paths/copilot/)  \n45. acbspjournal.org, accessed October 15, 2025, [https://acbspjournal.org/2025/06/01/beyond-content-leveraging-ai-and-metacognitive-strategies-for-transformative-learning-in-higher-education/\\#:\\~:text=AI%20tools%20like%20NotebookLM%20enhance,and%20refine%20their%20reflection%20processes.](https://acbspjournal.org/2025/06/01/beyond-content-leveraging-ai-and-metacognitive-strategies-for-transformative-learning-in-higher-education/#:"
  },
  {
    "id": "report_source",
    "chunk": "eir%20reflection%20processes.](https://acbspjournal.org/2025/06/01/beyond-content-leveraging-ai-and-metacognitive-strategies-for-transformative-learning-in-higher-education/#:~:text=AI%20tools%20like%20NotebookLM%20enhance,and%20refine%20their%20reflection%20processes.)  \n46. Advanced GenAI Development Practices | Coursera, accessed October 15, 2025, [https://www.coursera.org/learn/advanced-genai-development-practices](https://www.coursera.org/learn/advanced-genai-development-practices)  \n47. Generative AI for Software Development \\- DeepLearning.AI, accessed October 15, 2025, [https://www.deeplearning.ai/courses/generative-ai-for-software-development/](https://www.deeplearning.ai/courses/generative-ai-for-software-development/)  \n48. Generative AI for Software Development Skill Certificate \\- Coursera, accessed October 15, 2025, [https://www.coursera.org/professional-certificates/generative-ai-for-software-development](https://www.coursera.org/professional-certificates/generative-ai-for-software-development)  \n49. ChatGPT Prompt Engineering for Developers \\- DeepLearning.AI, accessed October 15, 2025, [https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)  \n50. Prompt Engineering for ChatGPT by Vanderbilt \\- Coursera, accessed October 15, 2025, [https://www.coursera.org/learn/prompt-engineering](https://www.coursera.org/learn/prompt-engineering)  \n51. Tips for programmers to stay ahead of generative AI | Hacker News, accessed October 15, 2025, [https://news.ycombinator.com/item?id=36586248](https://news.ycombinator.com/item?id=36586248)  \n52. Generative AI and the widening software developer knowledge ga"
  },
  {
    "id": "report_source",
    "chunk": "r 15, 2025, [https://news.ycombinator.com/item?id=36586248](https://news.ycombinator.com/item?id=36586248)  \n52. Generative AI and the widening software developer knowledge gap | Hacker News, accessed October 15, 2025, [https://news.ycombinator.com/item?id=39603163](https://news.ycombinator.com/item?id=39603163)  \n53. Context Engineering for Agents \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=4GiqzUHD5AA](https://www.youtube.com/watch?v=4GiqzUHD5AA)\n</file_artifact>\n\n<file path=\"context/v2v/research-proposals/09-V2V Pathway Research Proposal.md\">\n\n\n# **From Context to Cognition: A Foundational Report for the Vibecoding to Virtuosity Curriculum**\n\n## **Section 1: The Architectural Shift from Prompts to Context**\n\nThe development of applications leveraging large language models (LLMs) is undergoing a significant and rapid maturation. The initial phase, characterized by the craft of \"Prompt Engineering,\" is giving way to a more rigorous and systematic discipline: \"Context Engineering.\" This evolution is not merely a change in terminology but a fundamental shift in the architectural paradigm for building reliable, scalable, and intelligent AI systems. It redefines the role of the developer from that of a linguistic artist, crafting individual instructions, to that of a cognitive architect, designing the entire information environment in which an AI agent operates. This section will establish this foundational technical paradigm, demonstrating that while Prompt Engineering is a necessary skill, Context Engineering is the engineering discipline required for building the next generation of AI-powered applications. It will deconstruct this new paradigm, analyze its core components, and explore its traj"
  },
  {
    "id": "report_source",
    "chunk": "ering discipline required for building the next generation of AI-powered applications. It will deconstruct this new paradigm, analyze its core components, and explore its trajectory toward creating self-improving, agentic systems.\n\n### **1.1 From Instruction to Environment: Differentiating Prompt and Context Engineering**\n\nThe distinction between Prompt Engineering and Context Engineering is the foundational concept for understanding the construction of advanced AI systems. The former is a tactic for influencing a model's output in a single interaction, while the latter is a strategy for architecting a model's consistent and reliable performance over time. Prompt Engineering is a subset of Context Engineering, representing a crucial but limited component of a much larger system.1  \nPrompt Engineering can be understood as a form of \"linguistic tuning\".2 It is the iterative process of meticulously crafting the text inputthe promptto guide an LLM toward a desired response. This involves a range of techniques, including assigning a specific role or persona to the model (e.g., \"You are a professional translator\"), defining explicit formatting constraints (e.g., \"Provide the answer in JSON format\"), and using structured reasoning patterns like few-shot examples or chain-of-thought to illustrate the desired output logic.2 This practice is highly accessible, requiring little more than a text editor, and can be powerful for one-off tasks, generating creative variations, or producing impressive demonstrations.1 However, its core limitation is its brittleness. Minor variations in wording or the placement of examples can lead to significant and unpredictable changes in output quality, and the approach lacks mechanisms for persiste"
  },
  {
    "id": "report_source",
    "chunk": "s. Minor variations in wording or the placement of examples can lead to significant and unpredictable changes in output quality, and the approach lacks mechanisms for persistence, memory, or generalization across complex workflows.2 It is fundamentally focused on the immediate task: what to say to the model at a single moment in time.1  \nContext Engineering, in contrast, represents a shift to \"systems thinking\".2 It is the discipline of designing and managing the entire \"mental world\" or \"working memory\" in which an LLM operates.1 This is not about crafting a single, perfect instruction but about architecting automated pipelines that dynamically assemble and curate a rich set of information sources into the model's context window for each step of an interaction.2 These sources include not just the user's immediate query but also the system prompt defining the agent's core purpose, the dialogue history, real-time data retrieved from external tools and APIs, and relevant documents fetched from knowledge bases.2 The central challenge of Context Engineering is determining what the model needs to know at any given moment to perform its task reliably and why it should care about that information.1  \nThe scope is the most critical differentiator. Prompt Engineering operates *within* the context window, focusing on the clarity and structure of the instruction itself. Context Engineering is concerned with *what fills* the context window, managing the flow of information from multiple sources to frame the entire conversation and ensure consistency across sessions, users, and unexpected inputs.1 This distinction is akin to the difference between writing a single, compelling line of dialogue for a character versus directing an entir"
  },
  {
    "id": "report_source",
    "chunk": "ions, users, and unexpected inputs.1 This distinction is akin to the difference between writing a single, compelling line of dialogue for a character versus directing an entire filmmanaging the setting, backstory, props, and continuity to ensure a coherent narrative.6 This evolution is a natural progression of the field; as LLM applications move beyond simple, one-shot text generation to complex, multi-turn agentic workflows, the discrete task of writing a prompt evolves into the continuous, iterative process of curating context.7  \nThis evolution from an artisanal craft to a formal engineering discipline is a direct consequence of the changing requirements for AI systems. The initial phase of LLM adoption was driven by \"flashy demos\" and creative tasks where the \"quick-and-dirty,\" \"hit-or-miss\" nature of prompt crafting was acceptable.1 However, deploying these systems in production environmentsfor applications like customer support bots that cannot hallucinate or multi-step workflows that require predictabilitydemands a level of reliability that ad-hoc prompting cannot provide.1 The need for consistency, scalability, and maintainability in these production systems is the primary driver forcing the transition toward the more structured, systematic, and architectural approach of Context Engineering. This shift signals a professionalization of the field, moving from intuitive \"vibe coding\" to the deliberate design of robust cognitive systems.\n\n| Factor | Prompt Engineering | Context Engineering |\n| :---- | :---- | :---- |\n| **Mindset** | Creative Writing / Linguistic Tuning: Focuses on the art of wordsmithing and crafting the perfect instruction.1 | Systems Design / Software Architecture: Focuses on designing the entir"
  },
  {
    "id": "report_source",
    "chunk": "Writing / Linguistic Tuning: Focuses on the art of wordsmithing and crafting the perfect instruction.1 | Systems Design / Software Architecture: Focuses on designing the entire information flow and cognitive environment for an LLM.1 |\n| **Scope** | Single Turn / Input-Output Pair: Operates within a single interaction to elicit a specific response.1 | Multi-Turn Session / Workflow: Manages the state and information available to the model across an entire conversation or task.1 |\n| **Goal** | Specific, One-Off Response: Aims to get the best possible output for a single, discrete task.1 | Consistent, Reliable Performance: Aims to ensure the model performs well predictably across many users, sessions, and edge cases.1 |\n| **Methodology** | Wordsmithing & Formatting: Involves tweaking phrasing, providing few-shot examples, and defining output structures.2 | Information Orchestration: Involves building automated pipelines for retrieval, memory management, and tool integration.2 |\n| **Tools** | Text Editor / Prompt Box: Can be performed with basic tools like the ChatGPT interface.1 | RAG Systems, Memory Modules, APIs: Requires a backend infrastructure for managing data sources and state.1 |\n| **Scalability** | Brittle: Tends to break down with more users and complexity, requiring manual tweaks for new edge cases.1 | Robust: Designed with scale and consistency in mind from the outset to handle diverse and complex workflows.1 |\n| **Relationship** | A Subset: Prompt Engineering is a critical skill and component *within* the broader discipline of Context Engineering.1 | A Superset: Context Engineering encompasses prompt design as one of many elements to be managed within the context window.4 |\n\n### **1.2 Systemic Components of Cont"
  },
  {
    "id": "report_source",
    "chunk": "eering.1 | A Superset: Context Engineering encompasses prompt design as one of many elements to be managed within the context window.4 |\n\n### **1.2 Systemic Components of Context Engineering: RAG, Memory, and Tool Integration**\n\nA Context Engineer's primary role is to design and orchestrate the systems that populate an LLM's context window. This is not a monolithic task but involves the careful integration of several distinct architectural components, each serving a specific function in shaping the model's \"working memory.\" The most prominent of these components are Retrieval-Augmented Generation (RAG), memory management systems, and tool integration frameworks. Understanding how these pieces fit together is essential to moving beyond simple prompting and into the realm of true system design.  \nRetrieval-Augmented Generation (RAG) is a foundational tactic within the broader strategy of Context Engineering. It is an architectural pattern designed to address the inherent limitations of LLMs, such as their knowledge being frozen at the time of training and their propensity to hallucinate when faced with questions outside their training data.9 RAG works by connecting the LLM to an external knowledge base (e.g., a vector database of documents). When a user query is received, the system first retrieves relevant chunks of information from this knowledge base and then injects them into the context window alongside the user's prompt. This provides the model with timely, factual, and domain-specific information, effectively \"grounding\" its response in reality.6 It is crucial to understand that RAG is not a competitor to Context Engineering; rather, it is one of the primary mechanisms *for* engineering context.6 RAG is the system t"
  },
  {
    "id": "report_source",
    "chunk": ".6 It is crucial to understand that RAG is not a competitor to Context Engineering; rather, it is one of the primary mechanisms *for* engineering context.6 RAG is the system that provides the raw material (retrieved documents) that the Context Engineer must then prioritize and structure within the finite context window.  \nThe recent advent of models with extremely long context windows (e.g., one million tokens or more) has led some to question the continued relevance of RAG. This perspective, however, misunderstands the core challenges of context management. While a large context window offers more space, filling it indiscriminately creates significant problems. First, there are practical issues of cost and latency; processing a million tokens for every turn of a conversation is computationally expensive and slow.6 Second, and more importantly, there is the issue of model performance. Flooding the context window with excessive or irrelevant information acts as noise, which can degrade the model's ability to focus on the critical parts of the prompt, a phenomenon known as \"context confusion\".3 The quality of context matters far more than the quantity. Therefore, the need for intelligent retrievalthe ability to find and inject *only* the most relevant pieces of informationremains paramount. In this light, the more accurate framing is not that \"RAG is dead,\" but that the naive implementation of RAG is evolving into the more sophisticated and holistic discipline of Context Engineering.6  \nMemory management is another critical pillar of Context Engineering. For an AI agent to engage in a coherent, multi-turn conversation, it must have a mechanism for recalling past interactions. This is managed through two types of memory: "
  },
  {
    "id": "report_source",
    "chunk": "ring. For an AI agent to engage in a coherent, multi-turn conversation, it must have a mechanism for recalling past interactions. This is managed through two types of memory: short-term and long-term.2 Short-term memory, often referred to as chat history, pertains to the immediate conversation and allows the model to understand follow-up questions and maintain conversational flow.3 Long-term memory involves persisting information across sessions, such as user preferences, key facts, or summaries of past conversations, which allows for a personalized and continuous user experience.10 The Context Engineer's task is to design systems that manage this memory effectively, using techniques like summarization or trimming older messages to ensure the most relevant history fits within the context window without displacing other critical information.2 Poor memory management can lead to \"context poisoning,\" where an earlier hallucination or irrelevant detail is carried forward, derailing the agent's performance.3  \nFinally, tool integration allows an LLM to transcend its role as a text generator and become an actor in a digital environment. Tools are external functions or APIs that the model can call to perform actions like querying a database, booking a flight, or accessing real-time information.5 The process of engineering context for tool use involves several steps: providing the model with clear descriptions of the available tools and their parameters, invoking the chosen tool, and then feeding the tool's output back into the context window for the model to process and act upon.2 This dynamic loop of reasoning, acting, and observing is the foundation of modern AI agents.  \nThe core challenge for the developer, then, is akin to "
  },
  {
    "id": "report_source",
    "chunk": "o process and act upon.2 This dynamic loop of reasoning, acting, and observing is the foundation of modern AI agents.  \nThe core challenge for the developer, then, is akin to that of a cognitive psychologist managing the limited attention of a non-sentient intelligence. The context window is the LLM's entire field of awareness, and its performance is directly proportional to the signal-to-noise ratio within that field. The developer's job is not merely to provide information but to act as a curator and filter, protecting the core instruction from being drowned out by noisy RAG results, irrelevant chat history, or verbose tool outputs.1 This requires a deep, almost empathetic understanding of how the model \"thinks\"how it weighs different parts of its context and how easily it can be distracted. This reframes the technical task of software development into a socio-technical one, focused on managing the cognitive load and attention of an AI partner to achieve a shared goal.\n\n### **1.3 The Emergence of Agentic Systems: An Analysis of the Agentic Context Engineering (ACE) Framework**\n\nThe principles of Context Engineering provide the foundation for building reliable AI systems for specific, pre-defined tasks. However, the next frontier in AI development lies in creating systems that can learn, adapt, and improve their own performance over time based on experience. The Agentic Context Engineering (ACE) framework, as detailed in recent academic research, offers a concrete architectural pattern for achieving this goal.13 ACE represents a paradigm shift from dynamically *using* context to dynamically *improving* context, enabling the creation of self-improving systems without the need for costly and slow model retraining.  \nThe "
  },
  {
    "id": "report_source",
    "chunk": "m dynamically *using* context to dynamically *improving* context, enabling the creation of self-improving systems without the need for costly and slow model retraining.  \nThe primary goal of the ACE framework is to overcome the limitations of prior context adaptation methods, which often suffer from \"brevity bias\" (where important domain insights are lost in concise summaries) and \"context collapse\" (where iterative rewriting gradually erodes critical details over time).13 Instead of compressing or rewriting context, ACE treats it as an \"evolving playbook\"a structured, cumulative repository of strategies, rules, and insights that grows and refines itself through experience.14 This approach is designed to create a persistent, high-fidelity memory that allows an agent to learn from its successes and failures.  \nThe ACE framework operates on a three-part cycle that mimics a human learning loop: Generation, Reflection, and Curation.16\n\n1. **The Generator:** This component is the \"actor\" of the system. It receives a task and, guided by the current strategies in the context playbook, executes the task. It produces an output (e.g., a piece of code, a JSON object) and, crucially, logs the trajectory of its actions and reasoning steps.16 This log provides the raw data for the learning process.  \n2. **The Reflector:** This component is the \"analyst.\" After the Generator completes its task, the Reflector analyzes the \"execution feedback\"an automated signal from the environment that indicates success or failure (e.g., did the generated code pass its unit tests? Did the extracted JSON validate against its schema?).16 Based on this feedback, the Reflector performs a root cause analysis to identify why the task succeeded or failed an"
  },
  {
    "id": "report_source",
    "chunk": "s? Did the extracted JSON validate against its schema?).16 Based on this feedback, the Reflector performs a root cause analysis to identify why the task succeeded or failed and distills this analysis into a structured, key insight.16 For example, it might conclude, \"The code failed because the regex pattern did not account for decimal points in monetary values.\"  \n3. **The Curator:** This component is the \"librarian.\" It takes the structured insight from the Reflector and transforms it into a reusable, generalized rule or strategy. It then merges this new knowledge into the context playbook in a structured, incremental way, for instance, by adding a new rule: \"For monetary values, always use the regex pattern \\\\d+(\\\\.\\\\d+)?\".16 This updated playbook is then available to the Generator for all future tasks.\n\nThe most significant innovation of this framework is that the system's performance improves by modifying the *context* in which the LLM operates, not by altering the model's internal weights.16 This is a form of efficient, \"lifelong learning\" that allows the system to adapt to new domains and tasks by accumulating experiential knowledge. The empirical results of this approach are compelling: systems using the ACE framework have demonstrated significant performance gains, with one study reporting a \\+10.6% improvement on agent benchmarks and showing that a smaller, open-source model equipped with ACE could match the performance of a much larger, state-of-the-art proprietary model.13  \nThis framework provides a direct technical blueprint for achieving the pedagogical goal of \"virtuosity.\" Virtuosity in any complex domain is not a static state of knowledge but a dynamic capability for adaptation, self-correction, and cont"
  },
  {
    "id": "report_source",
    "chunk": "ng the pedagogical goal of \"virtuosity.\" Virtuosity in any complex domain is not a static state of knowledge but a dynamic capability for adaptation, self-correction, and continuous improvement. The Generator-Reflector-Curator loop is a direct computational analogue of the process a human expert undertakes: practice (generation), critical self-assessment (reflection), and the updating of mental models (curation). A developer who can design and implement ACE-like principles is therefore not just a user of AI tools but a builder of learning systems. This represents a fundamental step-change in skill and a tangible manifestation of what it means to progress from intuitive \"vibecoding\" to a state of engineering \"virtuosity.\"\n\n## **Section 2: The Pedagogical Landscape for AI-Driven Software Development**\n\nTo construct an effective and differentiated curriculum for the \"Vibecoding to Virtuosity\" (V2V) pathway, it is essential to first survey the existing educational landscape. An analysis of current courses and training programs offered by major online platforms reveals established pedagogical patterns, a consensus on core developer competencies, and, most importantly, significant gaps in the market. The current offerings are effective at teaching developers how to *use* AI as a discrete tool but fall short of teaching them how to *partner* with AI as a cognitive collaborator. This analysis will map the current terrain to identify the unique space the V2V curriculum is positioned to occupy.\n\n### **2.1 A Comparative Analysis of Curricula: From Foundational AI Literacy to Advanced Practices**\n\nThe current educational offerings for AI-assisted software development are largely fragmented, typically falling into one of three distin"
  },
  {
    "id": "report_source",
    "chunk": " AI Literacy to Advanced Practices**\n\nThe current educational offerings for AI-assisted software development are largely fragmented, typically falling into one of three distinct categories. This fragmentation presents an opportunity for a cohesive, integrated curriculum that guides a developer along a complete learning journey.  \nFirst, there are broad, high-level courses designed to foster general AI literacy, often targeting a non-technical audience of business leaders, managers, and students. Examples include DeepLearning.AI's \"Generative AI for Everyone\" 18 and Google's \"Fundamentals of Generative AI\".19 These courses excel at explaining the capabilities, limitations, and societal impact of AI, but they do not aim to teach practical software development skills. They build a conceptual foundation but are not a pathway to engineering proficiency.  \nSecond, and most relevant to the V2V pathway, are the specialized courses and professional certificates designed specifically for software developers. The \"Generative AI for Software Development\" Professional Certificate from DeepLearning.AI on Coursera is a prime example.20 This three-course series covers how to use LLMs to enhance productivity across the software development lifecycle, with modules on pair-coding, AI-assisted testing and documentation, and using AI for system design and database optimization.20 Similarly, Coursera's \"Advanced GenAI Development Practices\" course delves into more complex topics like multi-step prompt engineering, AI-driven API design, and full-stack integration.22 These programs represent the current state-of-the-art in formal online education for developers, focusing on applying AI to specific, practical engineering tasks.  \nThird, a new ca"
  },
  {
    "id": "report_source",
    "chunk": " programs represent the current state-of-the-art in formal online education for developers, focusing on applying AI to specific, practical engineering tasks.  \nThird, a new category of courses has emerged around the concept of \"vibe coding,\" primarily on platforms like Udemy.23 These courses often target non-coders, product managers, or entrepreneurs and focus on using natural language prompts with AI-native tools like Cursor, Lovable, and Windsurf to build fully functional applications with little to no traditional coding.23 This trend highlights a strong market demand for lowering the barrier to software creation and empowering a wider audience to build with AI.  \nFinally, there is a wealth of tool-specific training, such as the official learning paths for GitHub Copilot provided by Microsoft.24 These are essential for mastering the features and functionalities of a particular tool but, by design, do not typically address the broader, tool-agnostic principles of AI collaboration and workflow design.  \nThe following table provides a comparative synthesis of these offerings, illustrating the current state of the educational market.\n\n| Course/Certificate Title | Provider | Target Audience | Key Learning Objectives | Tools Taught |\n| :---- | :---- | :---- | :---- | :---- |\n| **Generative AI for Software Development** | DeepLearning.AI (Coursera) | Software Developers (Beginner-Intermediate) | Optimize code quality; enhance team collaboration; design AI-guided architectures; learn how LLMs work.20 | ChatGPT, LLMs (general) |\n| **Advanced GenAI Development Practices** | Coursera | Software Developers (Intermediate) | Construct multi-step prompts; design AI-driven APIs and databases; integrate AI across the full stack.22 | Ge"
  },
  {
    "id": "report_source",
    "chunk": "ent Practices** | Coursera | Software Developers (Intermediate) | Construct multi-step prompts; design AI-driven APIs and databases; integrate AI across the full stack.22 | Generative AI Tools (general) |\n| **GitHub Copilot Fundamentals** | Microsoft Learn | Developers, DevOps Engineers, Students | Understand Copilot features; use Copilot responsibly; apply advanced prompting; use across IDE, Chat, and CLI.24 | GitHub Copilot |\n| **The Complete Vibe Coding for Non-coders Guide** | Udemy | Non-coders, Beginners, Creatives | Build apps without coding; write effective natural language prompts; rapid prototyping.23 | Windsurf, Lovable, Cursor |\n| **Vibe Coding: AI-Driven Software Development and Testing** | Udemy | Developers, Product Managers | Build apps with AI agents; AI-guided debugging and refinement; version control and testing.23 | Cursor, Windsurf, GitHub Copilot, Lovable |\n| **The Complete AI Coding Course (2025)** | Udemy | Developers, SaaS Builders | Build web and mobile apps with AI; AI-assisted development from idea to deployment.23 | Cursor AI, Claude Code, ChatGPT |\n\nA critical analysis of these curricula reveals a significant pedagogical gap. The existing courses are highly effective at teaching developers how to *use* AI as a powerful tool to accomplish discrete, well-defined tasksfor example, \"use an LLM to generate unit tests\" or \"use Copilot to complete a function.\" This task-oriented approach treats the AI as a form of intelligent automation, a superior version of autocomplete or a conversational search engine. However, this approach fails to address the deeper, more complex skills required to truly *partner* with an AI in a collaborative workflow. The V2V pathway's emphasis on Cognitive Apprenticeship"
  },
  {
    "id": "report_source",
    "chunk": "ach fails to address the deeper, more complex skills required to truly *partner* with an AI in a collaborative workflow. The V2V pathway's emphasis on Cognitive Apprenticeship suggests a different kind of relationshipone of co-creation, mentorship, and joint problem-solving. This requires a different set of skills: the ability to strategically guide an AI through an ambiguous problem, the critical judgment to interpret and question the AI's suggestions, and the metacognitive awareness to co-debug a flawed solution that was jointly created. The current educational market is focused on the immediate productivity gains of AI tools, which is the \"low-hanging fruit.\" The more challenging, but ultimately more valuable and enduring, skill is mastering the cognitive workflow of human-AI collaboration. This is the unoccupied territory where the V2V curriculum can establish itself as a leader.\n\n### **2.2 Core Competencies for the AI-Assisted Developer**\n\nDespite the fragmentation in approach and target audience, a synthesis of the leading developer-focused curricula reveals a clear consensus on a set of foundational competencies. These skills represent the \"table stakes\" for any modern software developer seeking to effectively integrate AI into their workflow. A successful curriculum must not only cover these core areas but also build upon them to teach a more profound level of collaborative intelligence.  \nThe recurring, essential skills taught across these programs include:\n\n* **Code Generation and Refinement:** This is the most fundamental application. Developers are taught to use LLMs to generate boilerplate code, implement algorithms and functions from natural language descriptions, and iteratively refactor or improve existi"
  },
  {
    "id": "report_source",
    "chunk": "velopers are taught to use LLMs to generate boilerplate code, implement algorithms and functions from natural language descriptions, and iteratively refactor or improve existing code for clarity, efficiency, or style.20  \n* **AI-Assisted Testing and Debugging:** Curricula consistently emphasize using AI as a partner in quality assurance. This includes prompting an LLM to identify potential bugs, explain error messages, suggest fixes, and, most commonly, generate comprehensive unit tests for existing code.20  \n* **Documentation and Learning:** AI tools are positioned as powerful aids for comprehension and communication. Developers learn to use them to generate clear documentation for functions and classes, explain complex or unfamiliar codebases, and explore the application of software design patterns.20  \n* **AI-Guided System Design:** More advanced courses move beyond line-level code to higher levels of abstraction. They teach students how to leverage AI as a brainstorming partner for architectural decisions, API design, and the creation of database schemas from high-level requirements.20  \n* **Full-Stack and Multi-Layer Integration:** The most advanced curricula address the challenge of coordinating development across the entire software stack. This involves using AI to ensure consistency and resolve integration issues between the front-end, back-end, and database layers of an application.22  \n* **Foundational Prompt Engineering:** Underlying all these competencies is the skill of Prompt Engineering. Developers must learn how to craft clear, context-rich prompts that effectively guide the AI to perform each of the tasks listed above.20\n\nThe emergence of this consistent set of competencies signals a fundamental shift in"
  },
  {
    "id": "report_source",
    "chunk": "xt-rich prompts that effectively guide the AI to perform each of the tasks listed above.20\n\nThe emergence of this consistent set of competencies signals a fundamental shift in the nature of the software developer's role. The emphasis is moving away from the direct, manual implementation of every line of code and toward a higher-level, more strategic function. The verbs used in the learning objectives of these courses\"partner with,\" \"leverage,\" \"guided by\"are telling.20 They imply that the developer's primary activities are becoming specification, review, and integration. The human developer is increasingly the architect and the quality control engineer, while the AI is the tireless and infinitely fast implementation engine. This evolution gives rise to a \"meta-developer\" role. As AI tools become more proficient at the micro-level tasks of writing code, the differentiating value of human developers will increasingly lie in their macro-level skills: their ability to decompose complex problems, their strategic thinking, their holistic understanding of the system and its requirements, and their strong sense of product vision. A forward-looking curriculum must therefore be designed to explicitly cultivate these meta-skills. It is not enough to teach a student how to use AI to debug; the curriculum must teach them how to formulate a comprehensive debugging strategy for their AI partner to execute.\n\n### **2.3 Tool-Specific Pedagogy: A Case Study on GitHub Copilot Training**\n\nAn examination of the official training materials for a ubiquitous tool like GitHub Copilot provides a valuable model for foundational instruction. It also clearly illustrates the limitations of a purely feature-focused pedagogical approach, thereby reinf"
  },
  {
    "id": "report_source",
    "chunk": "Hub Copilot provides a valuable model for foundational instruction. It also clearly illustrates the limitations of a purely feature-focused pedagogical approach, thereby reinforcing the need for the more process-oriented methodology proposed by the V2V pathway.  \nMicrosoft's \"GitHub Copilot Fundamentals\" learning path is a well-structured and comprehensive introduction to the tool.24 It guides the learner through the essential knowledge required for competent use. The curriculum begins with the basics of installation and configuration, ensuring the user is set up for success.25 It then introduces the core concepts of prompt engineering as they apply to Copilot, teaching users how to transform comments into precise code suggestions.24 The path covers the tool's application across a variety of developer environments, including the IDE, the integrated Chat interface, and the command line, demonstrating its versatility.24 It provides concrete, practical use cases, such as a dedicated module on using Copilot to develop unit tests.24 Crucially, the curriculum also addresses higher-level concerns, with modules on the principles of responsible AI, security considerations, and the administrative features for managing Copilot in an enterprise setting.24 This official training is supplemented by a variety of third-party courses on platforms like Codecademy and YouTube, which often provide additional hands-on projects and workflow examples.27  \nThe key takeaway from analyzing these materials is that they excel at teaching the *features* of the tool. They effectively answer the question, \"What can this tool do, and how do I operate it?\" However, they are less focused on the deeper, more nuanced question of, \"How do I integrate this t"
  },
  {
    "id": "report_source",
    "chunk": "ively answer the question, \"What can this tool do, and how do I operate it?\" However, they are less focused on the deeper, more nuanced question of, \"How do I integrate this tool into a seamless and effective cognitive workflow?\" While the training uses the language of \"AI pair programming,\" the pedagogy is primarily centered on the tool's functions rather than the collaborative *process* of pairing. It teaches the user what buttons to press but does not deeply explore the art of the human-AI partnership.  \nThis observation leads to a crucial strategic conclusion for the V2V curriculum. The kind of foundational, tool-specific knowledge provided by Microsoft is necessary, but it is not sufficient to achieve virtuosity. A developer cannot become an expert partner with Copilot without first understanding its basic features, configuration options, and limitations. The V2V pathway should not seek to replicate or replace this essential baseline training. Instead, it should build a more advanced, conceptual layer on top of it. An effective curriculum cannot be purely abstract; it must be grounded in the practical realities of the tools developers use every day. Conversely, a curriculum that is *only* about the tools will fail to teach the enduring, transferable skills of cognitive collaboration that transcend any single product. Therefore, V2V can be powerfully positioned as the \"post-graduate\" program for developers who have already achieved basic tool competency. It could even list the Microsoft Learn path as a recommended prerequisite. The unique value proposition of V2V would then be clear: it teaches the art and science of collaboration that transforms a competent tool user into an expert AI partner.\n\n## **Section 3: Reima"
  },
  {
    "id": "report_source",
    "chunk": "proposition of V2V would then be clear: it teaches the art and science of collaboration that transforms a competent tool user into an expert AI partner.\n\n## **Section 3: Reimagining Cognitive Apprenticeship in the Age of AI**\n\nThe theoretical foundation of the \"Vibecoding to Virtuosity\" pathway rests on the Cognitive Apprenticeship model. This section will argue that this well-established pedagogical framework, designed specifically for teaching complex cognitive skills, is the ideal structure for a curriculum focused on human-AI collaboration. The central thesis is that modern AI, particularly large language models, can function as a scalable and tireless \"cognitive mentor,\" fulfilling the core requirements of the apprenticeship model in ways that were previously impossible with human-only instruction. By mapping the capabilities of AI to the tenets of Cognitive Apprenticeship, we can construct a powerful and effective learning environment.\n\n### **3.1 The Cognitive Apprenticeship Model Revisited: Core Tenets and Modern Relevance**\n\nThe Cognitive Apprenticeship model, first articulated by Collins, Brown, and Newman, is a pedagogical framework that adapts the traditional apprenticeship modellearning a craft by working alongside a masterto the learning of cognitive skills like reading comprehension, mathematical problem-solving, and scientific reasoning.29 Its central goal is to make the tacit, internal thought processes of experts visible and accessible to novices.31 The model is more relevant today than ever, as the primary challenge for developers is no longer just learning to code, but learning to effectively think and reason alongside a powerful, non-human intelligence.  \nThe framework is built upon six core teachin"
  },
  {
    "id": "report_source",
    "chunk": "s is no longer just learning to code, but learning to effectively think and reason alongside a powerful, non-human intelligence.  \nThe framework is built upon six core teaching methods, which are designed to guide a learner from observation to independent practice in a structured and supported manner 29:\n\n1. **Modeling:** The process begins with an expert performing a task while explicitly externalizing their thought processes. The expert \"thinks aloud,\" demonstrating not just the *what* of the task, but the *why*the strategies, heuristics, and self-correction they employ.  \n2. **Coaching:** As the novice begins to perform the task, the expert observes and provides real-time, specific, and contextual guidance. This can include offering hints, providing feedback, asking probing questions, and modeling correct performance when the novice is stuck.  \n3. **Scaffolding:** This refers to the support structures the expert provides to allow the novice to accomplish a task that would otherwise be beyond their current ability. This could be a template, a partial solution, or a simplified version of the problem. A key part of scaffolding is **fading**, the process of gradually removing these supports as the novice's proficiency increases.  \n4. **Articulation:** The model requires the novice to articulate their own knowledge, reasoning, and problem-solving processes. This can be done by having them explain their thinking, summarize their understanding, or answer diagnostic questions from the expert. This act of externalization forces them to solidify their internal models.  \n5. **Reflection:** The novice is prompted to compare their own problem-solving processes and results with those of the expert or other students. This comparati"
  },
  {
    "id": "report_source",
    "chunk": "ernal models.  \n5. **Reflection:** The novice is prompted to compare their own problem-solving processes and results with those of the expert or other students. This comparative analysis helps them identify their strengths, weaknesses, and misconceptions, leading to a more refined internal model of expertise.  \n6. **Exploration:** The final stage involves pushing the student to solve novel problems on their own, without guidance or scaffolding. This encourages them to generalize their learned skills and become independent practitioners.\n\nThe efficacy of this model has been demonstrated in a wide range of domains that require the mastery of complex, practice-based skills, from medical education to high school mathematics.29 Its structured yet flexible approach makes it an ideal framework for teaching the art and science of software development in the age of AI.  \nHowever, the traditional implementation of this model has one critical, inherent bottleneck: the availability of the human expert. Each of the core methodsmodeling, coaching, scaffoldingpresupposes the continuous, dedicated attention of a master practitioner. In a typical corporate or educational setting, this is the scarcest resource. A senior software architect cannot spend their entire day pair programming with a single junior developer, nor can a professor provide infinite one-on-one coaching to every student in a large class. This fundamental scaling problem is the primary reason that the highly effective apprenticeship model was largely supplanted by the more scalable but often less effective model of mass classroom instruction. This historical constraint is precisely what modern AI is poised to eliminate. An AI mentor can provide infinite, patient, perso"
  },
  {
    "id": "report_source",
    "chunk": "effective model of mass classroom instruction. This historical constraint is precisely what modern AI is poised to eliminate. An AI mentor can provide infinite, patient, personalized, one-on-one modeling and coaching, 24 hours a day. This creates the revolutionary possibility of delivering the profound benefits of apprenticeship at the scale of global education, forming the core strategic opportunity for the V2V curriculum.\n\n### **3.2 AI as the Cognitive Mentor: Making Expert Thought Processes Visible and Scalable**\n\nThe most critical function of the Cognitive Apprenticeship model is Modelingmaking expert thinking visible. It is here that modern AI tools offer a transformative capability. They can externalize complex problem-solving processes in a way that is explicit, repeatable, and interactive, directly addressing the primary challenge of learning from human experts: the \"expert blind spot.\"  \nThe expert blind spot is a well-documented cognitive bias where experts, whose knowledge has become automated and tacit through years of practice, find it difficult to perceive the struggles of a novice or to articulate the intermediate steps and foundational concepts they take for granted.33 A senior developer might solve a complex bug intuitively, compressing dozens of micro-decisions into a single, fluid action, making it nearly impossible for a junior developer to follow their reasoning. This is a major impediment to learning in any traditional apprenticeship setting.  \nAI, particularly an LLM prompted to use a chain-of-thought or step-by-step reasoning process, has no such blind spot. It can be explicitly instructed to \"think out loud,\" externalizing its entire logical pathway from problem statement to solution.34 It can b"
  },
  {
    "id": "report_source",
    "chunk": "g process, has no such blind spot. It can be explicitly instructed to \"think out loud,\" externalizing its entire logical pathway from problem statement to solution.34 It can break down a complex task, like refactoring a piece of legacy code, into a series of small, comprehensible steps, explaining the rationale for each decision along the way. Unlike a time-constrained human expert, an AI can be prompted to elaborate on any step with infinite patience, explaining foundational concepts or justifying its choices with references to established principles. This makes the AI an ideal cognitive mentor for the modeling phase of learning.  \nA powerful and direct analogue for this capability comes from the field of medical education. Recent studies have explored the use of ChatGPT to enhance the clinical reasoning skills of medical students.34 In this context, the AI acts as a \"surrogate expert.\" When presented with a patient's symptoms, it can verbalize a step-by-step diagnostic process, offer a list of differential diagnoses with justifications for each, and explain the evidence-based reasoning behind a proposed treatment plan.34 This allows a student to observe a modeled reasoning process in real time and engage in an iterative dialogue to deepen their understandinga scalable and consistent alternative to the often-limited time they can get with a senior clinician.34 This provides a perfect parallel for teaching complex software development skills like debugging, system design, or architectural trade-off analysis.  \nThis ability to externalize reasoning enables a fundamental shift in the learning process, from focusing on the \"what\" to focusing on the \"why.\" In traditional programming education, a significant portion of a stu"
  },
  {
    "id": "report_source",
    "chunk": "enables a fundamental shift in the learning process, from focusing on the \"what\" to focusing on the \"why.\" In traditional programming education, a significant portion of a student's cognitive load is consumed by the \"what\": remembering syntax, learning boilerplate patterns, and looking up specific API calls. AI coding assistants like GitHub Copilot automate a vast amount of this low-level implementation work. This automation frees up the learner's cognitive resources to engage with higher-order questionsthe \"why.\" The dialogue between the learner and their AI mentor can now be about strategy and design, not just syntax. Instead of asking \"How do I write a for-loop in Python?\", the learner can ask, \"Given these constraints, why is a microservices architecture a better choice here than a monolith?\" By automating the generation of code, the AI elevates the human's role to that of a strategic director and critic, allowing the educational process to focus on developing the deep, conceptual understanding that constitutes true expertise.\n\n### **3.3 Implementing AI-Powered Scaffolding, Coaching, and Reflection**\n\nBeyond modeling, AI tools can be strategically deployed to implement all six of the core methods of the Cognitive Apprenticeship model, creating a comprehensive and deeply interactive learning environment. By mapping specific AI capabilities to each pedagogical tenet, it becomes possible to design a curriculum that is both theoretically sound and practically effective.\n\n* **AI-Powered Modeling:** As established, an AI can demonstrate expert problem-solving by generating code while simultaneously articulating its step-by-step reasoning. A V2V module could present students with a pre-recorded video of an AI tackling a co"
  },
  {
    "id": "report_source",
    "chunk": "m-solving by generating code while simultaneously articulating its step-by-step reasoning. A V2V module could present students with a pre-recorded video of an AI tackling a complex debugging challenge, with the AI's \"thought process\" displayed in a separate panel alongside the code it generates, allowing students to pause and analyze its strategy at each step.34  \n* **AI-Powered Coaching:** AI can serve as a tireless, real-time pair programming partner. As a learner writes code, the AI can offer contextual hints, suggest completions for the current line, and provide immediate feedback on errors or stylistic issues. The growing field of \"AI coaching\" has shown that while AI may lack human empathy, it can be highly effective for specific, goal-oriented tasks like skill acquisition and reflection.36 A learner can be stuck on a problem at 2 AM and receive immediate, patient coaching that would be impossible to get from a human instructor.  \n* **AI-Powered Scaffolding:** The concept of \"AI scaffolding in education\" involves using AI to provide just-in-time support that enables a learner to complete a task they could not manage alone.38 In a V2V context, this could involve the AI generating the boilerplate for a new component, allowing the learner to focus on implementing the core business logic. It could provide a function signature or a class template to get them started. Crucially, as the curriculum progresses, these scaffolds can be gradually \"faded\" by instructing the AI to provide less and less support, pushing the learner toward independence.  \n* **AI-Powered Articulation:** The AI can be used to prompt and evaluate the learner's own thinking. A powerful exercise would be to have a student write a piece of code and then"
  },
  {
    "id": "report_source",
    "chunk": "*AI-Powered Articulation:** The AI can be used to prompt and evaluate the learner's own thinking. A powerful exercise would be to have a student write a piece of code and then instruct the AI: \"Act as a senior developer conducting a code review. I will now explain the logic of my function. Please ask me clarifying questions and critique my explanation for clarity and correctness.\" This forces the learner to externalize and solidify their own understanding in a safe, non-judgmental environment.  \n* **AI-Powered Reflection:** Reflection is fundamentally a comparative process, and AI can provide an excellent point of comparison. After completing a project, a student could submit their solution to an AI for a comprehensive critique. The AI could then generate its own alternative solution to the same problem and provide a detailed report comparing the two approaches in terms of efficiency, readability, maintainability, and adherence to best practices. This direct, evidence-based comparison is a powerful catalyst for reflective learning.  \n* **AI-Powered Exploration:** In the final stages of the curriculum, students can be given open-ended, portfolio-worthy projects. Here, the AI transitions from a coach to a consultant. The student drives the project, but they can use the AI as a brainstorming partner for ideas, a technical advisor for choosing libraries and frameworks, and a collaborator for solving novel problems they encounter along the way, fostering the skills of independent, exploratory problem-solving.\n\nThe following table provides a concrete blueprint for how these AI-powered techniques can be implemented within the V2V curriculum.\n\n| Cognitive Apprenticeship Tenet | V2V Implementation with AI |\n| :---- | :---- |\n| **"
  },
  {
    "id": "report_source",
    "chunk": "print for how these AI-powered techniques can be implemented within the V2V curriculum.\n\n| Cognitive Apprenticeship Tenet | V2V Implementation with AI |\n| :---- | :---- |\n| **Modeling** | Students analyze a recorded session of an AI solving a complex bug, with the AI's chain-of-thought reasoning displayed alongside the code. The AI explicitly calls out the strategies and hypotheses it is using at each step.34 |\n| **Coaching** | During a live coding exercise, a student works in an IDE with an AI pair programmer. When they get stuck, they can ask the AI for a hint (not the full solution), and the AI provides a targeted suggestion or a guiding question.36 |\n| **Scaffolding** | A project requires building a REST API. In an early module, the AI provides the complete boilerplate for the server and endpoints, letting the student focus on the business logic. In a later module, the AI only provides the function signatures, requiring the student to implement the rest (fading).38 |\n| **Articulation** | **Task:** A student writes a function and then prompts the AI: \"Act as a junior developer who is new to this codebase. I will explain my function to you. Please ask questions about anything that is unclear.\" This forces the student to articulate their reasoning clearly and simply.29 |\n| **Reflection** | After submitting a project, the student receives an automated code review from an AI. The AI scores the code on several metrics (e.g., complexity, security, style) and provides a \"Socratic\" critique by asking questions like, \"Have you considered the edge case where the input is null?\".29 |\n| **Exploration** | For a capstone project, the student is given a high-level goal (e.g., \"Build a tool to automate meeting summaries\"). They are r"
  },
  {
    "id": "report_source",
    "chunk": "re the input is null?\".29 |\n| **Exploration** | For a capstone project, the student is given a high-level goal (e.g., \"Build a tool to automate meeting summaries\"). They are required to use an AI as a brainstorming and research partner to define the project scope, select the technology stack, and solve implementation challenges independently.29 |\n\nIt is important to acknowledge the limitations of AI mentorship. Research into AI coaching highlights that current systems lack affective empathy, deep cultural understanding, and the ability to navigate complex, long-term human emotions and career aspirations.36 An AI cannot effectively mentor a student through a crisis of confidence or provide nuanced career advice. This is not a failure of the technology but a critical design constraint. A purely AI-driven apprenticeship would be technically effective but emotionally and socially sterile. Therefore, the optimal approach is a *hybrid* model. The V2V curriculum should leverage AI for the scalable, technical, and cognitive aspects of apprenticeshipthe line-by-line coaching, the infinite modeling, the patient scaffolding. Simultaneously, it must strategically deploy human instructors and peer groups for the tasks AI is ill-suited for: providing emotional support, fostering a sense of community, offering high-level strategic guidance, and mentoring the whole person, not just the coder. This human-in-the-loop design creates a learning environment that is both intellectually rigorous and humanistically supportive.\n\n## **Section 4: Synthesis and Strategic Recommendations for the V2V Pathway**\n\nThe preceding analysis of the technical landscape, pedagogical market, and theoretical frameworks provides a clear and compelling foundation"
  },
  {
    "id": "report_source",
    "chunk": "mmendations for the V2V Pathway**\n\nThe preceding analysis of the technical landscape, pedagogical market, and theoretical frameworks provides a clear and compelling foundation for the design of the \"Vibecoding to Virtuosity\" (V2V) curriculum. This final section synthesizes these findings into a set of concrete, actionable recommendations. The goal is to provide a strategic blueprint that will enable V2V to establish itself as a premier educational pathway, one that not only teaches the skills required for the present but also cultivates the mindset needed for the future of software development. The recommendations focus on formally defining the curriculum's core principles, structuring its learning path based on a proven pedagogical model, and designing unique learning modules that deliver a differentiated and transformative educational experience.\n\n### **4.1 Integrating Context Engineering as a Core V2V Principle**\n\n**Recommendation:** The V2V curriculum should be formally and explicitly structured around the mastery of Context Engineering as its core technical discipline. The term \"Vibecoding\" should be positioned as the intuitive, entry-level application of context engineering principlesthe art of getting into a productive flow with an AI partner. \"Virtuosity\" should be defined as the professional mastery of this disciplinethe science of architecting reliable, scalable, and self-improving agentic systems.  \n**Justification:** This strategic framing provides the curriculum with a rigorous and defensible intellectual foundation. It elevates the central concept from a potentially vague \"vibe\" into a defined engineering practice that is at the forefront of the AI industry. This aligns directly with the observed professi"
  },
  {
    "id": "report_source",
    "chunk": "entral concept from a potentially vague \"vibe\" into a defined engineering practice that is at the forefront of the AI industry. This aligns directly with the observed professionalization of AI development, where the ad-hoc craft of prompting is maturing into the systematic discipline of context architecture. For prospective students, this provides a powerful and clear narrative about their professional development: they are not just learning to use a new tool, but are training to become experts in a new and critical engineering role. This positioning differentiates V2V from courses that focus solely on prompting or the features of a specific tool, establishing it as a more advanced and career-focused program.\n\n### **4.2 A Proposed Curriculum Structure for \"Vibecoding to Virtuosity\"**\n\n**Recommendation:** The curriculum's primary modules should be structured to directly mirror the six progressive stages of the Cognitive Apprenticeship model. This creates a logical and pedagogically sound learning path that guides the student from passive observation to independent, creative problem-solving.  \n**Justification:** A structure based on the Cognitive Apprenticeship model directly addresses the primary pedagogical gap identified in the current market: the lack of focus on the *process* of human-AI collaboration. Instead of a curriculum organized by topic (e.g., Testing, Debugging, Documentation), this structure organizes the learning journey around the development of collaborative skills. This process-oriented approach is more likely to cultivate the deep, transferable skills of a \"meta-developer\" who can adapt to any tool or task.  \n**Proposed High-Level Curriculum Structure:**\n\n* **Phase 1: Observation (Modeling)**  \n  * **Mo"
  },
  {
    "id": "report_source",
    "chunk": " transferable skills of a \"meta-developer\" who can adapt to any tool or task.  \n**Proposed High-Level Curriculum Structure:**\n\n* **Phase 1: Observation (Modeling)**  \n  * **Module 1: Deconstructing the Expert \\- Observing the Ghost in the Machine.** In this initial phase, students are observers. They watch and analyze curated sessions of an expert AI system solving complex software development problems. The focus is on learning to \"read\" the AI's externalized thought process and identify the key strategies, heuristics, and patterns it employs.  \n* **Phase 2: Guided Practice (Coaching & Scaffolding)**  \n  * **Module 2: The Guided Partnership \\- The AI Pair Programmer.** Students move from observation to action. They tackle a series of well-defined coding exercises with an AI partner that provides real-time coaching (hints, feedback) and scaffolding (boilerplate code, templates). The goal is to develop a basic fluency in the give-and-take of AI-assisted development.  \n* **Phase 3: Articulation and Self-Correction (Articulation & Reflection)**  \n  * **Module 3: The Socratic Dialogue \\- Thinking Like a Meta-Developer.** This phase focuses on developing metacognitive skills. Students are required to articulate their design decisions and coding strategies to an AI for critique. They also engage in reflective exercises, comparing their solutions to AI-generated alternatives to identify gaps in their own thinking.  \n* **Phase 4: Independent Application (Exploration)**  \n  * **Module 4: The Creative Collaboration \\- From Prompt to Product.** In this final, capstone phase, students undertake open-ended projects. They are tasked with building a complete application from a high-level concept, using the AI not as a coach but as a con"
  },
  {
    "id": "report_source",
    "chunk": "apstone phase, students undertake open-ended projects. They are tasked with building a complete application from a high-level concept, using the AI not as a coach but as a consultant and collaborator. The goal is to demonstrate their ability to independently manage a complex, long-term, human-AI partnership to create a novel product.\n\n### **4.3 Key Learning Modules and Pedagogical Strategies**\n\n**Recommendation:** Within the broader Cognitive Apprenticeship structure, design specific learning modules and assessments that explicitly teach the \"meta-skills\" of AI collaboration and the advanced principles of agentic, self-improving systems. These unique modules will operationalize the key findings of this report and serve as the core differentiators of the V2V curriculum.  \n**Justification:** These modules and strategies make the theoretical framework of the curriculum tangible. They provide concrete learning experiences that directly cultivate the skills of a \"virtuoso\" Context Engineer, ensuring the program delivers on its unique value proposition.  \n**Example Modules and Pedagogical Strategies:**\n\n* **Specialized Module: \"Architecting Your Agent's Mind.\"** This module, situated within Phase 2 or 3, would be a deep dive into the practical skills of Context Engineering. Based on the analysis in Section 1.2, students would learn to design and manage an agent's context window by orchestrating RAG pipelines, implementing short- and long-term memory systems, and integrating external tools via function calling. The final project for this module would be to build a simple, stateful chatbot for a specific domain.  \n* **Signature Assessment: \"Metacognitive Debugging.\"** This assessment, part of Module 3, would directly test the me"
  },
  {
    "id": "report_source",
    "chunk": " build a simple, stateful chatbot for a specific domain.  \n* **Signature Assessment: \"Metacognitive Debugging.\"** This assessment, part of Module 3, would directly test the meta-skills of AI collaboration. Students would be given a complex, buggy, AI-generated codebase. Their task would be to use an AI partner to diagnose and fix the issues. The deliverable would be not just the corrected code, but also the complete, unedited transcript of their collaborative debugging session with the AI. They would be graded on their ability to formulate effective diagnostic strategies, ask clarifying questions, and guide the AI toward a solution.  \n* **Capstone Project: \"Building Your Personal Playbook.\"** This project, serving as the final assessment for Module 4, would require students to apply the principles of Agentic Context Engineering (ACE). They would design and implement a simple system to create a persistent, evolving \"playbook\" of their own successful coding strategies, custom prompts, and reusable code snippets. For example, after successfully refactoring a piece of code, they would prompt a \"Reflector\" agent to analyze the before-and-after and distill a reusable refactoring pattern, which a \"Curator\" agent would then save to a personal knowledge base that is automatically injected into their context in future sessions. This project would be a tangible demonstration of their ability to create a self-improving workflow, the hallmark of virtuosity.  \n* **Pedagogical Strategy: \"Hybrid Mentorship Pods.\"** To implement the hybrid apprenticeship model, students should be organized into small \"pods\" (4-6 students) with a dedicated human mentor. The AI will handle the vast majority of the day-to-day, code-level technical coaching."
  },
  {
    "id": "report_source",
    "chunk": "ents should be organized into small \"pods\" (4-6 students) with a dedicated human mentor. The AI will handle the vast majority of the day-to-day, code-level technical coaching. The human mentor's role will be to facilitate a weekly pod meeting focused on higher-level strategy, unblocking conceptual roadblocks, discussing career development, and fostering the socio-emotional aspects of learning and community that an AI cannot provide. This blended model optimizes for both scalability and human-centric support.\n\n#### **Works cited**\n\n1. Context Engineering vs Prompt Engineering | by Mehul Gupta | Data ..., accessed October 15, 2025, [https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d)  \n2. Understanding Prompt Engineering and Context Engineering \\- Walturn, accessed October 15, 2025, [https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering](https://www.walturn.com/insights/understanding-prompt-engineering-and-context-engineering)  \n3. Context Engineering \\- LangChain Blog, accessed October 15, 2025, [https://blog.langchain.com/context-engineering-for-agents/](https://blog.langchain.com/context-engineering-for-agents/)  \n4. Context Engineering: Going Beyond Prompt Engineering and RAG \\- The New Stack, accessed October 15, 2025, [https://thenewstack.io/context-engineering-going-beyond-prompt-engineering-and-rag/](https://thenewstack.io/context-engineering-going-beyond-prompt-engineering-and-rag/)  \n5. Context Engineering: The Dynamic Context Construction Technique for AI Agents | AWS Builder Center, accessed October 15, 2025, [https://builde"
  },
  {
    "id": "report_source",
    "chunk": "rompt-engineering-and-rag/)  \n5. Context Engineering: The Dynamic Context Construction Technique for AI Agents | AWS Builder Center, accessed October 15, 2025, [https://builder.aws.com/content/3064TwnFXzSYe6r2EpN6Ye2Q2u1/context-engineering-the-dynamic-context-construction-technique-for-ai-agents](https://builder.aws.com/content/3064TwnFXzSYe6r2EpN6Ye2Q2u1/context-engineering-the-dynamic-context-construction-technique-for-ai-agents)  \n6. Context Engineering ( RAG 2.0 ) : The Next Chapter in GenAI | by Ramakrishna Sanikommu, accessed October 15, 2025, [https://medium.com/@ramakrishna.sanikommu/context-engineering-rag-2-0-the-next-chapter-in-genai-4e53c0382bf4](https://medium.com/@ramakrishna.sanikommu/context-engineering-rag-2-0-the-next-chapter-in-genai-4e53c0382bf4)  \n7. Effective context engineering for AI agents \\- Anthropic, accessed October 15, 2025, [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)  \n8. Context Engineering vs Prompt Engineering : r/PromptEngineering \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/PromptEngineering/comments/1lmnftf/context\\_engineering\\_vs\\_prompt\\_engineering/](https://www.reddit.com/r/PromptEngineering/comments/1lmnftf/context_engineering_vs_prompt_engineering/)  \n9. What is Context Engineering for LLMs? | by Tahir | Medium, accessed October 15, 2025, [https://medium.com/@tahirbalarabe2/%EF%B8%8F-what-is-context-engineering-for-llms-90109f856c1c](https://medium.com/@tahirbalarabe2/%EF%B8%8F-what-is-context-engineering-for-llms-90109f856c1c)  \n10. What is Context Engineering? \\- Elasticsearch Labs, accessed October 15, 2025, [https://www.elastic.co/se"
  },
  {
    "id": "report_source",
    "chunk": "e2/%EF%B8%8F-what-is-context-engineering-for-llms-90109f856c1c)  \n10. What is Context Engineering? \\- Elasticsearch Labs, accessed October 15, 2025, [https://www.elastic.co/search-labs/blog/context-engineering-overview](https://www.elastic.co/search-labs/blog/context-engineering-overview)  \n11. Discussion: Context Engineering, Agents, and RAG. Oh My. : r/LangChain \\- Reddit, accessed October 15, 2025, [https://www.reddit.com/r/LangChain/comments/1m7qe3a/discussion\\_context\\_engineering\\_agents\\_and\\_rag\\_oh/](https://www.reddit.com/r/LangChain/comments/1m7qe3a/discussion_context_engineering_agents_and_rag_oh/)  \n12. Context Engineering \\- What it is, and techniques to consider \\- LlamaIndex, accessed October 15, 2025, [https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider](https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider)  \n13. \\[2510.04618\\] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models \\- arXiv, accessed October 15, 2025, [https://www.arxiv.org/abs/2510.04618](https://www.arxiv.org/abs/2510.04618)  \n14. arxiv.org, accessed October 15, 2025, [https://arxiv.org/html/2510.04618v1](https://arxiv.org/html/2510.04618v1)  \n15. Paper page \\- Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models \\- Hugging Face, accessed October 15, 2025, [https://huggingface.co/papers/2510.04618](https://huggingface.co/papers/2510.04618)  \n16. Agentic Context Engineering: Teaching Language Models to Learn ..., accessed October 15, 2025, [https://medium.com/@bingqian/agentic-context-engineering-teaching-language-models-to-learn-from-experience-706c31a872ca](https://medium.com/@bingqian/agentic-context-engine"
  },
  {
    "id": "report_source",
    "chunk": ", [https://medium.com/@bingqian/agentic-context-engineering-teaching-language-models-to-learn-from-experience-706c31a872ca](https://medium.com/@bingqian/agentic-context-engineering-teaching-language-models-to-learn-from-experience-706c31a872ca)  \n17. Agentic Context Engineering (ACE): Self-Improving LLMs via Evolving Contexts, Not Fine-Tuning \\- MarkTechPost, accessed October 15, 2025, [https://www.marktechpost.com/2025/10/10/agentic-context-engineering-ace-self-improving-llms-via-evolving-contexts-not-fine-tuning/](https://www.marktechpost.com/2025/10/10/agentic-context-engineering-ace-self-improving-llms-via-evolving-contexts-not-fine-tuning/)  \n18. Courses \\- DeepLearning.AI, accessed October 15, 2025, [https://www.deeplearning.ai/courses/](https://www.deeplearning.ai/courses/)  \n19. Best Generative AI Courses of 2025  Based on Your Profession \\- Class Central, accessed October 15, 2025, [https://www.classcentral.com/report/best-generative-ai-courses/](https://www.classcentral.com/report/best-generative-ai-courses/)  \n20. Generative AI for Software Development Skill Certificate | Coursera, accessed October 15, 2025, [https://www.coursera.org/professional-certificates/generative-ai-for-software-development](https://www.coursera.org/professional-certificates/generative-ai-for-software-development)  \n21. Generative AI for Software Development \\- DeepLearning.AI, accessed October 15, 2025, [https://www.deeplearning.ai/courses/generative-ai-for-software-development/](https://www.deeplearning.ai/courses/generative-ai-for-software-development/)  \n22. Advanced GenAI Development Practices | Coursera, accessed October 15, 2025, [https://www.coursera.org/learn/advanced-genai-development-practices](https://www.coursera.org/learn"
  },
  {
    "id": "report_source",
    "chunk": "dvanced GenAI Development Practices | Coursera, accessed October 15, 2025, [https://www.coursera.org/learn/advanced-genai-development-practices](https://www.coursera.org/learn/advanced-genai-development-practices)  \n23. Top 10 Udemy Courses to Learn Vibe Coding in 2025 | by javinpaul ..., accessed October 15, 2025, [https://medium.com/javarevisited/top-10-udemy-courses-to-learn-vibe-coding-in-2025-7a8df8036d7a](https://medium.com/javarevisited/top-10-udemy-courses-to-learn-vibe-coding-in-2025-7a8df8036d7a)  \n24. GitHub Copilot Fundamentals Part 1 of 2 \\- Training | Microsoft Learn, accessed October 15, 2025, [https://learn.microsoft.com/en-us/training/paths/copilot/](https://learn.microsoft.com/en-us/training/paths/copilot/)  \n25. Introduction to GitHub Copilot \\- Training \\- Microsoft Learn, accessed October 15, 2025, [https://learn.microsoft.com/en-us/training/modules/introduction-to-github-copilot/](https://learn.microsoft.com/en-us/training/modules/introduction-to-github-copilot/)  \n26. GitHub Copilot certified \\- GitHub Learn \\- Certification Details, accessed October 15, 2025, [https://learn.github.com/certification/COPILOT](https://learn.github.com/certification/COPILOT)  \n27. Intro to GitHub Copilot \\- Codecademy, accessed October 15, 2025, [https://www.codecademy.com/learn/intro-to-github-copilot](https://www.codecademy.com/learn/intro-to-github-copilot)  \n28. Master GitHub Copilot as a Beginner \\- YouTube, accessed October 15, 2025, [https://www.youtube.com/watch?v=FwKe2F7gxNw](https://www.youtube.com/watch?v=FwKe2F7gxNw)  \n29. Translating knowledge to practice: application of the public health ..., accessed October 15, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12230075/](https://pmc.ncbi.nlm.nih.gov/arti"
  },
  {
    "id": "report_source",
    "chunk": " knowledge to practice: application of the public health ..., accessed October 15, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12230075/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12230075/)  \n30. Investigating the Impact of the Stratified Cognitive Apprenticeship Model on High School Students' Math Performance \\- MDPI, accessed October 15, 2025, [https://www.mdpi.com/2227-7102/14/8/898](https://www.mdpi.com/2227-7102/14/8/898)  \n31. AI Personalized Learning: A New Era in Education \\- Apple Podcasts, accessed October 15, 2025, [https://podcasts.apple.com/lt/podcast/ai-personalized-learning-a-new-era-in-education/id1784701089](https://podcasts.apple.com/lt/podcast/ai-personalized-learning-a-new-era-in-education/id1784701089)  \n32. AI Personalized Learning: A New Era in Education \\- mission, accessed October 15, 2025, [https://podcasts.apple.com/ci/podcast/ai-personalized-learning-a-new-era-in-education/id1784701089](https://podcasts.apple.com/ci/podcast/ai-personalized-learning-a-new-era-in-education/id1784701089)  \n33. Instructor Prior Knowledge: Expert Blindspot  The Open Guide to Teaching and Learning in Higher Education \\- Pressbooks.pub, accessed October 15, 2025, [https://pressbooks.pub/etsu/chapter/instructor-prior-knowledge-expert-blindspot/](https://pressbooks.pub/etsu/chapter/instructor-prior-knowledge-expert-blindspot/)  \n34. ChatGPT as a Pedagogical Tool for Clinical Reasoning in Medical ..., accessed October 15, 2025, [https://eprints.uad.ac.id/88293/1/21-99-2-PB.pdf](https://eprints.uad.ac.id/88293/1/21-99-2-PB.pdf)  \n35. A Review of Cognitive Apprenticeship Methods in Computing Education Research, accessed October 15, 2025, [https://www.researchgate.net/publication/378815673\\_A\\_Review\\_of\\_Cognitive\\_"
  },
  {
    "id": "report_source",
    "chunk": "of Cognitive Apprenticeship Methods in Computing Education Research, accessed October 15, 2025, [https://www.researchgate.net/publication/378815673\\_A\\_Review\\_of\\_Cognitive\\_Apprenticeship\\_Methods\\_in\\_Computing\\_Education\\_Research](https://www.researchgate.net/publication/378815673_A_Review_of_Cognitive_Apprenticeship_Methods_in_Computing_Education_Research)  \n36. (PDF) A systematic literature review of artificial intelligence (AI) in ..., accessed October 15, 2025, [https://www.researchgate.net/publication/389166575\\_A\\_systematic\\_literature\\_review\\_of\\_artificial\\_intelligence\\_AI\\_in\\_coaching\\_Insights\\_for\\_future\\_research\\_and\\_product\\_development](https://www.researchgate.net/publication/389166575_A_systematic_literature_review_of_artificial_intelligence_AI_in_coaching_Insights_for_future_research_and_product_development)  \n37. Generative AI in Training and Coaching: Redefining the Design Process of Learning Materials \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2508.11662v1](https://arxiv.org/html/2508.11662v1)  \n38. arxiv.org, accessed October 15, 2025, [https://arxiv.org/abs/2501.06527](https://arxiv.org/abs/2501.06527)  \n39. Scaffolding Creativity: Integrating Generative AI Tools and Real-world Experiences in Business Education \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.06527v2](https://arxiv.org/html/2501.06527v2)  \n40. Scaffolding Creativity: Integrating Generative AI Tools and Real-world Experiences in Business Education \\- arXiv, accessed October 15, 2025, [https://arxiv.org/html/2501.06527v1](https://arxiv.org/html/2501.06527v1)\n</file_artifact>\n\n<file path=\"src/Artifacts/A62 - V2V Academy - Synthesis of Research Proposals.md\">\n# Artifact A62: V2V Academy - S"
  },
  {
    "id": "report_source",
    "chunk": "27v1](https://arxiv.org/html/2501.06527v1)\n</file_artifact>\n\n<file path=\"src/Artifacts/A62 - V2V Academy - Synthesis of Research Proposals.md\">\n# Artifact A62: V2V Academy - Synthesis of Research Proposals\n# Date Created: C60\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** A meta-reflection on the provided research proposals, summarizing key themes, strategic insights, and recurring patterns.\n- **Tags:** v2v, research, synthesis, meta-analysis, strategy\n\n## 1. Overview\n\nThis document provides a high-level synthesis of the key insights gleaned from the provided research proposals (`context/v2v/research-proposals/`). These proposals represent a deep dive into the transition from \"prompt engineering\" to \"context engineering\" and form the intellectual bedrock of the V2V Academy. This reflection consolidates the most critical themes that should guide our curriculum design and strategic positioning.\n\n## 2. Key Themes and Strategic Insights\n\n### 1. The Paradigm Shift is Real and Defensible\n*   **Insight:** The transition from \"prompt engineering\" to \"context engineering\" is not just a semantic change but a fundamental, industry-wide paradigm shift. The research consistently frames prompt engineering as a tactical, brittle, and introductory skill, while context engineering is positioned as a strategic, robust, and architectural discipline required for production-grade AI systems.\n*   **Strategic Implication:** This validates the core premise of the V2V curriculum. We should lean heavily into this distinction, positioning V2V as an advanced program that teaches AI *systems architecture*, not just prompt crafting. This creates a clear market differentiator.\n\n### 2. The Future is Agentic and Systemic\n*   **"
  },
  {
    "id": "report_source",
    "chunk": "dvanced program that teaches AI *systems architecture*, not just prompt crafting. This creates a clear market differentiator.\n\n### 2. The Future is Agentic and Systemic\n*   **Insight:** The research points toward a future dominated by \"agentic workflows,\" where autonomous or semi-autonomous AI agents execute complex, multi-step tasks. Building these agents requires a systems-thinking approach, focusing on memory, tool integration, and state management.\n*   **Strategic Implication:** The V2V curriculum must be forward-looking. The end goal should not be to create a better \"prompter,\" but a capable \"agent architect.\" The capstone projects and advanced modules should focus on designing and orchestrating these agentic systems.\n\n### 3. Pedagogy Must Evolve to Counter \"Pseudo-Apprenticeship\"\n*   **Insight:** The research highlights a critical pedagogical risk: learners using AI as an \"answer engine\" to bypass the productive struggle required for deep learning. The Cognitive Apprenticeship model is identified as the ideal framework, but it must be implemented in a way that forces learners to engage in metacognition, articulation, and reflection.\n*   **Strategic Implication:** Our curriculum design and exercises must be intentionally structured to mitigate this risk. We should prioritize activities that require students to critique AI output, justify their own design choices, and use the AI as a Socratic partner rather than a simple code generator. Assessment should focus on the student's *process* and *reasoning*, not just the final code.\n\n### 4. The \"Human-in-the-Loop\" is the \"Chief Validation Officer\"\n*   **Insight:** As AI automates more of the tactical implementation (the \"how\"), the human's value shifts to higher-order cog"
  },
  {
    "id": "report_source",
    "chunk": "n-in-the-Loop\" is the \"Chief Validation Officer\"\n*   **Insight:** As AI automates more of the tactical implementation (the \"how\"), the human's value shifts to higher-order cognitive functions: strategic intent (the \"why\"), critical validation, and ethical oversight.\n*   **Strategic Implication:** The V2V curriculum should explicitly train for this new role. We are not just training coders; we are training the next generation of technical leaders who can strategically direct and rigorously validate AI systems. Modules on AI-assisted Test-Driven Development (TDD) and spec-driven workflows are practical implementations of this principle.\n\n### 5. Context is the New Competitive Moat\n*   **Insight:** As powerful foundational models become commoditized, the source of competitive advantage is no longer the model itself, but the ability to effectively connect that model to unique, proprietary data and workflows. The context layerthe RAG pipelines, memory systems, and tool integrationsis the defensible asset.\n*   **Strategic Implication:** This reinforces the value proposition of the entire V2V program. By teaching the discipline of context engineering, we are equipping students with the skills to build these valuable, defensible systems, making them highly sought-after in the market.\n</file_artifact>\n\n<file path=\"src/Artifacts/A50 - V2V Academy - Core Principles & Philosophy.md\">\n- **Key/Value for A0:**\n- **Description:** Synthesizes the core principles and philosophical underpinnings of the \"Vibecoding to Virtuosity\" pathway, extracted from the curator's coaching transcripts.\n- **Tags:** v2v, philosophy, principles, cognitive apprenticeship, mental models\n\n## 1. Overview\n\nThis document codifies the foundational principles and "
  },
  {
    "id": "report_source",
    "chunk": "coaching transcripts.\n- **Tags:** v2v, philosophy, principles, cognitive apprenticeship, mental models\n\n## 1. Overview\n\nThis document codifies the foundational principles and philosophies that underpin the \"Vibecoding to Virtuosity\" (V2V) methodology. These concepts were synthesized from an analysis of the curator's 1-on-1 coaching transcripts and represent the \"why\" behind the practical workflows. They serve as the guiding ethos for the entire V2V curriculum.\n\n## 2. Core Principles\n\n### Principle 1: The AI is a Feedback Loop for Human Cognition\n\nThe most fundamental principle is that the AI is not just a tool for producing output; it is a mirror that creates a powerful feedback loop for human thought.\n*   **Expertise as a Prerequisite for Feedback:** To guide an AI effectively, one must have enough domain expertise to provide high-quality, \"expert feedback.\" If you aren't an expert, you cannot give expert feedback, and therefore cannot go deep with the AI.\n*   **Code Errors as Expert Feedback:** For a non-coder, a system-generated error (like a compiler error) *is* a form of expert feedback. It's an objective critique of the AI's output that the human doesn't have to generate themselves. By feeding this error back to the AI, the human enters the feedback loop and learns by observing the process of correction. This is the essence of learning to code in the AI era.\n\n### Principle 2: Data Curation is the Apex Skill\n\nThe V2V pathway posits that traditional programming syntax is becoming a secondary skill. The new apex skill for the AI era is **Data Curation**.\n*   **It's All Just Text:** All forms of informationcode, documents, images, errorscan be represented as text. The ability to select, organize, label, and annotate "
  },
  {
    "id": "report_source",
    "chunk": "ation**.\n*   **It's All Just Text:** All forms of informationcode, documents, images, errorscan be represented as text. The ability to select, organize, label, and annotate this text is the core competency.\n*   **The Internet is Your Hard Drive:** The modern developer's skill is not just knowing what's on their local machine, but knowing what data exists on the internet and how to pull it into their project context to solve a problem.\n*   **Context over Command:** The quality of an AI's output is a direct function of the quality of its input context. Therefore, the most leveraged activity is not perfecting the command (the prompt), but perfecting the data ecosystem (the context).\n\n### Principle 3: The Virtuous Cycle of Cognitive Apprenticeship\n\nThe V2V pathway is an implementation of the Cognitive Apprenticeship model, where the AI acts as the tireless master and the human is the apprentice.\n*   **Making the Hidden Curriculum Visible:** Expert thinking is often tacit and invisible. The AI, when prompted to explain its reasoning, makes this \"hidden curriculum\" explicit. The process of critically analyzing AI output, identifying its flaws, and guiding it to a better solution is how the apprentice internalizes the expert's thought patterns.\n*   **Every Prompt is a Lesson:** The developer builds a \"mental model of the model\" with every interaction. By observing the AI's response to a given context, the developer learns what the AI is capable of, where its knowledge gaps are, and how to structure information for better results.\n\n## 3. Key Mental Models & Analogies\n\nThe curator's transcripts are rich with analogies that simplify complex concepts. These will be central to the curriculum's teaching style.\n*   **The AI as an In"
  },
  {
    "id": "report_source",
    "chunk": " & Analogies\n\nThe curator's transcripts are rich with analogies that simplify complex concepts. These will be central to the curriculum's teaching style.\n*   **The AI as an Intern/Junior Developer:** Frame the AI as a very fast, very knowledgeable, but completely inexperienced junior partner. It needs clear instructions, well-defined context, and constant supervision. It will make mistakes, and your job is to catch them.\n*   **The Japanese Letter:** A single, tiny change to a prompt (one stroke on a character) can completely change the meaning and the output. This emphasizes the importance of precision in instruction.\n*   **The Game of Life:** Some problems cannot be solved in a single step. You must run the process, observe the outcome, and use that new state as the input for the next step. This is the essence of the iterative cycle.\n*   **Bitcoin without the Stress (AI Credits):** AI credits are an appreciating asset. The value of a credit increases over time as the underlying AI models become more powerful. This illustrates the compounding value of investing in AI skills and resources.\n\n## 4. The Strategic Vision: The \"Star Trek\" Motivation\n\nThe ultimate driver for this entire methodology is a desire to accelerate human progress to solve major world problems and explore the universe. The curator's stated \"selfish\" motivation is to \"be Captain Kirk.\"\n*   **Empowering the Citizen Architect:** The goal is to create a legion of \"sleeper agents\"individuals who acquire incredible problem-solving skills through AI collaboration and then apply those skills to solve problems in their own communities, accelerating progress from the bottom up.\n*   **AI as an Abundance Engine:** The V2V pathway is presented as a counter-strategy"
  },
  {
    "id": "report_source",
    "chunk": "lls to solve problems in their own communities, accelerating progress from the bottom up.\n*   **AI as an Abundance Engine:** The V2V pathway is presented as a counter-strategy to scarcity and conflict. By providing tools that create abundance (of knowledge, of solutions), it aims to elevate humanity's focus to higher-order challenges.\n</file_artifact>\n\n<file path=\"src/Artifacts/A51 - V2V Academy - The Virtuoso's Workflow.md\">\n- **Key/Value for A0:**\n- **Description:** A detailed, reverse-engineered breakdown of the curator's expert workflow, codifying the practical steps of the \"Vibecoding to Virtuosity\" pathway.\n- **Tags:** v2v, workflow, process, cognitive apprenticeship, reverse engineering\n\n## 1. Overview\n\nThis document reverse-engineers and codifies the curator's expert workflow for AI-assisted development. This process, referred to as the \"Virtuoso's Loop,\" represents the end-goal of the \"Vibecoding to Virtuosity\" pathway. It is a structured, iterative, and highly effective methodology for moving from a high-level goal to a tested and implemented solution in partnership with an AI. This workflow will serve as the \"north star\" for the V2V curriculum.\n\n## 2. The Virtuoso's Loop: A Step-by-Step Breakdown\n\nThe workflow is a cycle that integrates planning, AI interaction, and rigorous validation.\n\n### Step 1: Curation & Documentation (The \"Documentation First\" Principle)\n\nThe cycle begins not with a prompt, but with data and planning.\n1.  **Curate the Knowledge Base:** The curator gathers all relevant documents, code files, research, and raw data into a structured folder system. This becomes the AI's \"library.\"\n2.  **Define the Goal in an Artifact:** The curator creates or updates a planning artifact (e.g., `A[XX] - New"
  },
  {
    "id": "report_source",
    "chunk": "nto a structured folder system. This becomes the AI's \"library.\"\n2.  **Define the Goal in an Artifact:** The curator creates or updates a planning artifact (e.g., `A[XX] - New Feature Plan.md`). This document serves as the \"source of truth\" for the current task.\n3.  **Select Context:** Using the DCE's File Tree View, the curator selects the specific files and artifacts that are relevant to the immediate task, creating a precise context.\n\n### Step 2: Parallel Prompting & Response Triage\n\nThis step leverages parallelism to explore the solution space and select the most promising starting point.\n1.  **Generate `prompt.md`:** The curator uses the DCE to automatically generate a complete `prompt.md` file, which includes the project's interaction schema, the full cycle history, and the flattened content of the selected files.\n2.  **Execute Parallel Prompts:** The curator sends this identical `prompt.md` to multiple instances of the AI (e.g., 4-8 tabs in AI Studio).\n3.  **Parse and Sort:** The raw responses are pasted into the DCE's Parallel Co-Pilot Panel, parsed into a structured view, and then sorted by total token count. The curator starts their review with the longest response, which is often the most detailed.\n\n### Step 3: Critical Analysis & Selection (The Human-in-the-Loop)\n\nThis is the core human judgment step.\n1.  **Review the Plan:** The curator reviews the AI's proposed \"Course of Action\" and the list of \"Associated Files\" to ensure the AI's plan is logical and complete.\n2.  **Diff the Changes:** The curator uses the integrated diff viewer to compare the AI's proposed code changes against the current workspace files.\n3.  **Select the Best Response:** Based on the analysis, the curator clicks \"Select This Response\" o"
  },
  {
    "id": "report_source",
    "chunk": "ompare the AI's proposed code changes against the current workspace files.\n3.  **Select the Best Response:** Based on the analysis, the curator clicks \"Select This Response\" on the most promising solution, designating it as the primary candidate for the cycle.\n\n### Step 4: The Test-and-Revert Loop (Git-Integrated Validation)\n\nThis is the rapid, low-risk testing phase.\n1.  **Create a Baseline:** The curator clicks the \"Baseline (Commit)\" button, which creates a `git commit` of the current state of the workspace, providing a safe restore point.\n2.  **Accept Changes:** The curator selects the specific files from the chosen response they wish to test and clicks \"Accept Selected.\" This overwrites the local files with the AI's generated code.\n3.  **Test:** The curator runs the application, linter, or test suite to validate the changes.\n4.  **Decision:**\n    *   **If the test fails:** The curator clicks \"Restore Baseline.\" This command uses `git restore .` to instantly discard all changes, returning the workspace to its clean state. The curator can then choose to accept a different set of files or a different AI response and repeat the test.\n    *   **If the test succeeds:** The changes are kept, and the workflow proceeds.\n\n### Step 5: Finalize & Prepare for Next Cycle\n\nOnce a successful solution has been integrated, the curator prepares for the next iteration.\n1.  **Update Context:** The curator writes notes, feedback, or the next high-level goal into the \"Cycle Context\" and \"Cycle Title\" fields in the DCE.\n2.  **Start New Cycle:** The curator clicks the `+` button to create a new, empty cycle. The process then repeats from Step 1.\n\nThis entire loop codifies the principles of Cognitive Apprenticeship: the human **models** the "
  },
  {
    "id": "report_source",
    "chunk": "`+` button to create a new, empty cycle. The process then repeats from Step 1.\n\nThis entire loop codifies the principles of Cognitive Apprenticeship: the human **models** the high-level strategy through documentation, the AI is **coached** through iterative feedback, and the Git workflow provides **scaffolding** for safe exploration.\n</file_artifact>\n\n<file path=\"src/Artifacts/A53 - V2V Academy - Curriculum Outline.md\">\n# Artifact A53: V2V Academy - Curriculum Outline\n# Date Created: C58\n# Author: AI Model & Curator\n# Updated on: C73 (Add Lesson 4.3)\n\n- **Key/Value for A0:**\n- **Description:** Proposes a multi-module curriculum structure for the V2V Academy, designed to guide learners from the fundamentals of \"Vibecoding\" to the mastery of the \"Virtuoso's Workflow.\" Each lesson is tailored to three distinct learner personas.\n- **Tags:** v2v, curriculum design, instructional design, learning pathway, cognitive apprenticeship, persona\n\n## 1. Overview\n\nThis document outlines the proposed curriculum structure for the \"Vibecoding to Virtuosity\" (V2V) Academy. The curriculum is designed as a structured pathway that embodies the principles of Cognitive Apprenticeship. It follows a \"backwards design,\" starting with the end goalthe expert \"Virtuoso\"and progressively building the foundational skills required to reach that state.\n\nThe primary learning interface for all modules will be the `aiascent.dev` interactive report viewer, creating a consistent and immersive experience.\n\n## 2. The Three-Persona Approach\n\nTo provide a more personalized and effective learning experience, each lesson in the V2V curriculum is presented in three distinct versions, tailored to our primary learner personas (defined in `A58`):\n\n1.  **The Career Tr"
  },
  {
    "id": "report_source",
    "chunk": "arning experience, each lesson in the V2V curriculum is presented in three distinct versions, tailored to our primary learner personas (defined in `A58`):\n\n1.  **The Career Transitioner:** Content is framed around professional development, strategic advantage, and augmenting existing expertise.\n2.  **The Underequipped Graduate:** Content is focused on gaining a competitive edge, building a strong portfolio, and acquiring in-demand, practical skills for the job market.\n3.  **The Young Precocious:** Content uses more engaging, game-oriented language (\"level up,\" \"mastery\") and focuses on channeling raw talent into disciplined, powerful creation.\n\n## 3. The V2V Learning Pathway: A 4-Module Structure\n\nThe curriculum is divided into four core modules, each representing a stage in the developer's journey.\n\n---\n\n### **Module 1: The Virtuoso's Loop - Charting the Destination**\n\n*   **Objective:** To introduce the learner to the complete, end-to-end expert workflow as the \"north star\" for their journey. This corresponds to the **Modeling** phase of Cognitive Apprenticeship, where the expert's process is made visible.\n*   **Lessons:**\n    *   **1.1: The Virtuoso's Workflow** (See `A54`)\n        *   **Career Transitioner:** \"The Professional's Playbook: Mastering an Expert AI Workflow\"\n        *   **Underequipped Graduate:** \"The Unfair Advantage: Learning the Workflow That Gets You Hired\"\n        *   **Young Precocious:** \"Level Up Your Dev Game: Mastering the Virtuoso's Loop\"\n    *   **1.2: The Philosophy of V2V** (See `A63`)\n        *   **Career Transitioner:** \"Strategic Principles of Human-AI Collaboration\"\n        *   **Underequipped Graduate:** \"The Mindset for the Modern Tech Career\"\n        *   **Young Precocious:** \"The S"
  },
  {
    "id": "report_source",
    "chunk": "r:** \"Strategic Principles of Human-AI Collaboration\"\n        *   **Underequipped Graduate:** \"The Mindset for the Modern Tech Career\"\n        *   **Young Precocious:** \"The Secret Lore: Unlocking the V2V Philosophy\"\n    *   **1.3: The Citizen Architect** (See `A64`)\n        *   **Career Transitioner:** \"Becoming an AI-Powered Strategic Leader\"\n        *   **Underequipped Graduate:** \"Defining Your Role in the Future of Tech\"\n        *   **Young Precocious:** \"The End Game: Becoming a Citizen Architect\"\n*   **Capstone Project:** Learners will be given a complete, pre-packaged project and will follow a guided tutorial to execute a single, full cycle of the Virtuoso's Loop.\n\n---\n\n### **Module 2: The Curator's Toolkit - Mastering the Foundations**\n\n*   **Objective:** To build the foundational, data-centric skills identified in `A52`. This module focuses on teaching learners how to think like data architects and critical analysts.\n*   **Lessons:**\n    *   **2.1: Introduction to Data Curation** (See `A65`)\n        *   **Career Transitioner:** \"From Information Overload to Strategic Asset: The Principles of Data Curation\"\n        *   **Underequipped Graduate:** \"Skill #1: How to Build the High-Quality Context Employers Want\"\n        *   **Young Precocious:** \"The Ultimate Inventory Management: Mastering Your Data\"\n    *   **2.2: The Art of Annotation** (See `A66`)\n        *   **Career Transitioner:** \"Increasing Signal: How to Label Data for Maximum AI Leverage\"\n        *   **Underequipped Graduate:** \"Making Your Context Machine-Readable: A Guide to Annotation\"\n        *   **Young Precocious:** \"Enchanting Your Data: The Power of Labeling and Metadata\"\n    *   **2.3: Critical Analysis of AI Output** (See `A67`)\n        *   **"
  },
  {
    "id": "report_source",
    "chunk": "tation\"\n        *   **Young Precocious:** \"Enchanting Your Data: The Power of Labeling and Metadata\"\n    *   **2.3: Critical Analysis of AI Output** (See `A67`)\n        *   **Career Transitioner:** \"Quality Control: Vetting AI Output for Business-Critical Applications\"\n        *   **Underequipped Graduate:** \"Don't Trust, Verify: How to Spot AI Hallucinations and Errors\"\n        *   **Young Precocious:** \"Debuffing the AI: How to Find and Fix Flaws in AI Output\"\n*   **Capstone Project:** Learners will be given a large, disorganized collection of documents and tasked with curating and annotating them into a high-quality, structured knowledge base for a specific project.\n\n---\n\n### **Module 3: The Apprentice's Forge - Structured Interaction**\n\n*   **Objective:** To transition the learner from passive analysis to active, structured collaboration with the AI. This corresponds to the **Coaching and Scaffolding** phases of Cognitive Apprenticeship.\n*   **Lessons:**\n    *   **3.1: From Conversation to Command** (See `A68`)\n        *   **Career Transitioner:** \"Driving Outcomes: The Principles of Structured AI Interaction\"\n        *   **Underequipped Graduate:** \"Writing Prompts That Work: An Introduction to Interaction Schemas\"\n        *   **Young Precocious:** \"Casting Spells: Mastering the Syntax of Power\"\n    *   **3.2: The Feedback Loop in Practice** (See `A69`)\n        *   **Career Transitioner:** \"Leveraging Errors as Data Points for AI Refinement\"\n        *   **Underequipped Graduate:** \"Your First AI Debugging Session: Turning Errors into Progress\"\n        *   **Young Precocious:** \"Respawning with a Purpose: Using Errors to Level Up Your AI\"\n    *   **3.3: The Test-and-Revert Workflow** (See `A70`)\n        *   **Career "
  },
  {
    "id": "report_source",
    "chunk": "        *   **Young Precocious:** \"Respawning with a Purpose: Using Errors to Level Up Your AI\"\n    *   **3.3: The Test-and-Revert Workflow** (See `A70`)\n        *   **Career Transitioner:** \"Risk Mitigation: A Framework for Safely Testing AI-Generated Solutions\"\n        *   **Underequipped Graduate:** \"How to Test Code You Didn't Write: A Guide to the Git-Integrated Workflow\"\n        *   **Young Precocious:** \"Save Scumming for Coders: Mastering the Test-and-Revert Loop\"\n*   **Capstone Project:** Learners will be given a small, buggy codebase and a failing test suite. They must use the feedback loop and the test-and-revert workflow to guide an AI to fix all the bugs and make the tests pass.\n\n---\n\n### **Module 4: The Vibecoder's Canvas - Intuitive Exploration**\n\n*   **Objective:** To empower the learner to apply their skills to their own ideas, entering the **Exploration** phase of Cognitive Apprenticeship.\n*   **Lessons:**\n    *   **4.1: Defining Your Vision** (See `A71`)\n        *   **Career Transitioner:** \"From Business Need to Project Scope: Architecting Your Solution\"\n        *   **Underequipped Graduate:** \"Your Portfolio Starts Here: Creating a Professional Project Scope\"\n        *   **Young Precocious:** \"The Hero's Journey: Defining Your Quest\"\n    *   **4.2: The Blank Page Problem** (See `A72`)\n        *   **Career Transitioner:** \"Overcoming Inertia: AI-Powered Project Scaffolding\"\n        *   **Underequipped Graduate:** \"How to Start When You Don't Know Where to Start\"\n        *   **Young Precocious:** \"World-Building 101: Using AI to Generate Your Project's Lore\"\n    *   **4.3: Architecting Your MVP** (See `A73`)\n        *   **Career Transitioner:** \"From Scope to Structure: Generating Your Architectural Bl"
  },
  {
    "id": "report_source",
    "chunk": "enerate Your Project's Lore\"\n    *   **4.3: Architecting Your MVP** (See `A73`)\n        *   **Career Transitioner:** \"From Scope to Structure: Generating Your Architectural Blueprint\"\n        *   **Underequipped Graduate:** \"Creating the Blueprint: How to Get an AI to Build Your Starter Code\"\n        *   **Young Precocious:** \"The Architect's Table: Forging Your World's Foundation\"\n*   **Capstone Project:** The final project is open-ended. Learners must conceive of their own simple application or project, document their vision, and use the full V2V workflow to build the first functional version with an AI partner.\n</file_artifact>\n\n<file path=\"src/Artifacts/A55 - V2V Academy - Glossary of Terms.md\">\n# Artifact A55: V2V Academy - Glossary of Terms\n# Date Created: C59\n# Author: AI Model & Curator\n# Updated on: C73 (Add Technical Debt and Architectural Blueprint)\n\n- **Key/Value for A0:**\n- **Description:** A central glossary defining key terms, concepts, and acronyms used throughout the \"Vibecoding to Virtuosity\" curriculum and the broader aiascent.dev project.\n- **Tags:** v2v, documentation, glossary, definitions, cognitive apprenticeship\n\n## 1. Purpose\n\nThis document serves as the definitive glossary for the V2V Academy. Its purpose is to provide clear, consistent, and easily accessible definitions for the core concepts, specialized terminology, and acronyms that learners will encounter. This ensures a shared vocabulary and a deeper understanding of the underlying principles of the \"Vibecoding to Virtuosity\" pathway.\n\n## 2. Glossary\n\n### **A**\n\n*   **Agentic Workflow:** A development process where an AI agent can autonomously plan, reason, and execute complex, multi-step tasks, often involving the use of tools and memory."
  },
  {
    "id": "report_source",
    "chunk": "*Agentic Workflow:** A development process where an AI agent can autonomously plan, reason, and execute complex, multi-step tasks, often involving the use of tools and memory.\n*   **Annotation:** The process of adding descriptive metadata (labels, tags, names) to raw data to make it machine-readable and provide clear context to an AI. This is a core practice of Data Curation.\n*   **Apex Skill:** The pinnacle of the V2V pathway, defined as \"On-the-Fly Tooling.\" It is the ability to use AI not just as a tool to be used, but as a \"foundry\" to create bespoke tools and solutions in real-time to solve novel problems.\n*   **Architectural Blueprint:** A high-level plan or design document that outlines the structure, components, and interactions of a software system. It serves as a guide for the development team, similar to how a building's blueprint guides construction.\n*   **Artifact:** A formal, written document (e.g., project plan, requirements document, source code file) that serves as a \"source of truth\" for a specific part of a project. In the DCE workflow, artifacts are the primary medium for instructing and aligning with an AI.\n*   **Automation Bias:** The cognitive tendency for humans to over-trust and favor suggestions from automated systems, often ignoring contradictory information or failing to apply critical thinking to the system's output. In the V2V context, it's the dangerous trap of blindly accepting AI-generated code without rigorous validation.\n\n### **B**\n\n*   **Baseline (V2V Context):** The act of creating a safe restore point of a project using version control (`git commit`) before introducing new, potentially unstable code from an AI. This is the \"save\" step in the **Test-and-Revert Workflow**.\n*   **Blank "
  },
  {
    "id": "report_source",
    "chunk": "ct using version control (`git commit`) before introducing new, potentially unstable code from an AI. This is the \"save\" step in the **Test-and-Revert Workflow**.\n*   **Blank Page Problem:** The psychological and practical difficulty of starting a creative or technical project from a completely empty state. It represents the initial inertia that must be overcome to translate a plan into a tangible product.\n*   **Boilerplate Code:** Standardized, reusable sections of code that are included in many places with little or no alteration (e.g., configuration files, initial component skeletons). AI is excellent at generating this foundational code.\n\n### **C**\n\n*   **Citizen Architect:** A professional archetype who combines deep domain expertise with AI collaboration skills to design, build, and lead the development of complex systems, contributing meaningfully to their community and profession.\n*   **Cognitive Apprenticeship:** A pedagogical model where an expert (human or AI) makes their internal, tacit thought processes visible to a novice. The V2V curriculum is built on this model, using AI to model expert workflows, provide coaching, and offer scaffolding.\n*   **Cognitive Bandwidth Tax:** A concept from behavioral science describing how financial precarity or other stressors consume mental resources, measurably reducing a person's ability to perform complex cognitive tasks. The \"fissured workplace\" imposes this tax on its data annotators.\n*   **Cognitive Bias:** A systematic pattern of deviation from norm or rationality in judgment. In the V2V context, this refers to the human tendency to, for example, trust a confident-sounding AI (automation bias) or interpret its output in a way that confirms one's pre-existing beliefs "
  },
  {
    "id": "report_source",
    "chunk": ", this refers to the human tendency to, for example, trust a confident-sounding AI (automation bias) or interpret its output in a way that confirms one's pre-existing beliefs (confirmation bias). Acknowledging these biases is crucial for objective validation.\n*   **Cognitive Capital:** The collective problem-solving capacity of an individual, organization, or society. In the AI era, it is considered the primary strategic asset, representing the potential for innovation and adaptation.\n*   **Cognitive Security (COGSEC):** The practice of defending human perception and decision-making from online manipulation, propaganda, and deceptive information. It also refers to using AI modeled on human cognition to detect cybersecurity threats.\n*   **Cognitive Tutor:** An AI-powered system designed to provide personalized educational assistance. It models a student's knowledge, tracks their progress, and provides real-time feedback and hints to guide their learning process, mimicking a human tutor.\n*   **Commit:** A fundamental operation in Git that saves a snapshot of the current state of all tracked files in the repository. Each commit has a unique ID and a message describing the changes, creating a permanent part of the project's history.\n*   **Compiler Error:** An error detected by a compiler before a program is run, typically because the code violates the syntax or grammar rules of the programming language. It's like a spell-check for code.\n*   **Context Engineering:** The discipline of designing, organizing, and optimizing the complete informational payload (context) provided to a Large Language Model (LLM) to ensure reliable and accurate performance on complex tasks. It is the core technical skill of the \"Virtuoso.\"\n*   **Cont"
  },
  {
    "id": "report_source",
    "chunk": "d (context) provided to a Large Language Model (LLM) to ensure reliable and accurate performance on complex tasks. It is the core technical skill of the \"Virtuoso.\"\n*   **Context Rot:** The degradation of an AI's performance over a long conversation as the context window becomes filled with irrelevant, outdated, or contradictory information, reducing the signal-to-noise ratio.\n*   **Context Window:** The finite amount of information (measured in tokens) that an LLM can \"see\" and process at any given time. Effective management of this \"working memory\" is a core challenge of Context Engineering.\n*   **Critical Analysis:** The disciplined process of evaluating information (particularly AI-generated output) for its accuracy, logic, security, and alignment with project goals. It is the core \"human-in-the-loop\" skill that ensures quality and reliability.\n*   **Critical Thinking:** In the V2V context, this is the essential skill of evaluating AI-generated output for its accuracy, logic, relevance, and potential flaws. It is the core of the \"Don't Trust, Verify\" principle.\n*   **Cycle:** A single, complete iteration of the development workflow within the DCE. A cycle includes the curated context, the user's instructions, all AI-generated responses, and the user's final decision, all of which are saved to a persistent knowledge graph.\n\n### **D**\n\n*   **Data Curation:** The professional discipline of identifying, gathering, organizing, and structuring raw information to create a high-signal, machine-readable asset (context) that empowers an AI to perform complex tasks with precision and reliability. It is the foundational practice of Context Engineering.\n*   **Data Curation Environment (DCE):** A VS Code extension designed to stre"
  },
  {
    "id": "report_source",
    "chunk": "lex tasks with precision and reliability. It is the foundational practice of Context Engineering.\n*   **Data Curation Environment (DCE):** A VS Code extension designed to streamline the workflow of AI-assisted development. It provides tools for selecting context, managing parallel AI responses, and iterating on projects in a structured, auditable manner.\n*   **Data Labeling:** A specific type of annotation that focuses on classifying data by assigning predefined tags or categories to data points. It primarily answers the question \"What is this?\" (e.g., this image contains a \"cat\").\n*   **DCIA (Data Curator / Intelligence Analyst):** The peak archetype of the V2V pathway. A professional who combines the data-centric skills of a curator with the critical thinking and synthesis skills of an intelligence analyst.\n*   **Debugging (V2V Context):** The process of orchestrating the feedback loop between a human, an AI, and a computer system. The human's role is to execute the AI's code, capture any system-generated errors, and feed those errors back to the AI as context for the next corrective iteration.\n*   **Deliberate Practice:** A highly structured form of practice aimed at improving performance. It involves setting specific goals, maintaining intense focus, receiving immediate feedback, and constant refinement.\n*   **Development Cycle:** The core iterative loop of the V2V workflow. It consists of a sequence of phases: Curation & Documentation, Parallel Prompting & Triage, Critical Analysis & Selection, Test-and-Revert, and Finalization.\n*   **Diffing:** The process of comparing two versions of a file or text to see the exact differences (additions, deletions, modifications). A \"diff viewer\" is the tool used to visualize the"
  },
  {
    "id": "report_source",
    "chunk": "* The process of comparing two versions of a file or text to see the exact differences (additions, deletions, modifications). A \"diff viewer\" is the tool used to visualize these changes, and it is a primary tool for the critical analysis of AI-generated code.\n\n### **E**\n\n*   **Execution:** The act of carrying out a plan, order, or course of action. In the V2V workflow, this refers to the phase where a developer takes a selected AI response and applies it to their project to test its validity.\n*   **Experiential Blindness:** A state of not knowing what is possible or how to solve a problem due to a lack of relevant experience. The V2V pathway aims to cure this by providing a structured path to gaining experience in partnership with an AI.\n*   **External Brain:** A metaphor for the role of a well-curated project repository in the V2V workflow. The repository acts as a persistent, organized collection of knowledge that augments the developer's own memory and provides the AI with a comprehensive understanding of the project.\n\n### **F**\n\n*   **Feedback Loop:** The iterative process where the output of a system (e.g., an AI's code) is tested, the results (e.g., errors) are captured as feedback, and that feedback is used as input to guide the next iteration and improve the system.\n*   **Fissured Workplace:** An economic structure where large corporations distance themselves from their labor force by using layers of contractors and subcontractors. In the AI industry, this has led to a deprofessionalized and underpaid \"ghost workforce\" of data annotators.\n*   **Flattening:** The process of taking a selection of files (code, PDFs, etc.) and concatenating their content into a single, flat text file (e.g., `flattened_repo.md`) to be"
  },
  {
    "id": "report_source",
    "chunk": "  **Flattening:** The process of taking a selection of files (code, PDFs, etc.) and concatenating their content into a single, flat text file (e.g., `flattened_repo.md`) to be used as context for an AI.\n\n### **G**\n\n*   **Garbage In, Garbage Out (GIGO):** A fundamental principle in computing which states that the quality of the output is determined by the quality of the input. In the context of AI, it means that an LLM cannot produce high-quality results from low-quality (incomplete, incorrect, or irrelevant) data.\n*   **Genesis Prompt:** A specific, structured command given to an AI at the very beginning of a project (Cycle 0). Its purpose is to take a high-level `Project Scope` and generate the initial set of foundational planning artifacts and technical scaffolding, bootstrapping the entire project structure.\n*   **Git:** A free and open-source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. It is the underlying technology that powers the **Test-and-Revert Workflow**.\n\n### **H**\n\n*   **Hallucination:** A phenomenon where an AI model generates information that sounds plausible but is factually incorrect, nonsensical, or entirely fabricated. This can include inventing functions, libraries, API endpoints, or making up facts to complete a response.\n\n### **I**\n\n*   **Ideation:** The creative process of forming, entertaining, and developing new ideas. In the V2V curriculum, AI is used as a partner in the ideation phase to brainstorm potential project concepts and features.\n*   **Information Architecture:** The art and science of organizing and structuring shared information environments to support usability and findability. In the V2V curriculum, t"
  },
  {
    "id": "report_source",
    "chunk": " **Information Architecture:** The art and science of organizing and structuring shared information environments to support usability and findability. In the V2V curriculum, this refers to the practice of designing a logical and intuitive folder and file structure for a project repository.\n*   **Interaction Schema:** A template or a set of rules that defines a structured format for communicating with an AI. It ensures that all necessary information (like role, context, and output format) is provided in a clear, consistent, and machine-readable way, reducing ambiguity and improving the reliability of the AI's response.\n*   **Iterative Refinement:** A core principle of the V2V workflow where a solution is developed through repeated cycles of action, feedback, and improvement. Instead of aiming for a perfect solution on the first try, developers make small, incremental changes and use the results to guide their next step.\n*   **Iterative Development:** A software development methodology where a project is built through repeated cycles (iterations) of planning, building, testing, and refining. Instead of trying to build the entire system at once, features are developed and released in small, incremental pieces, allowing for flexibility and continuous feedback.\n\n### **K**\n\n*   **Knowledge Base (KB):** A curated collection of documents, data, and other information used to ground an AI model in a specific domain. In the V2V workflow, your curated project repository becomes the knowledge base.\n\n*   **Knowledge Graph:** A structured representation of a project's development history, as captured by the DCE. Each \"Cycle\" is a node in the graph, containing the context, prompts, AI responses, and developer decisions for that iteratio"
  },
  {
    "id": "report_source",
    "chunk": "ct's development history, as captured by the DCE. Each \"Cycle\" is a node in the graph, containing the context, prompts, AI responses, and developer decisions for that iteration.\n\n### **L**\n\n*   **Labeling:** See **Annotation**.\n*   **Logical Error:** A bug in a program that causes it to operate correctly but does not produce the intended result. The code runs without crashing, but its output is wrong because the underlying algorithm or strategy is flawed.\n\n### **M**\n\n*   **Machine-Readable Context:** Information that is structured and labeled in such a way that a machine (like an AI) can easily parse and understand its meaning, purpose, and relationship to other data.\n*   **Mental Model of the Model:** The intuitive understanding a developer builds over time of an AI's capabilities, limitations, and \"thought processes.\" Developing this mental model is key to effective collaboration and is a primary outcome of the V2V pathway.\n*   **Metacognition:** The ability to \"think about one's own thinking.\" In the V2V context, this involves critically analyzing one's own learning gaps and using AI as a \"meta-tool\" to build personalized learning accelerators.\n*   **Metadata:** Data that provides information about other data. In the V2V workflow, metadata includes descriptive file names, folder structures, and explicit tags that give context and meaning to raw information.\n*   **Minimum Viable Product (MVP):** A version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort. It is the simplest version of a product that can be released to the market.\n\n### **O**\n\n*   **On-the-Fly Tooling:** See **Apex Skill**.\n\n### **P**\n\n*   **Parallel Prompting:** The practice of"
  },
  {
    "id": "report_source",
    "chunk": "st version of a product that can be released to the market.\n\n### **O**\n\n*   **On-the-Fly Tooling:** See **Apex Skill**.\n\n### **P**\n\n*   **Parallel Prompting:** The practice of sending the same prompt to multiple AI instances simultaneously to generate a diverse set of solutions. This allows the developer to compare different approaches and select the most promising one, rather than being locked into a single, linear path.\n*   **Project Scope:** A formal document or artifact that defines the boundaries of a project. It outlines the project's objectives, deliverables, features, functions, tasks, deadlines, and costs. A clear project scope is essential for aligning human and AI collaborators.\n\n### **R**\n\n*   **Restore (V2V Context):** The act of instantly discarding all changes made by an AI and reverting the project to the last saved \"Baseline\" using version control (`git restore`). This is the \"revert\" step in the **Test-and-Revert Workflow**.\n*   **Retrieval-Augmented Generation (RAG):** A technique that enhances an LLM's response by dynamically retrieving relevant information from an external knowledge base and including it in the context provided to the model. This grounds the AI's answer in factual, up-to-date, or proprietary data.\n*   **Runtime Error:** An error that occurs while a program is actively running. It happens when the program encounters an unexpected condition or tries to perform an operation that is impossible to execute, such as dividing by zero or accessing a file that doesn't exist.\n\n### **S**\n\n*   **Scaffolding (Pedagogical Context):** Temporary support structures provided to a learner to help them complete a task that would otherwise be beyond their current capabilities. In the V2V workflow, AI is u"
  },
  {
    "id": "report_source",
    "chunk": "ext):** Temporary support structures provided to a learner to help them complete a task that would otherwise be beyond their current capabilities. In the V2V workflow, AI is used as a scaffolding engine to generate boilerplate code, offer hints, or provide templates, with the support being gradually removed as the learner's skills grow.\n*   **Signal-to-Noise Ratio:** A measure of the quality of the context provided to an AI. \"Signal\" is the precise, relevant information needed for a task, while \"Noise\" is any irrelevant, redundant, or distracting information. The goal of data curation is to maximize this ratio.\n*   **Source of Truth:** A canonical document, artifact, or repository that is designated as the single, authoritative source of information for a project. In the V2V workflow, the curated and version-controlled project repository serves as the Source of Truth to ensure consistency for both human and AI collaborators.\n*   **Specification (Software):** A detailed document that outlines the requirements, objectives, design, and constraints of a software project. It serves as a comprehensive blueprint for the development team, ensuring everyone has a consistent understanding of what needs to be built.\n*   **Stakeholder:** Any person, group, or organization that has an interest in, or is affected by, a project's outcome. This includes team members, customers, investors, and users.\n*   **Structured Interaction:** The practice of moving beyond casual, conversational prompts to providing the AI with clear, explicit, and repeatable commands, often using a template or \"Interaction Schema.\" This is a core skill for achieving reliable and predictable results from an AI.\n\n### **T**\n\n*   **Technical Debt:** The implied future "
  },
  {
    "id": "report_source",
    "chunk": "ing a template or \"Interaction Schema.\" This is a core skill for achieving reliable and predictable results from an AI.\n\n### **T**\n\n*   **Technical Debt:** The implied future cost of rework caused by choosing an easy (limited) solution now instead of using a better approach that would take longer. Over time, this \"debt\" accumulates \"interest,\" making future changes more difficult and costly.\n*   **Test-and-Revert Workflow:** A core practice of the Virtuoso's Loop where a developer creates a safe restore point (Baseline), applies AI-generated code, tests it, and then either keeps the changes or instantly discards them (Revert) if they are faulty. This enables rapid, low-risk experimentation.\n*   **Test-Driven Development (TDD):** A software development methodology where developers write a failing test *before* they write the functional code to make that test pass. This \"test-first\" approach follows a simple \"Red-Green-Refactor\" cycle and helps ensure code quality, correctness, and maintainability from the start.\n*   **Token:** The basic unit of text that an LLM processes. A token can be a word, part of a word, or a punctuation mark. The number of tokens in a prompt is a key metric for cost and performance.\n\n### **U**\n\n*   **User Story:** A simple, structured sentence used in agile development to define a feature from an end-user's perspective. The format is: \"As a `[type of user]`, I want to `[perform some action]`, so that `[I can achieve some goal]`.\"\n\n### **V**\n\n*   **Validation:** The process of confirming that an AI-generated output is correct, functional, and meets the specified requirements. This can involve running tests, performing a code review, or fact-checking generated text against reliable sources.\n*   **Ver"
  },
  {
    "id": "report_source",
    "chunk": "functional, and meets the specified requirements. This can involve running tests, performing a code review, or fact-checking generated text against reliable sources.\n*   **Verification:** The process of checking that an AI's output is factually correct and free of errors. It answers the question: \"Did we build the thing right?\"\n*   **Version Control:** A system that records changes to a file or set of files over time so that you can recall specific versions later. It allows developers to track project history, collaborate, and revert to previous stable states. **Git** is the most popular version control system.\n*   **Vibecoding:** The intuitive, conversational, and often imprecise starting point for interacting with generative AI. It is the process of translating a high-level goal or \"vibe\" into a functional output using natural language. It is the first stage on the pathway to Virtuosity.\n*   **Virtuosity:** The state of mastery at the end of the V2V pathway. It is characterized by the ability to systematically and reliably architect complex systems in partnership with AI, leveraging a deep understanding of Context Engineering and structured workflows.\n*   **Virtuoso's Loop:** The codified, step-by-step expert workflow for AI-assisted development that is taught in the V2V Academy. It encompasses Curation, Parallel Prompting, Critical Analysis, Git-Integrated Validation, and Finalization.\n*   **Vision Document:** A high-level strategic document that defines the purpose, goals, and long-term direction of a project. It answers the \"why\" and serves as a north star for all development decisions.\n*   **V2V (Vibecoding to Virtuosity):** The name of the pedagogical pathway and curriculum designed to guide learners from novice, "
  },
  {
    "id": "report_source",
    "chunk": "a north star for all development decisions.\n*   **V2V (Vibecoding to Virtuosity):** The name of the pedagogical pathway and curriculum designed to guide learners from novice, intuitive AI interaction to expert-level mastery in human-AI collaboration.\n</file_artifact>\n\n<file path=\"src/Artifacts/A54 - V2V Academy - Lesson 1.1 - The Virtuoso's Loop.md\">\n# Artifact A54: V2V Academy - Lesson 1.1 - The Virtuoso's Loop\n# Date Created: C58\n# Author: AI Model & Curator\n# Updated on: C61 (Expand to include three distinct versions of the lesson tailored to learner personas)\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 1.1 of the V2V Academy, \"The Virtuoso's Loop,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, workflow, interactive learning, persona\n\n## **Lesson 1.1: The Virtuoso's Loop**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: Introduction - The Professional's Playbook**\n*   **Page Title:** The Professional's Playbook: Mastering an Expert AI Workflow\n*   **Image Prompt:** A cinematic, wide-angle shot of a seasoned professional in a modern, minimalist office. They stand at a holographic interface, orchestrating a complex workflow visualized as a glowing, circular loop of data flowing between stages: \"Curation,\" \"Parallel Prompting,\" \"Validation,\" and \"Integration.\" The professional is calm and in control, conducting the flow with strategic intent.\n*   **TL;DR:** This lesson introduces the complete, end-to-end expert workflow for AI-assisted development. This is the professional playbook for leveraging AI as a strategic partner.\n*   **Content:** Welcome to t"
  },
  {
    "id": "report_source",
    "chunk": "the complete, end-to-end expert workflow for AI-assisted development. This is the professional playbook for leveraging AI as a strategic partner.\n*   **Content:** Welcome to the V2V Academy. Your journey to becoming an AI-powered leader begins here. Before we build the foundational skills, it's crucial to understand the destination: a state of fluid, powerful, and repeatable collaboration with AI. This expert workflow is the \"Virtuoso's Loop.\" It is a systematic process that transforms development from a series of tactical guesses into a disciplined engineering practice. In this lesson, we will walk through each step of this professional playbook.\n\n#### **Page 2: Step 1 - The \"Documentation First\" Principle**\n*   **Page Title:** Step 1: Curation & Documentation\n*   **Image Prompt:** An image depicting the \"Curation\" phase. On the left, a chaotic collection of business reports, spreadsheets, and emails. In the center, a project manager is using a clean interface to select specific documents. On the right, these items form an organized, high-signal data package labeled \"Curated Context.\"\n*   **TL;DR:** A successful initiative begins not with a command, but with planning and data. You must first build the AI's \"library\" and write its \"instructions\" before tasking it with execution.\n*   **Content:** Every successful cycle starts with preparation. This is the \"Documentation First\" principle. 1. **Curate the Knowledge Base:** You act as a strategist, gathering all relevant filescode, research, business requirementsinto your project. 2. **Define the Goal in an Artifact:** You act as an architect, creating a planning document that defines the objective for the current cycle. 3. **Select Context:** Finally, you act as a curator"
  },
  {
    "id": "report_source",
    "chunk": " in an Artifact:** You act as an architect, creating a planning document that defines the objective for the current cycle. 3. **Select Context:** Finally, you act as a curator, selecting only the specific files relevant to the objective, creating a focused, high-signal context for the AI.\n\n#### **Page 3: Step 2 - Parallel Prompting & Triage**\n*   **Page Title:** Step 2: Exploring the Solution Space\n*   **Image Prompt:** A visualization of \"Parallel Prompting.\" A single, well-defined business problem is sent out, which then splits and travels down eight parallel pathways to eight identical but separate AI analysts. The pathways return eight distinct, varied strategic proposals.\n*   **TL;DR:** Never rely on a single AI-generated strategy. By prompting multiple instances in parallel, you can evaluate a diverse set of solutions and select the most robust path forward.\n*   **Content:** LLMs are non-deterministic. The Virtuoso leverages this. 1. **Generate `prompt.md`:** The DCE automates the creation of a complete prompt file. 2. **Execute in Parallel:** You send this identical prompt to multiple AI instances. 3. **Parse and Sort:** The responses are brought into the DCE's Parallel Co-Pilot Panel, parsed, and sorted by size. Your review starts with the most detailed strategic option.\n\n#### **Page 4: Step 3 - Critical Analysis & Selection**\n*   **Page Title:** Step 3: The Executive Decision\n*   **Image Prompt:** A close-up of a leader's face, focused and analytical. They are reviewing a futuristic diff viewer comparing two versions of a technical blueprint. Their hand is poised over a glowing \"Select This Response\" button.\n*   **TL;DR:** The human's most important role is judgment. You must critically review the AI's proposed "
  },
  {
    "id": "report_source",
    "chunk": "t. Their hand is poised over a glowing \"Select This Response\" button.\n*   **TL;DR:** The human's most important role is judgment. You must critically review the AI's proposed plan and its tactical implementation before committing resources.\n*   **Content:** This is where your expertise as the \"Navigator\" is critical. The AI provides options; you provide the judgment. 1. **Review the Plan:** Read the AI's \"Course of Action.\" Is the strategy sound and complete? 2. **Diff the Changes:** Use the integrated diff viewer to see the exact changes the AI is proposing. Does the execution align with the strategy? 3. **Select the Best Path:** Based on your analysis, you select the single best response to move forward with.\n\n#### **Page 5: Step 4 - The Test-and-Revert Loop**\n*   **Page Title:** Step 4: Risk Mitigation & Rapid Validation\n*   **Image Prompt:** A simple, clear flowchart showing a Git-based workflow. A \"Baseline (Commit)\" button creates a \"Safe Restore Point.\" An \"Accept Selected\" arrow applies the AI code to a \"Staging Environment.\" A \"Test\" phase follows. An arrow labeled \"Failure\" leads to a \"Restore Baseline\" button. An arrow labeled \"Success\" moves forward.\n*   **TL;DR:** The Virtuoso's Loop uses Git to create a safe, low-risk environment for testing AI-generated solutions.\n*   **Content:** Never trust, always verify. This is the rapid validation phase. 1. **Create a Baseline:** Click \"Baseline (Commit)\" to create a Git commit. This is your safety net. 2. **Accept Changes:** Select which files you want to test and click \"Accept Selected.\" 3. **Test:** Run your application or test suite. 4. **Decide:** If the test fails, click \"Restore Baseline\" to instantly revert. If it succeeds, proceed.\n\n#### **Page 6: Step 5 - F"
  },
  {
    "id": "report_source",
    "chunk": "Test:** Run your application or test suite. 4. **Decide:** If the test fails, click \"Restore Baseline\" to instantly revert. If it succeeds, proceed.\n\n#### **Page 6: Step 5 - Finalize & Prepare for Next Cycle**\n*   **Page Title:** Step 5: Capture Learnings & Iterate\n*   **Image Prompt:** A shot of the DCE's Panel. The user is typing notes into the \"Cycle Context\" field, summarizing the key takeaways from the completed cycle. The \"Generate prompt.md\" button is highlighted, leading to a `+` (New Cycle) button.\n*   **TL;DR:** The loop completes by capturing institutional knowledge and preparing the context for the next strategic iteration.\n*   **Content:** A successful test sets the stage for the next initiative. 1. **Update Context:** You document what you've learned or define the next objective in the \"Cycle Context\" and \"Cycle Title\" fields. This becomes part of the permanent, auditable history. 2. **Start a New Cycle:** Click the `+` button to create a new cycle, and the Virtuoso's Loop begins again.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: Introduction - The Unfair Advantage**\n*   **Page Title:** The Unfair Advantage: Learning the Workflow That Gets You Hired\n*   **Image Prompt:** A cinematic shot of a recent graduate at a sleek, futuristic workstation. They are confidently orchestrating a complex coding project, visualized as a glowing loop of data. Around them, other graduates look stressed, buried in traditional textbooks and messy code. The title \"THE UNFAIR ADVANTAGE\" floats above the main subject.\n*   **TL;DR:** This lesson teaches you the complete, end-to-end expert workflow for AI-assisted development that employers are looking for. This is your new playbook for a successful tech caree"
  },
  {
    "id": "report_source",
    "chunk": "s lesson teaches you the complete, end-to-end expert workflow for AI-assisted development that employers are looking for. This is your new playbook for a successful tech career.\n*   **Content:** Welcome to the V2V Academy. Your journey to landing a great tech job starts now. The skills you learned in school are important, but the real world requires something more: the ability to partner with AI to build amazing things, fast. This expert workflow is called the \"Virtuoso's Loop,\" and it's your unfair advantage. Its a systematic process that will make you stand out. In this lesson, we'll break down every step.\n\n#### **Page 2: Step 1 - The \"Documentation First\" Principle**\n*   **Page Title:** Step 1: Plan Before You Prompt\n*   **Image Prompt:** An image depicting the \"Curation\" phase. On the left, a chaotic mess of project requirements on sticky notes. In the center, a developer uses a clean interface to organize these notes and select relevant code files. On the right, these items form a neat, organized stack labeled \"High-Quality Context.\"\n*   **TL;DR:** Great projects start with a great plan. Before you ask an AI to code, you need to give it a clear blueprint and the right materials.\n*   **Content:** Every successful project starts with preparation. 1. **Gather Your Files:** Collect all the relevant code, notes, and requirements for your task. 2. **Write a Plan:** Create a new document that clearly explains your goal for the current task. This is your blueprint. 3. **Select Context:** Using the DCE, select only the specific files that are relevant to your plan. This creates a focused, high-signal context for the AI.\n\n#### **Page 3: Step 2 - Parallel Prompting & Triage**\n*   **Page Title:** Step 2: Get Multiple Options\n*"
  },
  {
    "id": "report_source",
    "chunk": "to your plan. This creates a focused, high-signal context for the AI.\n\n#### **Page 3: Step 2 - Parallel Prompting & Triage**\n*   **Page Title:** Step 2: Get Multiple Options\n*   **Image Prompt:** A visualization of \"Parallel Prompting.\" A single coding problem is sent out, which then splits and travels down eight parallel pathways to eight AI assistants. The pathways return eight different code solutions.\n*   **TL;DR:** Don't settle for the first answer. By getting multiple AI responses at once, you can compare different coding approaches and pick the cleanest, most efficient solution.\n*   **Content:** A single prompt can have many right answers. The Virtuoso's Loop helps you find the best one. 1. **Generate `prompt.md`:** The DCE automatically creates a complete prompt file for you. 2. **Run in Parallel:** You send this prompt to multiple AI instances. 3. **Parse and Sort:** The responses are loaded into the DCE. With one click, they are parsed and sorted by size. Your review starts with the most detailed code.\n\n#### **Page 4: Step 3 - Critical Analysis & Selection**\n*   **Page Title:** Step 3: You're the Code Reviewer\n*   **Image Prompt:** A close-up of a developer's face, focused and analytical. They are looking at a futuristic diff viewer that highlights changes between two code files. Their hand is poised over a \"Select This Response\" button.\n*   **TL;DR:** The AI writes the code, but you are the lead engineer. Your most important job is to review the AI's work for quality and correctness.\n*   **Content:** This is where you apply your engineering judgment. 1. **Review the Plan:** Does the AI's proposed plan make sense? 2. **Diff the Code:** Use the diff viewer to see the exact code changes. Is it clean? Are there an"
  },
  {
    "id": "report_source",
    "chunk": "ng judgment. 1. **Review the Plan:** Does the AI's proposed plan make sense? 2. **Diff the Code:** Use the diff viewer to see the exact code changes. Is it clean? Are there any obvious bugs? 3. **Select the Best Code:** Based on your review, you select the best solution to test.\n\n#### **Page 5: Step 4 - The Test-and-Revert Loop**\n*   **Page Title:** Step 4: Test Without Fear\n*   **Image Prompt:** A simple diagram showing a Git-based workflow. A \"Baseline (Commit)\" button creates a \"Safe Restore Point.\" An \"Accept Selected\" arrow applies AI code to the \"Live Workspace.\" A \"Test\" phase follows. A \"Failure\" arrow leads to a \"Restore Baseline\" button. A \"Success\" arrow moves forward.\n*   **TL;DR:** The Virtuoso's Loop uses Git to let you test AI-generated code without any risk. If it breaks, you can go back to your last save point in one click.\n*   **Content:** Never trust, always verify. 1. **Create a Baseline:** Click \"Baseline (Commit)\" to create a Git commit. This is your safety net. 2. **Accept Changes:** Click \"Accept Selected\" to apply the AI's code to your project. 3. **Test:** Run your application or tests. 4. **Decide:** If it fails, click \"Restore Baseline\" to instantly undo the changes. If it works, you're ready for the next step.\n\n#### **Page 6: Step 5 - Finalize & Prepare for Next Cycle**\n*   **Page Title:** Step 5: Document and Repeat\n*   **Image Prompt:** A shot of the DCE Panel. The user is typing notes into the \"Cycle Context\" field. The \"Generate prompt.md\" button is highlighted, leading to a `+` (New Cycle) button.\n*   **TL;DR:** The loop finishes by documenting your work and preparing for the next task on your project plan.\n*   **Content:** A successful test sets up the next iteration. 1. **Update Contex"
  },
  {
    "id": "report_source",
    "chunk": " loop finishes by documenting your work and preparing for the next task on your project plan.\n*   **Content:** A successful test sets up the next iteration. 1. **Update Context:** Document what you did or define the next task in the \"Cycle Context\" field. This builds your project's history. 2. **Start a New Cycle:** Click the `+` button to create a new cycle, and the Virtuoso's Loop begins again.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: Introduction - Level Up Your Dev Game**\n*   **Page Title:** Level Up Your Dev Game: Mastering the Virtuoso's Loop\n*   **Image Prompt:** A cinematic shot of a young, focused gamer at a futuristic, multi-monitor battle station. They are orchestrating a complex coding project visualized as a glowing, circular loop of data. The aesthetic is inspired by high-end gaming setups, with RGB lighting and sleek peripherals. The title \"LEVEL UP YOUR DEV GAME\" is prominently displayed.\n*   **TL;DR:** This lesson reveals the secret \"pro-level\" workflow for building insane things with AI. This is the \"meta\" you need to go from hobbyist to master.\n*   **Content:** Welcome to the V2V Academy. You've probably already been \"vibecoding\"making cool stuff with AI just by talking to it. Now, it's time to level up. The \"Virtuoso's Loop\" is the expert-level workflow that transforms that raw creativity into a repeatable, powerful engineering discipline. It's the difference between messing around and building something legendary. Let's break down the combo.\n\n#### **Page 2: Step 1 - The \"Documentation First\" Principle**\n*   **Page Title:** Step 1: Gear Up - Prep Your Inventory\n*   **Image Prompt:** An image depicting the \"Curation\" phase, stylized like a video game inventory screen. On the left,"
  },
  {
    "id": "report_source",
    "chunk": " **Page Title:** Step 1: Gear Up - Prep Your Inventory\n*   **Image Prompt:** An image depicting the \"Curation\" phase, stylized like a video game inventory screen. On the left, a chaotic \"loot drop\" of files and data. In the center, a player is dragging specific items into their inventory slots. On the right, the organized inventory is labeled \"Curated Context.\"\n*   **TL;DR:** Every great quest starts with preparation. Before you command your AI, you need to equip it with the right gear (data) and give it a clear quest objective (a plan).\n*   **Content:** Every successful run starts with the right loadout. 1. **Gather Your Loot:** Collect all the files, notes, and assets you need for your mission. 2. **Write the Quest Log:** Create a new document that clearly defines what you're trying to build. This is your quest objective. 3. **Equip Your AI:** Using the DCE, select only the specific items from your inventory that are relevant to the current quest. This gives your AI a focused, high-power loadout.\n\n#### **Page 3: Step 2 - Parallel Prompting & Triage**\n*   **Page Title:** Step 2: Multi-Summoning Your AI\n*   **Image Prompt:** A visualization of \"Parallel Prompting\" in a fantasy style. A single, powerful spell is cast, which then splits and summons eight different AI familiars. Each familiar returns with a unique and powerful magic scroll (a code solution).\n*   **TL;DR:** Never rely on a single summon. By spawning multiple AI instances at once, you get a variety of solutions and can pick the most OP one.\n*   **Content:** RNG can be a pain. The Virtuoso's Loop lets you roll the dice multiple times at once. 1. **Generate `prompt.md`:** The DCE automatically forges your master spell. 2. **Multi-Summon:** You cast this spell o"
  },
  {
    "id": "report_source",
    "chunk": "o's Loop lets you roll the dice multiple times at once. 1. **Generate `prompt.md`:** The DCE automatically forges your master spell. 2. **Multi-Summon:** You cast this spell on multiple AI instances. 3. **Parse and Sort:** The results appear in the DCE. With one click, they're parsed and sorted by power level (size). You start by inspecting the legendary drops first.\n\n#### **Page 4: Step 3 - Critical Analysis & Selection**\n*   **Page Title:** Step 3: You're the Raid Leader\n*   **Image Prompt:** A close-up of a gamer's face, focused and intense. They are analyzing a futuristic diff viewer that shows the \"stat changes\" between two versions of a code file. Their hand is poised over a glowing \"Select This Build\" button.\n*   **TL;DR:** The AI generates the builds, but you're the raid leader who decides the strategy. Your most important job is to inspect the gear and pick the best one for the job.\n*   **Content:** This is where you make the strategic call. 1. **Check the Strategy:** Does the AI's proposed plan make sense? Is it going to pull the boss correctly? 2. **Inspect the Gear:** Use the diff viewer to check the stats on the code. Is it a clean build? Are there any hidden debuffs (bugs)? 3. **Equip the Best Build:** Based on your inspection, you select the best solution to try out.\n\n#### **Page 5: Step 4 - The Test-and-Revert Loop**\n*   **Page Title:** Step 4: Quick Save & Reload\n*   **Image Prompt:** A simple diagram showing a gaming-style workflow. A \"Quick Save\" button creates a \"Restore Point.\" An \"Equip Build\" arrow applies AI code to the \"Live Character.\" A \"Test in Dungeon\" phase follows. A \"Wipe\" arrow leads to a \"Reload Save\" button. A \"Success\" arrow moves forward.\n*   **TL;DR:** The Virtuoso's Loop has a built"
  },
  {
    "id": "report_source",
    "chunk": " Character.\" A \"Test in Dungeon\" phase follows. A \"Wipe\" arrow leads to a \"Reload Save\" button. A \"Success\" arrow moves forward.\n*   **TL;DR:** The Virtuoso's Loop has a built-in \"quick save\" and \"reload\" feature for your code. If an AI build sucks, you can instantly go back to your last save point.\n*   **Content:** Never trust a new build without testing it on a dummy first. 1. **Quick Save:** Click \"Baseline (Commit)\" to create a save state for your project. 2. **Equip Build:** Click \"Accept Selected\" to equip the AI's code. 3. **Run the Dungeon:** Run your app or tests. 4. **Decide:** If you wipe, just click \"Restore Baseline\" to reload your save. If you clear the dungeon, it's time for the next phase.\n\n#### **Page 6: Step 5 - Finalize & Prepare for Next Cycle**\n*   **Page Title:** Step 5: Log Your Win & Queue for the Next Raid\n*   **Image Prompt:** A shot of the DCE Panel. The user is typing notes into the \"Cycle Context\" field. The \"Generate prompt.md\" button is highlighted, leading to a `+` (New Cycle) button.\n*   **TL;DR:** The loop ends by logging your progress and gearing up for the next challenge.\n*   **Content:** A successful run sets you up for the next one. 1. **Update Quest Log:** Document your win or define the next objective in the \"Cycle Context\" field. This tracks your progress. 2. **Queue for the Next Raid:** Click the `+` button to start a new cycle, and the Virtuoso's Loop begins again.\n</file_artifact>\n\n<file path=\"src/Artifacts/A63 - V2V Academy - Lesson 1.2 - The Philosophy of V2V.md\">\n# Artifact A63: V2V Academy - Lesson 1.2 - The Philosophy of V2V\n# Date Created: C62\n# Author: AI Model & Curator\n# Updated on: C63 (Expand content for all personas and add new section on Cognitive Apprenticeship)\n\n"
  },
  {
    "id": "report_source",
    "chunk": " - The Philosophy of V2V\n# Date Created: C62\n# Author: AI Model & Curator\n# Updated on: C63 (Expand content for all personas and add new section on Cognitive Apprenticeship)\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 1.2 of the V2V Academy, \"The Philosophy of V2V,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, philosophy, interactive learning, persona\n\n## **Lesson 1.2: The Philosophy of V2V**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: The AI as a Feedback Loop**\n*   **Page Title:** Strategic Principle 1: The AI is a Feedback Loop for Your Expertise\n*   **Image Prompt:** A professional in a modern office looking at a holographic screen. The screen shows a circular diagram: \"Human Expertise\" -> \"Expert Feedback\" -> \"AI Action\" -> \"AI Output\" -> \"Human Analysis,\" which then loops back. The diagram illustrates a continuous cycle of refinement.\n*   **TL;DR:** The most powerful way to use AI is not as an instruction-taker, but as a feedback mechanism that amplifies your own cognitive and professional skills.\n*   **Content:** In the V2V pathway, the AI is more than a tool; it's a mirror that creates a feedback loop for your own thought processes. To guide an AI effectively on complex tasks, you must provide \"expert feedback.\" But what if you're not an expert in a new domain, like coding? The system itself provides the feedback. A compiler error, for instance, is an objective, expert critique of the AI's code. By taking that error and feeding it back to the AI with the instruction to \"fix it,\" you enter the loop. You are now directing the AI toward a cor"
  },
  {
    "id": "report_source",
    "chunk": "t critique of the AI's code. By taking that error and feeding it back to the AI with the instruction to \"fix it,\" you enter the loop. You are now directing the AI toward a correct solution while simultaneously learning from the process. This transforms you from a passive user into an active director, using the AI to build and validate your own growing expertise.\n\n#### **Page 2: Data Curation is the Apex Skill**\n*   **Page Title:** Strategic Principle 2: Data Curation is the New Apex Skill\n*   **Image Prompt:** An image of a digital librarian or archivist in a vast, futuristic library. Instead of books, they are organizing glowing blocks of data labeled \"Code,\" \"PDFs,\" and \"Research.\" Their work is precise and architectural, building a \"Source of Truth\" structure.\n*   **TL;DR:** In the AI era, the most valuable professional skill is not knowing how to code, but knowing how to curate the high-quality data that enables an AI to code for you.\n*   **Content:** The V2V methodology posits that traditional programming syntax is becoming a secondary, tactical skill. The new strategic apex skill is **Data Curation**, which is the foundational practice of **Context Engineering**. Why? Because the quality of an AI's output is a direct function of the quality of its input context. The most leveraged activity is not perfecting the command (the prompt), but perfecting the data ecosystem (the context). Your ability to identify, gather, organize, and label relevant informationto build a clean \"source of truth\"is what will differentiate you as a high-impact professional. It is the art of knowing what the AI needs to know.\n\n#### **Page 3: The \"Star Trek\" Motivation**\n*   **Page Title:** The Strategic Vision: Solving Problems of Abundance"
  },
  {
    "id": "report_source",
    "chunk": "ional. It is the art of knowing what the AI needs to know.\n\n#### **Page 3: The \"Star Trek\" Motivation**\n*   **Page Title:** The Strategic Vision: Solving Problems of Abundance\n*   **Image Prompt:** A stunning, cinematic shot of a Starship Enterprise-like vessel exploring a beautiful, colorful nebula. The image evokes a sense of hope, discovery, and a future where humanity has overcome petty conflicts to focus on grander challenges.\n*   **TL;DR:** The ultimate goal of mastering this workflow is to accelerate human progress, enabling us to solve major world problems and focus on a future of exploration and abundance.\n*   **Content:** The driving philosophy behind this work is deeply aspirational. We are building these tools and teaching these skills to accelerate human progress. In a world with seemingly infinite challenges, the V2V pathway provides a methodology to create an abundance of solutions. By empowering individuals to become \"Citizen Architects,\" we can tackle major societal problems from the bottom up. The ultimate motivation is to help create a \"Star Trek\" futurea world where our collective energy is focused on exploration, discovery, and solving the grand challenges of science and society, rather than being mired in conflicts born of scarcity.\n\n#### **Page 4: The AI as a Cognitive Mentor**\n*   **Page Title:** Pedagogical Model: The AI as a Cognitive Mentor\n*   **Image Prompt:** A wise, holographic mentor figure is shown guiding a professional through a complex strategic blueprint. The mentor is pointing out key connections and patterns that the professional had not seen, making the \"hidden curriculum\" of expert thinking visible.\n*   **TL;DR:** The V2V pathway is built on the Cognitive Apprenticeship model, wh"
  },
  {
    "id": "report_source",
    "chunk": "hat the professional had not seen, making the \"hidden curriculum\" of expert thinking visible.\n*   **TL;DR:** The V2V pathway is built on the Cognitive Apprenticeship model, where the AI serves as a tireless expert who makes their implicit thought processes explicit and learnable for you.\n*   **Content:** The V2V curriculum is structured around a powerful pedagogical model: Cognitive Apprenticeship. The central challenge in acquiring any new expertise is that an expert's most critical skillstheir intuition, their problem-solving heuristicsare often internal and invisible. Cognitive Apprenticeship makes this \"hidden curriculum\" visible. In our model, the AI acts as the expert. By prompting it to explain its reasoning, or by analyzing the code it produces, you are observing an expert's thought process. By critiquing its output and guiding it to a better solution, you are actively engaging in a dialogue that forces both you and the AI to articulate your reasoning. This process, facilitated by the AI mentor, is the engine of your skill development.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: The AI as a Feedback Loop**\n*   **Page Title:** Your Secret Weapon: The AI is a Feedback Loop for Learning\n*   **Image Prompt:** A student is shown working on a coding problem. A compiler error message appears. An arrow shows the student feeding this error message to an AI assistant, which then provides a corrected code snippet and an explanation. The student has a \"lightbulb\" moment of understanding.\n*   **TL;DR:** Don't fear errorsthey are your most powerful learning tool. The V2V method teaches you to use AI as a partner that turns your mistakes into immediate, practical lessons.\n*   **Content:** The V2V path"
  },
  {
    "id": "report_source",
    "chunk": "e your most powerful learning tool. The V2V method teaches you to use AI as a partner that turns your mistakes into immediate, practical lessons.\n*   **Content:** The V2V pathway redefines how you learn technical skills. The AI is your personal 24/7 tutor. Its most important function is to create a feedback loop that accelerates your learning. You don't need to be an expert to start. When the AI generates code that produces an error, that error message *is* a form of expert feedback. Your job is to take that feedback and give it back to the AI. By doing this, you enter a powerful learning loop where you guide the AI to the right answer and, in the process, learn exactly why the initial code was wrong. This is the fastest way to bridge the gap between academic knowledge and real-world skill.\n\n#### **Page 2: Data Curation is the Apex Skill**\n*   **Page Title:** The Skill That Gets You Hired: Data Curation\n*   **Image Prompt:** An image of a job description for a \"Next-Gen Software Engineer.\" The \"Required Skills\" section is highlighted, showing \"Data Curation,\" \"Context Engineering,\" and \"Critical Analysis of AI Output\" listed above \"Python/JavaScript.\"\n*   **TL;DR:** The job market is changing. Employers are looking for people who can direct AI, and the most important skill for that is the ability to curate high-quality data.\n*   **Content:** The V2V curriculum is designed to teach you the skills employers are actually looking for in the AI era. While coding is still important, the new apex skill is **Data Curation**, also known as **Context Engineering**. Why? Because an AI is only as good as the data you give it. Your ability to find, organize, and structure the right information for a task is what will make you a highl"
  },
  {
    "id": "report_source",
    "chunk": "ing**. Why? Because an AI is only as good as the data you give it. Your ability to find, organize, and structure the right information for a task is what will make you a highly effectiveand highly hirabledeveloper. This course focuses on making you an expert curator, the person who builds the \"source of truth\" that enables powerful AI performance and demonstrates a level of strategic thinking that will make your portfolio stand out.\n\n#### **Page 3: The \"Star Trek\" Motivation**\n*   **Page Title:** The Big Picture: Building a Better Future\n*   **Image Prompt:** A diverse group of young, brilliant engineers collaborating in a bright, solarpunk-style innovation hub. They are working on holographic interfaces, designing solutions for clean energy, sustainable cities, and space exploration. The atmosphere is optimistic and forward-looking.\n*   **TL;DR:** The skills you are learning aren't just for a job; they are the tools that will empower you and your generation to solve major world problems.\n*   **Content:** The V2V pathway is about more than just coding. It's about empowering you to build a better future. The ultimate vision is to accelerate human progress so we can tackle the big challengesfrom climate change to space exploration. By mastering these skills, you become part of a new generation of \"Citizen Architects\" who have the power to turn ambitious ideas into reality. This isn't just about building a career; it's about building a portfolio of impactful work that contributes to the world.\n\n#### **Page 4: The AI as a Cognitive Mentor**\n*   **Page Title:** Your Unfair Advantage: The AI as a Cognitive Mentor\n*   **Image Prompt:** A student is shown climbing a steep mountain labeled \"Skill Acquisition.\" A holographic me"
  },
  {
    "id": "report_source",
    "chunk": "e Title:** Your Unfair Advantage: The AI as a Cognitive Mentor\n*   **Image Prompt:** A student is shown climbing a steep mountain labeled \"Skill Acquisition.\" A holographic mentor figure is beside them, creating glowing handholds and footholds (scaffolding) just where the student needs them, making the difficult climb possible.\n*   **TL;DR:** The V2V curriculum uses the Cognitive Apprenticeship model, where the AI acts as an expert mentor who can show you exactly how a professional thinks and solves problems.\n*   **Content:** The V2V curriculum is built on a powerful learning secret: Cognitive Apprenticeship. When you learn from an expert, the hardest part is understanding *how* they think. Their best skills are often invisible. This is where the AI comes in. It acts as your expert mentor, and its biggest advantage is that it can make its thought process visible. By asking it to explain its code, or by analyzing why it made a certain choice, you get a direct look into an expert's mind. This process, where the AI models expert behavior and coaches you through your mistakes, is the fastest way to go from graduate to a sought-after professional.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: The AI as a Feedback Loop**\n*   **Page Title:** The Ultimate Power-Up: AI as a Feedback Loop\n*   **Image Prompt:** A video game-style UI. A character attempts a complex move and fails, with a \"COMBO FAILED\" message appearing. An AI companion analyzes the failure and provides a holographic overlay showing the correct button sequence. The character then successfully executes the move.\n*   **TL;DR:** Failure is part of the game. The V2V method teaches you to use AI as a co-op partner that instantly analyzes your fails and sh"
  },
  {
    "id": "report_source",
    "chunk": "n successfully executes the move.\n*   **TL;DR:** Failure is part of the game. The V2V method teaches you to use AI as a co-op partner that instantly analyzes your fails and shows you how to land the combo perfectly next time.\n*   **Content:** In the V2V pathway, every bug is a power-up. The AI is your ultimate co-op partner, creating a feedback loop to level up your skills at lightning speed. You don't need to be a pro to start. When the AI generates code that breaks, the error message is a \"boss pattern\" revealed. Your mission is to feed that pattern back to the AI. By doing this, you enter a learning loop where you're not just beating the levelyou're mastering the game's mechanics. It's the ultimate form of deliberate practice.\n\n#### **Page 2: Data Curation is the Apex Skill**\n*   **Page Title:** The New Meta: Data Curation is the Apex Skill\n*   **Image Prompt:** An image of a \"skill tree\" from an RPG. At the very top, in the \"Ultimate Skill\" slot, is an icon for \"Data Curation.\" Branching down from it are skills like \"Code Generation,\" \"Automation,\" and \"System Design,\" showing that they all depend on the master skill.\n*   **TL;DR:** The meta has shifted. The most OP skill in the AI era isn't codingit's knowing how to organize your loot (data) to craft the ultimate enchanted weapon (context) for your AI.\n*   **Content:** The V2V Academy teaches you the new meta. While knowing how to code is cool, the real S-tier skill is **Data Curation**, the core of **Context Engineering**. Why? Because an AI is only as powerful as the gear you equip it with. Your ability to find, organize, and label the right information for a quest is what separates the noobs from the legends. This course is designed to make you a master blacksm"
  },
  {
    "id": "report_source",
    "chunk": " Your ability to find, organize, and label the right information for a quest is what separates the noobs from the legends. This course is designed to make you a master blacksmith of context, forging the \"source of truth\" that unlocks your AI's ultimate power and lets you build truly epic things.\n\n#### **Page 3: The \"Star Trek\" Motivation**\n*   **Page Title:** The Endgame Quest: The \"Star Trek\" Future\n*   **Image Prompt:** A stunning, cinematic image of a player character standing on the bridge of a starship, looking out at a vast, unexplored galaxy. The image is filled with a sense of adventure, wonder, and limitless possibility.\n*   **TL;DR:** The ultimate quest is to use these skills to build a future worthy of a sci-fi epica world of exploration, discovery, and epic challenges.\n*   **Content:** The V2V pathway is about more than just building cool projects. It's about unlocking the \"endgame\" for humanity. The ultimate motivation is to build a \"Star Trek\" futurea world where we've beaten the boring bosses of scarcity and conflict and can focus on the epic raids of space exploration and solving the universe's greatest mysteries. By mastering these skills, you become a \"Citizen Architect,\" one of the heroes with the power to build that future. This isn't just a game; it's the greatest quest of all.\n\n#### **Page 4: The AI as a Cognitive Mentor**\n*   **Page Title:** The Secret Technique: The AI as a Cognitive Mentor\n*   **Image Prompt:** An apprentice is sparring with a holographic master warrior. The master perfectly executes a complex technique, then replays it in slow motion, highlighting the critical movements and explaining the strategy behind them.\n*   **TL;DR:** The V2V pathway is based on Cognitive Apprenticeship"
  },
  {
    "id": "report_source",
    "chunk": "hen replays it in slow motion, highlighting the critical movements and explaining the strategy behind them.\n*   **TL;DR:** The V2V pathway is based on Cognitive Apprenticeship. It's like learning a secret fighting style directly from a legendary master (the AI) who can show you not just the moves, but the thinking behind the moves.\n*   **Content:** The V2V curriculum is built on a secret technique: Cognitive Apprenticeship. The hardest part of learning from a master is that you can't see what they're thinking. Their best moves are invisible. This is where the AI becomes your sensei. It can make its thought process visible. By asking it to explain its code, or by analyzing its plan, you get a direct look into a master's mind. The AI models the perfect technique, coaches you when you mess up, and gives you the scaffolding you need to pull off moves you couldn't do on your own. This is the ultimate training arc.\n</file_artifact>\n\n<file path=\"src/Artifacts/A64 - V2V Academy - Lesson 1.3 - The Citizen Architect.md\">\n# Artifact A64: V2V Academy - Lesson 1.3 - The Citizen Architect\n# Date Created: C62\n# Author: AI Model & Curator\n# Updated on: C64 (Expand content for all personas and add new section on the architect's role in society)\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 1.3 of the V2V Academy, \"The Citizen Architect,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, citizen architect, interactive learning, persona\n\n## **Lesson 1.3: The Citizen Architect**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: Defining the Citizen Architect**\n*   **Page Title:**"
  },
  {
    "id": "report_source",
    "chunk": "arning, persona\n\n## **Lesson 1.3: The Citizen Architect**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: Defining the Citizen Architect**\n*   **Page Title:** The New Archetype: The Citizen Architect\n*   **Image Prompt:** A diverse group of professionalsa project manager, a military officer, a marketing strategistare depicted collaborating in a futuristic workspace. They are using holographic interfaces to assemble complex systems from glowing, modular components, demonstrating their ability to build without traditional coding.\n*   **TL;DR:** The Citizen Architect is a new professional archetype: a domain expert who leverages AI and structured workflows to design and build complex systems, contributing meaningfully to their community and profession.\n*   **Content:** The V2V pathway prepares you for a new and powerful role in the modern economy: the Citizen Architect. This is not a \"citizen developer\" who builds simple apps from templates. The Citizen Architect is a strategic thinker who combines their deep domain expertise with the power of AI to orchestrate the creation of sophisticated, mission-critical systems. They are the \"Navigators\" who provide the vision, the context, and the critical judgment, while the AI acts as the \"Driver,\" handling the tactical implementation. This role transcends traditional job titles, empowering you to become a creator and a systems builder within your field, using your unique talents to improve the community and human condition.\n\n#### **Page 2: The Core Competency: Cognitive Capital**\n*   **Page Title:** Your Core Asset: Cultivating Cognitive Capital\n*   **Image Prompt:** An image showing a human brain composed of glowing, interconnected circuits. Data streams represe"
  },
  {
    "id": "report_source",
    "chunk": "Title:** Your Core Asset: Cultivating Cognitive Capital\n*   **Image Prompt:** An image showing a human brain composed of glowing, interconnected circuits. Data streams representing \"Domain Expertise,\" \"Critical Thinking,\" and \"Systems Design\" flow into it, increasing its brightness and complexity.\n*   **TL;DR:** The primary function of the Citizen Architect is to generate and apply Cognitive Capitalthe collective problem-solving capacity of a team or organization.\n*   **Content:** As a Citizen Architect, your most valuable contribution is your ability to generate Cognitive Capital. This is the collective skill and creative potential of your team. In an age where AI can automate routine tasks, the ability to solve novel problems, innovate under pressure, and adapt to new challenges becomes the primary engine of value. The V2V workflow is a system for cultivating this asset. By learning to structure problems, curate data, and critically validate AI outputs, you are not just completing tasksyou are building your organization's most important strategic resource.\n\n#### **Page 3: The Architect as Storyteller and Collaborator**\n*   **Page Title:** The Architect's Role: Storyteller and Collaborator\n*   **Image Prompt:** A Citizen Architect stands before a diverse group of community stakeholders, presenting a holographic visualization of a new system. They are not just showing data; they are telling a compelling story about how the system will improve their lives. The atmosphere is one of collaboration and shared understanding.\n*   **TL;DR:** A Citizen Architect coordinates the social and design processes that lead to creation; communication and storytelling are fundamental to this collaborative process.\n*   **Content:** The te"
  },
  {
    "id": "report_source",
    "chunk": "hitect coordinates the social and design processes that lead to creation; communication and storytelling are fundamental to this collaborative process.\n*   **Content:** The term \"Citizen Architect\" has deep roots in the field of architecture, where it describes a professional who is not just a builder, but a community leader engaged in civic advocacy. This broader role emphasizes that architects do not simply build things; they coordinate the complex social and design processes that lead to building. As a Citizen Architect in the digital realm, your role is the same. Your ability to communicate a vision, engage with stakeholders, and tell a compelling story about the \"why\" behind your project is as important as your technical skill. The V2V pathway teaches you to be both a builder and a storyteller, enabling you to lead collaborative change.\n\n#### **Page 4: The Strategic Impact**\n*   **Page Title:** The Strategic Impact of the Citizen Architect\n*   **Image Prompt:** A \"before and after\" diptych. \"Before\": A traditional, hierarchical corporate structure, slow and bureaucratic. \"After\": A dynamic, decentralized network of empowered Citizen Architects, rapidly innovating and adapting to market changes.\n*   **TL;DR:** By empowering domain experts to build their own solutions, the Citizen Architect model creates more agile, resilient, and innovative organizations that can better serve society.\n*   **Content:** The rise of the Citizen Architect has profound strategic implications. It represents a shift from centralized, top-down innovation to a decentralized model where the individuals closest to a problem are empowered to solve it. This creates organizations that are faster, more agile, and more resilient. Citizen Architects "
  },
  {
    "id": "report_source",
    "chunk": "zed model where the individuals closest to a problem are empowered to solve it. This creates organizations that are faster, more agile, and more resilient. Citizen Architects are called to be aware of the social and ecological impacts of their design choices, ensuring that what they build serves the greater good. By mastering the V2V pathway, you are not just upgrading your personal skillset; you are becoming a catalyst for organizational transformation, equipped to lead with care and social responsibility in an era defined by rapid technological change.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: Defining the Citizen Architect**\n*   **Page Title:** Your New Job Title: The Citizen Architect\n*   **Image Prompt:** A young, confident developer stands before a holographic \"career path\" diagram. The traditional path (\"Junior Dev -> Mid-Level -> Senior\") is shown as a slow, linear ladder. A new, dynamic path labeled \"Citizen Architect\" branches off, leading directly to high-impact roles like \"AI Systems Designer\" and \"Solutions Architect.\"\n*   **TL;DR:** The Citizen Architect is the job title of the future. It's a role for developers who can think strategically, direct AI partners, and build complex systems, making them far more valuable than traditional coders.\n*   **Content:** The V2V Academy trains you for the jobs that will define the next decade of tech. The \"Citizen Architect\" is a new kind of developerone who combines technical skills with architectural vision and civic purpose. They are the ones who can lead a human-AI team, translating a high-level goal into a functional, robust application. They understand that their primary job is not just to write code, but to design the systems and curate "
  },
  {
    "id": "report_source",
    "chunk": "m, translating a high-level goal into a functional, robust application. They understand that their primary job is not just to write code, but to design the systems and curate the context that allows an AI to write *better* code. This is the role that commands a premium salary and offers a path to leadership by applying your talents to improve the community around you.\n\n#### **Page 2: The Core Competency: Cognitive Capital**\n*   **Page Title:** Your Killer Skill: Generating Cognitive Capital\n*   **Image Prompt:** An image of a developer's brain, glowing with activity. Connections are being forged between \"CS Fundamentals,\" \"AI Collaboration Skills,\" and \"Problem-Solving,\" creating a powerful, synergistic network.\n*   **TL;DR:** Your real value isn't just what you knowit's your ability to solve new, hard problems. This skill, called Cognitive Capital, is what you'll master in the V2V Academy.\n*   **Content:** As a Citizen Architect, your most valuable asset is your Cognitive Capital. This is your personal capacity to solve novel problems and innovate. In a world where AI can handle routine coding, employers are looking for people who can tackle the tough, unstructured challenges. The V2V workflow is a system for building this skill. By learning to structure problems, curate data, and critically validate AI outputs, you are building a powerful problem-solving engine that will make you indispensable to any team.\n\n#### **Page 3: The Architect as Storyteller and Collaborator**\n*   **Page Title:** More Than a Coder: The Architect as Storyteller\n*   **Image Prompt:** A young developer is confidently presenting a project to a team. On a large screen behind them is a clear, compelling visualization of the project's architecture a"
  },
  {
    "id": "report_source",
    "chunk": "e Prompt:** A young developer is confidently presenting a project to a team. On a large screen behind them is a clear, compelling visualization of the project's architecture and user flow. They are not just showing code; they are communicating a vision and telling a story.\n*   **TL;DR:** Top-tier architects are not just builders; they are great communicators who can coordinate the social and design processes that lead to a final product.\n*   **Content:** The most successful professionals in any field are effective communicators. In the traditional definition, a Citizen Architect is a storytellersomeone who can engage and converse with the world to coordinate the complex process of creation. As you build your career, your ability to articulate a technical vision to non-technical stakeholders will be a massive advantage. The V2V pathway doesn't just teach you how to build with AI; it teaches you how to think and communicate like an architect. You'll learn to document your process and justify your design decisions, skills that are fundamental to leading projects and teams.\n\n#### **Page 4: The Strategic Impact**\n*   **Page Title:** Why This Role Matters: Your Impact on the Future\n*   **Image Prompt:** A young developer is shown presenting a project to a group of impressed senior executives. The project, built using the V2V workflow, is a sleek, innovative application that solves a major company problem. The developer is seen as a key innovator, not just a junior coder.\n*   **TL;DR:** Citizen Architects are the new innovators. By mastering this workflow, you move from being a task-taker to a value-creator, the person who builds the solutions that drive a company forward.\n*   **Content:** The Citizen Architect is at the cente"
  },
  {
    "id": "report_source",
    "chunk": ", you move from being a task-taker to a value-creator, the person who builds the solutions that drive a company forward.\n*   **Content:** The Citizen Architect is at the center of modern innovation. They are the ones who can bridge the gap between a business need and a technical solution, using AI as a force multiplier. By mastering the V2V pathway, you position yourself not as someone who just closes tickets, but as someone who creates new products and new value. You become the engine of progress for your team and your company. This is about more than just getting a job; it's about building a career with real impact, where you are recognized for your insights and your ability to lead.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: Defining the Citizen Architect**\n*   **Page Title:** The Final Class: The Citizen Architect\n*   **Image Prompt:** A powerful, god-like figure is shown in a digital realm, effortlessly creating entire worlds and complex structures with gestures and thought. They are surrounded by AI companions who instantly execute their grand vision. The title \"THE CITIZEN ARCHITECT\" is emblazoned in epic, glowing letters.\n*   **TL;DR:** The Citizen Architect is the final evolution of the V2V pathway. It's a master-class developer who can build anything they can imagine by orchestrating legions of AI partners for the greater good.\n*   **Content:** You've learned the loops, you've mastered the skills. Now, it's time to understand the final class: the Citizen Architect. This isn't just a developer; it's a master builder. A Citizen Architect is a creative force who combines their unique vision with the power of AI to build complex, world-changing systems. They are the \"Navigators\" who chart the cou"
  },
  {
    "id": "report_source",
    "chunk": "tizen Architect is a creative force who combines their unique vision with the power of AI to build complex, world-changing systems. They are the \"Navigators\" who chart the course, while their AI crew acts as the \"Drivers,\" making it happen at light speed. This is the ultimate expression of creative power in the digital age, using your talents to improve the community and human condition.\n\n#### **Page 2: The Core Competency: Cognitive Capital**\n*   **Page Title:** Your Ultimate Stat: Cognitive Capital\n*   **Image Prompt:** An image of a character sheet from a futuristic RPG. The \"Primary Stat\" is highlighted: a glowing, maxed-out bar labeled \"Cognitive Capital,\" with an infinity symbol. Stats like \"Strength\" and \"Dexterity\" are shown as secondary.\n*   **TL;DR:** The most OP stat you can level up is your Cognitive Capitalyour raw problem-solving power. The V2V pathway is a system for grinding this stat to legendary levels.\n*   **Content:** As a Citizen Architect, your power isn't measured in lines of code; it's measured in Cognitive Capital. This is your ability to solve impossible problems and innovate on the fly. In a world where AI can handle the grind, the players who can think strategically and creatively are the ones who will dominate the leaderboards. The V2V workflow is your personal training dojo for this skill. Every cycle you run, every bug you fix, every piece of context you curate levels up your Cognitive Capital, making you an unstoppable creative force.\n\n#### **Page 3: The Architect as Storyteller and Collaborator**\n*   **Page Title:** The Lore Master: Architect as Storyteller\n*   **Image Prompt:** A character resembling a \"lore master\" or \"dungeon master\" is shown weaving a grand narrative on a holographic"
  },
  {
    "id": "report_source",
    "chunk": " The Lore Master: Architect as Storyteller\n*   **Image Prompt:** A character resembling a \"lore master\" or \"dungeon master\" is shown weaving a grand narrative on a holographic map. The story they tell is being instantly translated by AI companions into a living, breathing digital world that other players can explore.\n*   **TL;DR:** The greatest architects don't just build structures; they build worlds. To do that, you need to be a master storyteller who can communicate your vision and lead your team on an epic quest.\n*   **Content:** A key part of being a Citizen Architect is learning to be a storyteller. You don't just build things; you coordinate the entire creative process. Think of it like being a game master: you have to communicate the vision, describe the world, and guide the players (and your AI companions) through the adventure. The V2V pathway teaches you how to articulate your ideas with such clarity that your AI partners can execute your vision flawlessly. Mastering this skill is what separates a simple builder from a true world-creator.\n\n#### **Page 4: The Strategic Impact**\n*   **Page Title:** The Power of a World-Builder\n*   **Image Prompt:** A Citizen Architect is shown on a \"creator\" screen, similar to a game's map editor. They are designing and launching entire new \"game worlds\" (applications and systems) with a few clicks, which are then instantly populated by users.\n*   **TL;DR:** A Citizen Architect doesn't just play the game; they build new ones. Mastering this role gives you the power to create the platforms and systems that others will use.\n*   **Content:** The Citizen Architect is the ultimate game-changer. They don't just follow the questlinesthey write them. By mastering the V2V pathway, you g"
  },
  {
    "id": "report_source",
    "chunk": "thers will use.\n*   **Content:** The Citizen Architect is the ultimate game-changer. They don't just follow the questlinesthey write them. By mastering the V2V pathway, you gain the ability to build the tools, platforms, and worlds that will shape the future. You move from being a player in someone else's system to being the creator of your own. This is the highest level of agency and impact, giving you the power to bring any idea, no matter how ambitious, to life and use it for the greater good of society.\n</file_artifact>\n\n<file path=\"src/Artifacts/A65 - V2V Academy - Lesson 2.1 - Introduction to Data Curation.md\">\n# Artifact A65: V2V Academy - Lesson 2.1 - Introduction to Data Curation\n# Date Created: C65\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 2.1 of the V2V Academy, \"Introduction to Data Curation,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, data curation, context engineering, interactive learning, persona\n\n## **Lesson 2.1: Introduction to Data Curation**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: Defining Data Curation**\n*   **Page Title:** From Information Overload to Strategic Asset: The Principles of Data Curation\n*   **Image Prompt:** A seasoned professional stands before a chaotic storm of digital information (emails, reports, charts). With calm, deliberate gestures, they are selecting, organizing, and channeling this information into a clean, structured, and glowing data stream labeled \"High-Quality Context.\"\n*   **TL;DR:** Data Curation is the professional discipline of transforming raw, disorganize"
  },
  {
    "id": "report_source",
    "chunk": " into a clean, structured, and glowing data stream labeled \"High-Quality Context.\"\n*   **TL;DR:** Data Curation is the professional discipline of transforming raw, disorganized information into a high-signal, structured asset that empowers AI to perform complex tasks with precision and reliability.\n*   **Content:** In the age of AI, the ability to manage information is the ultimate strategic advantage. Data Curation is the process of transforming the chaotic flood of raw data that surrounds us into a focused, high-quality asset. It's the art and science of identifying what information is relevant, organizing it logically, and structuring it in a way that an AI can understand. For the professional, this is not a technical chore; it is a high-leverage activity. By mastering data curation, you move from being a consumer of AI to its director, ensuring that the AI's power is always aligned with your strategic intent.\n\n#### **Page 2: Why It's the Most Important Skill**\n*   **Page Title:** The \"Garbage In, Garbage Out\" Principle\n*   **Image Prompt:** A side-by-side comparison. On the left, a machine labeled \"AI\" is fed a pile of digital \"garbage\" (blurry images, jumbled text) and outputs a confusing, nonsensical blueprint. On the right, the same machine is fed a clean, organized stack of \"Curated Data\" and outputs a brilliant, precise architectural plan.\n*   **TL;DR:** An AI is only as good as the data you give it. Mastering data curation is the single most effective way to guarantee high-quality, reliable, and valuable AI outputs.\n*   **Content:** The oldest rule in computing is \"Garbage In, Garbage Out\" (GIGO), and it has never been more relevant than in the age of AI. An LLM, no matter how powerful, cannot produce a brillia"
  },
  {
    "id": "report_source",
    "chunk": "dest rule in computing is \"Garbage In, Garbage Out\" (GIGO), and it has never been more relevant than in the age of AI. An LLM, no matter how powerful, cannot produce a brilliant analysis from incomplete, incorrect, or irrelevant information. Its output is a direct reflection of its input. This is why Data Curation has become the new apex skill. While others focus on the tactical art of \"prompting,\" the Virtuoso focuses on the strategic discipline of building a superior context. By ensuring the AI receives a clean, well-organized, and highly relevant set of information, you eliminate the root cause of most AI failures and guarantee a higher quality of work.\n\n#### **Page 3: How to Curate Data**\n*   **Page Title:** The Curator's Method: Gather, Organize, Label\n*   **Image Prompt:** A three-panel diagram showing the core workflow. Panel 1: \"GATHER,\" showing a professional pulling in documents, code, and spreadsheets from various sources. Panel 2: \"ORGANIZE,\" showing them arranging the data into a logical folder structure. Panel 3: \"LABEL,\" showing them applying clear, descriptive names and tags to the organized data.\n*   **TL;DR:** The core process of data curation can be broken down into three simple steps: gathering all relevant information, organizing it into a logical structure, and labeling it for clarity.\n*   **Content:** The practice of data curation follows a straightforward, three-step process. First, you **Gather**. Think like an archivist: collect all the source materials relevant to your taskdocuments, code files, spreadsheets, research papers. Second, you **Organize**. Think like a librarian: arrange these materials into a logical folder structure that makes sense to both you and the AI. Group related items tog"
  },
  {
    "id": "report_source",
    "chunk": "ers. Second, you **Organize**. Think like a librarian: arrange these materials into a logical folder structure that makes sense to both you and the AI. Group related items together. Third, you **Label**. Think like a cataloger: give your files and folders clear, descriptive names. This process of creating a well-structured and clearly labeled \"library\" of information is the foundational act of building a high-quality context.\n\n#### **Page 4: The Curator's Toolkit**\n*   **Page Title:** The Right Tool for the Job: The Data Curation Environment (DCE)\n*   **Image Prompt:** A sleek, futuristic toolkit is open on a workbench. Inside are glowing digital tools labeled \"Context Selector,\" \"Parallel Co-Pilot,\" and \"Cycle Navigator.\" A professional is shown confidently selecting the \"Context Selector\" tool.\n*   **TL;DR:** The Data Curation Environment (DCE) is a specialized toolset built directly into VS Code, designed to make the process of gathering, organizing, and using curated data seamless and efficient.\n*   **Content:** To practice a professional discipline, you need professional tools. The Data Curation Environment (DCE) is the purpose-built toolkit for the Citizen Architect. It integrates the entire curation workflow directly into your development environment. Its File Tree View allows you to visually select your context with simple checkboxes, eliminating manual copy-pasting. Its Parallel Co-Pilot Panel allows you to manage and test the AI's output. The rest of this course will be dedicated to mastering this toolkit and applying it to build powerful, AI-driven solutions.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: Defining Data Curation**\n*   **Page Title:** Skill #1: How to Build the High-Quality "
  },
  {
    "id": "report_source",
    "chunk": " AI-driven solutions.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: Defining Data Curation**\n*   **Page Title:** Skill #1: How to Build the High-Quality Context Employers Want\n*   **Image Prompt:** A young graduate is at a job interview. The hiring manager is pointing to a section on their resume that is glowing: \"Proficient in Data Curation & Context Engineering.\" The manager looks impressed.\n*   **TL;DR:** Data Curation is the skill of organizing raw information into a clean, structured package that an AI can understand. It's one of the most in-demand skills in the new tech job market.\n*   **Content:** Welcome to the first and most important skill you'll learn in the V2V Academy: Data Curation. Think of it as preparing the ultimate \"cheat sheet\" for an AI. Its the process of taking a messy pile of project files, notes, and requirements and turning it into a perfectly organized, easy-to-understand package of information. In the real world, this is called \"Context Engineering,\" and it's what separates a junior developer from a high-impact engineer. Companies need people who can make their AI tools work effectively, and that all starts with building high-quality context.\n\n#### **Page 2: Why It's the Most Important Skill**\n*   **Page Title:** The \"Garbage In, Garbage Out\" Rule\n*   **Image Prompt:** A side-by-side comparison. On the left, a student hands a professor a messy, disorganized term paper and gets a \"C-\". On the right, a student hands in a clean, well-structured paper and gets an \"A+\". The professor is labeled \"AI.\"\n*   **TL;DR:** An AI can't give you \"A+\" work if you give it \"C-\" materials. Learning data curation is the fastest way to ensure you get high-quality, impressive results from you"
  },
  {
    "id": "report_source",
    "chunk": " **TL;DR:** An AI can't give you \"A+\" work if you give it \"C-\" materials. Learning data curation is the fastest way to ensure you get high-quality, impressive results from your AI partner every time.\n*   **Content:** There's a golden rule in tech: \"Garbage In, Garbage Out\" (GIGO). It means that if you put bad data into a system, you'll get bad results out. This is especially true for AI. An LLM can't write brilliant code if you give it a confusing, disorganized, or incomplete project description. Most AI failures aren't the AI's fault; they're the result of a poorly curated context. By mastering data curation, you learn how to control the quality of the input, which gives you control over the quality of the output. This is the key to building a portfolio of impressive, working projects.\n\n#### **Page 3: How to Curate Data**\n*   **Page Title:** The Curator's Method: Gather, Organize, Label\n*   **Image Prompt:** A three-panel diagram showing the workflow. Panel 1: \"GATHER,\" showing a student collecting all the files for a class project. Panel 2: \"ORGANIZE,\" showing them creating folders for \"Source Code,\" \"Assets,\" and \"Requirements.\" Panel 3: \"LABEL,\" showing them giving the files clear, descriptive names.\n*   **TL;DR:** The process is simple and follows three steps you already know: gather all your files, organize them into logical folders, and give everything a clear name.\n*   **Content:** The good news is that you already have the basic skills for data curation. The process follows three simple steps. First, **Gather**. Collect all the files and resources you need for your project into one place. Second, **Organize**. Create a clean folder structure. Group your source code, your documentation, and your reference materia"
  },
  {
    "id": "report_source",
    "chunk": "sources you need for your project into one place. Second, **Organize**. Create a clean folder structure. Group your source code, your documentation, and your reference materials into separate, logical folders. Third, **Label**. Use clear, descriptive names for your files and folders. Don't use generic names like `file1.js`. A name like `user-authentication-service.js` provides valuable context to both you and your AI partner.\n\n#### **Page 4: The Curator's Toolkit**\n*   **Page Title:** Your New Favorite Tool: The Data Curation Environment (DCE)\n*   **Image Prompt:** A sleek, powerful toolkit is open on a workbench. Inside are glowing digital tools labeled \"Context Selector,\" \"Parallel Co-Pilot,\" and \"Cycle Navigator.\" A young developer is shown confidently selecting the \"Context Selector\" tool.\n*   **TL;DR:** The Data Curation Environment (DCE) is a VS Code extension that makes the process of gathering, organizing, and using your curated data fast, easy, and professional.\n*   **Content:** To do a professional job, you need professional tools. The Data Curation Environment (DCE) is the toolkit you'll use throughout this course. It's a VS Code extension that brings the entire curation workflow into your code editor. Its File Tree View lets you select your context with simple checkboxes. Its Parallel Co-Pilot Panel lets you manage and test the AI's code. This course is designed to make you an expert user of this powerful tool, giving you a tangible, in-demand skill for your resume.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: Defining Data Curation**\n*   **Page Title:** The Ultimate Inventory Management: Mastering Your Data\n*   **Image Prompt:** A character from a video game stands before a massive, glowing "
  },
  {
    "id": "report_source",
    "chunk": "ata Curation**\n*   **Page Title:** The Ultimate Inventory Management: Mastering Your Data\n*   **Image Prompt:** A character from a video game stands before a massive, glowing inventory screen. On the left is a chaotic pile of unsorted \"loot\" (files, data, items). The character is skillfully dragging, sorting, and stacking this loot into a perfectly organized grid on the right, labeled \"Optimized Loadout.\"\n*   **TL;DR:** Data Curation is like expert-level inventory management for your projects. It's the skill of organizing all your digital \"loot\" into a perfect loadout that gives your AI companion a massive power boost.\n*   **Content:** Welcome to your first lesson in becoming a Virtuoso. The first secret technique you need to master is **Data Curation**. Think of it as the ultimate form of inventory management. When you start a new project, you have a massive pile of lootcode files, ideas, images, notes. Data Curation is the process of sorting that loot, equipping the best gear, and organizing it into an optimized loadout. This loadout is the \"context\" you give to your AI. A perfectly curated context is like giving your AI companion a full set of legendary enchanted gearit makes them exponentially more powerful.\n\n#### **Page 2: Why It's the Most Important Skill**\n*   **Page Title:** The \"Garbage In, Garbage Out\" Law\n*   **Image Prompt:** A side-by-side comparison in a fantasy game. On the left, a blacksmith is given rusty, broken materials (\"Garbage In\") and forges a weak, useless sword (\"Garbage Out\"). On the right, the same blacksmith is given glowing, high-quality ore (\"Curated Data\") and forges a legendary, epic sword. The blacksmith is labeled \"AI.\"\n*   **TL;DR:** You can't craft an epic weapon from trash material"
  },
  {
    "id": "report_source",
    "chunk": "lowing, high-quality ore (\"Curated Data\") and forges a legendary, epic sword. The blacksmith is labeled \"AI.\"\n*   **TL;DR:** You can't craft an epic weapon from trash materials. The \"Garbage In, Garbage Out\" law means your AI's creations will only be as good as the data you provide it.\n*   **Content:** There's a fundamental law in the universe of creation: \"Garbage In, Garbage Out\" (GIGO). It means the quality of your creation is determined by the quality of your starting materials. This is the most important rule when working with AI. An AI can't generate epic, bug-free code if you give it a messy, confusing, or incomplete set of instructions and files. Most of the time an AI \"fails,\" it's not because the AI is dumb; it's because it was given a bad loadout. Mastering data curation means you'll always be crafting with the best materials, which means you'll always be producing legendary results.\n\n#### **Page 3: How to Curate Data**\n*   **Page Title:** The Curator's Combo: Gather, Organize, Label\n*   **Image Prompt:** A three-panel comic strip showing the workflow. Panel 1: \"GATHER,\" showing a hero collecting loot from various chests and monsters. Panel 2: \"ORGANIZE,\" showing the hero back at their base, sorting the loot into different chests labeled \"Weapons,\" \"Armor,\" and \"Potions.\" Panel 3: \"LABEL,\" showing them applying custom names and icons to the sorted items.\n*   **TL;DR:** The core technique is a simple three-hit combo: gather all your loot, organize it into categories, and label everything so you know what it is.\n*   **Content:** The art of curation is a simple but powerful three-step combo. First, you **Gather**. Go on a loot run and collect every file, asset, and piece of information you need for your quest. Se"
  },
  {
    "id": "report_source",
    "chunk": "curation is a simple but powerful three-step combo. First, you **Gather**. Go on a loot run and collect every file, asset, and piece of information you need for your quest. Second, you **Organize**. Don't just dump everything in one chest. Create a clean folder structure. Put your code in one place, your art assets in another, and your quest logs (documentation) in a third. Third, you **Label**. Give your files and folders clear, descriptive names. This makes your inventory easy for both you and your AI sidekick to navigate.\n\n#### **Page 4: The Curator's Toolkit**\n*   **Page Title:** Your Legendary Gear: The Data Curation Environment (DCE)\n*   **Image Prompt:** A hero is shown equipping a set of glowing, futuristic armor and tools. The main tool is a powerful gauntlet labeled \"DCE,\" which has gems for \"Context Selector,\" \"Parallel Co-Pilot,\" and \"Cycle Navigator.\"\n*   **TL;DR:** The Data Curation Environment (DCE) is your legendary gear set for this quest. It's a VS Code extension packed with epic tools designed to make you a master data curator.\n*   **Content:** To become a master, you need legendary gear. The Data Curation Environment (DCE) is the epic-tier toolkit you'll be using in this academy. It's a VS Code extension that gives you all the power-ups you need for professional-grade curation. Its File Tree View is like an infinite bag of holding that lets you select your context with simple checkboxes. Its Parallel Co-Pilot Panel is like a summoning spell that lets you call on multiple AI familiars at once. This entire course is about mastering this gear set and using it to build whatever you can imagine.\n</file_artifact>\n\n<file path=\"src/Artifacts/A66 - V2V Academy - Lesson 2.2 - The Art of Annotation.md\">\n# Artifa"
  },
  {
    "id": "report_source",
    "chunk": " this gear set and using it to build whatever you can imagine.\n</file_artifact>\n\n<file path=\"src/Artifacts/A66 - V2V Academy - Lesson 2.2 - The Art of Annotation.md\">\n# Artifact A66: V2V Academy - Lesson 2.2 - The Art of Annotation\n# Date Created: C66\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 2.2 of the V2V Academy, \"The Art of Annotation,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, data annotation, metadata, context engineering, interactive learning, persona\n\n## **Lesson 2.2: The Art of Annotation**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: Defining Annotation**\n*   **Page Title:** Increasing Signal: The Professional's Guide to Data Annotation\n*   **Image Prompt:** A professional is shown adding clear, glowing labels and tags to various pieces of a complex digital blueprint. The labels (\"Version 2.1,\" \"Client: Acme Corp,\" \"Status: Approved\") create a layer of order and clarity over the raw design.\n*   **TL;DR:** Data annotation is the professional practice of adding descriptive metadatalabels, tags, and structureto raw information, transforming it into a high-signal asset that an AI can understand and act upon with precision.\n*   **Content:** Having gathered your data, the next critical step is to give it meaning. This is the discipline of **Data Annotation**, the process of adding a layer of descriptive information, or **metadata**, to your raw data. This metadata isn't the data itself, but data *about* the data: file names, dates, categories, and descriptive tags. For the professional, this is a high-leverag"
  },
  {
    "id": "report_source",
    "chunk": "our raw data. This metadata isn't the data itself, but data *about* the data: file names, dates, categories, and descriptive tags. For the professional, this is a high-leverage activity. Without clear annotation, an AI sees a folder of documents as a flat, undifferentiated wall of text. With annotation, it understands that one document is an approved project plan, another is an outdated draft, and a third is a client's feedback. This is how you increase the signal-to-noise ratio of your context and ensure the AI's actions are aligned with your strategic intent.\n\n#### **Page 2: Why It's Critically Important**\n*   **Page Title:** The Cost of Ambiguity\n*   **Image Prompt:** A split-panel image. On the left, an AI assistant looks confused, surrounded by identical, unlabeled file icons. On the right, the same AI is confidently and efficiently processing files that have clear, distinct labels and icons.\n*   **TL;DR:** An AI cannot read your mind or infer your intent. Without explicit labels, the AI is forced to guess, leading to costly errors, wasted time, and unreliable outputs.\n*   **Content:** In a professional environment, ambiguity is a liability. An AI, no matter how advanced, cannot infer the context, relevance, or purpose of a piece of data on its own. A file named `report.docx` could be the final version or a draft from six months ago. Without metadata, the AI has no way to know. Relying on it to guess is a recipe for disaster, leading to it referencing outdated information or applying the wrong logic. Proper annotation removes this ambiguity. It provides the explicit, machine-readable context the AI needs to make correct, reliable decisions every time. It is the primary mechanism for de-risking AI collaboration.\n\n###"
  },
  {
    "id": "report_source",
    "chunk": " provides the explicit, machine-readable context the AI needs to make correct, reliable decisions every time. It is the primary mechanism for de-risking AI collaboration.\n\n#### **Page 3: How to Annotate Effectively**\n*   **Page Title:** Practical Annotation: Naming, Structuring, Tagging\n*   **Image Prompt:** A three-panel diagram showing practical annotation. Panel 1: \"Descriptive Naming,\" showing a file being renamed from `final_draft.docx` to `Q3-Marketing-Strategy-v2.1-APPROVED.docx`. Panel 2: \"Logical Structure,\" showing files being moved into folders like `/Proposals/` and `/Contracts/`. Panel 3: \"Metadata Tags,\" showing a UI where tags like `client:acme` and `status:final` are being applied.\n*   **TL;DR:** Effective annotation doesn't require complex tools. It starts with disciplined habits: using clear, descriptive file names, organizing files into a logical folder structure, and applying consistent tags.\n*   **Content:** You can begin practicing professional-grade annotation immediately. The process starts with simple, disciplined habits. 1. **Use Descriptive Names:** Name your files and folders with clarity and consistency. `Q3-Marketing-Strategy-v2.1-APPROVED.docx` is infinitely more valuable as a piece of context than `draft_final_2.docx`. 2. **Structure Your Folders:** Your folder hierarchy is a form of metadata. A file in `/Proposals/Active/` has a clear context that a file sitting on your desktop does not. 3. **Apply Tags:** When possible, use systems that allow for explicit tagging. Even in a simple file system, you can embed tags in your filenames. This structured approach is the foundation of building a reliable \"source of truth\" for your AI partner.\n\n#### **Page 4: The Payoff: AI with Intent**\n*   **Pag"
  },
  {
    "id": "report_source",
    "chunk": "your filenames. This structured approach is the foundation of building a reliable \"source of truth\" for your AI partner.\n\n#### **Page 4: The Payoff: AI with Intent**\n*   **Page Title:** The Payoff: From Raw Data to Actionable Intelligence\n*   **Image Prompt:** A powerful AI is shown flawlessly executing a complex business workflow. It is pulling the correct, version-controlled documents, referencing the right client data, and assembling a perfect report, all guided by the glowing metadata attached to each piece of information.\n*   **TL;DR:** The result of diligent annotation is an AI that operates with a deep understanding of your intent, transforming it from a simple tool into a true strategic partner.\n*   **Content:** The return on investment for data annotation is immense. When your data is well-annotated, you unlock a new level of human-AI collaboration. You can issue high-level, strategic commands with confidence, knowing the AI has the context to execute them correctly. For example, you can say, \"Summarize the key findings from all *approved* Q3 client reports,\" and trust that the AI can identify the correct files based on their metadata. This transforms the AI from a simple text generator into a genuine partner in knowledge work, capable of understanding and acting upon your strategic intent.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: Defining Annotation**\n*   **Page Title:** Making Your Context Machine-Readable: An Intro to Annotation\n*   **Image Prompt:** A student is shown adding clear, glowing labels to digital flashcards. The labels (`Chapter 1`, `Key Concept`, `Final Exam`) help organize the raw information into a structured study guide.\n*   **TL;DR:** Data annotation is the process "
  },
  {
    "id": "report_source",
    "chunk": "ashcards. The labels (`Chapter 1`, `Key Concept`, `Final Exam`) help organize the raw information into a structured study guide.\n*   **TL;DR:** Data annotation is the process of adding labels and tags to your data. Its like adding helpful notes to your files that a computer can read, which is essential for getting an AI to understand what you want it to do.\n*   **Content:** You've learned how to gather your data. Now, you need to make it understandable to your AI partner. This is called **Data Annotation**. It's the process of adding descriptive \"labels\" or \"tags\"also known as **metadata**to your files and information. Think of it like this: a photo of a cat is just pixels to a computer. An annotation adds the label \"cat,\" which is what teaches the AI to recognize it. In your projects, this means giving your files clear names and organizing them in a way that tells the AI what they are and why they're important.\n\n#### **Page 2: Why It's Critically Important**\n*   **Page Title:** Don't Make the AI Guess\n*   **Image Prompt:** A split-panel image. On the left, a student gives a friend a pile of unlabeled, disorganized notes and asks them to write a paper; the friend looks confused. On the right, the student gives the friend a neatly organized binder with labeled tabs; the friend immediately starts writing with confidence. The friend is labeled \"AI.\"\n*   **TL;DR:** If you give an AI a messy, unlabeled pile of files, it has to guess what's important. This leads to mistakes. Clear annotations are like a perfect set of instructions that guarantee better results.\n*   **Content:** In any team project, clear communication is key. It's the same when your teammate is an AI. If you just dump a folder of files with names like `doc1"
  },
  {
    "id": "report_source",
    "chunk": " results.\n*   **Content:** In any team project, clear communication is key. It's the same when your teammate is an AI. If you just dump a folder of files with names like `doc1.js` and `final.txt` into your context, the AI has no idea what it's looking at. It's forced to guess, and its guesses are often wrong. This is where most junior developers fail when using AI. They blame the AI for being \"dumb\" when the real problem is that they provided a low-quality, ambiguous context. Learning to annotate your data properly is the skill that will prevent these errors and make you look like a pro.\n\n#### **Page 3: How to Annotate Effectively**\n*   **Page Title:** The Annotation Starter Pack: Naming & Structuring\n*   **Image Prompt:** A three-panel \"how-to\" guide. Panel 1 shows a file being renamed from `script.js` to `user-login-api.js`. Panel 2 shows a messy desktop of files being dragged into clean folders named `_src`, `_docs`, and `_assets`. Panel 3 shows a final, clean project structure.\n*   **TL;DR:** You can start annotating right now with two simple habits: give your files clear, descriptive names, and organize your project into a logical folder structure.\n*   **Content:** You don't need fancy tools to be a great data annotator. It starts with two foundational habits that will make your projects instantly more professional. 1. **Use Descriptive Names:** Always name your files based on what they do. `user-login-api.js` tells a story; `script.js` tells you nothing. 2. **Structure Your Folders:** Don't leave all your files in one giant folder. Create a logical structure. A common pattern is to have separate folders for your source code (`/src`), your documentation (`/docs`), and your images or other assets (`/assets`). This si"
  },
  {
    "id": "report_source",
    "chunk": "gical structure. A common pattern is to have separate folders for your source code (`/src`), your documentation (`/docs`), and your images or other assets (`/assets`). This simple organization is a powerful form of annotation that provides immediate clarity.\n\n#### **Page 4: The Payoff: Building a Killer Portfolio**\n*   **Page Title:** The Payoff: AI That Builds What You Actually Want\n*   **Image Prompt:** A young developer is proudly showing off a complex, polished application on their laptop screen. A glowing AI avatar is giving them a thumbs-up. The app looks clean, functional, and impressive.\n*   **TL;DR:** When you master annotation, you get an AI partner that understands your vision. This allows you to build more complex, impressive, and bug-free projects for your portfolio, faster.\n*   **Content:** The time you invest in annotating your data pays off massively. When your context is clean and well-structured, the AI understands your intent. It stops making dumb mistakes. It starts generating code that is more accurate, more relevant, and better organized. This means you spend less time debugging and more time building. For a graduate building a portfolio, this is a game-changer. It allows you to tackle more ambitious projects and produce higher-quality work, which is exactly what hiring managers want to see.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: Defining Annotation**\n*   **Page Title:** Enchanting Your Data: The Magic of Annotation\n*   **Image Prompt:** A hero in a fantasy world is shown holding a plain, unenchanted sword. They are applying glowing runes and gems to it. The runes are labeled \"Metadata.\" The finished sword on the right is glowing with power.\n*   **TL;DR:** Data annotation is l"
  },
  {
    "id": "report_source",
    "chunk": "d. They are applying glowing runes and gems to it. The runes are labeled \"Metadata.\" The finished sword on the right is glowing with power.\n*   **TL;DR:** Data annotation is like enchanting your gear. It's the process of adding magical labels and tags (metadata) to your raw data, which imbues it with power and makes it understandable to your AI familiar.\n*   **Content:** You've gathered your loot. Now it's time to enchant it. This is the art of **Data Annotation**the process of carving magical runes, known as **metadata**, onto your raw data. These runes are data *about* your data: names, categories, and tags that tell your AI familiar what an item is and what it does. Without these enchantments, a file is just a plain, useless item. With them, it becomes a powerful artifact that your AI can wield to cast incredible spells (like writing amazing code).\n\n#### **Page 2: Why It's Critically Important**\n*   **Page Title:** Your AI Can't Read Minds\n*   **Image Prompt:** A split-panel cartoon. On the left, a hero points at a pile of identical, unlabeled potions and yells \"Give me the healing potion!\" at their AI familiar, which looks confused. On the right, the hero points at a neatly organized shelf of potions, each with a clear label, and the familiar instantly grabs the correct one.\n*   **TL;DR:** Your AI companion is powerful, but it's not a mind reader. If you don't label your stuff, it has to guess what you want, and it will probably guess wrong.\n*   **Content:** Even the most legendary AI familiar has a critical weakness: it can't read your mind. If you throw a bag of unlabeled potions at it, it has no idea which one is for healing and which one will turn you into a frog. This is why most AI \"fails\" happen. It's not bec"
  },
  {
    "id": "report_source",
    "chunk": "f you throw a bag of unlabeled potions at it, it has no idea which one is for healing and which one will turn you into a frog. This is why most AI \"fails\" happen. It's not because the AI is weak; it's because its master gave it a confusing, unlabeled inventory. Annotation is how you give your AI perfect clarity. By labeling every item, you remove the guesswork and ensure your AI companion always knows exactly which spell to cast or item to use.\n\n#### **Page 3: How to Annotate Effectively**\n*   **Page Title:** The Annotator's Grimoire: Naming & Sorting\n*   **Image Prompt:** A page from a magical grimoire. It shows two primary \"spells.\" The first, \"Spell of True Naming,\" shows a generic sword being renamed to \"Sword of the Fire Lord +5.\" The second, \"Spell of Sorting,\" shows a messy pile of loot being automatically sorted into chests labeled \"Weapons,\" \"Armor,\" and \"Scrolls.\"\n*   **TL;DR:** You can start enchanting your data with two basic spells: the Spell of True Naming (giving files descriptive names) and the Spell of Sorting (organizing files into a logical folder structure).\n*   **Content:** You don't need to be a grand mage to start annotating. The grimoire starts with two simple but powerful spells. 1. **The Spell of True Naming:** Give your files names that reveal their true purpose. `dragon-slayer-sword.js` is a legendary weapon; `item1.js` is vendor trash. 2. **The Spell of Sorting:** Don't just dump your loot on the floor. Organize your project into a clean folder structure. Create separate \"chests\" for your code (`/src`), your lore (`/docs`), and your art (`/assets`). Mastering these two spells is the first step to becoming a master curator.\n\n#### **Page 4: The Payoff: God-Tier Loot**\n*   **Page Title:** The Pa"
  },
  {
    "id": "report_source",
    "chunk": "`), and your art (`/assets`). Mastering these two spells is the first step to becoming a master curator.\n\n#### **Page 4: The Payoff: God-Tier Loot**\n*   **Page Title:** The Payoff: Crafting God-Tier Gear\n*   **Image Prompt:** A young hero is proudly displaying a set of epic, glowing armor and a powerful weapon that they have crafted. An AI familiar floats beside them, giving a thumbs-up. The gear represents a complex, bug-free application.\n*   **TL;DR:** When you master annotation, your AI partner understands your vision perfectly. This lets you craft more complex, powerful, and bug-free projects, turning your ideas into god-tier loot.\n*   **Content:** The time you spend enchanting your data has an epic payoff. When your inventory is perfectly organized and labeled, your AI familiar becomes a master craftsman. It understands your grand design. It stops making rookie mistakes. It starts generating code that is more powerful, more elegant, and free of debuffs (bugs). This means you spend less time grinding and more time creating. It's the ultimate power-leveling strategy, allowing you to tackle epic \"raids\" (ambitious projects) and craft the god-tier gear you've always dreamed of building.\n</file_artifact>\n\n<file path=\"src/Artifacts/A67 - V2V Academy - Lesson 2.3 - Critical Analysis of AI Output.md\">\n# Artifact A67: V2V Academy - Lesson 2.3 - Critical Analysis of AI Output\n# Date Created: C67\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 2.3 of the V2V Academy, \"Critical Analysis of AI Output,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, crit"
  },
  {
    "id": "report_source",
    "chunk": "\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, critical thinking, ai literacy, validation, interactive learning, persona\n\n## **Lesson 2.3: Critical Analysis of AI Output**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: Why Critical Analysis is Essential**\n*   **Page Title:** Quality Control: Vetting AI Output for Business-Critical Applications\n*   **Image Prompt:** A seasoned professional in a high-tech quality control lab, wearing safety glasses and meticulously inspecting a glowing, holographic blueprint generated by an AI. They are using a digital magnifying glass to check for subtle flaws, demonstrating a high level of scrutiny and responsibility.\n*   **TL;DR:** In a professional setting, the human is the ultimate guarantor of quality. This lesson teaches the systematic process of critically analyzing AI output to ensure it is correct, reliable, and aligned with business objectives before deployment.\n*   **Content:** As you move into a role where you direct AI, you also assume responsibility for its output. An AI is a powerful but imperfect tool; it can generate code that contains subtle bugs, produce analyses based on flawed logic, or misinterpret key requirements. The most critical function of the human in the loop is to serve as the final checkpoint for quality and correctness. Critical analysis is the disciplined process of \"trusting, but verifying\" every AI output. It is the professional practice that transforms a promising AI-generated draft into a reliable, production-ready asset, mitigating risks and ensuring that all work aligns with strategic goals.\n\n#### **Page 2: Co"
  },
  {
    "id": "report_source",
    "chunk": " transforms a promising AI-generated draft into a reliable, production-ready asset, mitigating risks and ensuring that all work aligns with strategic goals.\n\n#### **Page 2: Common AI Failure Modes**\n*   **Page Title:** Know Your Enemy: Common AI Failure Modes\n*   **Image Prompt:** A \"rogue's gallery\" of digital phantoms. Each phantom represents a different AI failure mode: a ghost labeled \"Hallucination\" offers a non-existent API function; a tangled knot of wires labeled \"Flawed Logic\" shows a broken process; a block of code with a hidden skull-and-crossbones icon is labeled \"Security Vulnerability.\"\n*   **TL;DR:** To effectively critique AI output, you must be able to recognize its common failure patterns, including factual hallucinations, logical errors, security vulnerabilities, and stylistic misalignments.\n*   **Content:** An AI doesn't make mistakes like a human, so it's important to learn its unique failure patterns. **Hallucinations** are the most well-known issue, where the AI confidently invents facts, functions, or even entire libraries that don't exist. **Logical Errors** are more subtle; the code might run without crashing but produce the wrong result because of a flawed algorithm. **Security Vulnerabilities** can be introduced if the AI reproduces insecure coding patterns from its training data. Finally, **Stylistic & Architectural Misalignment** occurs when the AI's code works but doesn't follow your project's specific design patterns or coding standards. Recognizing these patterns is the first step in a professional code review process.\n\n#### **Page 3: The Curator's Method for Analysis**\n*   **Page Title:** The Analysis Workflow: From Diff to Decision\n*   **Image Prompt:** A professional is shown at a work"
  },
  {
    "id": "report_source",
    "chunk": "s.\n\n#### **Page 3: The Curator's Method for Analysis**\n*   **Page Title:** The Analysis Workflow: From Diff to Decision\n*   **Image Prompt:** A professional is shown at a workstation with a large, clear diff viewer. They are comparing the \"Original File\" on the left with the \"AI-Generated File\" on the right, with the changes clearly highlighted. Their process is methodical and focused.\n*   **TL;DR:** The primary tool for critical analysis is the diff viewer. The method involves a top-down review, starting with the overall plan, then examining the code's structure, and finally scrutinizing the line-by-line changes.\n*   **Content:** A systematic approach is key to an effective review. 1. **Review the Plan:** Start by re-reading the AI's \"Course of Action.\" Does the high-level strategy still make sense? 2. **Analyze the Diff:** Open the diff viewer. Don't just look at the highlighted lines; understand the *context* of the changes. Does the new code fit logically within the existing architecture? 3. **Scrutinize the Logic:** Read the new code carefully. Does the algorithm correctly solve the problem? Are there any obvious edge cases that have been missed? 4. **Validate Against Requirements:** Finally, test the code against the original requirements. Does it actually do what you asked it to do? This structured process ensures a thorough and efficient review.\n\n#### **Page 4: The Feedback Loop**\n*   **Page Title:** From Critique to Correction: Closing the Loop\n*   **Image Prompt:** A diagram showing a virtuous cycle. An \"AI Output\" is fed into a \"Human Critique\" phase. The output of the critique is a \"Refined Prompt,\" which is then fed back to the AI, resulting in an \"Improved Output.\"\n*   **TL;DR:** Finding a flaw is not a fai"
  },
  {
    "id": "report_source",
    "chunk": "tique\" phase. The output of the critique is a \"Refined Prompt,\" which is then fed back to the AI, resulting in an \"Improved Output.\"\n*   **TL;DR:** Finding a flaw is not a failure; it is an opportunity. A skilled architect uses their critique to create a more precise prompt for the next cycle, continuously improving the AI's performance.\n*   **Content:** The goal of critical analysis is not just to find errors, but to improve the system. Every flaw you identify is a valuable data point. Instead of manually fixing the AI's code, the Virtuoso's method is to use your critique to refine your instructions. Document the error you found and include it in the \"Ephemeral Context\" for your next cycle. For example: \"In the last cycle, you used a deprecated function. Please refactor this to use the new `processDataV2` API.\" This turns every error into a lesson for the AI, making the entire collaborative system smarter and more reliable over time.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: Why Critical Analysis is Essential**\n*   **Page Title:** Don't Trust, Verify: The Skill That Makes You a Senior Dev\n*   **Image Prompt:** A young developer is confidently presenting a code review to a senior engineer. The senior engineer looks impressed, giving a nod of approval. The young developer is shown pointing out a subtle but critical bug in a piece of AI-generated code on the screen.\n*   **TL;DR:** Junior developers trust AI-generated code. Senior developers verify it. This lesson teaches you how to develop the critical eye for quality that will accelerate your career.\n*   **Content:** One of the biggest mistakes junior developers make is blindly trusting AI-generated code. They copy, paste, and hope for the best. "
  },
  {
    "id": "report_source",
    "chunk": "ll accelerate your career.\n*   **Content:** One of the biggest mistakes junior developers make is blindly trusting AI-generated code. They copy, paste, and hope for the best. This is a recipe for introducing bugs and looking unprofessional. A senior developer, in contrast, treats every piece of AI-generated code as a suggestion to be rigorously verified. Critical analysis is the skill of looking at code and asking, \"Is this correct? Is this secure? Is this well-written?\" By mastering this skill, you move beyond being a simple \"coder\" and start thinking like an architect and a quality leadthe exact qualities that companies look for in senior talent.\n\n#### **Page 2: Common AI Failure Modes**\n*   **Page Title:** Spot the Bug: A Field Guide to AI Errors\n*   **Image Prompt:** A \"field guide\" page, like a bird-watching book. It shows different types of \"bugs.\" One is a \"Phantom Function\" (a function that doesn't exist). Another is a \"Logic Worm\" (code that runs but gives the wrong answer). A third is a \"Security Spider\" (a hidden vulnerability).\n*   **TL;DR:** To find bugs, you need to know what they look like. AI makes specific kinds of mistakes, like inventing functions, creating flawed logic, or introducing security holes.\n*   **Content:** AI doesn't get tired or make typos like humans, but it makes its own unique kinds of mistakes. Learning to spot them is a superpower. **Hallucinations** are the most common: the AI will confidently invent a function or library that sounds real but doesn't actually exist. **Flawed Logic** is trickier: the code runs, but it has a bug in its reasoning that makes it fail on certain inputs. **Security Flaws** are dangerous: the AI might use an outdated, insecure coding pattern it learned from"
  },
  {
    "id": "report_source",
    "chunk": "but it has a bug in its reasoning that makes it fail on certain inputs. **Security Flaws** are dangerous: the AI might use an outdated, insecure coding pattern it learned from old training data. Finally, you'll see **Style Mismatches**, where the code works but doesn't follow the formatting rules or design patterns of your project. Learning to spot these \"tells\" is the key to an effective code review.\n\n#### **Page 3: The Curator's Method for Analysis**\n*   **Page Title:** How to Review Code You Didn't Write\n*   **Image Prompt:** A young developer is shown with a checklist, methodically reviewing a piece of code on a screen. The checklist items are \"1. Understand the Goal,\" \"2. Check the Big Picture (Diff),\" \"3. Read the Code,\" and \"4. Run the Tests.\"\n*   **TL;DR:** Reviewing AI code is a skill. The best method is to start with the big picture, then zoom in: first understand the goal, then review the overall changes with a diff, then read the code line-by-line.\n*   **Content:** It can be intimidating to critique code from a super-intelligent AI, but a structured process makes it manageable. 1. **Understand the Goal:** Before you look at the code, re-read the AI's plan. What was it *trying* to do? 2. **See the Changes:** Use a \"diff\" tool. This is the most important step. A diff tool shows you exactly which lines were added or removed, letting you focus only on what's new. 3. **Read the Logic:** Now, read the new code blocks carefully. Follow the logic from top to bottom. Does it make sense? Can you spot any of the common AI errors? 4. **Test It:** The ultimate test is to run the code. Does it work as expected? Does it pass its tests? This simple, top-down process will turn you into a confident and effective code reviewer."
  },
  {
    "id": "report_source",
    "chunk": "e ultimate test is to run the code. Does it work as expected? Does it pass its tests? This simple, top-down process will turn you into a confident and effective code reviewer.\n\n#### **Page 4: The Feedback Loop**\n*   **Page Title:** Turn Bugs into Better Prompts\n*   **Image Prompt:** A diagram showing a cycle. An \"AI Bug\" is found. An arrow points to the developer writing a \"Better Prompt\" that says, \"Fix this bug by...\" The new prompt is fed back to the AI, which produces \"Better Code.\"\n*   **TL;DR:** When you find a bug in the AI's code, don't just fix it yourself. Use the bug to write a better prompt. This is how you train the AI to become a better partner.\n*   **Content:** Every bug you find is a learning opportunityfor you and for the AI. A junior dev might just manually fix the AI's mistake. A pro uses the mistake to improve the process. When you find a flaw, your next step should be to articulate that flaw in your next prompt. Add it to the \"Ephemeral Context\" in the DCE. For example: \"In the last attempt, the code failed because it didn't handle negative numbers. Please update the function to include a check for negative inputs.\" This does two things: it gets the AI to fix the bug for you, and it documents the requirement, making the entire system smarter for the next iteration.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: Why Critical Analysis is Essential**\n*   **Page Title:** Debuffing the AI: Mastering Critical Analysis\n*   **Image Prompt:** A hero in a video game is inspecting a powerful, glowing sword given to them by an NPC. The hero has a \"detect magic\" spell active, which reveals a hidden \"Cursed\" debuff on the sword that the NPC didn't mention.\n*   **TL;DR:** The AI can craft you legend"
  },
  {
    "id": "report_source",
    "chunk": "an NPC. The hero has a \"detect magic\" spell active, which reveals a hidden \"Cursed\" debuff on the sword that the NPC didn't mention.\n*   **TL;DR:** The AI can craft you legendary gear (code), but sometimes it's cursed. This lesson teaches you the \"Detect Curse\" skillthe power of critical analysis to find the hidden flaws in AI output before they blow up in your face.\n*   **Content:** In your quest to build epic things, the AI is your master blacksmith. It can forge powerful code and artifacts for you in seconds. But here's the secret: sometimes, the gear it crafts is cursed. It might look perfect, but it has a hidden bug or a security flaw that will cause a critical failure at the worst possible moment. Critical analysis is the \"Detect Curse\" spell of the V2V pathway. It's the skill of inspecting the AI's gifts and finding the hidden debuffs. Mastering this skill is what separates a true Virtuoso from a noob who gets wiped by their own cursed sword.\n\n#### **Page 2: Common AI Failure Modes**\n*   **Page Title:** Know Your Monsters: A Bestiary of AI Bugs\n*   **Image Prompt:** A page from a \"Monster Manual.\" It shows different types of digital monsters. The \"Hallucination\" is a shimmering, ghost-like creature that looks real but isn't. The \"Logic Gremlin\" is a small creature that secretly rewires a machine to make it do the wrong thing. The \"Security Serpent\" is a snake hiding inside a treasure chest.\n*   **TL;DR:** To be a master bug hunter, you need to know your prey. AI has its own unique set of monsters, like Hallucinations, Logic Gremlins, and Security Serpents.\n*   **Content:** AI doesn't spawn the same old bugs. It has its own bestiary of unique monsters you need to learn to hunt. **Hallucinations** are the trickiest"
  },
  {
    "id": "report_source",
    "chunk": "urity Serpents.\n*   **Content:** AI doesn't spawn the same old bugs. It has its own bestiary of unique monsters you need to learn to hunt. **Hallucinations** are the trickiest; they're like phantom enemies that look real but aren't. The AI will invent a function or a library that doesn't exist in the game world. **Logic Gremlins** are subtle saboteurs; they write code that seems to work but has a hidden flaw in its logic that causes it to fail in specific situations. **Security Serpents** are the most dangerous; the AI might accidentally leave a backdoor open in your code, creating a vulnerability that enemies can exploit. Learning the attack patterns of these monsters is the first step to becoming a legendary bug hunter.\n\n#### **Page 3: The Curator's Method for Analysis**\n*   **Page Title:** The Hunter's Strategy: Top-Down Takedown\n*   **Image Prompt:** A hero is shown planning an attack on a giant boss. They are looking at a map of the boss's weak points. Their strategy is clear: \"1. Analyze the Quest,\" \"2. Scan for Weak Points (Diff),\" \"3. Target the Core (Read the Code),\" and \"4. Final Blow (Run the Tests).\"\n*   **TL;DR:** The best way to take down a bug is with a strategy. Start with the big picture, then zoom in for the kill: first understand the quest, then scan for weak points with a diff, then target the core logic.\n*   **Content:** You don't just run headfirst at a boss; you use a strategy. The same goes for reviewing AI code. 1. **Analyze the Quest:** First, re-read the AI's plan. What was it supposed to do? 2. **Scan for Weak Points:** Use a \"diff\" tool. This is like a magical scanner that highlights all the changes the AI made. It lets you focus your attack on the new, untested parts. 3. **Target the Core:**"
  },
  {
    "id": "report_source",
    "chunk": "se a \"diff\" tool. This is like a magical scanner that highlights all the changes the AI made. It lets you focus your attack on the new, untested parts. 3. **Target the Core:** Now, read the new code. Follow its logic. Can you spot any of the monsters from our bestiary? 4. **The Final Blow:** Run the code. See if it survives the trial. This top-down strategy is the most effective way to hunt down and destroy any bug.\n\n#### **Page 4: The Feedback Loop**\n*   **Page Title:** Looting the Corpse: Turning Bugs into EXP\n*   **Image Prompt:** A hero is shown standing over a defeated bug-monster. The monster drops a glowing orb of light labeled \"Knowledge,\" which the hero absorbs, causing a \"LEVEL UP!\" graphic to appear.\n*   **TL;DR:** Every bug you defeat is a learning opportunity. A true Virtuoso loots the corpse for knowledge and uses it to craft a better \"spell\" (prompt) for the next fight.\n*   **Content:** In the V2V pathway, you never waste a kill. Every bug you find is a chance to level up. A rookie might just patch the bug and move on. A Virtuoso loots the corpse for experience points. When you find a flaw in the AI's code, you use that knowledge to craft a more powerful spell for next time. You add the bug description to your \"Ephemeral Context\" in the DCE. For example: \"Last time, your fireball spell didn't account for fire resistance. This time, add a 'check for resistance' step before casting.\" This forces the AI to learn from its mistake, making it a smarter and more powerful companion for all your future adventures.\n</file_artifact>\n\n<file path=\"src/Artifacts/A68 - V2V Academy - Lesson 3.1 - From Conversation to Command.md\">\n# Artifact A68: V2V Academy - Lesson 3.1 - From Conversation to Command\n# Date Created: C68\n#"
  },
  {
    "id": "report_source",
    "chunk": "\"src/Artifacts/A68 - V2V Academy - Lesson 3.1 - From Conversation to Command.md\">\n# Artifact A68: V2V Academy - Lesson 3.1 - From Conversation to Command\n# Date Created: C68\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 3.1 of the V2V Academy, \"From Conversation to Command,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, structured interaction, prompt engineering, context engineering, interactive learning, persona\n\n## **Lesson 3.1: From Conversation to Command**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: Defining Structured Interaction**\n*   **Page Title:** Driving Outcomes: The Principles of Structured AI Interaction\n*   **Image Prompt:** A seasoned executive is at a whiteboard, clearly outlining a project plan with boxes and arrows. An AI assistant is observing the whiteboard and translating the structured plan into a flawless, complex digital architecture on a holographic screen. The scene emphasizes clarity, precision, and strategic direction.\n*   **TL;DR:** Structured interaction is the practice of moving beyond casual conversation with an AI to giving it clear, explicit, and repeatable commands. It is the professional's method for ensuring reliability, reducing ambiguity, and driving predictable outcomes.\n*   **Content:** As a professional, your goal is to achieve reliable and predictable results. When collaborating with an AI, this requires a shift in communication stylefrom casual conversation to **Structured Interaction**. This is the practice of formalizing your requests into clear, unambiguous commands, much l"
  },
  {
    "id": "report_source",
    "chunk": "shift in communication stylefrom casual conversation to **Structured Interaction**. This is the practice of formalizing your requests into clear, unambiguous commands, much like writing a technical specification or a project brief. Instead of a vague, conversational prompt, you provide the AI with a structured set of instructions that define its role, the context, the required steps, and the expected output format. This discipline is the key to transforming the AI from a creative but sometimes unreliable brainstorming partner into a dependable execution engine for your strategic vision.\n\n#### **Page 2: The Interaction Schema**\n*   **Page Title:** The Briefing Document: Your Interaction Schema\n*   **Image Prompt:** A close-up of a futuristic digital document titled \"Interaction Schema.\" The document has clear sections for \"ROLE,\" \"CONTEXT,\" \"CONSTRAINTS,\" and \"OUTPUT_FORMAT.\" An AI is shown reading this document and giving a \"thumbs-up\" of understanding.\n*   **TL;DR:** An Interaction Schema is a template for your commands. It's a formal structure that ensures you provide the AI with all the critical information it needs to execute a task correctly and consistently.\n*   **Content:** The core of structured interaction is the **Interaction Schema**. Think of this as your standard operating procedure or briefing document for the AI. A robust schema ensures you never miss critical information. While it can be customized, a professional schema typically includes: 1. **Role & Goal:** Explicitly state the AI's persona and the high-level objective. 2. **Context:** Provide all necessary background information, data, or source files. 3. **Step-by-Step Instructions:** Break down the task into a clear, logical sequence of actions. 4."
  },
  {
    "id": "report_source",
    "chunk": "ext:** Provide all necessary background information, data, or source files. 3. **Step-by-Step Instructions:** Break down the task into a clear, logical sequence of actions. 4. **Constraints & Rules:** Define any \"guardrails\" or rules the AI must follow. 5. **Output Format:** Specify the exact format for the response (e.g., Markdown, JSON, a specific code structure). Using a consistent schema drastically reduces errors and ensures the output is always in a usable format.\n\n#### **Page 3: The Business Case: Why Structure Matters**\n*   **Page Title:** The Business Case: Repeatability, Reliability, Scalability\n*   **Image Prompt:** An architectural diagram showing a process. The \"Unstructured Prompt\" path leads to a chaotic, unpredictable branching of outcomes. The \"Structured Interaction\" path leads to a clean, straight, and predictable line from \"Input\" to \"Desired Outcome.\"\n*   **TL;DR:** An unstructured process is a business liability. A structured process is a scalable asset. Adopting this discipline ensures your AI-driven workflows are reliable enough for mission-critical applications.\n*   **Content:** In a business context, results cannot be left to chance. The reason to adopt structured interaction is purely strategic. **Repeatability:** A structured command can be run again and again, producing consistent results. **Reliability:** By removing ambiguity, you dramatically reduce the rate of AI errors and hallucinations. **Scalability:** A structured process can be documented, shared, and scaled across a team. It transforms an individual's \"prompting trick\" into a reliable, enterprise-grade workflow. While conversational AI is excellent for exploration, structured interaction is the required methodology for execution.\n\n"
  },
  {
    "id": "report_source",
    "chunk": "ng trick\" into a reliable, enterprise-grade workflow. While conversational AI is excellent for exploration, structured interaction is the required methodology for execution.\n\n#### **Page 4: Practical Application**\n*   **Page Title:** From Request to Command: A Practical Example\n*   **Image Prompt:** A \"before and after\" comparison. \"Before\" shows a simple chat bubble: \"Hey, can you make the user profile page better?\" \"After\" shows a structured command in a code-like block with clear sections for `// ROLE`, `// TASK`, `// CONTEXT`, and `// OUTPUT`.\n*   **TL;DR:** Let's translate a vague business request into a precise, structured command that guarantees a better result.\n*   **Content:** Consider this common but ineffective prompt: \"Review our project files and improve the user profile page.\" The AI has to guess what \"improve\" means. Now, consider a structured command: \n    ```\n    // ROLE: You are a senior UX designer and React developer.\n    // TASK: Refactor the user profile page to improve layout and add a password reset feature.\n    // CONTEXT: The relevant files are `ProfilePage.tsx` and `user-api.ts`. The current design lacks mobile responsiveness.\n    // INSTRUCTIONS:\n    // 1. Update `ProfilePage.tsx` to use a two-column responsive layout.\n    // 2. Add a 'Reset Password' button to the page.\n    // 3. Create a new function in `user-api.ts` to handle the password reset API call.\n    // OUTPUT_FORMAT: Provide the complete, updated content for both files in separate blocks.\n    ```\n    This command leaves no room for guessing. It is a professional directive that ensures the AI's output will be directly aligned with the specific business need. This is the V2V way.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#"
  },
  {
    "id": "report_source",
    "chunk": "al directive that ensures the AI's output will be directly aligned with the specific business need. This is the V2V way.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: Defining Structured Interaction**\n*   **Page Title:** Writing Prompts That Work: An Introduction to Interaction Schemas\n*   **Image Prompt:** A young developer is at their computer, looking frustrated at a screen full of messy, incorrect AI-generated code. A mentor figure points them to a clear, structured checklist, and the developer has a \"lightbulb\" moment of understanding.\n*   **TL;DR:** Stop getting bad results from the AI. The secret to getting the code you want is to stop chatting and start giving clear, structured commands using a template called an Interaction Schema.\n*   **Content:** If you're frustrated with getting unpredictable or wrong answers from an AI, this lesson is for you. The problem isn't the AI; it's the way you're asking. The shift from \"vibecoding\" to professional development is the shift from casual conversation to **Structured Interaction**. This means treating your prompts not as chat messages, but as technical commands. You give the AI a clear, step-by-step set of instructions, just like you would write a function. This method eliminates guesswork and forces the AI to give you the precise output you need.\n\n#### **Page 2: The Interaction Schema**\n*   **Page Title:** The Template for Perfect Prompts: The Interaction Schema\n*   **Image Prompt:** A clear, simple template is shown on a screen, like a form to be filled out. The fields are \"1. What is the AI's Role?\", \"2. What is the Task?\", \"3. What Files Does it Need?\", \"4. What are the Steps?\", and \"5. What Should the Output Look Like?\"\n*   **TL;DR:** An Intera"
  },
  {
    "id": "report_source",
    "chunk": ". What is the AI's Role?\", \"2. What is the Task?\", \"3. What Files Does it Need?\", \"4. What are the Steps?\", and \"5. What Should the Output Look Like?\"\n*   **TL;DR:** An Interaction Schema is a simple template for your prompts. Using it ensures you never forget to include the critical information the AI needs to do its job properly.\n*   **Content:** The best way to ensure your prompts are structured is to use a template. We call this an **Interaction Schema**. It's a checklist that guarantees you give the AI everything it needs. A good schema always includes: 1. **Role & Goal:** Tell the AI what its job is (e.g., \"You are a Python developer fixing a bug\"). 2. **Context:** List the exact files it needs to look at. 3. **Instructions:** Provide a numbered list of the steps you want it to take. 4. **Output Format:** Tell it exactly how you want the final code formatted. Using this simple template will instantly improve the quality of your results.\n\n#### **Page 3: The Business Case: Why Structure Matters**\n*   **Page Title:** Why This Gets You Hired: Reliability and Predictability\n*   **Image Prompt:** An engineering manager is reviewing two portfolios. One is a messy collection of one-off scripts. The other is a clean, organized project with clear documentation and a history of structured, repeatable processes. The manager is smiling and nodding at the second one.\n*   **TL;DR:** Companies hire engineers who produce reliable, predictable work. A developer who uses a structured workflow is seen as more professional and dependable than one who just \"wings it.\"\n*   **Content:** Why is this so important for your career? Because companies value reliability. A \"vibecoder\" who gets a cool result one time but can't reproduce it is a l"
  },
  {
    "id": "report_source",
    "chunk": "t.\"\n*   **Content:** Why is this so important for your career? Because companies value reliability. A \"vibecoder\" who gets a cool result one time but can't reproduce it is a liability. An engineer who uses a structured process to get a correct result every time is an asset. By learning structured interaction, you are demonstrating a professional engineering mindset. It shows that you can think systematically, communicate clearly, and produce work that is dependable and easy for others to understand. This is a massive differentiator in the job market.\n\n#### **Page 4: Practical Application**\n*   **Page Title:** Before and After: From Vague Request to Pro Command\n*   **Image Prompt:** A \"before and after\" comparison. \"Before\" shows a simple chat bubble: \"Can you fix the login page?\" \"After\" shows a structured command in a code-like block with clear sections for `// ROLE`, `// TASK`, `// CONTEXT`, and `// OUTPUT`.\n*   **TL;DR:** Let's see the difference between a junior-level prompt and a professional-level command.\n*   **Content:** Let's look at a real-world example. A junior-level prompt might be: \"My login page isn't working, can you fix it?\" The AI has no idea what's wrong. Now, look at a professional, structured command:\n    ```\n    // ROLE: You are a full-stack developer debugging a Next.js application.\n    // TASK: Fix the user login functionality.\n    // CONTEXT: The login form is in `LoginPage.tsx`. It calls an API route at `api/auth/login.ts`. I am getting a '401 Unauthorized' error.\n    // INSTRUCTIONS:\n    // 1. Analyze `api/auth/login.ts` to check the password validation logic.\n    // 2. Ensure the `LoginPage.tsx` is sending the email and password in the correct format.\n    // 3. Provide the corrected code for b"
  },
  {
    "id": "report_source",
    "chunk": "o check the password validation logic.\n    // 2. Ensure the `LoginPage.tsx` is sending the email and password in the correct format.\n    // 3. Provide the corrected code for both files.\n    // OUTPUT_FORMAT: Full file content for each file in separate blocks.\n    ```\n    This command gives the AI everything it needs. It's clear, specific, and actionable. This is the level of quality you should aim for in every interaction.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: Defining Structured Interaction**\n*   **Page Title:** Casting Spells: Mastering the Syntax of Power\n*   **Image Prompt:** A powerful mage is shown casting a complex spell. Instead of waving their hands randomly, they are tracing a precise, glowing geometric pattern in the air. The pattern is labeled \"Structured Interaction.\" The resulting spell is massive and perfectly formed.\n*   **TL;DR:** To cast the most powerful spells, you need more than just intent; you need to master the syntax. Structured interaction is the \"grammar\" of AI command, turning your creative \"vibe\" into focused, predictable power.\n*   **Content:** You've learned to \"vibe\" with the AI, using conversation to make cool stuff happen. That's like learning to use wild, unpredictable magic. Now, it's time to become a true sorcerer by learning **Structured Interaction**. This is the art of giving the AI commands with a precise, powerful syntax. Instead of just chatting, you'll learn to write \"spells\"structured blocks of instructions that tell the AI exactly what to do, how to do it, and what the result should look like. This is the difference between a cantrip and a world-changing epic spell.\n\n#### **Page 2: The Interaction Schema**\n*   **Page Title:** The Spellbook: Your Inter"
  },
  {
    "id": "report_source",
    "chunk": "uld look like. This is the difference between a cantrip and a world-changing epic spell.\n\n#### **Page 2: The Interaction Schema**\n*   **Page Title:** The Spellbook: Your Interaction Schema\n*   **Image Prompt:** A close-up of an ancient, magical spellbook. The page is a template for a spell, with sections for \"Target,\" \"Components,\" \"Incantation,\" and \"Effect.\"\n*   **TL;DR:** An Interaction Schema is your personal spellbook. It's a template that makes sure every spell you cast has all the right components, so it never fizzles out.\n*   **Content:** Every master mage has a spellbook. In the V2V world, this is your **Interaction Schema**. It's a template that ensures every command you give the AI is perfectly formed. Your spellbook should always include: 1. **Target & Intent:** What is the AI's role and what's the ultimate goal? (e.g., \"You are a game dev AI, and we're building the boss AI.\") 2. **Components:** What materials does the spell need? (List the files the AI should use). 3. **Incantation:** What are the step-by-step actions? (A numbered list of instructions). 4. **Effect:** What should the final result look like? (Specify the output format). Using your spellbook guarantees your magic is powerful and reliable.\n\n#### **Page 3: The Business Case: Why Structure Matters**\n*   **Page Title:** Why Pros Use Spellbooks: The Power of Repeatability\n*   **Image Prompt:** Two wizards are in a duel. One is frantically trying to remember a spell, looking stressed. The other calmly opens a spellbook, recites a perfectly structured incantation, and unleashes a flawless, powerful attack.\n*   **TL;DR:** A pro doesn't guess. They use a structured, repeatable process because it's more powerful and reliable. This is the path to becomin"
  },
  {
    "id": "report_source",
    "chunk": "a flawless, powerful attack.\n*   **TL;DR:** A pro doesn't guess. They use a structured, repeatable process because it's more powerful and reliable. This is the path to becoming a legendary creator.\n*   **Content:** Why do the pros use a structured approach? Because it's more powerful. Relying on \"vibing\" is like trying to remember a complex spell in the middle of a battleyou're going to mess it up. A structured interaction is like casting directly from a spellbook. It's **Repeatable:** you can cast the same perfect spell every time. It's **Reliable:** it removes the chance of the spell backfiring (AI errors). It's **Scalable:** you can share your spells with your guild, making your whole team more powerful. This is how you go from being a talented amateur to a legendary archmage.\n\n#### **Page 4: Practical Application**\n*   **Page Title:** From Wish to Incantation: A Practical Example\n*   **Image Prompt:** A \"before and after\" comparison. \"Before\" shows a simple chat bubble: \"yo, make the player's sword cooler.\" \"After\" shows a glowing, magical scroll with a structured incantation broken into sections for `// ROLE`, `// TASK`, `// CONTEXT`, and `// EFFECT`.\n*   **TL;DR:** Let's see how to level up a simple wish into a world-shaking incantation.\n*   **Content:** Let's see this in action. A beginner's prompt might be: \"make the player's attack animation better.\" The AI has no idea what that means. Now, check out this master-level incantation:\n    ```\n    // ROLE: You are a Unity C# and particle effects expert.\n    // TASK: Refactor the player's sword attack to be more visually impactful.\n    // CONTEXT: The current animation is in `PlayerAttack.cs`. The particle effect prefab is `SwordSlash.prefab`.\n    // INCANTATION:\n   "
  },
  {
    "id": "report_source",
    "chunk": "attack to be more visually impactful.\n    // CONTEXT: The current animation is in `PlayerAttack.cs`. The particle effect prefab is `SwordSlash.prefab`.\n    // INCANTATION:\n    // 1. In `PlayerAttack.cs`, increase the animation speed by 15%.\n    // 2. Add a new particle burst effect that triggers on a successful hit.\n    // 3. Add a subtle screen shake effect on hit.\n    // EFFECT: Provide the updated C# script and a description of the new particle system settings.\n    ```\n    This command is precise, powerful, and leaves nothing to chance. This is the syntax of power.\n</file_artifact>\n\n<file path=\"src/Artifacts/A69 - V2V Academy - Lesson 3.2 - The Feedback Loop in Practice.md\">\n# Artifact A69: V2V Academy - Lesson 3.2 - The Feedback Loop in Practice\n# Date Created: C69\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 3.2 of the V2V Academy, \"The Feedback Loop in Practice,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, feedback loop, debugging, cognitive apprenticeship, interactive learning, persona\n\n## **Lesson 3.2: The Feedback Loop in Practice**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: The AI as a Feedback Loop**\n*   **Page Title:** Leveraging Errors as Data Points for AI Refinement\n*   **Image Prompt:** A professional in a modern office looking at a holographic screen. The screen shows a circular diagram: \"Human Expertise\" -> \"Expert Feedback\" -> \"AI Action\" -> \"AI Output\" -> \"System Error\" -> \"Human Analysis,\" which then loops back. The diagram illustrates a continuous cycle of refinement where errors are a key input."
  },
  {
    "id": "report_source",
    "chunk": "\"AI Action\" -> \"AI Output\" -> \"System Error\" -> \"Human Analysis,\" which then loops back. The diagram illustrates a continuous cycle of refinement where errors are a key input.\n*   **TL;DR:** The most powerful way to use AI is not as an instruction-taker, but as a feedback mechanism that amplifies your own cognitive and professional skills. Errors are the fuel for this mechanism.\n*   **Content:** In the V2V pathway, the AI is more than a tool; it's a mirror that creates a feedback loop for your own thought processes. To guide an AI effectively on complex tasks, you must provide \"expert feedback.\" But what if you're not an expert in a new domain, like coding? The system itself provides the feedback. A compiler error, for instance, is an objective, expert critique of the AI's code. By taking that error and feeding it back to the AI with the instruction to \"fix it,\" you enter the loop. You are now directing the AI toward a correct solution while simultaneously learning from the process. This transforms you from a passive user into an active director, using the AI to build and validate your own growing expertise.\n\n#### **Page 2: Understanding Feedback Types**\n*   **Page Title:** Decoding System Feedback: A Professional's Guide to Errors\n*   **Image Prompt:** A clean, infographic-style diagram showing three types of errors. \"Compiler Error\" is represented by a document with grammatical mistakes highlighted. \"Runtime Error\" is a machine trying to perform an impossible action, like fitting a square peg in a round hole. \"Logical Error\" is a perfectly built machine that is driving in the wrong direction.\n*   **TL;DR:** To effectively manage an AI, you must understand the feedback it generates. This means learning to distinguish be"
  },
  {
    "id": "report_source",
    "chunk": "achine that is driving in the wrong direction.\n*   **TL;DR:** To effectively manage an AI, you must understand the feedback it generates. This means learning to distinguish between syntax errors, runtime errors, and subtle logical flaws.\n*   **Content:** System feedback primarily comes in the form of errors. Understanding the type of error is key to providing the right guidance to your AI partner. **Compiler/Syntax Errors** are like grammatical mistakes; the AI wrote code that violates the language's rules. **Runtime Errors** occur when the code is grammatically correct but tries to do something impossible during execution, like dividing by zero. **Logical Errors** are the most subtle and require the most human oversight. The code runs without crashing but produces an incorrect result because the underlying strategy is flawed. As a Citizen Architect, your role is to interpret these signals and translate them into clear, corrective instructions.\n\n#### **Page 3: The Workflow in Practice**\n*   **Page Title:** The Debugging Cycle: A Practical Workflow\n*   **Image Prompt:** A step-by-step diagram of the feedback loop. 1. An AI generates a block of code. 2. The code is run, and a red error message (stack trace) appears in a terminal. 3. The professional highlights and copies the full error message. 4. The error is pasted into the \"Ephemeral Context\" of the DCE with a new, simple prompt: \"Fix this.\"\n*   **TL;DR:** The practical workflow is simple: run the AI's code, capture the full error message when it fails, and provide that error back to the AI as context for the next iteration.\n*   **Content:** Let's walk through a real-world scenario. The AI generates a Python script. You run it, and the terminal returns a `TypeError`. Th"
  },
  {
    "id": "report_source",
    "chunk": "ext for the next iteration.\n*   **Content:** Let's walk through a real-world scenario. The AI generates a Python script. You run it, and the terminal returns a `TypeError`. The key is not to be intimidated by the technical jargon. Your task is to act as a conduit. You copy the *entire* error message, from top to bottom. You then paste this into the \"Ephemeral Context\" field in the DCE. Your prompt for the next cycle is simple and direct: \"The previous code produced the error included in the ephemeral context. Analyze the error and provide the corrected code.\" The AI, now armed with precise, expert feedback from the system, can diagnose and fix its own mistake.\n\n#### **Page 4: The Strategic Advantage**\n*   **Page Title:** The Strategic Advantage: Accelerating Your Learning Curve\n*   **Image Prompt:** A graph showing a steep, upward-curving line labeled \"V2V Learning Curve,\" demonstrating rapid skill acquisition. The line is fueled by small, iterative cycles of \"Error -> Feedback -> Correction.\"\n*   **TL;DR:** This feedback loop is the single fastest way to learn a new technical domain. Every error is a micro-lesson that builds your expertise and your mental model of the system.\n*   **Content:** This iterative feedback loop is more than just a debugging technique; it is a powerful engine for accelerated learning. Each time you witness the cycle of an error and its resolution, you internalize a new pattern. Your \"mental model of the model\"and of the programming language itselfbecomes more sophisticated. You begin to anticipate common errors and understand their root causes. This is the essence of Cognitive Apprenticeship in practice. The AI is not just fixing code for you; it is modeling an expert's debugging process, and"
  },
  {
    "id": "report_source",
    "chunk": "stand their root causes. This is the essence of Cognitive Apprenticeship in practice. The AI is not just fixing code for you; it is modeling an expert's debugging process, and you are learning by observation. This transforms you from someone who *manages* an AI into someone who *understands* the work at a deep, technical level.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: The AI as a Feedback Loop**\n*   **Page Title:** Your First AI Debugging Session: Turning Errors into Progress\n*   **Image Prompt:** A student is shown working on a coding problem, looking confused at an error message. An AI companion points to the error, then points to the prompt input field, encouraging the student to use the error as the next input. The student has a \"lightbulb\" moment.\n*   **TL;DR:** Don't fear errorsthey are your most powerful learning tool. The V2V method teaches you to use AI as a partner that turns your mistakes into immediate, practical lessons.\n*   **Content:** One of the most intimidating parts of learning to code is seeing a screen full of red error messages. The V2V pathway teaches you to see those errors not as a failure, but as progress. The AI is your 24/7 pair programming partner, and its most important job is to help you learn from mistakes. When AI-generated code fails, the error message is a free piece of \"expert feedback.\" Your job is to take that feedback and give it right back to the AI. This creates a powerful learning loop where you guide the AI to the right answer, and in the process, you learn exactly what the error means and how to fix it.\n\n#### **Page 2: Understanding Feedback Types**\n*   **Page Title:** A Field Guide to Code Bugs\n*   **Image Prompt:** A simple, friendly infographic sh"
  },
  {
    "id": "report_source",
    "chunk": " means and how to fix it.\n\n#### **Page 2: Understanding Feedback Types**\n*   **Page Title:** A Field Guide to Code Bugs\n*   **Image Prompt:** A simple, friendly infographic showing three types of \"bugs.\" A \"Syntax Bug\" is a bug with glasses on, reading a book of rules incorrectly. A \"Runtime Bug\" is a bug that trips over a wire while running. A \"Logic Bug\" is a bug that is following a map perfectly, but the map leads to the wrong treasure.\n*   **TL;DR:** To get good at debugging with an AI, you need to know the different kinds of bugs. The three main types are syntax errors, runtime errors, and logic errors.\n*   **Content:** Not all bugs are created equal. Learning to spot the different types will help you give the AI better instructions. **Compiler/Syntax Errors** are the easiest; they're like spelling or grammar mistakes in the code. The AI used a \"word\" the computer doesn't understand. **Runtime Errors** happen when the code is trying to run. It's grammatically correct, but it tries to do something impossible, like dividing a number by zero. **Logical Errors** are the sneakiest. The code runs without any errors, but it gives you the wrong answer. This means the AI's *idea* was wrong, and this is where your critical thinking is most needed.\n\n#### **Page 3: The Workflow in Practice**\n*   **Page Title:** The Debugging Cycle: A Step-by-Step Guide\n*   **Image Prompt:** A clear, step-by-step diagram. 1. A developer clicks \"Run.\" 2. A red error message appears in a terminal window. 3. The developer is shown highlighting and copying the entire error message. 4. The error is pasted into the \"Ephemeral Context\" field of the DCE, and the developer types the new prompt: \"Fix this error.\"\n*   **TL;DR:** The workflow is simple: run"
  },
  {
    "id": "report_source",
    "chunk": "age. 4. The error is pasted into the \"Ephemeral Context\" field of the DCE, and the developer types the new prompt: \"Fix this error.\"\n*   **TL;DR:** The workflow is simple: run the code, copy the *entire* error message when it breaks, and paste that error back into the context for your next prompt to the AI.\n*   **Content:** Let's walk through your first debugging cycle. It's a simple but powerful process. 1. **Run the Code:** The AI gives you a script. You run it. 2. **Get the Error:** The terminal shows a `TypeError` and a bunch of other lines. Don't worry if you don't understand it. 3. **Copy Everything:** This is the key step. Highlight and copy the *entire* error message, from the first line to the last. This is called the \"stack trace,\" and it's a map that tells the AI exactly where the problem is. 4. **Feed it Back:** Paste the full error into the \"Ephemeral Context\" field in the DCE. Your new prompt is as simple as: \"The last code you gave me produced this error. Please analyze it and provide the corrected code.\" That's it! You've just completed a professional debugging loop.\n\n#### **Page 4: The Strategic Advantage**\n*   **Page Title:** Why This is the Fastest Way to Learn\n*   **Image Prompt:** A graph shows two lines. One, labeled \"Traditional Learning,\" is a slow, steady incline. The other, labeled \"V2V Feedback Loop,\" is a steep, upward-curving rocket, showing much faster skill acquisition.\n*   **TL;DR:** This feedback loop is a learning hack. Every bug you and your AI partner fix together is a mini-lesson that builds your real-world coding skills faster than any textbook.\n*   **Content:** This iterative feedback loop is the ultimate learning accelerator. Textbooks can teach you theory, but the V2V workflow thr"
  },
  {
    "id": "report_source",
    "chunk": "g skills faster than any textbook.\n*   **Content:** This iterative feedback loop is the ultimate learning accelerator. Textbooks can teach you theory, but the V2V workflow throws you right into real-world problem-solving. Every error you encounter is a practical, in-context lesson. By seeing the AI identify a bug, explain the fix, and implement the solution, you learn faster than you would by just reading. This process builds your \"mental model\" of how code works and how to fix it when it breaks. It's the skill that separates a graduate with a degree from an engineer with experience, and this method helps you get that experience faster than any other.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: The AI as a Feedback Loop**\n*   **Page Title:** Respawning with a Purpose: Using Errors to Level Up\n*   **Image Prompt:** A video game character is defeated by a boss and respawns at the start of the level. This time, an AI companion replays a holographic recording of the failed fight, highlighting the boss's attack pattern that killed the player. The player nods in understanding, ready for the next attempt.\n*   **TL;DR:** In the V2V game, every \"Game Over\" screen (an error) is a chance to learn the boss's pattern. This lesson teaches you how to use your AI sidekick to analyze your fails and come back stronger.\n*   **Content:** In any tough game, you're going to wipe a few times before you beat the boss. In coding, these wipes are called \"errors.\" The V2V path teaches you that every error is a power-up. When your AI partner generates code and it crashes, the error message is a secret hint that reveals the boss's weak point. Your job is to grab that hint and feed it back to your AI. This creates an epic training m"
  },
  {
    "id": "report_source",
    "chunk": "d it crashes, the error message is a secret hint that reveals the boss's weak point. Your job is to grab that hint and feed it back to your AI. This creates an epic training montage loop. You're not just trying to win; you're learning the game's deep mechanics, turning every failure into a massive EXP gain.\n\n#### **Page 2: Understanding Feedback Types**\n*   **Page Title:** A Bestiary of Bugs\n*   **Image Prompt:** A page from a \"Monster Manual\" for code bugs. The \"Syntax Slug\" is a slow creature that breaks the rules of grammar. The \"Runtime Raptor\" is a fast monster that appears out of nowhere and crashes your game. The \"Logic Lich\" is a master of illusion who doesn't crash the game but subtly changes the rules to make you lose.\n*   **TL;DR:** To be a legendary bug hunter, you need to know your monsters. The three main types are Syntax Slugs (grammar mistakes), Runtime Raptors (crashes during play), and Logic Liches (the game runs, but the score is wrong).\n*   **Content:** Not all bugs are the same. Knowing which monster you're fighting is key to victory. **Syntax Slugs** are the easiest to squash; they're just grammar mistakes in the code's \"language.\" **Runtime Raptors** are more dangerous; they strike while the game is running and cause a full-on crash. They happen when the code tries to do something impossible. The **Logic Lich** is the ultimate villain. This bug doesn't crash the game; it's a master of illusion that subtly changes the rules so that you get the wrong outcome. It's the final boss of debugging and requires all your critical thinking skills to defeat.\n\n#### **Page 3: The Workflow in Practice**\n*   **Page Title:** The Debugging Combo\n*   **Image Prompt:** A four-panel comic strip showing the combo sequen"
  },
  {
    "id": "report_source",
    "chunk": "ng skills to defeat.\n\n#### **Page 3: The Workflow in Practice**\n*   **Page Title:** The Debugging Combo\n*   **Image Prompt:** A four-panel comic strip showing the combo sequence. 1. **EXECUTE:** A hero casts a spell (runs code). 2. **CRASH:** The spell backfires with a huge red \"ERROR!\" graphic. 3. **CAPTURE:** The hero uses a magic item to capture the full error message in a glowing orb. 4. **COUNTER:** The hero infuses their next spell with the captured error, launching a new, more powerful attack.\n*   **TL;DR:** The basic debugging combo is a simple four-hit sequence: Run the code, copy the *entire* error message when it crashes, paste it into your context, and tell the AI to launch a counter-attack.\n*   **Content:** Ready to learn your first debugging combo? It's easy to master. 1. **Cast the Spell:** Run the code your AI gave you. 2. **Analyze the Backfire:** The code crashes and a wall of red text appears. This is the \"stack trace.\" Don't panic. 3. **Capture the Essence:** This is the secret move. Copy the *entire* wall of text. Every single line. This contains the enemy's complete attack pattern. 4. **Launch Your Counter-Spell:** Paste the full error into the \"Ephemeral Context\" in the DCE. Your new prompt is your counter: \"The last spell backfired with this error. Analyze the pattern and craft a new spell that works.\" Boom. You've just executed a pro-level debugging cycle.\n\n#### **Page 4: The Strategic Advantage**\n*   **Page Title:** The Ultimate Training Montage\n*   **Image Prompt:** A hero is shown leveling up at an incredible speed. Each time they defeat a bug-monster, they absorb its energy and a \"+1 INT\" or \"+1 WIS\" stat increase appears over their head.\n*   **TL;DR:** This feedback loop is the ultimate EXP "
  },
  {
    "id": "report_source",
    "chunk": "time they defeat a bug-monster, they absorb its energy and a \"+1 INT\" or \"+1 WIS\" stat increase appears over their head.\n*   **TL;DR:** This feedback loop is the ultimate EXP farm. Every bug you and your AI partner squash together makes you a smarter, more powerful creator, faster than any other method.\n*   **Content:** This iterative feedback loop is the ultimate training montage. Forget grinding low-level mobs for hours. In the V2V Academy, every bug is a boss fight that grants a massive EXP boost. By working with your AI to analyze and defeat error after error, you're not just fixing codeyou're downloading expert-level knowledge directly into your brain. You're building a \"mental model\" of the game's code, learning its rules, its exploits, and its secret mechanics. This is the fastest path to becoming a god-tier developer, capable of building anything you can imagine.\n</file_artifact>\n\n<file path=\"src/Artifacts/A70 - V2V Academy - Lesson 3.3 - The Test-and-Revert Workflow.md\">\n# Artifact A70: V2V Academy - Lesson 3.3 - The Test-and-Revert Workflow\n# Date Created: C70\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 3.3 of the V2V Academy, \"The Test-and-Revert Workflow,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, git, version control, testing, cognitive apprenticeship, interactive learning, persona\n\n## **Lesson 3.3: The Test-and-Revert Workflow**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: The Professional's Safety Net**\n*   **Page Title:** Risk Mitigation: A Framework for Safely Testing AI-Generated Solutions\n*   **Im"
  },
  {
    "id": "report_source",
    "chunk": ": The Career Transitioner**\n\n#### **Page 1: The Professional's Safety Net**\n*   **Page Title:** Risk Mitigation: A Framework for Safely Testing AI-Generated Solutions\n*   **Image Prompt:** A professional engineer is shown working on a complex blueprint. To their side is a prominent, glowing \"UNDO\" button. The engineer is confidently making a bold change to the blueprint, knowing they can instantly revert it if it doesn't work. The scene conveys a sense of safety, confidence, and controlled experimentation.\n*   **TL;DR:** The Test-and-Revert workflow is a professional risk management strategy. It uses version control (Git) to create a safety net, allowing you to test potentially risky AI-generated solutions with the absolute confidence that you can instantly undo any negative consequences.\n*   **Content:** When integrating AI-generated code or content into a business-critical project, managing risk is paramount. The AI is a powerful but non-deterministic partner; its solutions can introduce unforeseen bugs or misalignments. The **Test-and-Revert Workflow** is a disciplined framework for mitigating this risk. It leverages a version control system called **Git** to create a \"baseline,\" or a safe snapshot of your project, before you introduce any changes. This allows you to freely experiment with the AI's output, and if it proves to be flawed, you can revert your entire project back to that clean baseline with a single command. This is the professional's method for enabling rapid innovation without compromising stability.\n\n#### **Page 2: Why It's Essential for AI Collaboration**\n*   **Page Title:** Managing Non-Determinism: Why You Need a Safety Net\n*   **Image Prompt:** A diagram shows a single prompt leading to three diffe"
  },
  {
    "id": "report_source",
    "chunk": "ial for AI Collaboration**\n*   **Page Title:** Managing Non-Determinism: Why You Need a Safety Net\n*   **Image Prompt:** A diagram shows a single prompt leading to three different AI-generated outcomes, visualized as branching, unpredictable paths. One path leads to a green checkmark (\"Success\"), while the other two lead to red X's (\"Bugs,\" \"Logic Flaw\"). A human figure stands at the branching point, protected by a glowing shield labeled \"Git Baseline.\"\n*   **TL;DR:** AI is not deterministic; the same prompt can yield different results, some of which may be flawed. The Test-and-Revert loop is the essential safety protocol for navigating this unpredictability.\n*   **Content:** Unlike traditional software, which is deterministic (the same input always produces the same output), LLMs are probabilistic. An AI might give you a perfect solution one minute and a buggy one the next, even for the same problem. This inherent unpredictability is a significant risk in a professional environment. You cannot afford to spend hours untangling a flawed solution that has been merged into your codebase. The Test-and-Revert workflow is the industry-standard solution to this problem. By creating a baseline before every test, you isolate the AI's changes in a temporary state, ensuring that any negative impacts are fully contained and easily reversible.\n\n#### **Page 3: The Workflow in Practice**\n*   **Page Title:** The Four-Step Validation Process\n*   **Image Prompt:** A clear, four-step flowchart. 1. **Baseline:** A developer clicks a \"Baseline (Commit)\" button, creating a snapshot. 2. **Accept:** The developer accepts AI-generated code into their project. 3. **Test:** The developer runs a series of automated tests, which show a \"FAIL\" status"
  },
  {
    "id": "report_source",
    "chunk": " a snapshot. 2. **Accept:** The developer accepts AI-generated code into their project. 3. **Test:** The developer runs a series of automated tests, which show a \"FAIL\" status. 4. **Restore:** The developer clicks a \"Restore Baseline\" button, and the project instantly reverts to the original snapshot.\n*   **TL;DR:** The workflow consists of four simple steps: create a safe restore point (Baseline), apply the AI's changes (Accept), check for issues (Test), and decide whether to keep or discard the changes (Proceed or Restore).\n*   **Content:** The Test-and-Revert loop is a straightforward but powerful four-step process integrated directly into the DCE. 1. **Baseline:** After selecting a promising AI response, you click the \"Baseline (Commit)\" button. This uses Git to save a snapshot of your project's current, working state. 2. **Accept:** You select the AI-generated files you wish to test and click \"Accept Selected,\" which overwrites your local files. 3. **Test:** You run your application's test suite or perform a manual functional test. 4. **Decide:** If the test fails or the changes are undesirable, you click \"Restore Baseline.\" This instantly discards all the AI's changes. If the test passes, you simply proceed to the next cycle, your successful changes now part of the project's history.\n\n#### **Page 4: The Strategic Advantage**\n*   **Page Title:** The Advantage: Innovation with Confidence\n*   **Image Prompt:** A graph shows two lines. The \"Traditional Workflow\" line shows slow, cautious, linear progress. The \"V2V Workflow\" line shows rapid, bold, upward spikes of experimentation, with small, quick dips representing instantly-reverted failures, resulting in a much faster overall rate of progress.\n*   **TL;DR:** This wo"
  },
  {
    "id": "report_source",
    "chunk": " upward spikes of experimentation, with small, quick dips representing instantly-reverted failures, resulting in a much faster overall rate of progress.\n*   **TL;DR:** This workflow removes the fear of breaking things, empowering you to experiment with more ambitious, innovative AI solutions and dramatically accelerating your development velocity.\n*   **Content:** The strategic advantage of the Test-and-Revert workflow cannot be overstated. By removing the fear of catastrophic failure, it fundamentally changes your relationship with the AI. You are no longer limited to accepting only the safest, most conservative suggestions. You are free to experiment with bold, creative, or highly complex solutions, knowing that the worst-case scenario is a single click away from being undone. This confidence enables a much higher tempo of innovation and experimentation, allowing you to find better solutions faster. It is the core mechanism that makes rapid, AI-driven development not just possible, but professionally responsible.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: The Professional's Safety Net**\n*   **Page Title:** How to Test Code You Didn't Write: The Git-Integrated Workflow\n*   **Image Prompt:** A young developer is shown confidently working on a complex project. To their side is a prominent, glowing \"UNDO\" button. They are applying a large, complex piece of AI-generated code to their project, smiling because they know they can instantly revert it if it breaks anything.\n*   **TL;DR:** The Test-and-Revert workflow is a professional developer's secret weapon. It uses a tool called Git to create a \"save point\" for your code, letting you test any AI suggestion without the fear of messing up your project."
  },
  {
    "id": "report_source",
    "chunk": "al developer's secret weapon. It uses a tool called Git to create a \"save point\" for your code, letting you test any AI suggestion without the fear of messing up your project.\n*   **Content:** One of the biggest challenges when starting out is being afraid to break things, especially when using code you didn't write yourself. The **Test-and-Revert Workflow** is the solution. It's a professional technique that uses a version control system called **Git** to create a \"baseline\"a safe \"save point\" for your projectbefore you try out any of the AI's code. This gives you a powerful safety net. You can accept any change, no matter how big, and if it causes a bug, you can press a single \"Restore\" button to go right back to the moment before the change was made. This is a core skill that shows employers you know how to work safely and efficiently.\n\n#### **Page 2: Why It's Essential for AI Collaboration**\n*   **Page Title:** Why You Need a Safety Net: The AI is Unpredictable\n*   **Image Prompt:** A diagram shows a developer asking an AI for a piece of code. The AI, represented as a friendly but slightly chaotic robot, offers three different code snippets. One has a green checkmark, but the other two have hidden red bug icons. A shield labeled \"Git Baseline\" protects the developer.\n*   **TL;DR:** AI doesn't always give you the same answer, and sometimes its answers have bugs. The Test-and-Revert loop is your shield, protecting your project from the AI's occasional mistakes.\n*   **Content:** Unlike the code you write, which does the same thing every time, an AI's output can be unpredictable. It might give you a perfect solution, or it might give you one with a hidden bug. You can't know until you test it. This is why a safety net "
  },
  {
    "id": "report_source",
    "chunk": "AI's output can be unpredictable. It might give you a perfect solution, or it might give you one with a hidden bug. You can't know until you test it. This is why a safety net is essential. Trying to manually undo a complex, multi-file change from an AI is a nightmare. The Test-and-Revert workflow makes this process trivial. By creating a baseline before you test, you ensure that any bugs or problems introduced by the AI are completely isolated and can be wiped away in an instant.\n\n#### **Page 3: The Workflow in Practice**\n*   **Page Title:** The Four-Step Validation Process\n*   **Image Prompt:** A clear, four-step flowchart. 1. **Baseline:** A developer clicks a \"Baseline (Commit)\" button, creating a \"Save Point.\" 2. **Accept:** The developer clicks \"Accept Selected\" to apply the AI's code. 3. **Test:** The developer runs the code, and a big \"TEST FAILED\" message appears. 4. **Restore:** The developer clicks a \"Restore Baseline\" button, and the project is instantly clean again.\n*   **TL;DR:** The workflow is a simple four-step combo: save your progress (Baseline), apply the AI's changes (Accept), see if it works (Test), and decide to keep it or go back (Proceed or Restore).\n*   **Content:** The Test-and-Revert loop is a simple but powerful process built right into the DCE. 1. **Baseline:** After you've picked an AI response you want to try, you click the \"Baseline (Commit)\" button. This uses Git to create a save point of your project. 2. **Accept:** You select the files the AI generated and click \"Accept Selected.\" 3. **Test:** You run your app and see if the new feature works or if anything broke. 4. **Decide:** If it's buggy, just click \"Restore Baseline\" to go back to your save point. It's that easy. If it works, you'"
  },
  {
    "id": "report_source",
    "chunk": "see if the new feature works or if anything broke. 4. **Decide:** If it's buggy, just click \"Restore Baseline\" to go back to your save point. It's that easy. If it works, you're all set to start the next cycle.\n\n#### **Page 4: The Strategic Advantage**\n*   **Page Title:** The Advantage: Build Faster, Learn Faster\n*   **Image Prompt:** A graph shows two learning curves. The \"Cautious Coder\" curve is slow and flat. The \"V2V Developer\" curve is steep and upward, showing rapid progress. The V2V curve is made of bold upward spikes (\"Experiments\") and tiny, quick dips (\"Reverts\").\n*   **TL;DR:** This workflow lets you experiment fearlessly. You'll be able to try out more ambitious ideas, learn from mistakes instantly, and build your skills and your portfolio much faster.\n*   **Content:** The real advantage of this workflow is speednot just in coding, but in learning. When you're not afraid of breaking your project, you're free to experiment. You can try the AI's most creative or complex suggestions just to see what happens. This fearless experimentation is the fastest way to learn. Every reverted failure is a quick, low-cost lesson. This high tempo of \"experiment -> validate -> learn\" will dramatically accelerate your development speed and, more importantly, your growth as an engineer. It's a skill that will set you apart.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: The Professional's Safety Net**\n*   **Page Title:** Save Scumming for Coders: Mastering the Test-and-Revert Loop\n*   **Image Prompt:** A gamer is shown playing a difficult video game. Just before entering the boss room, they hit a glowing \"QUICKSAVE\" button. The scene conveys a sense of smart preparation before a risky challenge.\n*   **TL;DR:** T"
  },
  {
    "id": "report_source",
    "chunk": "ideo game. Just before entering the boss room, they hit a glowing \"QUICKSAVE\" button. The scene conveys a sense of smart preparation before a risky challenge.\n*   **TL;DR:** The Test-and-Revert workflow is the coding equivalent of \"save scumming.\" It's a pro-gamer move that uses Git to create a perfect save state before you try a risky strategy (like using AI-generated code), letting you instantly reload if you wipe.\n*   **Content:** You know the feeling: you're about to fight a tough boss, so you create a save state. That way, if you mess up, you can just reload and try again without losing all your progress. This is called \"save scumming,\" and it's a core strategy for mastery. The **Test-and-Revert Workflow** is how you do this with code. It uses a powerful tool called **Git** to create a \"baseline\"a perfect \"save state\" of your projectbefore you try out the AI's unpredictable and potentially buggy code. If the AI's strategy fails, you just hit \"Restore,\" and you're right back where you started, ready to try a different approach.\n\n#### **Page 2: Why It's Essential for AI Collaboration**\n*   **Page Title:** Taming the RNG: Why You Need a Save State\n*   **Image Prompt:** A diagram shows a player asking an AI companion for a new weapon. The AI, represented as a chaotic but powerful entity, offers three glowing swords. One is \"Legendary,\" but the other two are \"Cursed.\" A magical shield labeled \"Git Baseline\" protects the player from the cursed items.\n*   **TL;DR:** Your AI companion is a master crafter, but its creations are based on RNG. Sometimes it crafts a legendary item, and sometimes it's cursed. The Test-and-Revert loop is your shield against the bad rolls.\n*   **Content:** Your AI partner is like a god-tier blac"
  },
  {
    "id": "report_source",
    "chunk": " it crafts a legendary item, and sometimes it's cursed. The Test-and-Revert loop is your shield against the bad rolls.\n*   **Content:** Your AI partner is like a god-tier blacksmith with a high crafting skill, but the results are still based on Random Number Generation (RNG). It might forge a legendary weapon for you, or it might hand you a cursed item that drains your HP. You won't know until you equip it and enter combat. This is why you always save before identifying a new item. The Test-and-Revert workflow is your save state. By creating a baseline before you test the AI's code, you guarantee that any \"curses\" (bugs) are contained and can be instantly cleansed from your project by reloading your save.\n\n#### **Page 3: The Workflow in Practice**\n*   **Page Title:** The Four-Hit Combo\n*   **Image Prompt:** A four-panel comic strip showing the workflow as a fighting game combo. 1. **SAVE:** The character hits a \"Baseline\" button, and a \"Game Saved\" message appears. 2. **EQUIP:** The character equips a new, AI-generated weapon. 3. **TEST:** The character swings the weapon at a training dummy, and it shatters (\"FAIL!\"). 4. **RELOAD:** The character hits a \"Restore\" button and instantly reappears at the save point with their old gear.\n*   **TL;DR:** The workflow is a simple four-hit combo: save your game (Baseline), equip the new gear (Accept), fight a mob (Test), and if you wipe, just reload your save (Restore).\n*   **Content:** The Test-and-Revert loop is a simple but devastatingly effective combo built into the DCE. 1. **Baseline (Quicksave):** After the AI drops some new loot, hit the \"Baseline (Commit)\" button. This is your save state. 2. **Accept (Equip):** Select the new code you want to try and hit \"Accept Selected."
  },
  {
    "id": "report_source",
    "chunk": "r the AI drops some new loot, hit the \"Baseline (Commit)\" button. This is your save state. 2. **Accept (Equip):** Select the new code you want to try and hit \"Accept Selected.\" 3. **Test (Enter Combat):** Run your program. Does it work? Does it crash and burn? 4. **Decide (Reload or Keep):** If it's a wipe, just hit \"Restore Baseline\" to instantly reload your save. No harm, no foul. If you win, the loot is yours, and you're ready for the next quest.\n\n#### **Page 4: The Strategic Advantage**\n*   **Page Title:** The Advantage: Fearless Speedrunning\n*   **Image Prompt:** A speedrunner is shown blazing through a difficult level, trying risky, high-level skips and strategies. They are not afraid of failing because a \"Reload Last Save\" button is always visible in the corner of their screen.\n*   **TL;DR:** This workflow removes all fear of failure. It lets you try the AI's most insane, high-risk, high-reward strategies, because you know a wipe costs you nothing. This is how you learn the game's deepest secrets and become a speedrunner.\n*   **Content:** The true power of the Test-and-Revert workflow is that it makes you fearless. When you know you can instantly undo any mistake, you're free to experiment with the AI's wildest suggestions. You can try that crazy, complex algorithm or that massive refactor just to see what happens. This is the mindset of a speedrunner, constantly pushing the boundaries and trying new routes because they know failure has no penalty. This fearless experimentation is the fastest way to discover the most powerful techniques and to master the game of AI-assisted development.\n</file_artifact>\n\n<file path=\"src/Artifacts/A71 - V2V Academy - Lesson 4.1 - Defining Your Vision.md\">\n# Artifact A71: V2V Academ"
  },
  {
    "id": "report_source",
    "chunk": "o master the game of AI-assisted development.\n</file_artifact>\n\n<file path=\"src/Artifacts/A71 - V2V Academy - Lesson 4.1 - Defining Your Vision.md\">\n# Artifact A71: V2V Academy - Lesson 4.1 - Defining Your Vision\n# Date Created: C71\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 4.1 of the V2V Academy, \"Defining Your Vision,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, project scope, mvp, planning, interactive learning, persona\n\n## **Lesson 4.1: Defining Your Vision**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: The Power of a Clear Vision**\n*   **Page Title:** From Business Need to Project Scope: Architecting Your Solution\n*   **Image Prompt:** A seasoned professional stands at a holographic whiteboard, sketching out a high-level strategic plan. The sketch shows a clear line from \"Problem\" to \"Target User\" to \"Proposed Solution.\" The scene is clean, focused, and professional, emphasizing strategic foresight.\n*   **TL;DR:** Before execution comes architecture. This lesson teaches you how to translate a raw business idea into a formal Project Scopethe foundational blueprint that guides all successful AI-driven development.\n*   **Content:** In any professional endeavor, a clear plan is the prerequisite for success. This is doubly true when collaborating with AI. An AI can execute complex tasks with incredible speed, but it cannot read your mind or infer your strategic intent. The first step of any project, therefore, is to create a **Project Scope**. This document is your architectural blueprint. It's where you define the "
  },
  {
    "id": "report_source",
    "chunk": "er your strategic intent. The first step of any project, therefore, is to create a **Project Scope**. This document is your architectural blueprint. It's where you define the problem you're solving, the audience you're serving, and the specific, measurable outcomes you intend to achieve. It is the ultimate \"source of truth\" that aligns both your efforts and the AI's, ensuring that every action taken is a step toward a well-defined goal.\n\n#### **Page 2: Deconstructing Your Idea**\n*   **Page Title:** The Discovery Phase: Answering the Three Core Questions\n*   **Image Prompt:** A three-panel diagram. Panel 1 shows a magnifying glass over a \"Problem Statement.\" Panel 2 shows a clear profile of a \"Target User Persona.\" Panel 3 shows a simple diagram of the \"Core Solution.\" Arrows connect the three, showing a logical progression.\n*   **TL;DR:** A strong project scope is built by answering three fundamental questions: What is the problem? Who has this problem? And what is the core function of my solution?\n*   **Content:** A powerful project scope doesn't need to be long, but it must be precise. The process of writing it forces you to deconstruct your idea by answering three core strategic questions. 1. **What is the core problem?** Articulate the specific pain point you are addressing in one or two clear sentences. 2. **Who is the target user?** Define your **User Persona**. Are you building this for expert analysts, for new hires, for an entire department? Be specific. 3. **What is the core solution?** Describe the single most important function your solution will perform to solve the user's problem. Answering these questions provides the foundational clarity needed for a successful project.\n\n#### **Page 3: Defining the Minimu"
  },
  {
    "id": "report_source",
    "chunk": "tion will perform to solve the user's problem. Answering these questions provides the foundational clarity needed for a successful project.\n\n#### **Page 3: Defining the Minimum Viable Product (MVP)**\n*   **Page Title:** The Principle of the MVP: Start Small, Scale Smart\n*   **Image Prompt:** An image showing the concept of an MVP. On the left, a team is trying to build a complex car all at once, resulting in a pile of unusable parts. On the right, a team builds a skateboard first, then a scooter, then a bicycle, and finally a car, delivering value at every stage.\n*   **TL;DR:** Don't try to build the entire system at once. Define the Minimum Viable Product (MVP)the smallest, simplest version of your idea that still solves the core problem for your target user.\n*   **Content:** The most common point of failure for ambitious projects is trying to do too much, too soon. The professional approach is to define a **Minimum Viable Product (MVP)**. The MVP is not a weak or incomplete version of your idea; it is the most focused version. Ask yourself: \"What is the absolute minimum set of features required to solve the core problem for my user?\" This is your MVP. By starting with a tightly defined scope, you can build, test, and deliver value quickly. This iterative approachbuilding and refining in small, manageable cyclesis far more effective and less risky than attempting a large, monolithic build.\n\n#### **Page 4: Writing the Project Scope Artifact**\n*   **Page Title:** Your First Artifact: The Project Scope Document\n*   **Image Prompt:** A professional is shown using the DCE to create their first artifact: `A1 - Project Scope.md`. The document is structured with clear headings for \"Vision Statement,\" \"Problem,\" \"User Persona"
  },
  {
    "id": "report_source",
    "chunk": "is shown using the DCE to create their first artifact: `A1 - Project Scope.md`. The document is structured with clear headings for \"Vision Statement,\" \"Problem,\" \"User Persona,\" and \"MVP Features.\"\n*   **TL;DR:** It's time to create your first and most important artifact. Use a simple template to document your vision, problem, user, and MVP, creating the \"source of truth\" for your project.\n*   **Content:** Let's put these principles into practice. Your first task in the V2V workflow is to create your Project Scope artifact. Use a simple structure to document your answers to the core questions. This document will become the primary context you provide to the AI in your first development cycle. A good starting template includes:\n    *   **Vision Statement:** A one-sentence, aspirational goal for your project.\n    *   **Problem Statement:** A clear description of the pain point you are solving.\n    *   **Target User Persona:** A brief description of who you are building this for.\n    *   **MVP Feature List:** A short, bulleted list of the core features for your first version.\n    This artifact is your contract with the AI. It is the blueprint that will guide every subsequent step.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: The Power of a Clear Vision**\n*   **Page Title:** Your Portfolio Starts Here: Creating a Professional Project Scope\n*   **Image Prompt:** A hiring manager is reviewing a recent graduate's portfolio. They are zoomed in on a well-written, professional Project Scope document, looking very impressed. The document is the first item in the portfolio.\n*   **TL;DR:** The difference between a student project and a professional portfolio piece is a clear plan. This lesson teaches you how to"
  },
  {
    "id": "report_source",
    "chunk": " is the first item in the portfolio.\n*   **TL;DR:** The difference between a student project and a professional portfolio piece is a clear plan. This lesson teaches you how to write a Project Scopethe document that shows employers you can think like a real engineer.\n*   **Content:** Welcome to the first step of building a project that will get you hired. In the professional world, development doesn't start with code; it starts with a plan. A **Project Scope** is the formal document that outlines what you're building, for whom, and why. It's the blueprint that guides the entire project. Learning to write a clear and concise project scope is a critical skill. It proves to potential employers that you can think strategically, communicate clearly, and manage a project from concept to completion. It's the first and most important piece of any professional portfolio.\n\n#### **Page 2: Deconstructing Your Idea**\n*   **Page Title:** From Cool Idea to Concrete Plan\n*   **Image Prompt:** A three-panel diagram. Panel 1: A lightbulb labeled \"Cool Idea!\" Panel 2: A series of question marks around the lightbulb (\"Who is this for?\", \"What problem does it solve?\"). Panel 3: A simple, clear blueprint labeled \"Actionable Plan.\"\n*   **TL;DR:** A great idea isn't enough. You need to be able to answer three key questions: What is the problem? Who has this problem? And what is the core function of your solution?\n*   **Content:** A great portfolio piece starts with a great idea, but a great *project* starts with a great plan. The process of writing a project scope forces you to get specific about your idea by answering three core questions. 1. **What is the problem?** What specific pain point are you trying to solve? Be precise. 2. **Who is you"
  },
  {
    "id": "report_source",
    "chunk": "ou to get specific about your idea by answering three core questions. 1. **What is the problem?** What specific pain point are you trying to solve? Be precise. 2. **Who is your user?** Define your **User Persona**. Who are you building this for? A \"user persona\" is a short description of your ideal user. 3. **What is the core solution?** What is the one key thing your project will do to solve the problem for that user? Answering these questions is how you turn a vague idea into an actionable engineering plan.\n\n#### **Page 3: Defining the Minimum Viable Product (MVP)**\n*   **Page Title:** The MVP Strategy: How to Actually Finish Your Projects\n*   **Image Prompt:** An image showing two paths. One, labeled \"Build Everything,\" leads to an unfinished, complex mess of code. The other, labeled \"Build the MVP,\" leads to a small but complete, polished, and working application.\n*   **TL;DR:** The secret to finishing projects is to start small. Define the Minimum Viable Product (MVP)the simplest version of your app that still works and provides value.\n*   **Content:** The biggest reason personal projects fail is because their scope is too big. The professional solution is to build a **Minimum Viable Product (MVP)**. The MVP isn't your dream version of the app with every feature you can imagine; it's the simplest, most focused version that solves the core problem. Ask yourself: \"What's the smallest thing I can build that is still useful?\" That's your MVP. This approach is powerful because it's achievable. It allows you to get a finished, polished piece for your portfolio quickly. You can always add more features later.\n\n#### **Page 4: Writing the Project Scope Artifact**\n*   **Page Title:** Your First Artifact: The Project Scope Do"
  },
  {
    "id": "report_source",
    "chunk": "r portfolio quickly. You can always add more features later.\n\n#### **Page 4: Writing the Project Scope Artifact**\n*   **Page Title:** Your First Artifact: The Project Scope Document\n*   **Image Prompt:** A student is shown using the DCE to create their first artifact: `A1 - Project Scope.md`. The document is structured with clear headings for \"Vision Statement,\" \"Problem,\" \"User Persona,\" and \"MVP Features.\"\n*   **TL;DR:** Let's create your first portfolio document. Use this simple template to write down your vision, problem, user, and MVP. This will be the blueprint you give to your AI partner.\n*   **Content:** It's time to create the first official document for your portfolio. This Project Scope artifact is what you'll use to guide your AI partner in the next lessons. Create a new file and use this simple template:\n    *   **Vision Statement:** A single, exciting sentence about your project's goal.\n    *   **Problem Statement:** What pain point are you solving?\n    *   **Target User Persona:** Who are you building this for?\n    *   **MVP Feature List:** A short, bulleted list of the essential features for your first version.\n    Completing this document is a huge step. You now have a professional plan that will guide you and your AI toward a finished, portfolio-ready project.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: The Power of a Clear Vision**\n*   **Page Title:** The Hero's Journey: Defining Your Quest\n*   **Image Prompt:** A hero stands before a massive, ancient map spread out on a stone table. They are plotting a course from their starting village to a distant, glowing castle. The map is labeled \"Project Scope.\" The scene is epic and full of purpose.\n*   **TL;DR:** Every legendary adventure sta"
  },
  {
    "id": "report_source",
    "chunk": "om their starting village to a distant, glowing castle. The map is labeled \"Project Scope.\" The scene is epic and full of purpose.\n*   **TL;DR:** Every legendary adventure starts with a quest. This lesson teaches you how to create your Project Scopethe sacred map that will guide you and your AI companion on your epic build.\n*   **Content:** Every great story, every epic game, starts with a quest. Before you set out on your adventure, you need a map. In the world of V2V, that map is your **Project Scope**. This is the artifact where you define your epic quest: the evil you will vanquish (the problem), the people you will save (the users), and the legendary weapon you will forge (the solution). Creating this map is the first and most important step. It's the \"source of truth\" that aligns you and your AI familiar, ensuring every step you take is a step toward your ultimate goal.\n\n#### **Page 2: Deconstructing Your Idea**\n*   **Page Title:** The Quest Giver's Riddle\n*   **Image Prompt:** A wise, old quest giver is shown presenting a riddle to a young hero. The riddle is broken into three parts, represented by glowing runes: a \"Problem\" rune, a \"Hero\" rune (representing the user), and a \"Solution\" rune.\n*   **TL;DR:** To accept the quest, you must first solve the Quest Giver's riddle by answering three questions: What is the evil you must defeat? Who are you fighting for? And what is your ultimate weapon?\n*   **Content:** A great quest is more than just a cool idea; it's a clear mission. To build your map, you must first solve the Quest Giver's riddle by answering three questions. 1. **What is the core problem?** What evil dragon or corrupt king are you setting out to defeat? Define your villain clearly. 2. **Who is your use"
  },
  {
    "id": "report_source",
    "chunk": "y answering three questions. 1. **What is the core problem?** What evil dragon or corrupt king are you setting out to defeat? Define your villain clearly. 2. **Who is your user?** Who are the villagers or kingdom you are fighting for? Create a **User Persona**a profile of the hero who will use what you build. 3. **What is the core solution?** What is the one legendary sword or powerful spell that will win the day? Answering these questions is how you transform a vague desire for adventure into a clear, epic quest.\n\n#### **Page 3: Defining the Minimum Viable Product (MVP)**\n*   **Page Title:** The First Dungeon: Conquering the MVP\n*   **Image Prompt:** An image shows a video game world map. The final boss castle is far in the distance. The player's current objective is highlighted: a small, nearby dungeon labeled \"The First Dungeon (MVP).\" A clear path is shown from this dungeon to the next, and so on, toward the final boss.\n*   **TL;DR:** Don't try to fight the final boss at Level 1. Your first quest is to clear the Minimum Viable Product (MVP)the smallest, first dungeon that still gives you loot and EXP.\n*   **Content:** The biggest mistake a hero can make is trying to fight the final boss at Level 1. You'll get wiped every time. The path to victory is to start with the first dungeon. In V2V, this is your **Minimum Viable Product (MVP)**. The MVP isn't the full, epic game; it's the first, complete, playable level. Ask yourself: \"What's the smallest quest I can complete that is still fun and rewarding?\" That's your MVP. This strategy is how you actually finish things. You clear one dungeon at a time, leveling up your skills and your gear, until you're powerful enough to take on the final boss.\n\n#### **Page 4: Writing t"
  },
  {
    "id": "report_source",
    "chunk": "tually finish things. You clear one dungeon at a time, leveling up your skills and your gear, until you're powerful enough to take on the final boss.\n\n#### **Page 4: Writing the Project Scope Artifact**\n*   **Page Title:** Inscribing Your Map: The Project Scope Artifact\n*   **Image Prompt:** A hero is shown carefully inscribing their quest details onto a magical scroll. The scroll has glowing sections for \"Prophecy\" (Vision), \"The Evil\" (Problem), \"The Chosen One\" (User), and \"The First Trial\" (MVP).\n*   **TL;DR:** It's time to create your map. Use this sacred template to inscribe your prophecy, your enemy, your hero, and your first trial. This scroll will be the source of your AI's power.\n*   **Content:** Let's forge your map. This Project Scope artifact is the sacred scroll you will give to your AI familiar to begin your quest. Create a new file and use this legendary template:\n    *   **Vision Statement (The Prophecy):** Your one-sentence epic goal.\n    *   **Problem Statement (The Great Evil):** The villain you must defeat.\n    *   **Target User Persona (The Chosen One):** The hero who will use your creation.\n    *   **MVP Feature List (The First Trial):** The list of tasks to complete the first dungeon.\n    Once this map is inscribed, your great adventure can truly begin.\n</file_artifact>\n\n<file path=\"src/Artifacts/A72 - V2V Academy - Lesson 4.2 - The Blank Page Problem.md\">\n# Artifact A72: V2V Academy - Lesson 4.2 - The Blank Page Problem\n# Date Created: C72\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 4.2 of the V2V Academy, \"The Blank Page Problem,\" designed for the interactive report viewer. It includes three parallel versions of the content for differe"
  },
  {
    "id": "report_source",
    "chunk": "content for Lesson 4.2 of the V2V Academy, \"The Blank Page Problem,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, project scope, scaffolding, planning, interactive learning, persona\n\n## **Lesson 4.2: The Blank Page Problem**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: The Challenge of Project Initiation**\n*   **Page Title:** Overcoming Inertia: The Challenge of Project Initiation\n*   **Image Prompt:** A professional stands before a vast, empty, and intimidatingly white digital canvas. They hold a single glowing seed of an idea, looking uncertain about where to plant it. The scene conveys the daunting nature of starting a complex project from a completely blank state.\n*   **TL;DR:** The \"Blank Page Problem\" is the initial hurdle of translating a well-defined vision into the first tangible steps of a project. This lesson provides a systematic, AI-driven approach to overcome this inertia.\n*   **Content:** You have a clear vision and a defined project scope. Now comes one of the most challenging phases in any project: starting. The \"Blank Page Problem\" is a well-known phenomenon in creative and technical fields. It's the psychological and practical inertia we face when converting a plan into the first lines of code, the first document, or the first directory structure. An unstructured approach at this stage can lead to a poorly organized foundation, creating technical debt before a single feature is built. The V2V pathway addresses this challenge head-on with a structured, AI-driven methodology for project scaffolding.\n\n#### **Page 2: The V2V Solution: AI as a Scaffolding Engine**\n*"
  },
  {
    "id": "report_source",
    "chunk": "V2V pathway addresses this challenge head-on with a structured, AI-driven methodology for project scaffolding.\n\n#### **Page 2: The V2V Solution: AI as a Scaffolding Engine**\n*   **Page Title:** AI as a Strategic Partner for Project Scaffolding\n*   **Image Prompt:** A professional is shown presenting their \"Project Scope\" document to a powerful AI. The AI processes the document and, in response, generates a complete and perfectly organized architectural blueprint, including folder structures, foundational code files, and key planning artifacts.\n*   **TL;DR:** The V2V workflow leverages AI as a \"scaffolding engine.\" By providing your Project Scope artifact as context, you can command the AI to generate the entire foundational structure of your project automatically.\n*   **Content:** The solution to the blank page is to never start with one. In the V2V workflow, your first step is not to write code, but to delegate the initial setup to your AI partner. By providing the AI with your `Project Scope` artifact (created in Lesson 4.1), you give it the blueprint it needs to act as a scaffolding engine. You can instruct it to perform the foundational tasks that consume significant time and effort: creating a logical directory structure, generating initial boilerplate code for your chosen tech stack, and even producing a starter set of more detailed planning artifacts based on your high-level vision. This transforms the daunting first step into a simple, automated process.\n\n#### **Page 3: Case Study: The DCE's \"Cycle 0\" Onboarding**\n*   **Page Title:** Case Study: The DCE's Own Onboarding Workflow\n*   **Image Prompt:** A close-up of the DCE extension's UI in \"Cycle 0.\" It shows a user typing their project scope into a text area. An"
  },
  {
    "id": "report_source",
    "chunk": "udy: The DCE's Own Onboarding Workflow\n*   **Image Prompt:** A close-up of the DCE extension's UI in \"Cycle 0.\" It shows a user typing their project scope into a text area. An arrow points from this to a generated `prompt.md` file, which then leads to a neatly organized `src/Artifacts` folder filled with new planning documents.\n*   **TL;DR:** The DCE itself is the perfect example of this principle. Its \"Cycle 0\" onboarding experience is a built-in scaffolding engine that takes your high-level vision and automatically generates the foundational artifacts for your project.\n*   **Content:** The best evidence for this workflow is the tool you are using. The Data Curation Environment's \"Cycle 0\" onboarding is a real-world implementation of AI-driven scaffolding. When you first open a new workspace, the DCE prompts you for your project scope. When you click \"Generate Initial Artifacts Prompt,\" it doesn't just create an empty file; it uses your input to construct a complex prompt that instructs an AI to create a full suite of starter documentationa Master Artifact List, a Project Vision document, a Technical Scaffolding Plan, and more. It solves the blank page problem by ensuring you never have to face one.\n\n#### **Page 4: Your First Step: Generating the Blueprint**\n*   **Page Title:** Your First Command: \"Architect the Foundation\"\n*   **Image Prompt:** A professional is shown at their workstation, confidently typing a clear, structured prompt. The prompt instructs the AI to use the attached Project Scope to generate a file structure and initial artifacts for a new project.\n*   **TL;DR:** Your first practical step is to use your Project Scope artifact to command your AI partner to build the project's foundation, creating the i"
  },
  {
    "id": "report_source",
    "chunk": " for a new project.\n*   **TL;DR:** Your first practical step is to use your Project Scope artifact to command your AI partner to build the project's foundation, creating the initial set of files and folders for your MVP.\n*   **Content:** Now it's your turn to apply this principle. Take the `Project Scope` artifact you developed in the previous lesson. This document is the high-quality context you need. Your task for the next cycle is to craft a prompt that instructs your AI to act as a project architect. A powerful prompt would be: \"You are a senior software architect. Based on the attached Project Scope artifact, please generate the complete directory structure and create placeholder files for the Minimum Viable Product. Additionally, create a more detailed technical plan as a new artifact.\" This command delegates the foundational work, allowing you to begin your project with a clean, well-structured, and AI-generated starting point.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: The Challenge of Project Initiation**\n*   **Page Title:** How to Start a Project When You Don't Know Where to Start\n*   **Image Prompt:** A recent graduate sits in front of a computer with a completely empty code editor, looking overwhelmed and uncertain. Question marks float around their head.\n*   **TL;DR:** Facing a \"blank page\" is one of the most intimidating parts of starting a new project. This lesson gives you a powerful technique to use AI to build your project's foundation for you, so you always start with a clear structure.\n*   **Content:** You've got a great idea for your portfolio and you've written a solid project scope. So, what's next? For many new developers, this is the hardest part. Staring at an empty fold"
  },
  {
    "id": "report_source",
    "chunk": "u've got a great idea for your portfolio and you've written a solid project scope. So, what's next? For many new developers, this is the hardest part. Staring at an empty folder and a blank code editor can be paralyzing. Where do you even begin? What should the folder structure look like? What's the first file you should create? This is the \"Blank Page Problem,\" and it's a major hurdle. The good news is that as a V2V developer, you have a partner who can solve this for you.\n\n#### **Page 2: The V2V Solution: AI as a Scaffolding Engine**\n*   **Page Title:** Using AI to Build Your Project's Skeleton\n*   **Image Prompt:** A developer hands their \"Project Scope\" document to a friendly AI robot. The robot reads the document and then quickly assembles a perfect, clean \"skeleton\" of a project, complete with a folder structure and initial files.\n*   **TL;DR:** You can use the Project Scope you already wrote to command the AI to build the entire \"skeleton\" of your projectthe folders, the initial files, the configurationautomatically.\n*   **Content:** The secret to overcoming the blank page problem is to use your AI partner as a \"scaffolding engine.\" \"Scaffolding\" is the initial structure that holds a project together. Instead of trying to figure out the \"right\" way to structure your project from scratch, you can delegate that task to the AI. You provide it with your `Project Scope` artifact as the blueprint, and you command it to create the foundational structure. This includes creating all the necessary folders and generating the initial \"boilerplate\" codethe standard, repetitive code that every project needs.\n\n#### **Page 3: Case Study: The DCE's \"Cycle 0\" Onboarding**\n*   **Page Title:** Case Study: How the DCE Does it for Y"
  },
  {
    "id": "report_source",
    "chunk": "the standard, repetitive code that every project needs.\n\n#### **Page 3: Case Study: The DCE's \"Cycle 0\" Onboarding**\n*   **Page Title:** Case Study: How the DCE Does it for You\n*   **Image Prompt:** A close-up of the DCE extension's UI in \"Cycle 0.\" It shows a user typing their project scope. An arrow points to a generated `prompt.md` file, which then leads to a neatly organized `src/Artifacts` folder filled with new planning documents.\n*   **TL;DR:** The DCE tool itself uses this exact technique. Its \"Cycle 0\" feature takes your high-level idea and uses it to prompt an AI to create all the initial planning documents for your project.\n*   **Content:** The V2V workflow has this powerful principle built right into its core tool. Think back to when you started your first project with the Data Curation Environment. The \"Cycle 0\" onboarding experience is a perfect example of AI-driven scaffolding. You provided a high-level description of your project. The DCE then used that description to generate a prompt that told an AI to create a full set of professional planning documents for you. It automatically created your Master Artifact List, your Project Vision, your Technical Plan, and more. It solved the blank page problem for you from the very beginning.\n\n#### **Page 4: Your First Step: Generating the Blueprint**\n*   **Page Title:** Your First Command: \"Build Me a Starter Project\"\n*   **Image Prompt:** A developer is shown confidently typing a clear prompt into their AI chat. The prompt instructs the AI to use their Project Scope to generate a complete starter project for a Next.js application, including all the initial folders and config files.\n*   **TL;DR:** Your next step is to take your Project Scope and use it to prompt y"
  },
  {
    "id": "report_source",
    "chunk": "arter project for a Next.js application, including all the initial folders and config files.\n*   **TL;DR:** Your next step is to take your Project Scope and use it to prompt your AI partner to build the starter files for your portfolio project.\n*   **Content:** It's time to put this into practice. Your `Project Scope` artifact is the key. Your task for the next cycle is to write a prompt that uses this artifact to get the AI to build your project's foundation. A great prompt would be: \"You are a senior developer setting up a new project. Based on my attached Project Scope, please generate the complete folder structure and all the initial configuration files for a Next.js and TypeScript application. Create placeholder files for the main components described in the MVP.\" This command gets the AI to do the boring setup work, giving you a clean, professional, and ready-to-code project structure from the start.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: The Challenge of Project Initiation**\n*   **Page Title:** World-Building 101: Conquering the Blank Canvas\n*   **Image Prompt:** A game developer is staring at a completely empty, white grid in a game engine, looking stumped. The screen is labeled \"Level 1: The Blank Canvas.\" It's an intimidating, empty world with no starting point.\n*   **TL;DR:** The \"Blank Page Problem\" is the ultimate first boss fight in any creative quest. It's that moment you have a great idea but a totally empty screen. This lesson teaches you the ultimate cheat code to beat it.\n*   **Content:** You've defined your epic quest in your Project Scope. Now what? You open your editor and... it's empty. A blank canvas. A fresh world with nothing in it. This is the first and scariest boss in a"
  },
  {
    "id": "report_source",
    "chunk": " epic quest in your Project Scope. Now what? You open your editor and... it's empty. A blank canvas. A fresh world with nothing in it. This is the first and scariest boss in any creative journey: the **Blank Page Problem**. Its that moment of paralysis when you have a huge vision but no idea where to lay the first brick or write the first line of code. But fear not, as a V2V hero, you have a legendary power to summon a world into existence.\n\n#### **Page 2: The V2V Solution: AI as a Scaffolding Engine**\n*   **Page Title:** The Genesis Spell: AI as a World-Building Engine\n*   **Image Prompt:** A hero holds up their \"Project Scope\" scroll. They cast a spell, and the scroll's text is consumed by a powerful AI familiar. The familiar then unleashes a massive wave of creation magic, instantly generating the entire \"world map\" (folder structure) and \"starting cities\" (foundational files) for the project.\n*   **TL;DR:** The V2V meta is to use your Project Scope as a magic scroll to cast a \"Genesis Spell.\" This commands your AI familiar to instantly generate the entire starting zone for your project.\n*   **Content:** The secret to beating the Blank Page boss is to use a \"Genesis Spell.\" Your AI partner is your world-building engine. You give it your `Project Scope` artifactyour sacred scrolland command it to construct the world for you. This is called **scaffolding**. The AI will create your world map (the folder structure), build the starting towns and dungeons (the initial boilerplate code and config files), and even write the first chapters of your lore book (the planning artifacts). This spell turns the most boring part of any projectthe setupinto an instant, epic act of creation.\n\n#### **Page 3: Case Study: The DCE's \"Cy"
  },
  {
    "id": "report_source",
    "chunk": " book (the planning artifacts). This spell turns the most boring part of any projectthe setupinto an instant, epic act of creation.\n\n#### **Page 3: Case Study: The DCE's \"Cycle 0\" Onboarding**\n*   **Page Title:** The Built-in Tutorial Level: DCE's \"Cycle 0\"\n*   **Image Prompt:** A close-up of the DCE extension's UI in \"Cycle 0.\" It shows a user typing their \"world idea.\" An arrow points to a `prompt.md` file, which then magically transforms into a full set of \"Lore Books\" (planning artifacts) in an `src/Artifacts` folder.\n*   **TL;DR:** The DCE tool has this Genesis Spell built-in. The \"Cycle 0\" onboarding is the tutorial level where you give it your main quest idea, and it automatically summons all the starter lore books for you.\n*   **Content:** You've already seen this spell in action. The Data Curation Environment itself uses this exact technique. The \"Cycle 0\" onboarding is the game's tutorial level for world-building. You gave it your high-level quest idea in the \"Project Scope\" text box. Then, when you clicked \"Generate Initial Artifacts Prompt,\" the DCE cast a Genesis Spell. It used your idea to prompt an AI to forge a full set of legendary planning artifactsyour Master Artifact List, your Project Vision, your Technical Scaffolding Plan, and more. It instantly built the lore foundation for your entire journey.\n\n#### **Page 4: Your First Step: Generating the Blueprint**\n*   **Page Title:** Your First Quest: \"Forge My World\"\n*   **Image Prompt:** A hero is shown confidently giving a command to their AI familiar. The prompt is clear: \"Use my Project Scope scroll to forge the world for my MVP. Create the map, the starting towns, and all the necessary artifacts.\"\n*   **TL;DR:** Your next quest is to use your Projec"
  },
  {
    "id": "report_source",
    "chunk": " Project Scope scroll to forge the world for my MVP. Create the map, the starting towns, and all the necessary artifacts.\"\n*   **TL;DR:** Your next quest is to use your Project Scope scroll to command your AI to build the starting zone for your own epic project.\n*   **Content:** Now it's your turn to cast the spell. Your `Project Scope` artifact is your scroll of power. Your next quest is to write an incantation that commands your AI partner to build the foundation for your game, app, or world. A powerful incantation would be: \"You are a master world-builder. Using the attached Project Scope, forge the complete world map (folder structure) and the starting zone (initial files and boilerplate code) for my Minimum Viable Product. Also, inscribe the detailed technical blueprints as a new artifact.\" This command delegates the initial grind, letting you jump straight into the adventure with a fully formed world ready to be explored and built upon.\n</file_artifact>\n\n<file path=\"src/Artifacts/A73 - V2V Academy - Lesson 4.3 - Architecting Your MVP.md\">\n# Artifact A73: V2V Academy - Lesson 4.3 - Architecting Your MVP\n# Date Created: C73\n# Author: AI Model & Curator\n\n- **Key/Value for A0:**\n- **Description:** The detailed content for Lesson 4.3 of the V2V Academy, \"Architecting Your MVP,\" designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.\n- **Tags:** v2v, curriculum, lesson plan, project scope, architecture, planning, interactive learning, persona\n\n## **Lesson 4.3: Architecting Your MVP**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: From Scope to Structure**\n*   **Page Title:** From Scope to Structure: Generating Your Architectural Blue"
  },
  {
    "id": "report_source",
    "chunk": "VP**\n\n---\n\n### **Version 1: The Career Transitioner**\n\n#### **Page 1: From Scope to Structure**\n*   **Page Title:** From Scope to Structure: Generating Your Architectural Blueprint\n*   **Image Prompt:** A professional is shown presenting their \"Project Scope\" document to a powerful AI, which is depicted as a master architect. The AI processes the document and generates a detailed, glowing holographic blueprint of a software application, showing the folder structure, key components, and data flows.\n*   **TL;DR:** An architectural blueprint translates your strategic \"why\" (the scope) into a technical \"how\" (the structure). This lesson teaches you to command an AI to act as your lead architect, generating the foundational structure for your MVP.\n*   **Content:** You've defined your project's vision and scope. The next step is to translate that strategic plan into a technical one. This is your **Architectural Blueprint**. It's the high-level design that outlines your project's file structure, technology stack, and core components. A clear blueprint is essential for building a maintainable and scalable application. It prevents the accumulation of **Technical Debt**the long-term cost of short-term shortcuts. In the V2V workflow, you don't have to create this blueprint from scratch; you will command your AI partner to create it for you.\n\n#### **Page 2: The AI as Your Architect**\n*   **Page Title:** The AI as Your Technical Architect\n*   **Image Prompt:** A side-by-side comparison. On the left, a developer is manually creating folders and empty files, a slow and tedious process. On the right, a developer gives a single command to an AI, which instantly generates a complete, perfectly organized project structure.\n*   **TL;DR:** "
  },
  {
    "id": "report_source",
    "chunk": "ow and tedious process. On the right, a developer gives a single command to an AI, which instantly generates a complete, perfectly organized project structure.\n*   **TL;DR:** Leverage your AI partner's vast knowledge of software design patterns and best practices. It can act as your senior architect, taking your project scope and generating an optimal, professional-grade project structure in seconds.\n*   **Content:** Your AI partner has been trained on millions of open-source projects. It has a deep, implicit understanding of software architecture, design patterns, and best practices for virtually any technology stack. By providing it with your clear Project Scope, you can leverage this expertise. The AI's role in this phase is to act as your lead technical architect. It will take your high-level requirements and translate them into a concrete file structure and the initial \"boilerplate\" code needed to get the project running. This saves you hours of setup time and ensures your project is built on a solid, professional foundation from day one.\n\n#### **Page 3: Writing the Architectural Prompt**\n*   **Page Title:** The Architect's Command: Writing the Scaffolding Prompt\n*   **Image Prompt:** A professional is shown typing a clear, structured prompt. The prompt instructs the AI to \"Act as a senior software architect\" and \"Generate the file and folder structure\" for a specific tech stack (e.g., Next.js, TypeScript, TailwindCSS) based on the provided project scope.\n*   **TL;DR:** The key to this step is a precise prompt that assigns the AI the role of an architect and clearly specifies the technology stack and the desired output (a file structure and initial code).\n*   **Content:** To get a high-quality architectural blueprin"
  },
  {
    "id": "report_source",
    "chunk": " architect and clearly specifies the technology stack and the desired output (a file structure and initial code).\n*   **Content:** To get a high-quality architectural blueprint from your AI, your prompt needs to be specific and role-oriented. This is a perfect application of the Structured Interaction principles you've learned. A powerful architectural prompt includes: 1. **The Role:** \"You are an expert software architect specializing in [Your Tech Stack].\" 2. **The Context:** \"Using the provided Project Scope artifact...\" 3. **The Task:** \"...generate the complete file and folder structure for the MVP.\" 4. **The Deliverables:** \"Create the initial boilerplate code for the main components, including configuration files, the main server file, and placeholder UI components.\" This command gives the AI a clear mandate and a well-defined set of deliverables.\n\n#### **Page 4: Kicking Off Cycle 1**\n*   **Page Title:** Cycle 1: From Blueprint to Live Application\n*   **Image Prompt:** The newly generated project structure is shown inside the DCE. The developer clicks the \"Accept Selected\" button, and the files instantly appear in their workspace. The final shot shows them running the application for the first time, with a \"Hello World\" screen visible.\n*   **TL;DR:** The AI's architectural output becomes the basis for your first development cycle. You accept the generated files, run the application, and begin the iterative process of building out your vision.\n*   **Content:** The AI's response to your architectural prompt will be a set of new files and folders. This is the starting point for your first true development cycle. Using the DCE's Parallel Co-Pilot Panel, you will review the proposed structure, select the response you l"
  },
  {
    "id": "report_source",
    "chunk": "s. This is the starting point for your first true development cycle. Using the DCE's Parallel Co-Pilot Panel, you will review the proposed structure, select the response you like best, and \"Accept\" the new files into your workspace. With that single click, your project is born. You can then install any dependencies and run the application for the first time. You have successfully overcome the Blank Page Problem and established a solid, scalable foundation upon which you will build your MVP, one cycle at a time.\n\n---\n\n### **Version 2: The Underequipped Graduate**\n\n#### **Page 1: From Scope to Structure**\n*   **Page Title:** Creating the Blueprint: How to Get an AI to Build Your Starter Code\n*   **Image Prompt:** A student presents their \"Project Scope\" document to a friendly AI robot. The robot processes the document and generates a perfect, professional-looking \"Architectural Blueprint\" for a software project, showing a clean folder structure and key components.\n*   **TL;DR:** A project scope tells you *what* to build. An architectural blueprint tells you *how* to build it. This lesson teaches you how to use your scope to get an AI to create a professional blueprint and starter code for your project.\n*   **Content:** You've created a professional Project Scope. Now, it's time to turn that plan into a real project. The next step is to create an **Architectural Blueprint**. This is the technical plan that maps out your project's folder structure, files, and core components. A good blueprint is what prevents your project from becoming a messy, unmanageable tangle of code, a problem known as **Technical Debt**. The best part is, you don't have to guess how to do this. Your AI partner can act as your senior developer, creatin"
  },
  {
    "id": "report_source",
    "chunk": "able tangle of code, a problem known as **Technical Debt**. The best part is, you don't have to guess how to do this. Your AI partner can act as your senior developer, creating a perfect project structure for you based on your scope.\n\n#### **Page 2: The AI as Your Architect**\n*   **Page Title:** The AI as Your Senior Dev Partner\n*   **Image Prompt:** A side-by-side comparison. On the left, a student is struggling, manually creating folders and empty files with generic names. On the right, a student gives a single command to an AI, which instantly generates a complete, perfectly organized project with folders like `/components`, `/lib`, and `/app`.\n*   **TL;DR:** Your AI partner knows the best practices for structuring a professional-grade application. Let it do the boring setup work for you, so you can focus on building the cool features.\n*   **Content:** Why should you let an AI build your initial project structure? Because it knows best practices you haven't learned yet. It has analyzed millions of professional projects and understands the optimal way to organize files for different technology stacks. By giving it your Project Scope, you're essentially asking a senior developer to set up the project for you. The AI will create a clean, logical directory structure and generate the \"boilerplate\" codeall the initial config files and startup scriptsthat every project needs. This saves you a massive amount of time and ensures your project is built on a solid foundation that will impress any hiring manager.\n\n#### **Page 3: Writing the Architectural Prompt**\n*   **Page Title:** The \"Build My Project\" Prompt\n*   **Image Prompt:** A developer is shown typing a clear, concise prompt. The prompt is: \"Act as a senior Next.js dev"
  },
  {
    "id": "report_source",
    "chunk": "ompt**\n*   **Page Title:** The \"Build My Project\" Prompt\n*   **Image Prompt:** A developer is shown typing a clear, concise prompt. The prompt is: \"Act as a senior Next.js developer. Use my Project Scope to scaffold the complete starter project using the App Router, TypeScript, and TailwindCSS.\"\n*   **TL;DR:** The prompt for this step is straightforward. You tell the AI to act as an expert in your chosen technology, give it your scope, and ask it to build the \"scaffolding\" for your project.\n*   **Content:** To get your AI to act as your architect, you need to give it a clear and specific command. This is a great time to practice the Structured Interaction skills you've learned. A powerful prompt for this task would look something like this: 1. **The Role:** \"You are an expert full-stack developer specializing in Next.js, TypeScript, and TailwindCSS.\" 2. **The Context:** \"Using the Project Scope I've provided...\" 3. **The Task:** \"...scaffold the complete initial project structure for the MVP.\" 4. **The Deliverables:** \"Generate all necessary configuration files (`package.json`, `tailwind.config.ts`, etc.) and create placeholder component files for the main features.\" This tells the AI exactly what you need to get started.\n\n#### **Page 4: Kicking Off Cycle 1**\n*   **Page Title:** Cycle 1: Your Project is Born\n*   **Image Prompt:** The new project structure is shown inside the DCE. The developer clicks \"Accept Selected,\" and the files appear in their VS Code explorer. The final shot shows them typing `npm run dev` in the terminal and seeing a \"Welcome to Next.js\" page in their browser.\n*   **TL;DR:** The AI's response will be a complete set of starter files. You'll accept them into your project, run the installation comman"
  },
  {
    "id": "report_source",
    "chunk": " to Next.js\" page in their browser.\n*   **TL;DR:** The AI's response will be a complete set of starter files. You'll accept them into your project, run the installation command, and officially begin your first development cycle on a real, working application.\n*   **Content:** The AI's response will be your complete starter project. Inside the DCE, you'll review the files it generated, select the best response, and click \"Accept Selected.\" Just like that, your empty folder will be populated with a professional, well-organized project structure. Your next step is to open the terminal, run `npm install` to get all the necessary packages, and then `npm run dev` to start your application for the first time. You have now successfully gone from a simple idea to a running application. This is the start of your first real development cycle and the next major piece for your portfolio.\n\n---\n\n### **Version 3: The Young Precocious**\n\n#### **Page 1: From Scope to Structure**\n*   **Page Title:** The Architect's Table: Forging Your World's Foundation\n*   **Image Prompt:** A hero lays their magical \"Project Scope\" scroll on a massive, ancient forge. As they do, the forge glows with power and begins to automatically construct the foundational \"blueprint\" of a massive, complex castle, showing its walls, towers, and internal layout.\n*   **TL;DR:** Your quest map (Project Scope) contains the secret runes to summon your project's foundation. This lesson teaches you how to use your map to command your AI familiar to forge the architectural blueprint for your epic creation.\n*   **Content:** You have your sacred mapthe Project Scope that defines your epic quest. Now it's time to lay the foundation of your fortress. The next step is to create th"
  },
  {
    "id": "report_source",
    "chunk": ".\n*   **Content:** You have your sacred mapthe Project Scope that defines your epic quest. Now it's time to lay the foundation of your fortress. The next step is to create the **Architectural Blueprint**. This is the master plan that shows where every wall, tower, and secret passage of your project will go. A good blueprint is what keeps your castle from collapsing into a pile of buggy code, a trap known as **Technical Debt**. But you don't have to draw this blueprint by hand. You will command your AI familiar to forge it for you, using the magic of your quest map.\n\n#### **Page 2: The AI as Your Architect**\n*   **Page Title:** The AI as Your Master Blacksmith\n*   **Image Prompt:** A side-by-side comparison. On the left, a novice adventurer is clumsily trying to build a shack out of mismatched logs. On the right, a hero commands a powerful AI golem, which is expertly and instantly constructing the strong, perfectly designed foundation of a massive castle.\n*   **TL;DR:** Your AI companion is a master blacksmith who knows all the secret techniques for building a legendary fortress. Let it handle the boring foundation work so you can focus on designing the epic throne room.\n*   **Content:** Why let your AI build the foundation? Because it's a master craftsman trained by the ancients (i.e., millions of GitHub repos). It knows all the secret techniques for building strong, scalable project structures. By giving it your Project Scope, you're basically telling a legendary blacksmith, \"Here's the plan for my castle; forge me the foundation.\" The AI will create the perfect directory structure and all the initial \"boilerplate\" codethe boring but essential magic runes and configuration scrollsthat every project needs. This lets y"
  },
  {
    "id": "report_source",
    "chunk": "ate the perfect directory structure and all the initial \"boilerplate\" codethe boring but essential magic runes and configuration scrollsthat every project needs. This lets you skip the grind and jump straight to the fun part: building out your world.\n\n#### **Page 3: Writing the Architectural Prompt**\n*   **Page Title:** The Incantation of Creation\n*   **Image Prompt:** A hero is shown reciting a powerful spell from a scroll. The incantation has clear, structured verses: \"ROLE: Master Architect,\" \"CONTEXT: The Sacred Scroll of Scope,\" \"TASK: Forge the Foundation,\" \"DELIVERABLES: The Skeleton of the World.\"\n*   **TL;DR:** To summon your project's foundation, you need to recite the Incantation of Creation. This spell tells the AI its role, gives it your map, and commands it to build the world's skeleton.\n*   **Content:** To get your AI to forge your world, you need to use a powerful, structured incantation. This is where you practice your spellcasting. A master-level incantation would be: 1. **The Role:** \"You are a grand architect of digital realms, a master of the [Your Tech Stack] arts.\" 2. **The Context:** \"Using the sacred Project Scope scroll I have provided...\" 3. **The Task:** \"...forge the foundational scaffolding for my new world.\" 4. **The Deliverables:** \"Summon the complete directory map, the essential configuration scrolls, and the placeholder souls (component files) for the main structures of the MVP.\" This incantation is a clear and powerful command that will bring your project into existence.\n\n#### **Page 4: Kicking Off Cycle 1**\n*   **Page Title:** Cycle 1: The World is Born\n*   **Image Prompt:** The newly generated project structure is shown inside the DCE. The hero clicks \"Accept,\" and the files materi"
  },
  {
    "id": "report_source",
    "chunk": " **Page Title:** Cycle 1: The World is Born\n*   **Image Prompt:** The newly generated project structure is shown inside the DCE. The hero clicks \"Accept,\" and the files materialize in their world. The final shot shows them taking their first steps into the newly created world, which is now live and running.\n*   **TL;DR:** The AI's creation spell will fill your world with its foundational structures. You'll accept this creation, breathe life into it with a command, and begin your first true adventure in a world you designed.\n*   **Content:** The AI's response to your incantation will be the skeleton of your new world. In the DCE, you'll see all the new files and folders it has forged. You'll choose the best creation and click \"Accept.\" With that one click, your world is born. Your next step is to perform the Ritual of Awakening: open the terminal, type `npm install` and `npm run dev`. This will breathe life into your creation. You have officially gone from a blank canvas to a living, running world. This is the start of Cycle 1. Your adventure has begun.\n</file_artifact>\n\n"
  }
]