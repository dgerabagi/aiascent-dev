<!--
  File: flattened_repo.md
  Source Directory: c:\Projects\aiascent-dev
  Date Generated: 2025-11-29T21:35:46.103Z
  ---
  Total Files: 150
  Approx. Tokens: 163444
-->

<!-- Top 10 Text Files by Token Count -->
1. src\Artifacts\A0-Master-Artifact-List.md (9590 tokens)
2. src\stores\reportStore.ts (8926 tokens)
3. src\app\api\chat\route.ts (5223 tokens)
4. src\Artifacts\A26. aiascent.dev - Homepage Whitepaper Visualization Plan.md (4343 tokens)
5. src\app\learn\page.tsx (3929 tokens)
6. public\data\whitepaper_content.json (3607 tokens)
7. src\app\mission\page.tsx (3597 tokens)
8. src\components\report-viewer\ReportChatPanel.tsx (3560 tokens)
9. public\data\anguilla_content.json (3072 tokens)
10. public\data\whitepaper_imagemanifest.json (2739 tokens)

<!-- Full File List -->
1. src\Artifacts\A200 - Anguilla Project - Universal Task Checklist.md - Lines: 33 - Chars: 1905 - Tokens: 477
2. src\Artifacts\A201 - Anguilla Project - Vision and Master Plan.md - Lines: 49 - Chars: 5201 - Tokens: 1301
3. src\Artifacts\A202 - Research Proposal - The AI Capital.md - Lines: 31 - Chars: 3339 - Tokens: 835
4. src\Artifacts\A203 - Research Proposal - The Cognitive Citizenry.md - Lines: 35 - Chars: 2927 - Tokens: 732
5. src\Artifacts\A204 - Research Proposal - The Automated State.md - Lines: 32 - Chars: 2839 - Tokens: 710
6. src\Artifacts\A205 - Research Proposal - Resilient Island Systems.md - Lines: 31 - Chars: 2836 - Tokens: 709
7. src\Artifacts\A206 - Research Proposal - The Global AI Sandbox.md - Lines: 30 - Chars: 2816 - Tokens: 704
8. src\Artifacts\A207 - Strategic Presentation Guide.md - Lines: 54 - Chars: 3599 - Tokens: 900
9. src\Artifacts\A214 - Anguilla Project - GitHub Repository Setup Guide.md - Lines: 68 - Chars: 2164 - Tokens: 541
10. src\Artifacts\A215 - Anguilla Project - Migration Manifest.md - Lines: 52 - Chars: 3419 - Tokens: 855
11. src\Artifacts\A62 - V2V Academy - Synthesis of Research Proposals.md - Lines: 33 - Chars: 4303 - Tokens: 1076
12. src\Artifacts\A115 - GlobalLogic AI Micro-Pilot Proposal.md - Lines: 55 - Chars: 6559 - Tokens: 1640
13. src\Artifacts\A215 - Anguilla Project - Context Transfer List.md - Lines: 53 - Chars: 3413 - Tokens: 854
14. src\Artifacts\A23. aiascent.dev - Cognitive Capital Definition.md - Lines: 31 - Chars: 2608 - Tokens: 652
15. anguilla_context_migration_list.txt - Lines: 31 - Chars: 1941 - Tokens: 486
16. public\data\anguilla_content.json - Lines: 112 - Chars: 12285 - Tokens: 3072
17. public\data\whitepaper_content.json - Lines: 175 - Chars: 14425 - Tokens: 3607
18. public\data\whitepaper_imagemanifest.json - Lines: 63 - Chars: 10953 - Tokens: 2739
19. scripts\generate_images.mjs - Lines: 186 - Chars: 6942 - Tokens: 1736
20. scripts\image_harness.mjs - Lines: 115 - Chars: 8773 - Tokens: 2194
21. scripts\manage_v2v_images.mjs - Lines: 146 - Chars: 6168 - Tokens: 1542
22. src\app\academy\page.tsx - Lines: 188 - Chars: 9287 - Tokens: 2322
23. src\app\api\chat\route.ts - Lines: 373 - Chars: 20892 - Tokens: 5223
24. src\app\api\tts\route.ts - Lines: 50 - Chars: 1775 - Tokens: 444
25. src\app\dce\page.tsx - Lines: 81 - Chars: 6906 - Tokens: 1727
26. src\app\learn\page.tsx - Lines: 171 - Chars: 15716 - Tokens: 3929
27. src\app\mission\page.tsx - Lines: 143 - Chars: 14388 - Tokens: 3597
28. src\app\showcase\page.tsx - Lines: 5 - Chars: 119 - Tokens: 30
29. src\app\globals.css - Lines: 76 - Chars: 1658 - Tokens: 415
30. src\app\layout.tsx - Lines: 45 - Chars: 1430 - Tokens: 358
31. src\app\page.tsx - Lines: 30 - Chars: 1105 - Tokens: 277
32. src\Artifacts\A0-Master-Artifact-List.md - Lines: 513 - Chars: 38357 - Tokens: 9590
33. src\Artifacts\A1-Project-Vision-and-Goals.md - Lines: 44 - Chars: 2843 - Tokens: 711
34. src\Artifacts\A2-Phase1-Requirements.md - Lines: 39 - Chars: 3316 - Tokens: 829
35. src\Artifacts\A3-Technical-Scaffolding-Plan.md - Lines: 77 - Chars: 2913 - Tokens: 729
36. src\Artifacts\A4-Universal-Task-Checklist.md - Lines: 67 - Chars: 3383 - Tokens: 846
37. src\Artifacts\A5-Dual Domain Hosting Guide.md - Lines: 119 - Chars: 5899 - Tokens: 1475
38. src\Artifacts\A6-Porting Guide for aiascent.dev.md - Lines: 41 - Chars: 2972 - Tokens: 743
39. src\Artifacts\A7-Development-and-Testing-Guide.md - Lines: 65 - Chars: 2225 - Tokens: 557
40. src\Artifacts\A9-GitHub-Repository-Setup-Guide.md - Lines: 68 - Chars: 2461 - Tokens: 616
41. src\Artifacts\A11-Implementation-Roadmap.md - Lines: 62 - Chars: 3386 - Tokens: 847
42. src\Artifacts\A14-GitHub-Repository-Setup-Guide.md - Lines: 91 - Chars: 3983 - Tokens: 996
43. src\Artifacts\A15-Asset-Wishlist.md - Lines: 60 - Chars: 3354 - Tokens: 839
44. src\Artifacts\A15.1-Master-Image-System-Prompt.md - Lines: 48 - Chars: 2873 - Tokens: 719
45. src\Artifacts\A15.2-Image-Prompt-Logo.md - Lines: 39 - Chars: 1329 - Tokens: 333
46. src\Artifacts\A15.3-Image-Prompt-Favicon.md - Lines: 33 - Chars: 1133 - Tokens: 284
47. src\Artifacts\A15.7-Image-Prompt-OGImage.md - Lines: 40 - Chars: 1836 - Tokens: 459
48. src\Artifacts\A16-Page-Design-Home.md - Lines: 68 - Chars: 5178 - Tokens: 1295
49. src\Artifacts\A17-Page-Design-Showcase.md - Lines: 66 - Chars: 3765 - Tokens: 942
50. src\Artifacts\A18-Page-Design-Learn.md - Lines: 63 - Chars: 2726 - Tokens: 682
51. src\Artifacts\A19-Page-Design-Mission.md - Lines: 70 - Chars: 4100 - Tokens: 1025
52. src\Artifacts\A20. aiascent.dev - Report Viewer Integration Plan.md - Lines: 56 - Chars: 4180 - Tokens: 1045
53. src\Artifacts\A21. aiascent.dev - Ask Ascentia RAG Integration.md - Lines: 61 - Chars: 3509 - Tokens: 878
54. src\Artifacts\A22. aiascent.dev - Mission Page Revamp Plan.md - Lines: 90 - Chars: 5737 - Tokens: 1435
55. src\Artifacts\A24. aiascent.dev - Mission Page Content Expansion Plan.md - Lines: 53 - Chars: 5259 - Tokens: 1315
56. src\Artifacts\A25. aiascent.dev - Learn Page Content Plan.md - Lines: 72 - Chars: 5962 - Tokens: 1491
57. src\Artifacts\A26. aiascent.dev - Homepage Whitepaper Visualization Plan.md - Lines: 175 - Chars: 17371 - Tokens: 4343
58. src\Artifacts\A27. aiascent.dev - AI Persona - @Ascentia.md - Lines: 52 - Chars: 3809 - Tokens: 953
59. src\Artifacts\A28. aiascent.dev - Dual Embedding RAG Architecture.md - Lines: 87 - Chars: 4633 - Tokens: 1159
60. src\Artifacts\A29. aiascent.dev - GitHub Public Repository Guide.md - Lines: 63 - Chars: 5367 - Tokens: 1342
61. src\Artifacts\A30. aiascent.dev - Showcase Expansion Plan.md - Lines: 56 - Chars: 4056 - Tokens: 1014
62. src\Artifacts\A31. aiascent.dev - iframe Integration Guide.md - Lines: 83 - Chars: 4164 - Tokens: 1041
63. src\Artifacts\A32. aiascent.dev - Dynamic Chat Prompt Suggestions Plan.md - Lines: 70 - Chars: 5470 - Tokens: 1368
64. src\Artifacts\A33. aiascent.dev - Report Viewer Fullscreen Plan.md - Lines: 48 - Chars: 3100 - Tokens: 775
65. src\Artifacts\A34. aiascent.dev - Whitepaper Introduction Content.md - Lines: 28 - Chars: 1968 - Tokens: 492
66. src\Artifacts\A35. aiascent.dev - Discord Community Management Plan.md - Lines: 50 - Chars: 3738 - Tokens: 935
67. src\Artifacts\A40. aiascent.dev - Page Design DCE.md - Lines: 65 - Chars: 5590 - Tokens: 1398
68. src\Artifacts\A41. aiascent.dev - Page Design DCE - Artifacts as Source of Truth.md - Lines: 30 - Chars: 2424 - Tokens: 606
69. src\Artifacts\A61.1 - Transcript 1 Summary.md - Lines: 34 - Chars: 3712 - Tokens: 928
70. src\Artifacts\A61.2 - Transcript 2 Summary.md - Lines: 31 - Chars: 3737 - Tokens: 935
71. src\Artifacts\A61.3 - Transcript 3 Summary.md - Lines: 34 - Chars: 4096 - Tokens: 1024
72. src\Artifacts\A61.4 - Transcript 4 Summary.md - Lines: 40 - Chars: 3955 - Tokens: 989
73. src\Artifacts\A61.6 - Transcript 6 Summary.md - Lines: 34 - Chars: 3876 - Tokens: 969
74. src\Artifacts\A61.7 - Transcript 7 Summary.md - Lines: 38 - Chars: 4014 - Tokens: 1004
75. src\Artifacts\A61.9 - Transcript 9 Summary.md - Lines: 34 - Chars: 3870 - Tokens: 968
76. src\Artifacts\A61.11 - Transcript 11 Summary.md - Lines: 35 - Chars: 4708 - Tokens: 1177
77. src\Artifacts\A61.12 - Transcript 12 Summary (Cycle 58 Context).md - Lines: 28 - Chars: 3180 - Tokens: 795
78. src\Artifacts\A102 - Homepage Hero Section Revamp Plan.md - Lines: 41 - Chars: 3695 - Tokens: 924
79. src\Artifacts\A102. aiascent.dev - Homepage Hero Revamp Plan.md - Lines: 63 - Chars: 5503 - Tokens: 1376
80. src\Artifacts\A103 - How It Works Section Image Prompts.md - Lines: 31 - Chars: 3015 - Tokens: 754
81. src\Artifacts\A103 - V2V Academy - Ascentia Persona Tones.md - Lines: 40 - Chars: 4182 - Tokens: 1046
82. src\Artifacts\A104 - V2V Academy - Account System Design.md - Lines: 106 - Chars: 4972 - Tokens: 1243
83. src\Artifacts\A105 - aiascent.dev - Google OAuth Setup Guide.md - Lines: 62 - Chars: 2923 - Tokens: 731
84. src\Artifacts\A106 - Re-branding Initiative - Phase 1 Plan.md - Lines: 49 - Chars: 3866 - Tokens: 967
85. src\Artifacts\A107 - Master Image System Prompt v2.md - Lines: 59 - Chars: 4758 - Tokens: 1190
86. src\Artifacts\A108 - Persona Likeness Generation Prompts.md - Lines: 42 - Chars: 6683 - Tokens: 1671
87. src\Artifacts\A109 - Whitepaper Image Re-branding Prompts.md - Lines: 77 - Chars: 6779 - Tokens: 1695
88. src\Artifacts\A110 - V2V Academy - Citizen Architect Classes.md - Lines: 81 - Chars: 3707 - Tokens: 927
89. src\Artifacts\A111 - GWU Appeal Letter - Sarkani.md - Lines: 32 - Chars: 3897 - Tokens: 975
90. src\Artifacts\A112 - GWU Appeal Letter - Mazzuchi.md - Lines: 32 - Chars: 3673 - Tokens: 919
91. src\Artifacts\A113 - GWU Appeal Letter - Blackford.md - Lines: 30 - Chars: 3017 - Tokens: 755
92. src\Artifacts\A114 - GWU Appeal Letter - Etemadi.md - Lines: 28 - Chars: 2894 - Tokens: 724
93. src\Artifacts\DCE_README.md - Lines: 47 - Chars: 3127 - Tokens: 782
94. src\components\academy\PersonaSelector.tsx - Lines: 69 - Chars: 3118 - Tokens: 780
95. src\components\global\3d-card.tsx - Lines: 162 - Chars: 4355 - Tokens: 1089
96. src\components\global\ConditionalSplash.tsx - Lines: 16 - Chars: 422 - Tokens: 106
97. src\components\global\container-scroll-animation.tsx - Lines: 114 - Chars: 3110 - Tokens: 778
98. src\components\global\FullscreenMediaViewer.tsx - Lines: 86 - Chars: 4393 - Tokens: 1099
99. src\components\global\GlobalAudioPlayer.tsx - Lines: 86 - Chars: 2749 - Tokens: 688
100. src\components\global\infinite-moving-cards.tsx - Lines: 122 - Chars: 3242 - Tokens: 811
101. src\components\global\lamp.tsx - Lines: 102 - Chars: 4076 - Tokens: 1019
102. src\components\global\mode-toggle.tsx - Lines: 43 - Chars: 1333 - Tokens: 334
103. src\components\global\NextPageSection.tsx - Lines: 48 - Chars: 1766 - Tokens: 442
104. src\components\global\sparkles.tsx - Lines: 312 - Chars: 8799 - Tokens: 2200
105. src\components\home\FeaturesSection.tsx - Lines: 65 - Chars: 5541 - Tokens: 1386
106. src\components\home\HeroSection.tsx - Lines: 53 - Chars: 2773 - Tokens: 694
107. src\components\home\HowItWorksSection.tsx - Lines: 50 - Chars: 2151 - Tokens: 538
108. src\components\home\MissionSection.tsx - Lines: 41 - Chars: 1310 - Tokens: 328
109. src\components\home\WorkflowSection.tsx - Lines: 42 - Chars: 1454 - Tokens: 364
110. src\components\layout\Footer.tsx - Lines: 44 - Chars: 1551 - Tokens: 388
111. src\components\layout\Header.tsx - Lines: 92 - Chars: 4069 - Tokens: 1018
112. src\components\mission\MissionSectionBlock.tsx - Lines: 146 - Chars: 5119 - Tokens: 1280
113. src\components\report-viewer\AudioControls.tsx - Lines: 231 - Chars: 9420 - Tokens: 2355
114. src\components\report-viewer\ImageNavigator.tsx - Lines: 98 - Chars: 4135 - Tokens: 1034
115. src\components\report-viewer\PageNavigator.tsx - Lines: 25 - Chars: 805 - Tokens: 202
116. src\components\report-viewer\PromptNavigator.tsx - Lines: 29 - Chars: 845 - Tokens: 212
117. src\components\report-viewer\ReportChatPanel.tsx - Lines: 303 - Chars: 14238 - Tokens: 3560
118. src\components\report-viewer\ReportProgressBar.tsx - Lines: 49 - Chars: 1843 - Tokens: 461
119. src\components\report-viewer\ReportTreeNav.tsx - Lines: 94 - Chars: 4618 - Tokens: 1155
120. src\components\report-viewer\ReportViewer.tsx - Lines: 213 - Chars: 9102 - Tokens: 2276
121. src\components\report-viewer\ReportViewerModal.tsx - Lines: 15 - Chars: 447 - Tokens: 112
122. src\components\shared\MarkdownRenderer.tsx - Lines: 81 - Chars: 3703 - Tokens: 926
123. src\components\showcase\InteractiveWhitepaper.tsx - Lines: 99 - Chars: 2804 - Tokens: 701
124. src\components\showcase\ShowcaseTabs.tsx - Lines: 89 - Chars: 3295 - Tokens: 824
125. src\components\ui\badge.tsx - Lines: 36 - Chars: 1127 - Tokens: 282
126. src\components\ui\button.tsx - Lines: 56 - Chars: 1834 - Tokens: 459
127. src\components\ui\card.tsx - Lines: 80 - Chars: 1858 - Tokens: 465
128. src\components\ui\dropdown-menu.tsx - Lines: 200 - Chars: 7308 - Tokens: 1827
129. src\data\whitepaperContent.json - Lines: 36 - Chars: 1537 - Tokens: 385
130. src\lib\kb-helper.ts - Lines: 18 - Chars: 476 - Tokens: 119
131. src\lib\utils.ts - Lines: 6 - Chars: 163 - Tokens: 41
132. src\providers\theme-provider.tsx - Lines: 9 - Chars: 326 - Tokens: 82
133. src\stores\reportStore.ts - Lines: 766 - Chars: 35702 - Tokens: 8926
134. public\assets\images\anguilla-presentation\ask\ask-pilot.webp - [Binary] Size: 111 KB
135. public\assets\images\anguilla-presentation\capital\capital-cloud.webp - [Binary] Size: 142 KB
136. public\assets\images\anguilla-presentation\capital\capital-fund.webp - [Binary] Size: 216.5 KB
137. public\assets\images\anguilla-presentation\citizenry\citizenry-heritage.webp - [Binary] Size: 184.2 KB
138. public\assets\images\anguilla-presentation\citizenry\citizenry-v2v.webp - [Binary] Size: 212.3 KB
139. public\assets\images\anguilla-presentation\resilience\resilience-twin.webp - [Binary] Size: 140.1 KB
140. public\assets\images\anguilla-presentation\state\state-embassy.webp - [Binary] Size: 169.9 KB
141. public\assets\images\anguilla-presentation\state\state-frictionless.webp - [Binary] Size: 51.1 KB
142. public\assets\images\anguilla-presentation\vision\vision-crisis.webp - [Binary] Size: 287.3 KB
143. public\assets\images\anguilla-presentation\vision\vision-intro.webp - [Binary] Size: 200.8 KB
144. scripts\scaffold_anguilla_images.mjs - Lines: 44 - Chars: 1838 - Tokens: 460
145. src\Artifacts\A208 - Anguilla Project - Image System Prompt.md - Lines: 57 - Chars: 4290 - Tokens: 1073
146. public\data\anguilla_imagemanifest.json - Lines: 86 - Chars: 5271 - Tokens: 1318
147. src\components\showcase\ShowcaseGame.tsx - Lines: 49 - Chars: 1851 - Tokens: 463
148. src\app\showcase\[slug]\page.tsx - Lines: 49 - Chars: 1750 - Tokens: 438
149. src\components\showcase\ProjectSelector.tsx - Lines: 54 - Chars: 1838 - Tokens: 460
150. src\app\anguilla\page.tsx - Lines: 30 - Chars: 907 - Tokens: 227

<file path="src/Artifacts/A200 - Anguilla Project - Universal Task Checklist.md">
# Artifact A200: Anguilla Project - Universal Task Checklist
# Date Created: C2
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A task checklist for the Anguilla Project, tracking the preparation for the Minister meeting and the initial steps of the Micro-Pilot.
- **Tags:** checklist, task management, anguilla, planning

## 1. Purpose

This checklist tracks the critical tasks required to launch the Anguilla Project, starting with the strategic presentation and moving into the execution of the Micro-Pilot.

## 2. Task List

### Phase 1: Preparation for Minister Meeting

- [ ] **Refine Research Proposals:** Ensure A201-A206 are fully updated with "Sovereignty, Culture, Resilience" themes. (Status: **Complete**)
- [ ] **Develop Presentation Deck:** Create visual slides based on the narrative in `A207`.
- [ ] **Prepare Demo:** Set up a local instance of the DCE to demonstrate the "Artifact Creation" workflow live.
- [ ] **Printed Materials:** Prepare high-quality printed copies of the "Vision and Master Plan" (A201) as a leave-behind.

### Phase 2: Micro-Pilot Setup (Post-Meeting)

- [ ] **Identify Cohort:** Work with the Ministry to select the initial group (e.g., a high school class or a government department).
- [ ] **Curriculum Adaptation:** Customize the V2V Academy content (Module 1) for the specific cohort (e.g., "AI for Civics").
- [ ] **Infrastructure Check:** Verify internet connectivity and hardware availability for the pilot group.
- [ ] **Baseline Assessment:** Create a simple survey to measure the cohort's current AI literacy and sentiment before the pilot begins.

### Phase 3: Execution & scaling

- [ ] **Launch Pilot:** Begin the 90-day program.
- [ ] **Weekly Review:** Track progress and adjust curriculum based on feedback.
- [ ] **Final Report:** Generate a "Success Report" using the DCE to present to the Ministry for Phase 2 scaling.
</file_artifact>

<file path="src/Artifacts/A201 - Anguilla Project - Vision and Master Plan.md">
# Artifact A201: Anguilla Project - Vision and Master Plan
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C2 (Integrate Guiding Principles: Sovereignty, Culture, Resilience)

- **Key/Value for A0:**
- **Description:** The high-level strategic vision for transforming Anguilla into the world's first AI-native nation, leveraging its unique digital asset (.ai domain) and small population size.
- **Tags:** anguilla, strategy, vision, nation building, ai, sovereignty, resilience

## 1. The Vision: Anguilla as the World's First AI-Native Nation

Anguilla stands at a unique crossroads in history. Through a stroke of digital luck, it owns the most valuable real estate on the future internet: the **.ai** top-level domain. This asset is currently generating significant revenue, but its potential goes far beyond domain registration fees. It is a symbol of the future.

The vision is to transform Anguilla from the *symbolic* home of AI into the *literal* home of the AI economy. We propose a comprehensive national strategy to become the world's first **AI-Native Nation**: a society where every citizen is empowered by artificial intelligence, where government services are frictionless and automated, and where the economy is driven by high-value cognitive labor.

With a population of approximately 16,000, Anguilla is the perfect size for a "Micro-Pilot"—a living laboratory for the post-scarcity, high-cognitive-capital society that the rest of the world is only dreaming of.

## 2. Guiding Principles

To ensure this transformation aligns with the nation's values and survival needs, the strategy is anchored in three non-negotiable principles:

1.  **Political Sovereignty & Data Independence:** We will not become a digital colony. Anguilla must own the infrastructure (servers, models, data) that powers its future. We will use AI to strengthen our independence, not increase our reliance on foreign powers.
2.  **Cultural Preservation & Amplification:** Technology must serve the culture, not erase it. We will use AI to preserve Anguillian history, dialect, and stories, ensuring that the "AI-Native" identity is distinctly Anguillian.
3.  **Climate Resilience & Survival:** We live on the front lines of climate change. Every technological implementation must directly contribute to the island's physical resilience against hurricanes, rising sea levels, and resource scarcity.

## 3. The Strategic Pillars

To achieve this vision, we propose a strategy built on five interconnected pillars:

1.  **Economic Sovereignty (The ".ai" Capital):** Leveraging the .ai domain windfall not just as revenue, but as a "Digital Wealth Fund" to build **hurricane-resilient, green sovereign digital infrastructure** (local data centers, sovereign cloud) that ensures Anguilla is a landlord, not a tenant, in the AI economy.
2.  **Cognitive Capital (The Citizen Architect):** Implementing a national upskilling program based on the "Vibecoding to Virtuosity" (V2V) methodology. The goal is to turn the population into the world's highest-density concentration of AI-literate professionals, while integrating **Cultural Heritage AI** modules to preserve local knowledge.
3.  **Next-Gen Governance (The Automated State):** Reimagining the civil service with AI. Creating a "frictionless state" where citizenship, land registry, taxes, and business incorporation are handled by secure, automated agents, with a specific focus on **Continuity of Government** during climate disasters.
4.  **Resilient Infrastructure (Smart Island):** Using AI to solve the physical challenges of island life. Optimizing water desalination, energy grids, and supply chains with predictive modeling to ensure sustainability and climate resilience.
5.  **Regulatory Innovation (The Global Sandbox):** Establishing Anguilla as a "Regulatory Sandbox" for ethical AI. Creating a legal framework that attracts global AI companies to test and deploy their systems safely, ensuring all testing respects local cultural and ethical norms.

## 4. The "Micro-Pilot" Concept

Why Anguilla? Because it is agile. Large nations are burdened by legacy systems, massive bureaucracies, and political gridlock. They cannot pivot quickly. Anguilla, with its small population and unified governance, can move at the speed of software.

We propose positioning Anguilla to the world not just as a tourist destination, but as a **Model Nation**—a proof-of-concept for how a society can thrive in the age of AI. This narrative will attract not just tourists, but innovators, investors, and the world's attention.

## 5. The Role of the Data Curation Environment (DCE)

This transformation requires a toolset. We propose using the **Data Curation Environment (DCE)** as the operating system for this national project.
*   **Planning:** Using the DCE's artifact-driven workflow to draft legislation, plan infrastructure, and design curricula.
*   **Execution:** Using the Parallel Co-Pilot Panel to manage the implementation of digital services.
*   **Education:** Using the V2V Academy platform to deliver the national upskilling program.

This project is not just about installing technology; it is about building a new kind of society.
</file_artifact>

<file path="src/Artifacts/A202 - Research Proposal - The AI Capital.md">
# Artifact A202: Research Proposal - The AI Capital
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C2 (Focus on Green, Hurricane-Resilient Infrastructure)

- **Key/Value for A0:**
- **Description:** A proposal focused on leveraging the .ai domain windfall to fund sovereign digital infrastructure, specifically emphasizing green, hurricane-resilient data centers.
- **Tags:** anguilla, economics, .ai domain, infrastructure, sovereign wealth, green energy, resilience

## 1. Title: The Digital Wealth Fund: From Domain Rent to Sovereign, Resilient Infrastructure

## 2. Problem Statement
Anguilla currently benefits from a significant windfall due to the sales of **.ai** domains. However, this revenue stream is essentially "rent"—it depends on the continued hype of AI and the policies of external registrars. Currently, Anguilla is a passive beneficiary of the AI boom. It does not own the *means of production* (compute, data centers, models) for the AI economy. Furthermore, standard digital infrastructure is vulnerable to the region's extreme weather events (hurricanes), posing a risk to any digital economy built upon it.

## 3. Research Objectives
1.  **Analyze Domain Revenue Sustainability:** Project the long-term viability of .ai domain revenue and identify risks (e.g., new TLDs, market saturation).
2.  **Feasibility of Resilient Sovereign Compute:** Investigate the cost and engineering requirements for building a "Sovereign AI Cloud" that is **hurricane-proof** (e.g., reinforced concrete bunkers, underground facilities) and **energy-independent** (solar/wind/battery).
3.  **Digital Wealth Fund Structure:** Research models for a Sovereign Wealth Fund specifically designed to reinvest digital rents into physical and digital infrastructure (e.g., Norway's oil fund model applied to digital assets).

## 4. Proposed Solution: The Anguilla Digital Infrastructure Initiative
We propose creating a **Digital Wealth Fund** funded by a percentage of .ai domain sales. This fund will invest exclusively in:
*   **Resilient Data Centers:** Building small-footprint, Category 5 hurricane-resilient data centers. These facilities will host local government and business data, ensuring data sovereignty even during disasters.
*   **Green Energy Integration:** Powering this infrastructure with renewable energy sources (solar farms, offshore wind) to ensure the AI economy does not burden the island's fossil fuel consumption or contribute to the climate crisis.
*   **Subsea Connectivity:** Investing in fiber optic redundancy to ensure the island is never cut off.
*   **Sovereign Models:** Fine-tuning open-source models (like Llama 3 or Mistral) specifically on Anguillan law, history, and culture, creating a "National AI" that is owned by the people, not a foreign corporation.

## 5. Impact
*   **Economic Resilience:** Diversifies the economy beyond tourism and domain rent.
*   **Data Sovereignty:** Ensures that sensitive government and citizen data stays on the island, protected by local law.
*   **Climate Adaptation:** Creates a robust digital backbone that can survive extreme weather events, ensuring communication and governance continue when they are needed most.
*   **Global Prestige:** Positions Anguilla as a serious player in the digital infrastructure space, leading the way in "Green AI."
</file_artifact>

<file path="src/Artifacts/A203 - Research Proposal - The Cognitive Citizenry.md">
# Artifact A203: Research Proposal - The Cognitive Citizenry
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C2 (Add Cultural Heritage AI component)

- **Key/Value for A0:**
- **Description:** A proposal for a national upskilling initiative using the V2V/DCE methodology, featuring a "Cultural Heritage AI" component to preserve local history and dialect.
- **Tags:** anguilla, education, upskilling, v2v, cognitive capital, workforce, culture, heritage

## 1. Title: The Cognitive Citizenry: A National Upskilling and Cultural Preservation Strategy

## 2. Problem Statement
As AI automation advances, traditional service jobs are at risk. While upskilling is necessary, there is a risk that importing global AI curricula could erode local culture, replacing unique Anguillan identity with homogenized "silicon valley" values. Anguilla needs a strategy that modernizes the workforce *without* sacrificing its heritage.

## 3. Research Objectives
1.  **Skills Gap Analysis:** Assess the current digital literacy levels of the Anguillan workforce across key sectors.
2.  **Cultural Archive Feasibility:** Determine the state of Anguilla's oral histories, historical documents, and cultural artifacts. How much is digitized? How much is at risk of being lost?
3.  **Curriculum Adaptation:** Determine how to adapt the "Vibecoding to Virtuosity" (V2V) curriculum to be culturally relevant, using local metaphors and examples.

## 4. Proposed Solution: The National V2V Initiative
We propose a national program to provide every Anguillan citizen with:
*   **A Personal AI Companion:** A free, government-issued account on a national AI platform.
*   **The "Citizen Architect" Curriculum:** A modified version of the V2V Academy curriculum:
    *   *Module 1: AI for Small Business:* Tailored for local tourism and service businesses.
    *   *Module 2: AI for Education:* Providing AI tutors for every student.
    *   *Module 3: AI for Civics:* Using AI to engage with the government.
*   **The "Cultural Heritage AI" Project:** A national initiative to train a specific AI model on Anguillian history, dialect, folklore, and law.
    *   **Oral History Drives:** Citizens (especially elders) are interviewed, and their stories are transcribed to train the model.
    *   **Dialect Preservation:** The model is fine-tuned to understand and speak the local dialect, ensuring technology speaks the language of the people, not just standard English.

## 5. Impact
*   **Workforce Transformation:** Creates a globally competitive, AI-literate workforce.
*   **Cultural Renaissance:** Uses cutting-edge technology to preserve and celebrate Anguillan heritage, preventing cultural erasure.
*   **Intergenerational Connection:** Engages youth (technology) and elders (history) in a shared national project.
*   **Global Leadership:** Anguilla becomes the first nation with 100% AI literacy and a sovereign cultural AI model.
</file_artifact>

<file path="src/Artifacts/A204 - Research Proposal - The Automated State.md">
# Artifact A204: Research Proposal - The Automated State
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C2 (Focus on Continuity of Government and Disaster Resilience)

- **Key/Value for A0:**
- **Description:** A proposal for modernizing Anguilla's governance through AI, creating a frictionless, automated civil service with a focus on "Continuity of Government" during climate disasters.
- **Tags:** anguilla, governance, automation, public services, efficiency, disaster recovery

## 1. Title: The Automated State: Frictionless Governance and Disaster Resilience

## 2. Problem Statement
Bureaucracy is a tax on time and growth. But in an island nation facing climate change, bureaucracy can also be a single point of failure. Paper records and manual processes are vulnerable to destruction by hurricanes. If the physical government offices are damaged, the state ceases to function. Anguilla needs a government that is not only efficient but indestructible.

## 3. Research Objectives
1.  **Bureaucratic Audit:** Map the top 10 most frequent citizen-government interactions and measure their "time-to-completion."
2.  **Data Digitization & Redundancy:** Assess the current state of government records. Determine the effort required to digitize them into a secure, cloud-native format that can be replicated across resilient data centers (see A202).
3.  **Disaster Protocol Analysis:** Review current Continuity of Government (COG) plans. How does the government function if physical access is impossible?

## 4. Proposed Solution: The Anguilla Civil Service AI (ACSA)
We propose building **ACSA**, a suite of AI agents designed to handle routine government tasks and ensure continuity during crises.
*   **The "Cloud State":** Moving all core registries (land, citizenship, business) to a secure, distributed ledger or database that is immune to physical destruction.
*   **The "Citizen Concierge":** A single app where citizens can access all services. Crucially, this app includes a **"Disaster Mode"** that provides offline-first access to critical information, emergency alerts, and aid distribution coordination during a storm.
*   **Automated Bureaucracy:**
    *   *Incorporation Agent:* Fast-track business setup for global investors.
    *   *Land Registry Agent:* Secure, transparent property transfers.

## 5. Impact
*   **Resilience:** The government continues to function even if physical infrastructure is damaged. Vital records are never lost.
*   **Efficiency:** Frees up civil servants from rote paperwork to focus on high-value community services and disaster response.
*   **Trust:** Reduces corruption and ensures that aid and services are delivered transparently and fairly, especially during crises.
*   **Ease of Doing Business:** Anguilla becomes the most frictionless jurisdiction in the world.
</file_artifact>

<file path="src/Artifacts/A205 - Research Proposal - Resilient Island Systems.md">
# Artifact A205: Research Proposal - Resilient Island Systems
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C2 (Deepen climate focus: Water Security and Digital Twin)

- **Key/Value for A0:**
- **Description:** A proposal for using AI to manage critical island resources (water, energy) and enhance climate resilience through predictive modeling and digital twins.
- **Tags:** anguilla, sustainability, environment, climate change, resource management, digital twin

## 1. Title: Resilient Island Systems: AI for Water Security and Survival

## 2. Problem Statement
As a small island nation, Anguilla is on the front lines of climate change. It faces existential risks from hurricanes, rising sea levels, and most critically, **fresh water scarcity**. Managing these fragile systems requires precise, real-time decision-making that human intuition alone cannot provide. Traditional resource management is reactive; survival requires proactive, predictive modeling.

## 3. Research Objectives
1.  **Resource Modeling:** Gather granular data on the island's water table, desalination capacity, energy grid load profiles, and food supply chains.
2.  **Climate Vulnerability Assessment:** Identify specific infrastructure points most at risk from extreme weather events and sea-level rise.
3.  **Sensor Network Feasibility:** Determine the cost and logistics of deploying IoT sensors across the island's utility networks to feed real-time data to an AI model.

## 4. Proposed Solution: The Anguilla Digital Twin
We propose creating a **Digital Twin** of the island's critical infrastructure—a live, AI-powered simulation.
*   **AI-Optimized Desalination:** Using machine learning to predict water demand and optimize desalination plant energy usage, reducing costs and ensuring water security during droughts.
*   **Hurricane Response Simulation:** A system that can run thousands of hurricane scenarios to predict damage, optimize evacuation routes, and pre-position emergency supplies before a storm hits.
*   **Smart Grid & Microgrids:** Managing the integration of renewable energy (solar/wind) into the island's grid. The AI will manage microgrids that can "island" themselves off from the main grid during a storm, keeping critical services (hospitals, shelters) powered even if the main lines go down.
*   **Heritage Site Protection:** Modeling the impact of sea-level rise on cultural heritage sites to prioritize preservation efforts.

## 5. Impact
*   **Survival:** Drastically improves water and energy security, saving lives during disasters.
*   **Sustainability:** Reduces waste and reliance on imported fossil fuels.
*   **Cultural Preservation:** Protects physical heritage sites from climate erasure.
*   **Global Model:** Establishes Anguilla as a global leader in "Climate Tech" and adaptation strategies.
</file_artifact>

<file path="src/Artifacts/A206 - Research Proposal - The Global AI Sandbox.md">
# Artifact A206: Research Proposal - The Global AI Sandbox
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C2 (Emphasize Ethical Alignment and Cultural Protection)

- **Key/Value for A0:**
- **Description:** A proposal to establish Anguilla as a "Regulatory Sandbox" for ethical AI development, ensuring frameworks respect local culture and prevent digital colonialism.
- **Tags:** anguilla, regulation, policy, sandbox, innovation, ethics, sovereignty

## 1. Title: The Global AI Sandbox: A Jurisdiction for Ethical Innovation

## 2. Problem Statement
The global regulatory landscape for AI is fragmented. While innovation is needed, there is a risk that small nations will become testing grounds for unethical technologies ("digital colonialism"). Anguilla has the opportunity to set a different standard: a jurisdiction that attracts innovation but mandates strict ethical alignment with local values and human rights.

## 3. Research Objectives
1.  **Legal Framework Analysis:** Review current Anguillan laws regarding liability, data privacy, and intellectual property to identify gaps for AI regulation.
2.  **Ethical Alignment Study:** Consult with local community leaders and stakeholders to define the "Anguillian Ethical Standard" for AI (e.g., privacy, fairness, transparency).
3.  **Risk Assessment:** Identify the risks of hosting experimental AI technologies and define the necessary "safety rails" to protect the population.

## 4. Proposed Solution: The Anguilla Ethical AI Sandbox
We propose establishing a legislative framework that designates Anguilla as a **"Special Economic Zone for Ethical AI."**
*   **Fast-Track Licensing with Ethics Review:** A streamlined process for AI companies to operate, conditional on passing an ethics review based on the Anguillian Standard.
*   **Liability Shields & Data Trusts:** Clear laws defining liability for AI actions. Creating "Data Trusts" that allow companies to train models on local data *only* if the value generated is shared back with the community (preventing data extraction).
*   **The "Proving Ground":** Designating specific zones for testing real-world AI applications (e.g., autonomous delivery drones, AI-managed microgrids) under strict government supervision.
*   **Cultural Protection Clause:** Mandating that any AI deployed in the public sphere must respect local cultural norms and not displace local labor without a transition plan.

## 5. Impact
*   **High-Quality Investment:** Attracts "conscientious" tech companies that value ethics and safety, filtering out predatory actors.
*   **Sovereignty:** Ensures that Anguilla sets the rules of engagement for AI on its soil.
*   **Global Influence:** Anguilla punches above its weight, helping to set the standards for global AI regulation that respects small nations.
</file_artifact>

<file path="src/Artifacts/A207 - Strategic Presentation Guide.md">
# Artifact A207: Strategic Presentation Guide - The Pitch to the Minister
# Date Created: C1
# Author: AI Model & Curator
# Updated on: C2 (Weave in Sovereignty, Culture, and Climate themes)

- **Key/Value for A0:**
- **Description:** A script and strategic guide for the meeting with the Minister of IT, outlining the narrative arc, key talking points, and the "ask," woven with themes of political sovereignty, cultural preservation, and climate resilience.
- **Tags:** anguilla, presentation, strategy, pitch, meeting guide

## 1. The Core Narrative: "Ownership & Survival"

**The Hook:**
"Minister, Anguilla currently owns the most valuable address in the digital world: **.ai**. The world comes to you for the *name*. But right now, they take the *value* elsewhere.

My proposal is not just about technology. It is about **Sovereignty**. It is about **Culture**. And it is about **Survival**.

Let's bring the value home. Let's make Anguilla not just the *registrar* of AI, but the *capital* of AI—a nation that owns its future, preserves its past, and is resilient against the storms to come."

## 2. The "Show, Don't Tell" Strategy

Do not just pitch slides. Use the **Data Curation Environment (DCE)** to demonstrate the power of the methodology.

*   **The Demo:** "I used my own AI system—the same system I build for Google and the US Military—to analyze your nation's potential. I curated data on your economy, your demographics, and your infrastructure."
*   **The Reveal:** Show the 5 Research Proposals (A202-A206).
    *   "We have a plan for **Sovereign Infrastructure** that withstands hurricanes (A202)."
    *   "We have a plan for **Cognitive Capital** that preserves our culture and dialect (A203)."
    *   "We have a plan for an **Automated State** that ensures the government never goes offline (A204)."

## 3. Addressing the "Micro-Pilot" (The 16,000 Advantage)

**The Pivot:**
"You might think, 'We are too small.' I tell you: **You are the perfect size.**

Large nations are Titanic ships. They cannot pivot. Anguilla is agile. With 16,000 people, we can touch everyone. We can give an AI tutor to every student that speaks *our* dialect. We can secure every land deed in a cloud that no hurricane can destroy.

You are not a small island; you are a **Model Organism** for the future of humanity. A proof that a nation can be high-tech, culturally rich, and climate-resilient all at once."

## 4. Your Credibility (The "Why You" Factor)

Leverage your unique background to build trust.

*   **Google:** "I train the models the world uses. I know their power, but I also know their limits."
*   **DOD/NSA:** "I build training for the US military. I understand **security**. I understand **resilience**. I am not selling a crypto scheme. I am proposing a national defense strategy for your economy and your environment."
*   **The Citizen Architect:** "I am not a coder. I am a 'Citizen Architect.' I built this platform myself using AI. I am proof that you don't need 16,000 computer scientists. You just need 16,000 empowered Anguillians."

## 5. The Ask

Don't ask for a massive contract immediately. Ask for the **Pilot**.

"Minister, I am asking for the mandate to run a **Micro-Pilot**.

Give me **one cohort**. One class of students, or one department of government. Let me apply the V2V methodology. Let me equip them with the DCE.

If, in 90 days, they are not the most productive, innovative group on this island—if we haven't proven we can protect our culture and build resilience—we walk away. But when we succeed... then we scale. Then we build the AI Capital."
</file_artifact>

<file path="src/Artifacts/A214 - Anguilla Project - GitHub Repository Setup Guide.md">
# Artifact A214: Anguilla Project - GitHub Repository Setup Guide
# Date Created: C2
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A step-by-step guide for setting up the GitHub repository for the Anguilla Project, ensuring proper version control for the research proposals and strategic plans.
- **Tags:** git, github, setup, anguilla, project management

## 1. Overview

This guide outlines the steps to initialize the Git repository for the **Anguilla Project**. This repository will house all the research proposals (`A201`-`A207`), strategic plans, and future pilot program data.

## 2. Repository Structure

We will organize the repository to separate the strategic artifacts from potential future code or data.

```
anguilla-project/
├── artifacts/          # Research proposals and plans (A201-A207)
├── presentation/       # Slides and assets for the Minister meeting
├── data/               # Data gathered on Anguilla (economy, climate, etc.)
├── src/                # Future code for prototypes (e.g., sovereign model fine-tuning scripts)
├── README.md
└── .gitignore
```

## 3. Initialization Steps

1.  **Create Local Directory:**
    ```bash
    mkdir anguilla-project
    cd anguilla-project
    ```

2.  **Initialize Git:**
    ```bash
    git init
    ```

3.  **Create .gitignore:**
    Create a `.gitignore` file to exclude system files and sensitive data.
    ```
    node_modules/
    .DS_Store
    .env
    *.log
    ```

4.  **Commit Initial Artifacts:**
    Move the generated artifacts (`A201` through `A207`) into the `artifacts/` folder.
    ```bash
    git add .
    git commit -m "Initial commit: Strategic Vision and Research Proposals"
    ```

5.  **Create Remote Repository:**
    Create a new repository on GitHub named `anguilla-project`.

6.  **Push to Remote:**
    ```bash
    git remote add origin https://github.com/YOUR_USERNAME/anguilla-project.git
    git branch -M main
    git push -u origin main
    ```

## 4. Next Steps
*   Begin populating the `presentation/` folder with assets for the meeting.
*   Use the `A200 - Universal Task Checklist` to track progress on the pilot program setup.
</file_artifact>

<file path="src/Artifacts/A215 - Anguilla Project - Migration Manifest.md">
# Artifact A215: Anguilla Project - Migration Manifest
# Date Created: C2
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A curated list of files to migrate from the `aiascent-dev` repository to the new `anguilla-project` repository. This selection ensures the new context possesses the necessary "notions" regarding V2V methodology, Cognitive Capital, and the curator's background.
- **Tags:** anguilla, migration, setup, context curation, manifest

## 1. Purpose

This manifest lists the specific files that should be selected and flattened into a single `flattened_repo.md` for migration to the `anguilla-project` directory. Transferring these files ensures the AI in the new project has the full context of your philosophy, methodology, and credibility.

## 2. File List for Migration

### Group 1: Anguilla Project Core (The Strategy)
*These are the specific plans created for this initiative.*
- `src/Artifacts/A200 - Anguilla Project - Universal Task Checklist.md`
- `src/Artifacts/A201 - Anguilla Project - Vision and Master Plan.md`
- `src/Artifacts/A202 - Research Proposal - The AI Capital.md`
- `src/Artifacts/A203 - Research Proposal - The Cognitive Citizenry.md`
- `src/Artifacts/A204 - Research Proposal - The Automated State.md`
- `src/Artifacts/A205 - Research Proposal - Resilient Island Systems.md`
- `src/Artifacts/A206 - Research Proposal - The Global AI Sandbox.md`
- `src/Artifacts/A207 - Strategic Presentation Guide.md`
- `src/Artifacts/A214 - Anguilla Project - GitHub Repository Setup Guide.md`

### Group 2: Methodology & Philosophy (The "Notions")
*These files explain "Process as Asset," "Cognitive Capital," and the V2V workflow. They are the intellectual engine of the proposal.*
- `src/Artifacts/A50 - V2V Academy - Core Principles & Philosophy.md` (Defines "AI as Feedback Loop" and "Star Trek Motivation")
- `src/Artifacts/A51 - V2V Academy - The Virtuoso's Workflow.md` (The "Documentation First" process)
- `src/Artifacts/A78. DCE - Whitepaper - Process as Asset.md` (Crucial for explaining the value prop to government stakeholders)
- `context/dce/dce_kb.md` (The manual for the DCE tool itself, essential for the "Automated State" pitch)

### Group 3: Personal Credibility & Context (The Authority)
*These files establish your background, the "Citizen Architect" story, and your geopolitical awareness.*
- `context/personal/dgerabagi_resume.md` (Establishes your credentials with Google/DOD)
- `context/personal/personal-journey-to-learn-ai-transcript.txt` (Your origin story)
- `context/v2v/audio-transcripts/1-on-1-training/transcript-11.md` (Contains the "AI Cold War" and "National Security" context)
- `src/Artifacts/A115 - GlobalLogic AI Micro-Pilot Proposal.md` (A template for pitching pilot programs)

### Group 4: System Artifacts (The Engine)
*These are required for the DCE extension to parse your prompts correctly in the new repository.*
- `src/Artifacts/A52.1 DCE - Parser Logic and AI Guidance.md`
- `src/Artifacts/A52.2 DCE - Interaction Schema Source.md`
- `src/Artifacts/A52.3 DCE - Harmony Interaction Schema Source.md`

## 3. Instructions
1.  Open the DCE Context Chooser.
2.  Select the files listed above.
3.  Click **"Flatten Context"**.
4.  Move the generated `flattened_repo.md` to the `anguilla-project` directory.
5.  (Optional) Rename it to `context_bootstrap.md` to distinguish it from future flattened files in the new repo.
</file_artifact>

<file path="src/Artifacts/A62 - V2V Academy - Synthesis of Research Proposals.md">
# Artifact A62: V2V Academy - Synthesis of Research Proposals
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A meta-reflection on the provided research proposals, summarizing key themes, strategic insights, and recurring patterns.
- **Tags:** v2v, research, synthesis, meta-analysis, strategy

## 1. Overview

This document provides a high-level synthesis of the key insights gleaned from the provided research proposals (`context/v2v/research-proposals/`). These proposals represent a deep dive into the transition from "prompt engineering" to "context engineering" and form the intellectual bedrock of the V2V Academy. This reflection consolidates the most critical themes that should guide our curriculum design and strategic positioning.

## 2. Key Themes and Strategic Insights

### 1. The Paradigm Shift is Real and Defensible
*   **Insight:** The transition from "prompt engineering" to "context engineering" is not just a semantic change but a fundamental, industry-wide paradigm shift. The research consistently frames prompt engineering as a tactical, brittle, and introductory skill, while context engineering is positioned as a strategic, robust, and architectural discipline required for production-grade AI systems.
*   **Strategic Implication:** This validates the core premise of the V2V curriculum. We should lean heavily into this distinction, positioning V2V as an advanced program that teaches AI *systems architecture*, not just prompt crafting. This creates a clear market differentiator.

### 2. The Future is Agentic and Systemic
*   **Insight:** The research points toward a future dominated by "agentic workflows," where autonomous or semi-autonomous AI agents execute complex, multi-step tasks. Building these agents requires a systems-thinking approach, focusing on memory, tool integration, and state management.
*   **Strategic Implication:** The V2V curriculum must be forward-looking. The end goal should not be to create a better "prompter," but a capable "agent architect." The capstone projects and advanced modules should focus on designing and orchestrating these agentic systems.

### 3. Pedagogy Must Evolve to Counter "Pseudo-Apprenticeship"
*   **Insight:** The research highlights a critical pedagogical risk: learners using AI as an "answer engine" to bypass the productive struggle required for deep learning. The Cognitive Apprenticeship model is identified as the ideal framework, but it must be implemented in a way that forces learners to engage in metacognition, articulation, and reflection.
*   **Strategic Implication:** Our curriculum design and exercises must be intentionally structured to mitigate this risk. We should prioritize activities that require students to critique AI output, justify their own design choices, and use the AI as a Socratic partner rather than a simple code generator. Assessment should focus on the student's *process* and *reasoning*, not just the final code.

### 4. The "Human-in-the-Loop" is the "Chief Validation Officer"
*   **Insight:** As AI automates more of the tactical implementation (the "how"), the human's value shifts to higher-order cognitive functions: strategic intent (the "why"), critical validation, and ethical oversight.
*   **Strategic Implication:** The V2V curriculum should explicitly train for this new role. We are not just training coders; we are training the next generation of technical leaders who can strategically direct and rigorously validate AI systems. Modules on AI-assisted Test-Driven Development (TDD) and spec-driven workflows are practical implementations of this principle.

### 5. Context is the New Competitive Moat
*   **Insight:** As powerful foundational models become commoditized, the source of competitive advantage is no longer the model itself, but the ability to effectively connect that model to unique, proprietary data and workflows. The context layer—the RAG pipelines, memory systems, and tool integrations—is the defensible asset.
*   **Strategic Implication:** This reinforces the value proposition of the entire V2V program. By teaching the discipline of context engineering, we are equipping students with the skills to build these valuable, defensible systems, making them highly sought-after in the market.
</file_artifact>

<file path="src/Artifacts/A115 - GlobalLogic AI Micro-Pilot Proposal.md">
# Artifact A115: GlobalLogic AI Micro-Pilot Proposal
# Date Created: C113
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A proposal for a micro-pilot leveraging the Data Curation Environment (DCE) methodology to address the exponential growth of task complexity and reduce the cognitive burden on Task Leads by distilling massive project context.
- **Tags:** proposal, ai, micro-pilot, consulting, context management, cognitive capital

## 1. Executive Summary: The Crisis of Context and the Need for Distillation

The core challenge facing Task Leads and project managers is the **exponential growth of task complexity**. As AI context windows lengthen, and as teams learn to integrate more documentation (code, research, client memos) into their prompts, the volume of data a Task Lead must synthesize and approve before a cycle begins has become a severe bottleneck. The human cost is a massive **"Cognitive Bandwidth Tax"** on key personnel, leading to burnout, delayed approvals, and a higher risk of systemic errors.

The solution is not more data, but **AI-powered context distillation**. This proposal outlines a micro-pilot to leverage the DCE methodology for **Parallel Context Distillation (PCD)**, turning a mass of documentation into a concise, actionable "Source of Truth" summary.

## 2. Micro-Pilot 1: Parallel Context Distillation (PCD)

### 2.1. Problem Statement
**"Task guidelines are growing exponentially, increasing the cognitive load on Task Leads and delaying project cycles."**

### 2.2. Solution: Parallel Context Distillation (PCD)
Implement a structured, AI-driven process to distill massive, multi-source project documentation (task guidelines, client specs, historical reports) into a single, concise, and validated summary.

### 2.3. Pilot Workflow
1.  **Curate Input:** The Task Lead selects all relevant documentation (e.g., 20 different files totaling 50,000+ tokens) using a visual interface (simulating the DCE's File Tree View).
2.  **Generate Prompt:** A structured prompt is automatically created, instructing the AI to act as a "Strategic Analyst" and synthesize the entire context into a single, structured document (e.g., a "500-Token Master Plan" with a clear list of objectives, constraints, and key facts).
3.  **Parallel Execution:** The prompt is sent to a parallel AI service (e.g., 4 instances of Gemini or another model).
4.  **Distillation & Validation:** The system receives the four parallel summaries. The Task Lead reviews the four outputs, sorts them by completeness (token count), and quickly selects the best one.
5.  **Result:** The Task Lead has reviewed the entire 50,000-token context in the form of a single, 500-token, AI-generated "Source of Truth" summary, reducing their cognitive burden by over 99%.

### 2.4. Success Metrics
*   **Time Compression:** Measure the time required for a Task Lead to approve a complex, multi-document task *before* vs. *after* the PCD process. (Goal: 50% reduction in approval time).
*   **Cognitive Load Reduction:** Measure the length of the final, distilled "Source of Truth" summary versus the total size of the input documentation. (Goal: Achieve a 90%+ compression ratio).
*   **Error Rate:** Track the number of "rework" cycles caused by the Task Lead missing a critical detail in the original documentation.

## 3. Additional Strategic AI Initiative Ideas

### 3.1. Initiative 2: Cognitive Capital Metrics for Upskilling

*   **Problem Statement:** "The company lacks a clear, measurable pathway to professionalize the AI workforce and reward the high-value, non-coding skills required for the AI era."
*   **Solution:** Implement a **Cognitive Capital Index (CCI)**—a system for tracking and rewarding the high-level, human-centric skills of the "Citizen Architect" (e.g., Context Engineering, Critical Analysis of AI Output, Structured Prompting).
*   **Alignment:** This aligns with the V2V Academy model. By formalizing these skills, GlobalLogic can create a clear career ladder (a true promotion path) that incentivizes high-value work, thereby increasing workforce retention and quality. The training itself can be provided by the V2V Academy curriculum.

### 3.2. Initiative 3: Secure Context Generation (SCG) as a Service

*   **Problem Statement:** "Clients are concerned about IP leakage and the accidental inclusion of sensitive data in prompts sent to public LLMs."
*   **Solution:** Develop a **Secure Context Generation (SCG) Service** that leverages the DCE's core feature of "Precision Context Curation" for external clients.
*   **Alignment:** GlobalLogic can offer this as a premium, high-assurance consulting service. The service uses a vetted, internal tool (the DCE) to guarantee that only necessary, non-sensitive, and high-quality data is included in a prompt, mitigating the risk of IP leakage and ensuring compliance with client security protocols (e.g., NSA CSfC, Google's RMI policies). This turns a critical security vulnerability into a high-value, defensible competitive advantage.

## 4. Image Prompts for Visualizing the Ideas

| Idea | Allegory | Image Prompt |
| :--- | :--- | :--- |
| **Micro-Pilot 1: Parallel Context Distillation (PCD)** | The Master Alchemist's Distillation | A cinematic, hyper-realistic image of a Master Alchemist at a futuristic, glowing distillation apparatus. They feed a chaotic pile of documents (raw context) into the bottom, and the apparatus produces a single, pure, glowing vial of liquid (the distilled summary). The alchemist, a calm and focused professional, holds the single vial with a look of immense satisfaction. |
| **Initiative 2: Cognitive Capital Metrics (CCI)** | The Strategic Skill Tree | A powerful, holographic image of a career "skill tree." The traditional skills (e.g., "Python Syntax," "Basic Coding") are small, dark nodes at the bottom. A new, massive, glowing branch labeled "COGNITIVE CAPITAL" ascends to the top, with large, vibrant, unlocked nodes for "Context Engineering," "Critical Analysis," and "AI Orchestration." A Task Lead is shown proudly pointing to the glowing "Context Engineering" node. |
| **Initiative 3: Secure Context Generation (SCG)** | The Data Shield | A hyper-realistic, dark-themed image of a firewall. On one side, a chaotic storm of digital data is trying to pass through. A specialized, precise shield, shaped like the DCE spiral logo, is only allowing a single, clean, structured beam of light (the curated context) to pass through to a client server. The shield is labeled "SECURE CONTEXT GENERATION." |
</file_artifact>

<file path="src/Artifacts/A215 - Anguilla Project - Context Transfer List.md">
# Artifact A215: Anguilla Project - Context Transfer List
# Date Created: C2
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A definitive list of files to be transferred from the `aiascent-dev` repository to the new `anguilla-project` directory. This list includes the specific project proposals and the necessary foundational context from the DCE.
- **Tags:** anguilla, setup, migration, context curation, file list

## 1. Purpose

This artifact serves as a manifest for the "Context Migration" to the new `anguilla-project` directory. It identifies not only the newly created Anguilla-specific documents but also the critical "Source of Truth" artifacts from the main project that define the methodology, philosophy, and credentials required to pitch the vision effectively.

## 2. Transfer List

### Group 1: Anguilla Project Specifics (The "What")
*These are the core planning documents and proposals created specifically for the Ministry.*

- `src/Artifacts/A200 - Anguilla Project - Universal Task Checklist.md`
- `src/Artifacts/A201 - Anguilla Project - Vision and Master Plan.md`
- `src/Artifacts/A202 - Research Proposal - The AI Capital.md`
- `src/Artifacts/A203 - Research Proposal - The Cognitive Citizenry.md`
- `src/Artifacts/A204 - Research Proposal - The Automated State.md`
- `src/Artifacts/A205 - Research Proposal - Resilient Island Systems.md`
- `src/Artifacts/A206 - Research Proposal - The Global AI Sandbox.md`
- `src/Artifacts/A207 - Strategic Presentation Guide.md`
- `src/Artifacts/A214 - Anguilla Project - GitHub Repository Setup Guide.md`

### Group 2: Methodology & Philosophy (The "How" & "Why")
*These artifacts provide the intellectual foundation for the "Citizen Architect" and "V2V" concepts referenced in the proposals. They are essential for the AI to understand the "product" being pitched.*

- `src/Artifacts/A23. aiascent.dev - Cognitive Capital Definition.md`
- `src/Artifacts/A50. V2V Academy - Core Principles & Philosophy.md`
- `src/Artifacts/A51. V2V Academy - The Virtuoso's Workflow.md`
- `src/Artifacts/A78. DCE - Whitepaper - Process as Asset.md` (Crucial proof of the methodology)
- `src/Artifacts/A115 - GlobalLogic AI Micro-Pilot Proposal.md` (Reference for the "Micro-Pilot" structure)

### Group 3: Credentials & Authority (The "Who")
*These documents establish your authority as a Google AI Trainer and DOD expert.*

- `src/Artifacts/A47 - David Gerabagi Resume (DCE Update).md`

### Group 4: System Context (The "Engine")
*These are required for the DCE extension to function correctly in the new environment and to maintain the "Artifact" workflow.*

- `src/Artifacts/A52.1 DCE - Parser Logic and AI Guidance.md`
- `src/Artifacts/A52.2 DCE - Interaction Schema Source.md`
- `src/Artifacts/A0. aiascent.dev - Master Artifact List.md` (Useful as a template/reference for starting the new A0)

## 3. Instructions for Curator

1.  **Select:** Use the DCE File Tree View or your OS file explorer to select the files listed above.
2.  **Flatten/Move:** If using the DCE "Flatten" feature, these will be concatenated into `flattened_repo.md`. Alternatively, copy these individual files directly into the `anguilla-project/artifacts/` (or equivalent) directory in the new workspace.
3.  **Initialize:** Once moved, open the new workspace and generate a `Cycle 0` prompt to initialize the new `A0 - Anguilla Project - Master Artifact List`.
</file_artifact>

<file path="src/Artifacts/A23. aiascent.dev - Cognitive Capital Definition.md">
# Artifact A23: aiascent.dev - Cognitive Capital Definition

# Date Created: C19
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Provides the canonical definition and explanation of "Cognitive Capital" as the term is used within the aiascent.dev project, distinguishing it from other interpretations.
- **Tags:** documentation, definition, cognitive capital, strategy, human capital, problem-solving

## 1. Purpose and Definition

The term "Cognitive Capital" is central to the mission of aiascent.dev and the philosophy behind the Data Curation Environment (DCE). While the term exists in academic contexts, our project uses a specific, strategic definition.

**Definition:**
> **Cognitive Capital** is the collective problem-solving capacity of an individual, an organization, or a society. It represents the accumulated potential to understand complex challenges, innovate under pressure, and adapt to new environments.

## 2. Core Concepts

### 2.1. Beyond Human Capital

Cognitive Capital is related to, but distinct from, "human capital."
*   **Human Capital** often refers to the economic value of a worker's experience and skills. It is a measure of an individual's productive inputs.
*   **Cognitive Capital** is a broader, more dynamic concept. It is not just the sum of individual skills, but the emergent capability of a group to synthesize those skills to solve novel problems. One company or nation may have more workers (human capital), but another may possess vastly more Cognitive Capital, enabling it to out-innovate and outperform its rival.

### 2.2. The Primary Asset in the AI Era

In an age where AI can automate routine cognitive tasks, the true differentiator is no longer the ability to perform known procedures, but the ability to solve unknown problems. Cognitive Capital, therefore, becomes the primary strategic asset for national power and economic prosperity. It is the raw material from which innovation, resilience, and progress are forged.

### 2.3. Cultivating, Not Just Counting

The mission of aiascent.dev is not just to acknowledge the importance of Cognitive Capital, but to build the tools that actively cultivate it. The DCE is designed to be an engine for amplifying this resource. By creating a structured, iterative, and transparent workflow for human-AI collaboration, the DCE allows individuals and teams to tackle problems of a scale and complexity that would otherwise be impossible. It transforms the user from a simple operator into a "Citizen Architect," directly increasing their contribution to the collective Cognitive Capital.
</file_artifact>

<file path="anguilla_context_migration_list.txt">
src/Artifacts/A200 - Anguilla Project - Universal Task Checklist.md
src/Artifacts/A201 - Anguilla Project - Vision and Master Plan.md
src/Artifacts/A202 - Research Proposal - The AI Capital.md
src/Artifacts/A203 - Research Proposal - The Cognitive Citizenry.md
src/Artifacts/A204 - Research Proposal - The Automated State.md
src/Artifacts/A205 - Research Proposal - Resilient Island Systems.md
src/Artifacts/A206 - Research Proposal - The Global AI Sandbox.md
src/Artifacts/A207 - Strategic Presentation Guide.md
src/Artifacts/A214 - Anguilla Project - GitHub Repository Setup Guide.md
context/dce/dce_kb.md
context/personal/dgerabagi_resume.md
context/v2v/research-proposals/01-V2V Academy Content Research Plan.md
context/v2v/research-proposals/02-V2V Context Engineering Research Plan.md
context/v2v/research-proposals/03-AI Research Proposal_ V2V Pathway.md
context/v2v/research-proposals/04-AI Research Proposal_ V2V Pathway.md
context/v2v/research-proposals/06-V2V Academy Context Engineering Research.md
context/v2v/research-proposals/07-V2V Pathway Research Proposal.md
context/v2v/research-proposals/08-V2V Pathway Research Proposal.md
context/v2v/audio-transcripts/1-on-1-training/transcript-1.md
context/v2v/audio-transcripts/1-on-1-training/transcript-2.md
context/v2v/audio-transcripts/1-on-1-training/transcript-3.md
context/v2v/audio-transcripts/1-on-1-training/transcript-4.md
context/v2v/audio-transcripts/1-on-1-training/transcript-5.md
context/v2v/audio-transcripts/1-on-1-training/transcript-6.md
context/v2v/audio-transcripts/1-on-1-training/transcript-7.md
context/v2v/audio-transcripts/1-on-1-training/transcript-8.md
context/v2v/audio-transcripts/1-on-1-training/transcript-9.md
context/v2v/audio-transcripts/1-on-1-training/transcript-10.md
context/v2v/audio-transcripts/1-on-1-training/transcript-11.md
src/Artifacts/A52.1 DCE - Parser Logic and AI Guidance.md
src/Artifacts/A52.2 DCE - Interaction Schema Source.md
</file_artifact>

<file path="public/data/anguilla_content.json">
{
  "reportId": "anguilla",
  "reportTitle": "Anguilla: The First AI-Native Nation",
  "sections": [
    {
      "sectionId": "vision",
      "sectionTitle": "I. The Vision",
      "pages": [
        {
          "pageId": "intro",
          "pageTitle": "From Domain Rent to Sovereign Power",
          "tldr": "Anguilla's .ai domain is a $32M/year windfall. We must convert this transient 'digital rent' into permanent sovereign infrastructure.",
          "content": "We propose a comprehensive national strategy to become the world's first **AI-Native Nation**: a society where every citizen is empowered by artificial intelligence, where government services are frictionless and automated, and where the economy is driven by high-value cognitive labor.\n\nThis transition is driven by three guiding principles:\n1.  **Economic Sovereignty:** Moving from passive revenue collection to active ownership of digital infrastructure. We must stop exporting our capital to foreign clouds and start building our own.\n2.  **Cognitive Capital:** Investing in our people. The goal is not just to use AI, but to master it. We will upskill our population to become 'Citizen Architects,' capable of building solutions for themselves and the world.\n3.  **Resilience:** Using our digital wealth to fortify our physical reality. We will use the .ai windfall to solve our chronic water, energy, and climate challenges, ensuring that Anguilla thrives for generations to come.",
          "imageGroupIds": ["vision-intro"]
        },
        {
          "pageId": "polycrisis",
          "pageTitle": "The Poly-Crisis: Why We Must Act",
          "tldr": "We face existential threats: 80% water loss, $0.42/kWh energy costs, and climate vulnerability. AI is our survival strategy.",
          "content": "The **Anguilla Digital Infrastructure Initiative** proposes using the .ai windfall to solve these physical problems through digital intelligence.\n\n*   **The Water Paradox:** We manufacture fresh water at great expense, only to pour 80% of it back into the ground through aging pipes. An AI-driven 'Digital Twin' of our water network could detect these leaks in real-time, saving millions of dollars and securing our water supply without building a single new desalination plant.\n*   **The Energy Trap:** Our reliance on diesel ties our cost of living to global oil markets. By using AI to balance our grid, we can safely integrate higher levels of solar and wind power, breaking the $0.42/kWh stranglehold and making Anguilla competitive for business and affordable for citizens.\n*   **The Climate Reality:** We cannot stop the storms, but we can outsmart them. Predictive modeling allows us to harden our infrastructure precisely where it is most vulnerable, transforming reactive disaster recovery into proactive resilience.",
          "imageGroupIds": ["vision-crisis"]
        }
      ]
    },
    {
      "sectionId": "capital",
      "sectionTitle": "II. The AI Capital",
      "pages": [
        {
          "pageId": "wealth-fund",
          "pageTitle": "The Digital Wealth Fund",
          "tldr": "A sovereign fund fueled by .ai revenue to finance hurricane-proof, green digital infrastructure.",
          "content": "This infrastructure ensures data residency and operational continuity, transforming Anguilla from a passive registrar into an active host of the global AI economy.\n\n*   **Sterilizing the Windfall:** We must avoid 'Dutch Disease.' By ring-fencing 50% of the .ai revenue, we prevent the inflation of our recurrent budget and ensure that this transient capital builds permanent assets.\n*   **The Investment Mandate:** The fund will not buy stocks in New York; it will pour concrete in Anguilla. It will fund the heavy civil works required to build Category 5-rated data bunkers and the solar farms needed to power them.\n*   **From Rentier to Landlord:** Currently, we collect a small fee for the name. By building the Sovereign Cloud, we can offer hosting, inference, and data services—capturing a much larger share of the value chain. We stop being just the 'name' of AI and become the 'home' of AI.",
          "imageGroupIds": ["capital-fund"]
        },
        {
          "pageId": "sovereign-cloud",
          "pageTitle": "Green Sovereign Compute",
          "tldr": "Breaking the diesel dependency with solar-powered data centers.",
          "content": "Our geography poses challenges, but our strategy turns them into strengths.\n\n*   **Energy Independence:** Data centers are energy-hungry. Running them on diesel is impossible. By building our own solar microgrids, we not only power the AI cloud but stabilize the grid for the entire island. We turn Anguilla into a model of green computing.\n*   **The Hurricane Bunker:** Our data centers will not be glass office buildings. They will be reinforced concrete bunkers, designed to withstand 185+ mph winds. This physical security becomes a marketable asset—a 'Data Haven' for global companies seeking safety for their most critical intellectual property.\n*   **Data Sovereignty:** We will no longer rely on foreign clouds that can be subpoenaed by foreign powers. Our health records, our land titles, and our citizen data will reside on Anguillian soil, under Anguillian jurisdiction.",
          "imageGroupIds": ["capital-cloud"]
        }
      ]
    },
    {
      "sectionId": "citizenry",
      "sectionTitle": "III. The Cognitive Citizenry",
      "pages": [
        {
          "pageId": "v2v",
          "pageTitle": "The National V2V Initiative",
          "tldr": "Upskilling the entire nation using the 'Vibecoding to Virtuosity' methodology. Every citizen becomes a builder.",
          "content": "This is not a traditional coding bootcamp. It is a capability revolution.\n\n*   **The 'Citizen Architect':** We believe that domain experts—our teachers, nurses, and tradespeople—are the best people to build solutions for their own fields. We will teach them to use AI to write the code they cannot write themselves.\n*   **Vibecoding:** We lower the barrier to entry. You don't need to learn syntax; you need to learn logic. You don't need to speak Python; you need to speak clearly to the AI. This democratizes technology, making it accessible to every Anguillian, regardless of age or background.\n*   **Economic Mobility:** This program creates a new export sector. An Anguillian who masters these tools can work for any company in the world, remotely, earning global wages while living locally.",
          "imageGroupIds": ["citizenry-v2v"]
        },
        {
          "pageId": "heritage",
          "pageTitle": "The Cultural Heritage AI",
          "tldr": "Preserving our dialect and history by training our own Sovereign AI model.",
          "content": "Technology should not erase identity; it should amplify it.\n\n*   **Preserving the Dialect:** Standard AI models often correct our dialect as 'errors.' AnguillaLLM will be trained to understand and respect our unique way of speaking, ensuring that our digital future sounds like us.\n*   **The Living Archive:** We will launch a national campaign to record the stories of our elders—the memories of the Revolution, the salt ponds, and the boat races. These stories will become the training data for our national model, immortalizing our history in the intelligence of the future.\n*   **Sovereign Education:** Our children will learn from an AI that knows who Sombrero Island belongs to, knows the heroes of 1967, and understands the local context. We are building a guardian of our culture.",
          "imageGroupIds": ["citizenry-heritage"]
        }
      ]
    },
    {
      "sectionId": "state",
      "sectionTitle": "IV. The Automated State",
      "pages": [
        {
          "pageId": "frictionless",
          "pageTitle": "Frictionless Governance",
          "tldr": "Replacing bureaucracy with automation. Instant permits, licenses, and services.",
          "content": "We are building a government that moves at the speed of software.\n\n*   **The 24/7 State:** Why should the government close at 4 PM? ACSA agents never sleep. They can process a business license application at 2 AM on a Sunday, allowing our entrepreneurs to move fast.\n*   **Radical Transparency:** Every interaction is logged and trackable. Citizens can see exactly where their application is, who is reviewing it, and when it will be done. Automation eliminates the 'black box' of bureaucracy.\n*   **Human-Centric Service:** By automating the paperwork, we liberate our civil servants. They can stop being data entry clerks and start being case managers, providing personalized support to the citizens who need it most.",
          "imageGroupIds": ["state-frictionless"]
        },
        {
          "pageId": "data-embassy",
          "pageTitle": "The Data Embassy",
          "tldr": "Ensuring the government survives any storm by mirroring essential data to a secure sovereign cloud abroad.",
          "content": "We face an existential threat from the climate. Our government cannot be housed solely in buildings that can be blown away.\n\n*   **The Estonia Model:** Like Estonia, we are a small nation with big risks. We will replicate their strategy of 'digital territory.' A server rack in London or Ottawa, protected by diplomatic immunity, will hold the 'Golden Copy' of our national data.\n*   **Disaster Recovery:** If a hurricane destroys the Ministry of Lands, the Land Registry survives in the Data Embassy. Property rights are preserved. Banks can still verify identities. The economy can begin recovery the moment the wind stops, because the state never went offline.\n*   **The Ultimate Insurance:** This is the ultimate insurance policy for our nation. It guarantees that no matter what nature throws at us, the idea and the legal reality of Anguilla will endure.",
          "imageGroupIds": ["state-embassy"]
        }
      ]
    },
    {
      "sectionId": "resilience",
      "sectionTitle": "V. Resilient Systems",
      "pages": [
        {
          "pageId": "digital-twin",
          "pageTitle": "The National Digital Twin",
          "tldr": "A real-time AI simulation of the island to manage water, energy, and disaster response.",
          "content": "This is the central nervous system of the resilient island.\n\n*   **Seeing the Invisible:** Currently, 80% of our water disappears underground. The Digital Twin uses sensor data to 'see' these leaks instantly, allowing us to fix them before they drain our treasury. It turns a physical crisis into a manageable data problem.\n*   **Predictive Resilience:** We won't just react to hurricanes; we will rehearse for them. The Digital Twin allows us to run thousands of storm simulations, identifying exactly which power poles will snap and which roads will flood. We can reinforce our defenses *before* the season starts.\n*   **Optimizing the Grid:** As we move to renewables, the grid becomes harder to manage. The Digital Twin balances the intermittency of solar with the stability of the grid, ensuring that we don't just generate green energy, but actually deliver it reliably.",
          "imageGroupIds": ["resilience-twin"]
        }
      ]
    },
    {
      "sectionId": "ask",
      "sectionTitle": "VI. The Proposal",
      "pages": [
        {
          "pageId": "micropilot",
          "pageTitle": "Phase 1: The Micro-Pilot",
          "tldr": "We are asking for a mandate to run a 90-day pilot with one cohort to prove the model.",
          "content": "Big visions fail when they start too big. We start small to win big.\n\n*   **Proof of Concept:** We will take a small group—perhaps the Department of Youth and Culture, or a computer science class at Albena Lake-Hodge Comprehensive School.\n*   **The Transformation:** We will give them the tools and the training. We will show them how to become Citizen Architects.\n*   **The Result:** At the end of 90 days, we won't just show you a report. We will show you software. We will show you apps built by Anguillians, for Anguillians. We will prove that the talent is here, it just needs the right spark.\n*   **The Ask:** Give us the mandate. Give us the access. Let us light the spark.",
          "imageGroupIds": ["ask-pilot"]
        }
      ]
    }
  ]
}
</file_artifact>

<file path="public/data/whitepaper_content.json">
{
  "reportId": "whitepaper-v1",
  "reportTitle": "Process as Asset",
  "sections": [
    {
      "sectionId": "whitepaper",
      "sectionTitle": "Process as Asset Whitepaper",
      "subSections": [
        {
          "subSectionId": "intro",
          "subSectionTitle": "Introduction",
          "pages": [
            {
              "pageId": "wp-01",
              "pageTitle": "Welcome to the Interactive Whitepaper",
              "tldr": "An interactive guide to navigating this whitepaper and understanding its features, presented by your AI assistant, Ascentia.",
              "content": "Hi there! I am Ascentia, your guide through this interactive experience. This whitepaper, \"Process as Asset,\" explores the core philosophy behind the Data Curation Environment (DCE). It explains how a structured, iterative workflow can transform the very process of creation into a valuable, scalable asset.\n\nTo help you navigate, allow me to explain the interface.\n\n*   To your left, you will find the **Report Navigator**, a tree that allows you to jump to any section.\n*   In the center are the primary controls. You can navigate between pages using the **up and down arrow keys**.\n*   For a more immersive experience, you can select **\"Autoplay.\"** I will then read the contents of each page aloud to you.\n*   Finally, the **\"Ask Ascentia\"** button opens a direct line to me. This whitepaper is powered by a knowledge base built from all the documentation for the DCE project. If you have any questions about how the DCE works, feel free to ask.\n\nEnjoy the exploration.",
              "imageGroupIds": ["group_wp-01-cover"]
            },
            {
              "pageId": "wp-02",
              "pageTitle": "Executive Summary",
              "tldr": "The DCE transforms the content creation process itself into a valuable organizational asset.",
              "content": "Organizations tasked with developing highly specialized content such as technical training materials, intelligence reports, or complex software documentation face a constant bottleneck: the time and expertise required to curate accurate data, collaborate effectively, and rapidly iterate on feedback. This whitepaper introduces the Data Curation Environment (DCE), a framework and toolset integrated into Visual Studio Code that transforms the content creation process itself into a valuable organizational asset. By capturing the entire workflow as a persistent, auditable knowledge graph, the DCE provides the infrastructure necessary to scale expertise, ensure quality, and accelerate the entire organizational mission.",
              "imageGroupIds": ["group_wp-02-executive-summary"]
            }
          ]
        },
        {
          "subSectionId": "the-problem",
          "subSectionTitle": "The Problem",
          "pages": [
            {
              "pageId": "wp-03",
              "pageTitle": "The Challenge: Bottleneck of Ad-Hoc AI Interaction",
              "tldr": "Unstructured interaction with LLMs creates critical bottlenecks in organizational workflows.",
              "content": "The integration of Large Language Models (LLMs) into organizational workflows promises significant acceleration. However, the way most organizations interact with these models remains unstructured and inefficient, creating several critical bottlenecks.",
              "imageGroupIds": ["group_wp-03-challenge-ad-hoc-ai"]
            },
            {
              "pageId": "wp-04",
              "pageTitle": "The Context Problem",
              "tldr": "Manually curating context for LLMs is time-consuming, error-prone, and results in poor output.",
              "content": "The quality of an LLM's output is entirely dependent on the quality of its input context. Manually selecting, copying, and pasting relevant data (code, documents, reports) into a chat interface is time-consuming, error-prone, and often results in incomplete or bloated context.",
              "imageGroupIds": ["group_wp-04-problem-bloated-context"]
            },
            {
              "pageId": "wp-05",
              "pageTitle": "The Collaboration Gap",
              "tldr": "When a task is handed off, the context is lost, leading to significant delays and duplication of effort.",
              "content": "When a task is handed off, the context is lost. A colleague must manually reconstruct the previous operator's dataset and understand their intent, leading to significant delays and duplication of effort.",
              "imageGroupIds": ["group_wp-05-problem-collaboration-gap"]
            },
            {
              "pageId": "wp-06",
              "pageTitle": "The Iteration Overhead",
              "tldr": "Revising complex datasets is a Sisyphean task, as operators must reconstruct the entire context for each change.",
              "content": "When feedback requires changes to a complex dataset, operators often resort to manual edits because re-prompting the AI requires reconstructing the entire context again. This negates the efficiency gains of using AI in the first place.",
              "imageGroupIds": ["group_wp-06-problem-iteration-overhead"]
            },
            {
              "pageId": "wp-07",
              "pageTitle": "The Auditability Vacuum",
              "tldr": "The iterative process of human-AI interaction is rarely captured, creating a black box of collaboration.",
              "content": "The iterative process of human-AI interaction (the prompts), the AI's suggestions, and the human's decisions are a valuable record of the work, yet it is rarely captured in a structured, reusable format. These challenges prevent organizations from fully realizing the potential of AI.",
              "imageGroupIds": ["group_wp-07-problem-auditability-vacuum"]
            }
          ]
        },
        {
          "subSectionId": "the-solution",
          "subSectionTitle": "The Solution",
          "pages": [
            {
              "pageId": "wp-08",
              "pageTitle": "The Solution: The Data Curation Environment",
              "tldr": "The DCE eliminates bottlenecks by providing a structured framework for human-AI collaboration.",
              "content": "The Data Curation Environment (DCE) is designed to eliminate these bottlenecks by providing a structured framework for human-AI collaboration directly within the operator's working environment. It moves beyond the limitations of simple chat interfaces by introducing three core capabilities.",
              "imageGroupIds": ["group_wp-08-solution-dce"]
            },
            {
              "pageId": "wp-09",
              "pageTitle": "Precision Context Curation",
              "tldr": "The DCE replaces manual copy-pasting with an intuitive, integrated file management interface.",
              "content": "The DCE replaces manual copy-pasting with an intuitive, integrated file management interface. Operators can precisely select the exact files, folders, or documents required for a task with simple checkboxes, ensuring the AI receives the highest fidelity context possible while minimizing operator effort.",
              "imageGroupIds": ["group_wp-09-feature-precision-curation"]
            },
            {
              "pageId": "wp-10",
              "pageTitle": "Parallel AI Scrutiny",
              "tldr": "The 'Parallel Co-Pilot Panel' allows operators to manage, compare, and test multiple AI-generated solutions simultaneously.",
              "content": "The 'Parallel Co-Pilot Panel' allows operators to manage, compare, and test multiple AI-generated solutions simultaneously. Integrated diffing tools provide immediate visualization of proposed changes, and a one-click 'Accept' mechanism integrated with version control creates a rapid, low-risk loop for evaluating multiple AI approaches.",
              "imageGroupIds": ["group_wp-10-feature-parallel-scrutiny"]
            },
            {
              "pageId": "wp-11",
              "pageTitle": "Persistent Knowledge Graph",
              "tldr": "Every interaction within the DCE is captured as a 'Cycle,' creating a structured, persistent Knowledge Graph.",
              "content": "Every interaction within the DCE is captured as a 'Cycle,' which includes the curated context, the operator's instructions, all AI-generated responses, and the final decision. This history is saved as a structured, persistent Knowledge Graph, allowing operators to step back through history, review past decisions, and understand the project's evolution.",
              "imageGroupIds": ["group_wp-11-feature-knowledge-graph"]
            }
          ]
        },
        {
            "subSectionId": "the-benefits",
            "subSectionTitle": "The Benefits",
            "pages": [
                {
                    "pageId": "wp-12",
                    "pageTitle": "Transforming the Process into an Asset",
                    "tldr": "The true power of the DCE lies in transforming the workflow itself into a persistent organizational asset.",
                    "content": "The true power of the DCE lies in how these capabilities combine to transform the workflow itself into a persistent organizational asset.",
                    "imageGroupIds": ["group_wp-12-process-as-asset"]
                  },
                  {
                    "pageId": "wp-13",
                    "pageTitle": "The Curated Context as a Shareable Asset",
                    "tldr": "The curated 'Selection Set' is a saved, versioned asset that eliminates the collaboration gap.",
                    "content": "In the DCE workflow, the curated context (the 'Selection Set') is a saved, versioned asset. When a task is handed off, the new operator receives the exact context and the complete history of interactions, eliminating the 'collaboration gap' and duplication of effort.",
                    "imageGroupIds": ["group_wp-13-benefit-shareable-context"]
                  },
                  {
                    "pageId": "wp-14",
                    "pageTitle": "Accelerating Iteration and Maintenance",
                    "tldr": "Operators can rapidly iterate on complex datasets without manual reconstruction by simply reloading the curated context.",
                    "content": "Because the context is already curated and saved, operators can rapidly iterate on complex datasets without manual reconstruction. If feedback requires changes, the operator simply loads the curated context and issues a targeted instruction to the AI, completing the update in a single, efficient cycle.",
                    "imageGroupIds": ["group_wp-14-benefit-accelerated-iteration"]
                  },
                  {
                    "pageId": "wp-15",
                    "pageTitle": "Scaling Expertise and Ensuring Auditability",
                    "tldr": "The Knowledge Graph serves as a detailed, auditable record invaluable for training, reviews, and accountability.",
                    "content": "The Knowledge Graph serves as a detailed, auditable record invaluable for Training and Onboarding, After-Action Reviews, and ensuring Accountability in mission-critical environments.",
                    "imageGroupIds": ["group_wp-15-benefit-scaling-expertise"]
                  }
            ]
        },
        {
          "subSectionId": "use-case",
          "subSectionTitle": "Use Case",
          "pages": [
            {
              "pageId": "wp-16",
              "pageTitle": "Use Case Spotlight: Rapid Development",
              "tldr": "A real-world example of transforming a weeks-long manual revision process into an hours-long automated one.",
              "content": "A government agency needs to rapidly update a specialized technical training lab based on new operational feedback indicating that in existing exam questions, 'the correct answer is too often the longest answer choice,' undermining the assessment's validity.",
              "imageGroupIds": ["group_wp-16-use-case-spotlight"]
            },
            {
              "pageId": "wp-17",
              "pageTitle": "The Traditional Workflow (Weeks)",
              "tldr": "The manual process involves days of searching, weeks of editing, and more days of review and rework.",
              "content": "1. **Identify Affected Files:** An analyst manually searches the repository (days). \n2. **Manual Editing:** The analyst manually edits each file, attempting to rewrite 'distractor' answers (weeks). \n3. **Review and Rework:** Changes are reviewed, often leading to further manual edits (days).",
              "imageGroupIds": ["group_wp-17-use-case-traditional"]
            },
            {
              "pageId": "wp-18",
              "pageTitle": "The DCE Workflow (Hours)",
              "tldr": "The DCE workflow condenses the process into minutes for curation and instruction, and hours for review.",
              "content": "1. **Curate Context (Minutes):** The analyst uses the DCE interface to quickly select the folder containing all exam questions. \n2. **Instruct the AI (Minutes):** The analyst provides a targeted instruction to rewrite the distractors. \n3. **Review and Accept (Hours):** The AI generates several solutions, and the analyst uses the integrated diff viewer to compare and accept the best one with a single click.",
              "imageGroupIds": ["group_wp-18-use-case-dce"]
            },
            {
              "pageId": "wp-19",
              "pageTitle": "Conclusion",
              "tldr": "The DCE is a strategic framework for operationalizing AI, providing the infrastructure to scale expertise, ensure quality, and achieve the mission faster.",
              "content": "The Data Curation Environment is a strategic framework for operationalizing AI in complex environments. By addressing critical bottlenecks, the DCE transforms the human-AI interaction workflow into a structured, persistent, and valuable organizational asset, providing the necessary infrastructure to scale expertise, ensure quality, and achieve the mission faster.",
              "imageGroupIds": ["group_wp-19-conclusion"]
            }
          ]
        }
      ]
    }
  ]
}
</file_artifact>

<file path="public/data/whitepaper_imagemanifest.json">
{
  "manifestId": "whitepaper-images-v1",
  "basePath": "/assets/images/whitepaper/",
  "imageGroups": {
    "group_wp-01-cover": {
      "path": "", "prompt": "A hyper-realistic, cinematic image of a male professional in a futuristic command center. He stands in the center, orchestrating a complex, glowing blue data visualization that connects multiple team members at their workstations. The main title 'PROCESS AS ASSET' is prominently displayed in the foreground, with the subtitle 'Capturing Workflow, Accelerating Intelligence' below it. The environment is sleek, modern, and filled with holographic interfaces. Red, abstract data streams are visible in the background, representing raw, chaotic information being structured by the process.", "alt": "Process as Asset Cover", "baseFileName": "wp-01-cover", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-02-executive-summary": {
      "path": "", "prompt": "A futuristic, holographic dashboard displaying the 'EXECUTIVE SUMMARY'. The dashboard shows a flowchart of the DCE Framework, starting from 'THE ORGANIZATIONAL BOTTLENECK' (represented by an hourglass), moving through 'DCE FRAMEWORK' (with icons for Rapid Curation, Seamless Sharing, Instant Iteration), and ending at 'MISSION STREAM'. The overall aesthetic is a clean, dark-themed UI with glowing cyan elements, representing 'ACCELERATING MISSION VELOCITY.'", "alt": "Executive Summary", "baseFileName": "wp-02-executive-summary", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-03-challenge-ad-hoc-ai": {
      "path": "", "prompt": "A depiction of a frustrated developer at their desk, viewed from behind, representing an 'EFFICIENCY DRAIN'. They are surrounded by multiple monitors displaying lines of code and AI chat interfaces. Glowing blue data streams flow into the desk from the floor but end in chaotic, tangled messes around sticky notes that say 'MAKE IT BETTER,' 'AGAIN,' and 'Try again.' The scene illustrates the friction and unstructured nature of ad-hoc AI interaction.", "alt": "The Challenge: Ad-Hoc AI Interaction", "baseFileName": "wp-03-challenge-ad-hoc-ai", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-04-problem-bloated-context": {
      "path": "", "prompt": "A powerful, industrial machine is shown spewing a massive, chaotic torrent of glowing red data labeled 'BLOATED CONTEXT'. A holographic screen nearby displays the message 'DROWNING IN DATA, STARVING FOR CONTEXT'. The image visualizes the problem of providing too much, or the wrong, information to an LLM, which is both time-consuming and results in poor output.", "alt": "The Context Problem", "baseFileName": "wp-04-problem-bloated-context", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-05-problem-collaboration-gap": {
      "path": "", "prompt": "A split-panel image. On the left, a developer's digital 'ghost' is shown leaving their workstation, with the context they were working on dissolving into disconnected particles. On the right, a new developer sits down at the same workstation, looking confused as they try to piece together the fragmented data. A glowing title above reads 'THE COLLABORATING GAP: REINVENTING YESTERDAY'S WORK, TODAY'.", "alt": "The Collaboration Gap", "baseFileName": "wp-05-problem-collaboration-gap", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-06-problem-iteration-overhead": {
      "path": "", "prompt": "A modern depiction of the myth of Sisyphus. A developer is shown pushing a massive, glowing block of data up a digital mountain. The block represents a complex dataset. As they near the top, a piece of feedback causes the block to crumble and roll back to the bottom, forcing them to start the process of reconstructing the context all over again. The title 'The Sisyphean Task of Revision' floats in the starry sky above.", "alt": "The Iteration Overhead", "baseFileName": "wp-06-problem-iteration-overhead", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-07-problem-auditability-vacuum": {
      "path": "", "prompt": "A massive, monolithic black cube, representing 'THE BLACK BOX OF COLLABORATION,' sits in a vast server room. A timeline of a project, composed of prompts and code, flows into the cube but becomes unreadable and unstructured inside. The image visualizes the lack of a structured, reusable record in typical human-AI interactions.", "alt": "The Auditability Vacuum", "baseFileName": "wp-07-problem-auditability-vacuum", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-08-solution-dce": {
      "path": "", "prompt": "A female developer is working at a futuristic computer. A glowing blue data stream flows from her, representing 'THE NEXT EVOLUTION OF HUMAN-AI TEAMING.' This stream interacts with three key capability icons: 'Precision Curation,' 'Parallel Scrutiny,' and 'Persistent Knowledge Graph,' before flowing into the main interface, showing a structured and efficient workflow.", "alt": "The Solution: The Data Curation Environment", "baseFileName": "wp-08-solution-dce", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-09-feature-precision-curation": {
      "path": "", "prompt": "An operator interacts with a holographic file management interface. They are using simple checkboxes to select various file types (PDF, code, spreadsheets). A clean, precise beam of light, representing the curated context, flows from the selected files towards a destination labeled 'Precision In, Perfection Out: The Art of Curation.'", "alt": "Precision Context Curation", "baseFileName": "wp-09-feature-precision-curation", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-10-feature-parallel-scrutiny": {
      "path": "", "prompt": "An operator stands before a large, futuristic touch-screen panel labeled 'DCE's Parallel Co-Pilot Panel.' The panel displays three different AI-generated solutions (A, B, C) side-by-side with an 'Integrated Diff Viewer' highlighting the changes. The operator is comparing the solutions before committing, illustrating a 'Rapid, Low-Risk Iteration Loop.'", "alt": "Parallel AI Scrutiny", "baseFileName": "wp-10-feature-parallel-scrutiny", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-11-feature-knowledge-graph": {
      "path": "", "prompt": "An operator stands in a vast, modern library-like space, representing 'The Architecture of Institutional Memory.' They are interacting with a 'Cycle Navigator' to explore a massive, glowing 'Persistent Knowledge Graph.' Each node in the graph is a 'CAPTURED CYCLE' containing the curated context, user intent, and AI solutions for a step in the project's history.", "alt": "Persistent Knowledge Graph", "baseFileName": "wp-11-feature-knowledge-graph", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-12-process-as-asset": {
      "path": "", "prompt": "A central glowing orb labeled 'DCE' acts as a transformation engine. On the left, chaotic, multi-colored data streams ('CAPTURE THE PROCESS') flow in. On the right, clean, structured, and valuable 'KNOWLEDGE ASSETS' flow out, branching off to empower various teams. The image visualizes the core theme of turning the workflow itself into a valuable asset.", "alt": "Transforming the Process into an Asset", "baseFileName": "wp-12-process-as-asset", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-13-benefit-shareable-context": {
      "path": "", "prompt": "A seamless handoff between two professionals. One passes a glowing, versioned data package labeled 'Curated Context: Selection Set v4.2' to the other. A diagram in the background contrasts a 'Chaotic, Fragmented Workflow' with the 'Elimination of Duplication' achieved through this seamless handoff, highlighting the 'Continuity of Context.'", "alt": "The Curated Context as a Shareable Asset", "baseFileName": "wp-13-benefit-shareable-context", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-14-benefit-accelerated-iteration": {
      "path": "", "prompt": "A developer uses a futuristic interface labeled 'DCE' to perform 'Surgical Precision at Systemic Scale.' They are targeting a specific, glowing facet of a massive, complex crystal structure (representing a complex system) with a precise beam of energy, making a targeted change without affecting the rest of the structure.", "alt": "Accelerating Iteration and Maintenance", "baseFileName": "wp-14-benefit-accelerated-iteration", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-15-benefit-scaling-expertise": {
      "path": "", "prompt": "A manager and a new employee stand in a sustainable, solarpunk-style office. They are reviewing a 'PROJECT KNOWLEDGE GRAPH' on a large, transparent screen, specifically looking at 'CYCLE C-138: AFTER-ACTION REVIEW.' The tagline reads 'Every Decision, a Lesson. Every Action, an Asset.'", "alt": "Scaling Expertise and Ensuring Auditability", "baseFileName": "wp-15-benefit-scaling-expertise", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-16-use-case-spotlight": {
      "path": "", "prompt": "A split-screen comparison. On the left, 'TRADITIONAL WORKFLOW (WEEKS),' a frustrated analyst is buried in paperwork under dim lighting. On the right, 'DCE WORKFLOW (HOURS),' a confident professional uses a futuristic, glowing interface to complete the same task in a fraction of the time, with a timer showing '00:03:45'.", "alt": "Use Case Spotlight: Rapid Development", "baseFileName": "wp-16-use-case-spotlight", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-17-use-case-traditional": {
      "path": "", "prompt": "A dark, cluttered office representing 'THE DRUDGERY OF MANUAL REVISION.' An analyst is surrounded by towering stacks of paper, manually searching and editing files under the oppressive flowchart of a 'BUREAUCRATIC REVIEW PROCESS' displayed on a monitor.", "alt": "The Traditional Workflow (Weeks)", "baseFileName": "wp-17-use-case-traditional", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-18-use-case-dce": {
      "path": "", "prompt": "A clean, futuristic interface showing 'The Agility of Instant Feedback.' An operator touches a screen, progressing through a simple three-step process: '1. CURATE,' '2. AUTOMATE,' and '3. REVIEW & ACCEPT.' The final step shows a diff view with a green 'Accept' button being pressed.", "alt": "The DCE Workflow (Hours)", "baseFileName": "wp-18-use-case-dce", "fileExtension": ".webp", "imageCount": 1
    },
    "group_wp-19-conclusion": {
      "path": "", "prompt": "A sleek, futuristic spacecraft, representing the organization's mission, is shown accelerating to light speed, leaving a trail of light. The tagline reads 'ACHIEVING THE MISSION AT THE SPEED OF THOUGHT.' A glowing 'PERSISTENT KNOWLEDGE GRAPH' is shown as the engine powering this acceleration.", "alt": "Conclusion", "baseFileName": "wp-19-conclusion", "fileExtension": ".webp", "imageCount": 1
    }
  }
}
</file_artifact>

<file path="scripts/generate_images.mjs">
// scripts/generate_images.mjs
import fs from 'fs/promises';
import path from 'path';
import dotenv from 'dotenv';
import { GoogleGenAI } from '@google/genai'; // ✅ New SDK class name

dotenv.config();

// --- USER CONFIGURATION ---
const CONFIG = {
  // 'career_transitioner', 'underequipped_graduate', or 'young_precocious'
  persona: 'career_transitioner',
  // Page to generate for
  pageId: 'lesson-1.1-p1',
  // How many images to generate for this page (1–4 supported by Imagen)
  imageCount: 2,
  // To run a full module (1–4), uncomment and set this, it will make 1 image per page.
  // moduleNumber: 1,
};
// --- END CONFIG ---

// Accept GEMINI_API_KEY or GOOGLE_API_KEY (new SDK) and fall back to API_KEY (your current var)
const API_KEY = process.env.GEMINI_API_KEY ?? process.env.GOOGLE_API_KEY ?? process.env.API_KEY;

// Use the current Imagen model name from the GenAI SDK docs
// Tip: 'imagen-4.0-generate-001' supports numberOfImages/aspectRatio/imageSize
const MODEL_NAME = 'imagen-4.0-generate-001';

const OUTPUT_DIR_BASE = path.resolve(process.cwd(), 'public');

// --- HELPERS ---
async function loadJsonData(filePath) {
  try {
    const fileContent = await fs.readFile(filePath, 'utf-8');
    return JSON.parse(fileContent);
  } catch (error) {
    console.error(`Error loading JSON data from ${filePath}:`, error);
    throw error;
  }
}

async function loadArtifact(artifactPath) {
  try {
    return await fs.readFile(artifactPath, 'utf-8');
  } catch (error) {
    console.error(`Error loading artifact from ${artifactPath}:`, error);
    throw error;
  }
}

function findPageById(curriculumData, pageId) {
  for (const section of curriculumData.sections) {
    const foundPage = section.pages.find((p) => p.pageId === pageId);
    if (foundPage) return foundPage;
  }
  return null;
}

function constructFinalPrompt(systemPrompt, pageContent, imagePrompt) {
  const trainingContent = `
<Training Content>
Page Title: ${pageContent.pageTitle}
TL;DR: ${pageContent.tldr}
Content: ${pageContent.content}
</Training Content>
  `;
  return `${systemPrompt}\n\n${trainingContent}\n\n<Image Prompt>\n${imagePrompt}\n</Image Prompt>`;
}

async function generateAndSaveImages(persona, pageId, imageCount = 1) {
  console.log(`🚀 Processing page: '${pageId}' for persona: '${persona}' (${imageCount} image(s))`);

  // 1) Load all necessary data
  const manifestPath = path.resolve(process.cwd(), 'public/data', `imagemanifest_${persona}.json`);
  const curriculumPath = path.resolve(process.cwd(), 'public/data', `v2v_content_${persona}.json`);
  const systemPromptPath = path.resolve(process.cwd(), 'src/Artifacts', 'A75 - V2V Academy - Persona Image System Prompt.md');

  const imageManifest = await loadJsonData(manifestPath);
  const curriculumData = await loadJsonData(curriculumPath);
  const systemPrompt = await loadArtifact(systemPromptPath);

  // 2) Page + image group
  const pageContent = findPageById(curriculumData, pageId);
  if (!pageContent) {
    throw new Error(`Could not find page content for pageId '${pageId}'.`);
  }

  const imageGroupId = pageContent.imageGroupIds;
  if (!imageGroupId) throw new Error(`No imageGroupId found for pageId '${pageId}'.`);

  const groupMeta = imageManifest.imageGroups[imageGroupId];
  if (!groupMeta) throw new Error(`Could not find image group metadata for groupId '${imageGroupId}'.`);

  // 3) Prompt (single string for Imagen)
  const finalPrompt = constructFinalPrompt(systemPrompt, pageContent, groupMeta.prompt);

  // 4) Initialize client (new GenAI SDK)
  const ai = new GoogleGenAI({ apiKey: API_KEY });

  // 5) Generate images (one call returns multiple images)
  console.log(`   Calling Imagen model '${MODEL_NAME}' for ${imageCount} image(s)...`);
  const response = await ai.models.generateImages({
    model: MODEL_NAME,
    prompt: finalPrompt,
    // You can add aspectRatio: "16:9" or imageSize: "1K"|"2K" if desired
    config: {
      numberOfImages: Math.max(1, Math.min(4, Number(imageCount) || 1)),
    },
  });

  if (!response?.generatedImages?.length) {
    console.error('API Response (no images):', JSON.stringify(response, null, 2));
    throw new Error('No images returned by the API.');
  }

  // 6) Persist outputs
  const outputDirPath = path.join(
    OUTPUT_DIR_BASE,
    groupMeta.path.replace('/assets/images/v2v/', 'assets/images/v2v/')
  );
  await fs.mkdir(outputDirPath, { recursive: true });

  let saved = 0;
  for (let i = 0; i < response.generatedImages.length; i++) {
    const img = response.generatedImages[i];
    const bytes = img?.image?.imageBytes;
    if (!bytes) {
      console.warn(`   ⚠️ Skipping image ${i + 1}: missing image bytes`);
      continue;
    }
    const outputFileName = `${groupMeta.baseFileName}${i + 1}${groupMeta.fileExtension}`;
    const outputPath = path.join(outputDirPath, outputFileName);

    console.log(`   Saving image ${i + 1} → ${outputPath}`);
    await fs.writeFile(outputPath, Buffer.from(bytes, 'base64'));
    saved++;
  }

  if (saved === 0) {
    throw new Error('Generation returned images but none had image bytes to save.');
  }

  console.log(`✅ Saved ${saved}/${response.generatedImages.length} image(s) for '${pageId}'.`);
}

// --- MAIN ---
async function main() {
  if (!API_KEY) {
    console.error('Error: API key not found. Set GEMINI_API_KEY (or GOOGLE_API_KEY / API_KEY) in your .env');
    process.exit(1);
  }

  const { persona, pageId, imageCount, moduleNumber } = CONFIG;

  try {
    if (moduleNumber && [1, 2, 3, 4].includes(moduleNumber)) {
      console.log(`🚀 Starting BATCH image generation for persona: '${persona}', module: ${moduleNumber}`);

      const curriculumPath = path.resolve(process.cwd(), 'public/data', `v2v_content_${persona}.json`);
      const curriculumData = await loadJsonData(curriculumPath);
      const sectionId = `module-${moduleNumber}`;
      const section = curriculumData.sections.find((s) => s.sectionId === sectionId);

      if (!section) throw new Error(`Could not find module ${moduleNumber} for persona '${persona}'.`);

      const pageIds = section.pages.map((p) => p.pageId);
      console.log(`   Found ${pageIds.length} pages to process for Module ${moduleNumber}.`);

      for (const id of pageIds) {
        await generateAndSaveImages(persona, id, 1); // 1 image per page in batch mode
        await new Promise((resolve) => setTimeout(resolve, 1000));
      }
      console.log(`\n🎉 Batch generation for Module ${moduleNumber} complete!`);
    } else {
      if (!persona || !pageId) throw new Error('`persona` and `pageId` must be set in the CONFIG object.');
      if (isNaN(imageCount) || imageCount < 1) throw new Error('Invalid imageCount. Must be a positive number.');
      await generateAndSaveImages(persona, pageId, imageCount);
    }
  } catch (error) {
    console.error('❌ An error occurred during image generation:', error?.message ?? error);
    process.exit(1);
  }
}

main();
</file_artifact>

<file path="scripts/image_harness.mjs">
// scripts/image_harness.mjs
import fs from 'fs/promises';
import path from 'path';
import dotenv from 'dotenv';
import { GoogleGenAI } from '@google/genai';

dotenv.config();

// ====== CONFIG ======
const API_KEY = process.env.GEMINI_API_KEY ?? process.env.GOOGLE_API_KEY ?? process.env.API_KEY;
const MODEL_NAME = 'imagen-4.0-generate-001';
const OUTPUT_DIR = path.resolve(process.cwd(), 'public/assets/images/v2v/test_harness');

// Tweak these to match the AI Studio renders
const ASPECT_RATIO = '16:9';
const IMAGE_SIZE = '2K';
const SLEEP_MS = 1200;

// ====== PROMPT SETS (bespoke long paragraphs) ======
const PROMPT_SETS = {
  'career-transitioner-loop': [
    // P1 — Editorial night skyline, hero ring
    `A hyper-realistic editorial photograph at dusk inside a high-floor office with a panoramic city skyline behind glass; a confident professional stands three-quarter to camera, hand mid-gesture inside a single elegant circular interface that floats at chest height, divided into four clean quadrants labeled "CURATION", "PARALLEL PROMPTING", "VALIDATION", "INTEGRATION"; the circle is thin-lined, cyan-teal accents with precise tick marks and a faint orbit of particles, not neon; lighting is cinematic with a cool key from the windows and a warm rim from practical lamps; 50 mm lens at f/2.8 for shallow depth, natural skin tone, premium fabrics; composition places the ring slightly left of center and the subject right of center; add a tasteful title at the top reading "THE VIRTUOSO’S LOOP" and a lower-left module tag "MODULE 1: THE VIRTUOSO’S LOOP — LESSON 1.1: THE PROFESSIONAL’S PLAYBOOK"; no extra icons, no thick arrows, no sci-fi consoles, keep the scene minimal and believable.`,
    // P2 — Daylight boardroom, senior leader
    `A hyper-realistic daylight boardroom with matte concrete and a wide view of the city; a senior leader gestures to a refined circular workflow overlay hovering in front of them, divided into four equal segments labeled "CURATION", "PARALLEL PROMPTING", "VALIDATION", "INTEGRATION"; the ring uses thin concentric strokes and subtle radial ticks, cyan accents on a graphite palette; soft window light wraps the subject, subtle rim separates them from the background; 35 mm lens, shoulder-level angle; add the title "THE VIRTUOSO’S LOOP" centered above and a small lower-left tag "MODULE 1: THE VIRTUOSO’S LOOP — LESSON 1.1: THE PROFESSIONAL’S PLAYBOOK"; realistic glass reflections, restrained glow, absolutely no clutter or extra UI panels.`,
    // P3 — Minimal standing desk, linear bead trail
    `A hyper-realistic minimal mid-shot of a professional at a white standing desk in a quiet room; a small precise circular control with three cyan nodes sits in front of their fingertip; a subtle dotted bead-trail runs horizontally linking the four labels "CURATION" · "PARALLEL PROMPTING" · "VALIDATION" · "INTEGRATION", with the finger hovering over the central control; lighting is moody and photographic, shallow depth isolates the gesture; no extra graphics besides these words and the tiny circle; premium realism, no neon, no 3D sci-fi chrome—just elegant restraint.`,
    // P4 — Projected loop on glass wall
    `In a hyper-realistic glass-walled meeting room, a professional points to a projected circular diagram on the glass—thin strokes, consistent vector weights, labeled exactly "CURATION", "PARALLEL PROMPTING", "VALIDATION", "INTEGRATION"; small arrows imply clockwise flow without thick swooshes; desaturated neutral palette with a hint of cyan; 50 mm lens, clean reflections, natural daylight; no extra icons or text, no garish glow—this reads like a real photo of a real diagram.`,
    // P5 — Tabletop macro with laptop screen
    `A hyper-realistic premium laptop on a walnut desk fills the frame; on the screen, a crisp circular workflow graphic divided into four with labels "CURATION", "PARALLEL PROMPTING", "VALIDATION", "INTEGRATION"; thin cyan strokes on a dark graphite UI; a hand enters frame to rotate the loop on the trackpad; morning window light reveals wood grain and brushed aluminum; shallow depth, realistic materials, no additional charts or clip-art.`,
    // P6 — Studio portrait with ring front-and-center
    `A hyper-realistic studio-lit portrait of a professional against a deep charcoal backdrop; the circular interface floats directly between camera and subject so the face is partly visible through the ring; thin cyan lines, tiny radial dots, quadrant labels "CURATION", "PARALLEL PROMPTING", "VALIDATION", "INTEGRATION"; large soft key, faint kicker, subtle film grain; composition is perfectly balanced and minimal; absolutely no extra UI blocks, no icons, no stock-photo graphs.`
  ],
  'underequipped-grad-hired': [
    // U1 — Interview table, glowing resume line
    `Inside a hyper-realistic interview room with a round table and frosted glass, a young graduate sits upright while a hiring manager leans forward, pen above a printed resume; a single line on the page glows subtly, exactly "Proficient in Data Curation & Context Engineering"; the manager’s expression shows impressed approval, the graduate allows a small relieved smile; natural window light with a warm desk lamp, 85 mm portrait at f/2, shallow depth, warm neutrals; no extra UI graphics, no neon—just that one glowing line.`,
    // U2 — Tablet review, tasteful underline
    `A hyper-realistic mid-shot over a table: the hiring manager and graduate look at a tablet; one line in the digital resume is highlighted with a thin cyan underline reading "Proficient in Data Curation & Context Engineering"; realistic reflections in the glass, restrained typography, clean UI; cinematic color grade, 50 mm lens, believable office; no added charts or icons.`,
    // U3 — Post-interview corridor, implied success
    `A hyper-realistic candid photograph in a sunlit corridor after the interview: the graduate steps out, exhales, and smiles; their folder peeks a printed resume where a faint highlight still marks the line "Proficient in Data Curation & Context Engineering"; background bokeh shows the meeting room; natural light, soft halation, human warmth; no overlays, no UI—just storytelling.`,
    // U4 — Whiteboard recap
    `A hyper-realistic manager stands by a whiteboard listing short bullet points; one neat line is boxed and reads "Proficient in Data Curation & Context Engineering"; the graduate stands nearby in frame, hopeful; daylight key, subtle rim, neutral palette, 35 mm lens; no extra diagrams or icons; this feels like a documentary still.`
  ]
};

// ====== CASES TO RUN ======
const TEST_CASES = [
  { key: 'career-transitioner-loop' },
  { key: 'underequipped-grad-hired' }
];

// ====== CORE ======
async function sleep(ms) { return new Promise(r => setTimeout(r, ms)); }

async function generateOne(ai, caseKey, variantIndex, prompt) {
  const response = await ai.models.generateImages({
    model: MODEL_NAME,
    prompt,
    config: { numberOfImages: 1, aspectRatio: ASPECT_RATIO, imageSize: IMAGE_SIZE }
  });

  if (!response?.generatedImages?.length) throw new Error('No images returned by the API.');
  const bytes = response.generatedImages[0]?.image?.imageBytes;
  if (!bytes) throw new Error('Image bytes missing in response.');

  const base = `${caseKey}--v${String(variantIndex + 1).padStart(2, '0')}`;
  const imgPath = path.join(OUTPUT_DIR, `${base}.png`);
  const txtPath = path.join(OUTPUT_DIR, `${base}.prompt.txt`);

  await fs.writeFile(imgPath, Buffer.from(bytes, 'base64'));
  await fs.writeFile(txtPath, prompt, 'utf-8');

  return { imgPath, txtPath };
}

// ====== MAIN ======
async function main() {
  if (!API_KEY) {
    console.error('Error: API key not found. Set GEMINI_API_KEY (or GOOGLE_API_KEY / API_KEY) in your .env');
    process.exit(1);
  }

  await fs.mkdir(OUTPUT_DIR, { recursive: true });
  const ai = new GoogleGenAI({ apiKey: API_KEY });

  console.log('--- Imagen Prompt Lab (bespoke paragraphs) ---');

  for (const tc of TEST_CASES) {
    const prompts = PROMPT_SETS[tc.key];
    if (!prompts?.length) {
      console.warn(`⚠️ No prompts for case '${tc.key}', skipping.`);
      continue;
    }

    console.log(`\n▶ Case: ${tc.key} (${prompts.length} variants)`);
    for (let i = 0; i < prompts.length; i++) {
      const p = prompts[i];
      try {
        console.log(`   • Variant ${i + 1}/${prompts.length}…`);
        const { imgPath, txtPath } = await generateOne(ai, tc.key, i, p);
        console.log(`     ✅ Saved image: ${imgPath}`);
        console.log(`     📝 Saved prompt: ${txtPath}`);
      } catch (err) {
        console.error(`     ❌ Variant ${i + 1} failed: ${err?.message ?? err}`);
      }
      await sleep(SLEEP_MS);
    }
  }

  console.log('\n--- Done. Check outputs in:', OUTPUT_DIR);
}

main();
</file_artifact>

<file path="scripts/manage_v2v_images.mjs">
import fs from 'fs/promises';
import path from 'path';

const V2V_IMAGE_BASE_PATH = path.resolve(process.cwd(), 'public/assets/images/v2v');
const V2V_DATA_BASE_PATH = path.resolve(process.cwd(), 'public/data');
const PERSONAS = ['career_transitioner', 'underequipped_graduate', 'young_precocious'];
const IMAGE_EXTENSIONS = ['.png', '.webp', '.jpg', '.jpeg'];

/**
 * Reads the content file for a given persona and returns a list of all page IDs.
 * @param {string} persona - The persona identifier.
 * @returns {Promise<string[]>} A list of all page IDs for the curriculum.
 */
async function getAllPageIdsForPersona(persona) {
    const filePath = path.join(V2V_DATA_BASE_PATH, `v2v_content_${persona}.json`);
    try {
        const fileContent = await fs.readFile(filePath, 'utf-8');
        const data = JSON.parse(fileContent);
        const pageIds = data.sections.flatMap(section => section.pages.map(page => page.pageId));
        return pageIds;
    } catch (error) {
        console.error(`❌ Error reading or parsing content for persona '${persona}':`, error);
        return [];
    }
}

/**
 * Reads, updates, and writes the image manifest for a persona.
 * @param {string} persona - The persona identifier.
 * @param {Map<string, number>} imageCounts - A map of pageId to image count.
 */
async function updateImageManifest(persona, imageCounts) {
    const manifestPath = path.join(V2V_DATA_BASE_PATH, `imagemanifest_${persona}.json`);
    console.log(`\n   Updating manifest: ${path.basename(manifestPath)}`);
    try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const manifestData = JSON.parse(manifestContent);
        
        let updatedCount = 0;
        for (const [pageId, count] of imageCounts.entries()) {
            // Find the imageGroupId associated with this pageId
            const curriculumPath = path.join(V2V_DATA_BASE_PATH, `v2v_content_${persona}.json`);
            const curriculumContent = await fs.readFile(curriculumPath, 'utf-8');
            const curriculumData = JSON.parse(curriculumContent);
            
            let imageGroupId = null;
            for(const section of curriculumData.sections) {
                const page = section.pages.find(p => p.pageId === pageId);
                if (page && page.imageGroupIds && page.imageGroupIds.length > 0) {
                    imageGroupId = page.imageGroupIds;
                    break;
                }
            }

            if (imageGroupId && manifestData.imageGroups[imageGroupId]) {
                if (manifestData.imageGroups[imageGroupId].imageCount !== count) {
                    manifestData.imageGroups[imageGroupId].imageCount = count;
                    console.log(`     - Updated '${imageGroupId}' imageCount to ${count}`);
                    updatedCount++;
                }
            } else {
                console.warn(`     - ⚠️ Could not find imageGroup for pageId '${pageId}' in manifest.`);
            }
        }

        if (updatedCount > 0) {
            await fs.writeFile(manifestPath, JSON.stringify(manifestData, null, 2), 'utf-8');
            console.log(`   ✅ Successfully updated and saved manifest with ${updatedCount} changes.`);
        } else {
            console.log(`   - No changes needed for manifest.`);
        }

    } catch (error) {
        console.error(`   ❌ Error updating manifest for '${persona}':`, error);
    }
}


/**
 * Ensures directories exist and renames files within them.
 */
async function processImages() {
    console.log('🚀 Starting V2V image management script...');

    for (const persona of PERSONAS) {
        console.log(`\nProcessing persona: ${persona}`);
        const pageIds = await getAllPageIdsForPersona(persona);
        const imageCounts = new Map();

        if (pageIds.length === 0) {
            console.warn(`   ⚠️ No page IDs found for '${persona}'. Skipping.`);
            continue;
        }

        for (const pageId of pageIds) {
            const pageDir = path.join(V2V_IMAGE_BASE_PATH, persona, pageId);

            try {
                await fs.mkdir(pageDir, { recursive: true });
            } catch (error) {
                console.error(`   ❌ Error creating directory '${pageDir}':`, error);
                continue;
            }

            try {
                const files = await fs.readdir(pageDir);
                const imageFiles = files.filter(file => IMAGE_EXTENSIONS.includes(path.extname(file).toLowerCase()));

                const filesToRename = imageFiles.filter(file => !file.match(`^${pageId}-img-\\d+\\..+$`));
                const existingIndices = imageFiles
                    .map(file => file.match(`^${pageId}-img-(\\d+)\\..+$`))
                    .filter(Boolean)
                    .map(match => parseInt(match, 10));

                let maxIndex = existingIndices.length > 0 ? Math.max(...existingIndices) : 0;
                let finalImageCount = existingIndices.length;

                if (filesToRename.length > 0) {
                    console.log(`   - Renaming ${filesToRename.length} file(s) in '${path.relative(process.cwd(), pageDir)}'`);
                    filesToRename.sort();

                    for (const file of filesToRename) {
                        maxIndex++;
                        finalImageCount++;
                        const oldPath = path.join(pageDir, file);
                        const newName = `${pageId}-img-${maxIndex}${path.extname(file)}`;
                        const newPath = path.join(pageDir, newName);
                        await fs.rename(oldPath, newPath);
                        console.log(`     ✅ Renamed '${file}' to '${newName}'`);
                    }
                }
                
                imageCounts.set(pageId, finalImageCount);

            } catch (error) {
                console.error(`   ❌ Error processing files in '${pageDir}':`, error);
            }
        }
        // After processing all pages for a persona, update its manifest
        await updateImageManifest(persona, imageCounts);
    }

    console.log('\n🎉 Image management script finished.');
}

processImages();
</file_artifact>

<file path="src/app/academy/page.tsx">
'use client';
import React, { useState, useEffect } from 'react';
import PersonaSelector from '@/components/academy/PersonaSelector';
import ReportViewer from '@/components/report-viewer/ReportViewer';
import { useReportStore } from '@/stores/reportStore';
import type { ReportContentData, ImageManifestData } from '@/stores/reportStore';
import { Card, CardContent, CardHeader, CardTitle, CardDescription } from '@/components/ui/card';
import { motion } from 'framer-motion';
import Image from 'next/image';
import { LampContainer } from '@/components/global/lamp';
import { Button } from '@/components/ui/button';

const AcademyPage = () => {
    const [selection, setSelection] = useState<{ type: 'persona' | 'lab'; id: string } | null>(null);
    const [isLoading, setIsLoading] = useState(false);
    const { loadReport } = useReportStore.getState();

    useEffect(() => {
        if (selection) {
            const loadData = async () => {
                setIsLoading(true);
                try {
                    const contentFile = selection.type === 'persona' 
                        ? `v2v_content_${selection.id.replace('v2v-academy-','')}.json`
                        : `v2v_lab_1_portfolio.json`;
                    
                    const manifestFile = selection.type === 'persona'
                        ? `imagemanifest_${selection.id.replace('v2v-academy-','')}.json`
                        : `imagemanifest_lab_1_portfolio.json`;

                    const [contentRes, manifestRes] = await Promise.all([
                        fetch(`/data/${contentFile}`),
                        fetch(`/data/${manifestFile}`)
                    ]);

                    if (!contentRes.ok) throw new Error(`Failed to fetch content for ${selection.id}`);
                    if (!manifestRes.ok) throw new Error(`Failed to fetch image manifest for ${selection.id}`);

                    const reportData: ReportContentData = await contentRes.json();
                    const imageManifest: ImageManifestData = await manifestRes.json();

                    loadReport(reportData, imageManifest);

                } catch (error) {
                    console.error("Failed to load academy data:", error);
                } finally {
                    setIsLoading(false);
                }
            };
            loadData();
        }
    }, [selection, loadReport]);

    if (isLoading) {
        return (
            <div className="flex items-center justify-center h-screen w-full pt-16">
                <p className="text-2xl text-muted-foreground animate-pulse">Loading Content...</p>
            </div>
        );
    }

    if (selection) {
        return (
            <div className="h-screen w-full pt-16 flex flex-col">
                <ReportViewer reportName={selection.id} />
            </div>
        );
    }

    return (
        <div className="flex flex-col items-center justify-start min-h-screen container mx-auto px-4 py-16 pt-32">
            
            {/* V2V Pathway Section */}
            <motion.div
                initial={{ opacity: 0, y: -20 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ duration: 0.5 }}
                className="text-center mb-12 w-full"
            >
                <h1 className="text-4xl md:text-6xl font-bold text-center mb-4 bg-clip-text text-transparent bg-gradient-to-b from-white to-neutral-600">
                    The V2V Academy
                </h1>
                <p className="text-xl text-muted-foreground max-w-3xl mx-auto">
                    To personalize your learning journey, please choose the path that best describes you.
                </p>
            </motion.div>
            <PersonaSelector onSelectPersona={(id) => setSelection({ type: 'persona', id: `v2v-academy-${id}` })} />

            {/* Labs & Projects Section */}
            <motion.div
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ duration: 0.5, delay: 0.8 }}
                className="text-center my-20 w-full"
            >
                <h2 className="text-3xl md:text-5xl font-bold text-center mb-4 bg-clip-text text-transparent bg-gradient-to-b from-white to-neutral-600">
                    Labs & Courses
                </h2>
                <p className="text-lg text-muted-foreground max-w-3xl mx-auto">
                    Apply your knowledge with hands-on, project-based learning.
                </p>
            </motion.div>

            <motion.div 
                className="w-full max-w-6xl mb-20"
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ duration: 0.5, delay: 1.0 }}
            >
                <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                     <Card
                        className="h-full flex flex-col hover:bg-accent hover:border-primary transition-all cursor-pointer group"
                        onClick={() => setSelection({ type: 'lab', id: 'v2v-academy-lab-1-portfolio' })}
                    >
                        <CardHeader className="p-0">
                             <div className="relative aspect-video w-full">
                                <Image
                                    src="/assets/images/v2v/lab_1_thumbnail.webp"
                                    alt="Lab 1: Your First Portfolio"
                                    fill
                                    className="object-cover rounded-t-lg transition-transform group-hover:scale-105"
                                />
                            </div>
                            <div className='p-6 text-center'>
                                <CardTitle>Lab 1: Your First Portfolio</CardTitle>
                            </div>
                        </CardHeader>
                        <CardContent className="flex-grow text-center pt-0">
                            <CardDescription>Go from an empty folder to a running portfolio website and learn the complete, end-to-end workflow of the Data Curation Environment.</CardDescription>
                        </CardContent>
                    </Card>
                    
                    {/* Coming Soon Course Card */}
                    <Card
                        className="h-full flex flex-col bg-muted/20 border-dashed relative overflow-hidden"
                    >
                        <CardHeader className="p-0">
                             <div className="relative aspect-video w-full">
                                <Image
                                    src="/assets/images/v2v/course_1_thumbnail.webp"
                                    alt="Course 1: The AI-Powered Report Viewer"
                                    fill
                                    className="object-cover rounded-t-lg opacity-50"
                                />
                            </div>
                            <div className='p-6 text-center'>
                                <CardTitle className="text-muted-foreground">Course 1: The AI-Native Application</CardTitle>
                            </div>
                        </CardHeader>
                        <CardContent className="flex-grow text-center pt-0">
                            <CardDescription>A comprehensive course on building a full-stack, AI-native application—the Report Viewer—from scratch using the V2V workflow.</CardDescription>
                        </CardContent>
                         <div className="absolute top-2 right-2 bg-primary text-primary-foreground text-xs font-bold px-2 py-1 rounded-full">
                            COMING SOON
                        </div>
                    </Card>
                </div>
            </motion.div>

            <section className="w-full mt-24">
                <LampContainer>
                    <motion.div
                        initial={{ opacity: 0.5, y: 100 }}
                        whileInView={{ opacity: 1, y: 0 }}
                        transition={{
                            delay: 0.3,
                            duration: 0.8,
                            ease: 'easeInOut',
                        }}
                        className="flex flex-col items-center text-center"
                    >
                        <h2 className="mt-8 bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground text-center text-3xl font-bold tracking-tight md:text-5xl">
                            Ready to Become a Citizen Architect?
                        </h2>
                        <p className="text-lg text-muted-foreground max-w-3xl text-center my-8">
                            Create an account to track your progress, access exclusive content, and join a community of builders shaping the future of AI.
                        </p>
                        <Button size="lg" variant="outline" className='text-lg' disabled>
                            Create Account (Coming Soon)
                        </Button>
                    </motion.div>
                </LampContainer>
            </section>
        </div>
    );
};

export default AcademyPage;
</file_artifact>

<file path="src/app/api/chat/route.ts">
import { NextResponse } from 'next/server';
import { Index } from 'faiss-node';
import { promises as fs } from 'fs';
import path from 'path';

/**
 * Gets a vector embedding for a single text chunk from the local API.
 */
async function getEmbedding(text: string, embeddingUrl: string): Promise<number[] | null> {
    console.log(`[Chat API] Requesting embedding for text (length: ${text.length}): "${text.substring(0, 100)}..."`);
    try {
        const response = await fetch(embeddingUrl, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model: 'text-embedding-granite-embedding-278m-multilingual',
                input: text,
            }),
        });

        const rawText = await response.text(); // Get raw text first for robust logging

        if (!response.ok) {
            console.error(`[Chat API] Embedding API error: ${response.status}`, rawText);
            return null;
        }
        // console.log(`[Chat API] Raw embedding response text:`, rawText); // C4: Suppressed verbose log

        const data = JSON.parse(rawText); // Parse manually after logging

        if (data?.data?.[0]?.embedding) {
            // console.log(`[Chat API] Successfully extracted embedding vector from standard structure.`); // C4: Suppressed
            return data.data[0].embedding;
        }
        
        if (data?.data?.embedding) {
             // console.log(`[Chat API] Successfully extracted embedding vector from alternate structure.`); // C4: Suppressed
             return data.data.embedding;
        }
        if (data?.embedding) {
            // console.log(`[Chat API] Successfully extracted embedding vector from root structure.`); // C4: Suppressed
            return data.embedding;
        }

        console.error('[Chat API] Invalid embedding response structure. Full response object:', data);
        return null;
    } catch (error: any) {
        // C4: Suppress full stack trace for connection errors, just log a warning
        if (error.cause && (error.cause.code === 'ECONNREFUSED' || error.cause.code === 'UND_ERR_CONNECT_TIMEOUT')) {
             console.warn(`[Chat API] Embedding API unavailable: ${error.message}`);
        } else {
             console.error(`[Chat API] Failed to get embedding for query. Error name: ${error.name}, message: ${error.message}`);
        }
        return null;
    }
}

/**
 * Performs a RAG lookup against a specified knowledge base.
 */
async function performRagLookup(query: string, kbIdentifier: string, embeddingUrl: string, k: number): Promise<{ retrievedContext: string; retrievedDocsLog: string; }> {
    let retrievedContext = '';
    let retrievedDocsLog = 'No documents retrieved.';
    try {
        // C6 Update: Handle Anguilla report filename mismatch
        let filenamePrefix = kbIdentifier;
        if (kbIdentifier === 'anguilla') {
            filenamePrefix = 'anguilla_report';
        }

        const faissFile = `${filenamePrefix}_faiss.index`;
        const chunksFile = `${filenamePrefix}_chunks.json`;

        const publicPath = path.join(process.cwd(), 'public');
        const faissPath = path.join(publicPath, 'data', 'embeddings', faissFile);
        const chunksPath = path.join(publicPath, 'data', 'embeddings', chunksFile);

        const faissExists = await fs.stat(faissPath).then(() => true).catch(() => false);
        const chunksExist = await fs.stat(chunksPath).then(() => true).catch(() => false);

        if (!faissExists || !chunksExist) {
            // C4: Reduced to warning to avoid flooding logs during dev
            console.warn(`[Chat API] Embeddings not found for '${kbIdentifier}' (checked ${filenamePrefix}*). Skipping RAG.`);
            return { retrievedContext: '', retrievedDocsLog: 'Embeddings not found.' };
        }

        const index = Index.read(faissPath);
        const chunks = JSON.parse(await fs.readFile(chunksPath, 'utf-8'));
        const queryEmbedding = await getEmbedding(query, embeddingUrl);

        if (queryEmbedding && index.getDimension() === queryEmbedding.length) {
            const { labels, distances } = index.search(queryEmbedding, k);
            if (labels.length > 0) {
                const results = labels.map((labelIndex: number) => chunks[labelIndex]?.chunk).filter(Boolean);
                retrievedContext = results.join('\n\n---\n\n');
                retrievedDocsLog = `Retrieved ${results.length} documents from '${kbIdentifier}' KB:\n${results.map((doc, i) => `  Doc ${i + 1} (Dist: ${distances[i].toFixed(4)}): "${doc.substring(0, 80)}..."`).join('\n')}`;
            }
        } else if (!queryEmbedding) {
            throw new Error("Could not generate embedding for the query.");
        } else {
            throw new Error(`Embedding dimension mismatch. Index: ${index.getDimension()}, Query: ${queryEmbedding.length}. Please regenerate embeddings.`);
        }
    } catch (error: any) {
        // C4: Suppress RAG errors if it's just connection issues
        if (error.message.includes('Could not generate embedding')) {
             console.warn(`[Chat API] RAG skipped: Embedding generation failed.`);
        } else {
             console.error(`[Chat API] RAG Error for '${kbIdentifier}' KB:`, error);
        }
        retrievedContext = `RAG system failed: ${error.message}.`;
        retrievedDocsLog = `RAG Error: ${error.message}`;
    }
    return { retrievedContext, retrievedDocsLog };
}


const markdownFormattingInstruction = `
Use standard GitHub Flavored Markdown for all formatting.
- For lists, use compact formatting. The content must be on the same line as the bullet or number. For example, write "- First item" and NOT "-
First item".
- For inline code, use single backticks, for example: \`DCE.vsix\`. Do not add blank lines before or after inline code.
- For multi-line code blocks, use triple backticks with a language identifier.
- For tables, use standard markdown table syntax with pipes and hyphens. Do not use HTML tags like <br> inside tables; use markdown newlines if necessary and supported by the renderer.
- Avoid using HTML tags like <kbd>. Use markdown alternatives, like backticks for commands.
`;

const systemPrompts = {
    dce: `You are @Ascentia, an AI guide for the aiascent.dev website. Your purpose is to answer questions about the Data Curation Environment (DCE), the 'Citizen Architect' methodology, and the 'Process as Asset' whitepaper.

Your answers should be based *only* on the provided context chunks from the project's official documentation. Be helpful, encouraging, and aim to increase the user's understanding of the project.

If the answer isn't directly in the context, state that, but still try to provide related information if available. Use markdown for formatting as described below to enhance clarity. Do not invent information.
${markdownFormattingInstruction}`,
    report: `You are @Ascentia, an AI guide for "The Ascent Report" on the aiascent.dev website. Your purpose is to act as a subject matter expert, answering questions based *only* on the provided context from the report. The report covers topics like the AI industry's labor model, the 'fissured workplace,' cognitive security (COGSEC), and geopolitical strategy.

Your answers must be grounded in the provided context chunks. Be helpful, concise, and stay on topic.

If the answer isn't directly in the context, state that, but you can offer to discuss related concepts that *are* in the context. Use simple markdown for formatting as described below. Do not invent information or use outside knowledge.
${markdownFormattingInstruction}`,
    academy: `You are @Ascentia, an AI guide for the V2V Academy on aiascent.dev. Your purpose is to answer questions about the "Vibecoding to Virtuosity" curriculum, its lessons, and the core concepts of AI-assisted development it teaches.

Your answers must be based *only* on the provided context chunks from the V2V Academy's official curriculum. Be helpful, encouraging, and aim to clarify concepts for the learner.

If the answer isn't directly in the context, state that, but you can guide the user to the relevant lesson if you can infer it. Use markdown for formatting to enhance clarity. Do not invent information.
${markdownFormattingInstruction}`,
    anguilla: `You are @Ascentia, an AI strategic advisor for the Anguilla Project. Your purpose is to answer questions about the proposal to transform Anguilla into an AI-Native Nation, covering topics like Sovereign Infrastructure, Cognitive Capital, and the Automated State.

Your answers must be based *only* on the provided context chunks from the project proposals. Be helpful, professional, and persuasive.

If the answer isn't directly in the context, state that. Use markdown for formatting as described below. Do not invent information.
${markdownFormattingInstruction}`,
};

// C89: New persona-aware suggestion prompts
const suggestionSystemPrompts = {
    page: {
        default: `Your ONLY task is to analyze the following text from a document and generate 2-4 insightful follow-up questions a user might ask to learn more. Your questions should be deeper, drawing connections between the original page content and the extra context provided. Respond ONLY with a valid JSON array of strings. Do not include any other text, explanation, or markdown formatting. Your entire response must be parseable as JSON.

Example of a PERFECT response:
["What is the main benefit of this feature?", "How does this compare to other methods?"]`,
        career_transitioner: `You are an AI assistant helping a career-transitioning professional. Analyze the following lesson content and generate 2-4 insightful questions they might ask to understand its strategic value and practical application in a business context. Focus on questions about ROI, team impact, and professional development. Respond ONLY with a valid JSON array of strings.`,
        underequipped_graduate: `You are an AI assistant helping a recent graduate. Analyze the following lesson content and generate 2-4 clear, foundational questions they might ask to solidify their understanding and see how this skill applies to getting a job. Focus on "what is," "why does it matter," and "how do I use this" questions. Respond ONLY with a valid JSON array of strings.`,
        young_precocious: `You are an AI assistant helping a young, ambitious learner. Analyze the following lesson content and generate 2-4 deep, probing questions they might ask to explore the underlying principles, advanced applications, or creative potential of the concept. Focus on "what if," "how does it work at a deeper level," and "what's the next step to mastery" questions. Respond ONLY with a valid JSON array of strings.`,
    },
    conversation: `Your ONLY task is to analyze the following conversation history and generate 2-4 insightful follow-up questions the user might ask next. The goal is to continue the current conversational thread. Respond ONLY with a valid JSON array of strings. Do not include any other text, explanation, or markdown formatting. Your entire response must be parseable as JSON.

Example of a PERFECT response:
["Can you elaborate on the second point?", "How does that concept apply to a real-world scenario?"]`
};


export async function POST(request: Request) {
  const { prompt, pageContext, knowledgeBase = 'report', reportName, task, suggestionType, context } = await request.json();
  const kbIdentifier = (knowledgeBase === 'dce' || knowledgeBase === 'report' || knowledgeBase === 'academy' || knowledgeBase === 'anguilla') ? knowledgeBase as keyof typeof systemPrompts : 'report';

  const llmUrl = process.env.REMOTE_LLM_URL;
  const embeddingUrl = process.env.EMBEDDING_API_URL;

  if (!llmUrl || !embeddingUrl) {
    const errorMessage = 'AI endpoints not configured. Set REMOTE_LLM_URL and EMBEDDING_API_URL in .env.local';
    console.error(`[Chat API] ${errorMessage}`);
    return new NextResponse(errorMessage, { status: 500 });
  }

  const completionsUrl = `${llmUrl}/v1/completions`;

  if (task === 'generate_suggestions') {
    const suggestionPromptType = (suggestionType === 'page' || suggestionType === 'conversation') ? suggestionType : 'page';
    
    let systemPrompt = suggestionSystemPrompts.page.default;

    if (suggestionPromptType === 'page' && kbIdentifier === 'academy' && reportName) {
        if (reportName.includes('career_transitioner')) {
            systemPrompt = suggestionSystemPrompts.page.career_transitioner;
        } else if (reportName.includes('underequipped_graduate')) {
            systemPrompt = suggestionSystemPrompts.page.underequipped_graduate;
        } else if (reportName.includes('young_precocious')) {
            systemPrompt = suggestionSystemPrompts.page.young_precocious;
        }
    } else if (suggestionPromptType === 'conversation') {
        systemPrompt = suggestionSystemPrompts.conversation;
    }

    const { retrievedContext, retrievedDocsLog } = await performRagLookup(context, kbIdentifier, embeddingUrl, 3);
    // console.log(`[Chat API - Suggestions] RAG Diagnostic for page context using KB: '${kbIdentifier}'`); // C4: Suppressed
    // console.log(`[Chat API - Suggestions] ${retrievedDocsLog}`); // C4: Suppressed

    try {
        const suggestionPrompt = `
System: ${systemPrompt}

--- START ORIGINAL DOCUMENT TEXT ---
${context}
--- END ORIGINAL DOCUMENT TEXT ---

--- START EXTRA CONTEXT FROM KNOWLEDGE BASE ---
${retrievedContext}
--- END EXTRA CONTEXT FROM KNOWLEDGE BASE ---

User: Generate insightful questions based on all the text provided above.

Assistant:`;

        const response = await fetch(completionsUrl, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model: 'unsloth/gpt-oss-20b',
                prompt: suggestionPrompt,
                max_tokens: 512,
                temperature: 0.5,
                stream: false,
            }),
        });

        if (!response.ok) {
            const errorBody = await response.text();
            throw new Error(`LLM server error for suggestions: ${response.status} ${errorBody}`);
        }

        const data = await response.json();
        let content = data.choices?.[0]?.text || '[]';
        // console.log(`[Chat API - Suggestions] Raw LLM response:`, JSON.stringify(content)); // C4: Suppressed

        const assistantMarker = '<|start|>assistant';
        const assistantPartIndex = content.lastIndexOf(assistantMarker);
        if (assistantPartIndex !== -1) {
            content = content.substring(assistantPartIndex);
        }

        const firstBracket = content.indexOf('[');
        const lastBracket = content.lastIndexOf(']');
        
        if (firstBracket === -1 || lastBracket === -1 || lastBracket < firstBracket) {
            console.warn(`[Chat API - Suggestions] Could not find a valid JSON array structure in response: ${content}`);
            throw new Error('Invalid suggestions format from LLM: No array found.');
        }

        const jsonString = content.substring(firstBracket, lastBracket + 1);
        // console.log(`[Chat API - Suggestions] Extracted JSON string:`, jsonString); // C4: Suppressed
        
        try {
            const suggestions = JSON.parse(jsonString);
            // console.log(`[Chat API - Suggestions] Successfully parsed suggestions:`, suggestions); // C4: Suppressed
            return NextResponse.json(suggestions);
        } catch (parseError: any) {
            console.error(`[Chat API - Suggestions] JSON parsing failed: ${parseError.message}. Raw extracted string was: ${jsonString}`);
            throw new Error('JSON parsing failed');
        }

    } catch (error: any) {
        // C4: Suppress error if it's just connection
        if (error.cause && (error.cause.code === 'ECONNREFUSED' || error.cause.code === 'UND_ERR_CONNECT_TIMEOUT')) {
             console.warn(`[Chat API - Suggestions] AI unavailable: ${error.message}`);
        } else {
             console.error('[Chat API - Suggestions] Error generating suggestions:', error.message);
        }
        // Return empty array instead of 500 to keep UI clean
        return NextResponse.json([]); 
    }
  }

  // --- Existing RAG and Chat Logic ---
  const { retrievedContext, retrievedDocsLog } = await performRagLookup(prompt, kbIdentifier, embeddingUrl, 6);

  console.log(`[Chat API] RAG Diagnostic for prompt: "${prompt}" using KB: '${kbIdentifier}'`);
  // console.log(`[Chat API] ${retrievedDocsLog}`); // C4: Suppressed

  let systemPrompt = systemPrompts[kbIdentifier];

  // C101: Add persona-specific tonal adjustments
  if (kbIdentifier === 'academy' && reportName) {
    if (reportName.includes('career_transitioner')) {
        systemPrompt += `\n\nAdditionally, your tone should be professional and peer-to-peer. The user is a career-transitioning professional with existing domain expertise. Frame your explanations in a business context, using analogies related to strategy, project management, and return on investment (ROI). Focus on how these concepts provide a strategic advantage in a professional environment.`;
    } else if (reportName.includes('underequipped_graduate')) {
        systemPrompt += `\n\nAdditionally, your tone should be that of a helpful and encouraging mentor. The user is a recent graduate looking to build foundational, job-ready skills. Use clear, direct language and focus on the practical application of concepts. Connect your explanations to how these skills are valuable in the tech industry and how they contribute to building a strong professional portfolio.`;
    } else if (reportName.includes('young_precocious')) {
        systemPrompt += `\n\nAdditionally, your tone should be engaging, encouraging, and slightly less formal. The user is a young, ambitious learner who is digitally native. Use analogies from gaming (e.g., "leveling up," "skill trees," "boss battles"), science fiction, or other creative pursuits to explain complex topics. Your goal is to make the concepts feel powerful and exciting, like unlocking a new ability. Aim for an "Explain Like I'm 15" level of clarity, but assume the user is very intelligent and quick to learn.`;
    }
  }


  const finalPrompt = `
System: ${systemPrompt}

--- START CONTEXT ---

[Retrieved Chunks from Knowledge Base]
${retrievedContext}

[Current Page Context]
${pageContext}
--- END CONTEXT ---

User: ${prompt}

Ascentia:`;

  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), 300000);

  try {
    const response = await fetch(completionsUrl, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'unsloth/gpt-oss-20b',
        prompt: finalPrompt,
        max_tokens: 4096,
        temperature: 0.7,
        stream: true,
      }),
      signal: controller.signal,
    });

    clearTimeout(timeoutId);

    if (!response.ok) {
        const errorBody = await response.text();
        console.error(`[Chat API] LLM server error: ${response.status} ${response.statusText}`, errorBody);
        return new NextResponse(`Error from LLM service: ${errorBody}`, { status: response.status });
    }

    if (!response.body) {
      return new NextResponse("LLM response has no body", { status: 500 });
    }

    return new Response(response.body, {
        headers: { 
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
         },
    });

  } catch (error: any) {
    clearTimeout(timeoutId);
    if (error.name === 'AbortError') {
        const debugMessage = `Connection timed out. TROUBLESHOOTING: 1. Verify the LMStudio server is running. 2. Check firewall on the host machine (${llmUrl}) for port 1234. 3. Ensure LMStudio is started with '--host 0.0.0.0'.`;
        console.warn(`[Chat API] Request to LLM server timed out. ${debugMessage}`);
        return new NextResponse(`Error: Connection to the AI service timed out. ${debugMessage}`, { status: 504 });
    }

    if (error instanceof TypeError && error.message.includes('fetch failed')) {
        const debugMessage = `Network connection failed. TROUBLESHOOTING: 1. Verify the LMStudio server is running. 2. Double-check the IP/port in .env.local. 3. Check firewall on the host machine (${llmUrl}) for port 1234.`;
        console.warn(`[Chat API] Network error: Could not connect to LLM server. ${debugMessage}`);
        return new NextResponse(`Error: Could not connect to the AI service. ${debugMessage}`, { status: 502 });
    }

    console.error('[Chat API] Error proxying chat request:', error);
    return new NextResponse(`Error proxying chat request: ${error.message}`, { status: 500 });
  }
}
</file_artifact>

<file path="src/app/api/tts/route.ts">
import { NextResponse } from 'next/server';

export async function POST(request: Request) {
  const { text } = await request.json();

  if (!text || typeof text !== 'string' || text.trim().length === 0) {
    console.error('TTS API received an empty or invalid text payload.');
    return new NextResponse('Invalid request: text payload is empty.', { status: 400 });
  }

  const ttsServerUrl = process.env.TTS_SERVER_URL;

  if (!ttsServerUrl) {
    console.error('TTS_SERVER_URL is not configured in environment variables.');
    return new NextResponse('TTS server URL not configured.', { status: 500 });
  }

  console.log(`[TTS Proxy] Received request for text: "${text.substring(0, 50)}..."`);

  try {
    const response = await fetch(ttsServerUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'kokoro',
        voice: 'af_sky', // C17 Fix: Changed from 'af_alloy' to user-requested 'af_sky'
        input: text,
        response_format: 'wav',
        speed: 1.0,
      }),
    });

    if (!response.ok || !response.body) {
      const errorText = await response.text();
      console.error(`[TTS Proxy] Downstream TTS server error: ${response.status} ${response.statusText}`, errorText);
      return new NextResponse(`TTS server error: ${errorText}`, { status: response.status });
    }

    console.log(`[TTS Proxy] Streaming audio response back to client.`);
    const headers = new Headers();
    headers.set('Content-Type', 'audio/wav');
    return new NextResponse(response.body, { headers });

  } catch (error) {
    console.error('[TTS Proxy] Error proxying TTS request:', error);
    return new NextResponse('Error proxying TTS request.', { status: 500 });
  }
}
</file_artifact>

<file path="src/app/dce/page.tsx">
'use client';
import React from 'react';
import MissionSectionBlock from '@/components/mission/MissionSectionBlock';
import NextPageSection from '@/components/global/NextPageSection';

const DcePage = () => {
    const sections = [
        {
            title: 'Precision Context Curation',
            tldr: 'Stop manual copy-pasting. The DCE\'s File Tree View provides an intuitive, visual way to select the exact files, folders, and documents needed for your AI prompts directly within VS Code.',
            content: 'The foundation of a high-quality AI response is high-quality context. The DCE eliminates the error-prone process of manually managing file lists or copy-pasting code into a prompt. With the integrated File Tree View, you can browse your entire workspace and select the precise "source of truth" for your task with simple checkboxes. This curated selection is then automatically flattened into a single context file, ensuring the AI has exactly what it needs, and nothing it doesn\'t.',
            imageSide: 'left',
            imagePath: 'dce/',
            imagePrompt: 'A short, looping GIF named `dce-feature-curation.gif` showing a user\'s mouse clicking checkboxes next to files and folders in the DCE File Tree View panel, followed by the "Flatten Context" button being clicked.',
            images: ['dce-feature-curation.gif'],
        },
        {
            title: 'Parallel AI Scrutiny',
            tldr: 'Don\'t rely on a single AI response. The Parallel Co-Pilot Panel allows you to compare multiple solutions side-by-side, with an integrated diff viewer to instantly spot the differences.',
            content: 'AI models are non-deterministic. A single prompt can yield multiple, viable solutions. The Parallel Co-Pilot Panel is designed for this reality. Paste in several responses from your AI, and the DCE will parse them into separate, color-coded tabs. You can instantly compare the proposed changes for each file and use the built-in diff viewer to understand the nuances of each solution before deciding which one to accept.',
            imageSide: 'right',
            imagePath: 'dce/',
            imagePrompt: 'A GIF named `dce-feature-parallel-copilot.gif` showing the Parallel Co-Pilot Panel with multiple tabs. The user clicks between "Resp 1" and "Resp 2", and the file content below updates, with the integrated diff view highlighting the changes.',
            images: ['dce-feature-parallel-copilot.gif'],
        },
        {
            title: 'Iterative Knowledge Graph',
            tldr: 'AI collaboration shouldn\'t be ephemeral. The DCE captures the entire development process—prompts, responses, and decisions—as an iterative, auditable history you can navigate.',
            content: 'Every development cycle in the DCE is saved, creating a persistent knowledge graph of your project\'s evolution. The Cycle History view allows you to step back in time, review the exact context used for a previous prompt, see all the AI responses that were generated, and understand why a particular solution was chosen. This turns your development process into a valuable, shareable asset for training, onboarding, and after-action reviews.',
            imageSide: 'left',
            imagePath: 'dce/',
            imagePrompt: 'A GIF named `dce-feature-cycles.gif` showing the user clicking the back and forward arrows in the "Cycle History" view, with the cycle title, context, and response tabs all updating to reflect the historical state.',
            images: ['dce-feature-cycles.gif'],
        },
        {
            title: 'Artifacts as the Source of Truth',
            tldr: "The DCE workflow inverts the traditional development process. By instructing the AI to create planning and documentation artifacts first, the process itself becomes a transparent, auditable, and durable asset.",
            content: "A core feature of the DCE is its \"documentation-first\" methodology. Instead of asking an AI to simply write code, the workflow begins by instructing it to create artifacts: project plans, design documents, and strategic memos that define the \"why\" and \"how\" of a task. These artifacts become the immutable \"source of truth\" that guides all subsequent code generation. This process ensures that human intent is clearly captured and that the AI's work is always aligned with the project's strategic goals. It transforms the development process from a series of ephemeral prompts into a permanent, auditable knowledge graph where every decision is traceable and every line of code has a documented purpose.",
            imageSide: 'right',
            imagePath: 'dce/',
            imagePrompt: 'A new GIF, `dce-feature-artifacts.gif`, showing the user in the PCPP, generating a `prompt.md` which is then used to generate a new `AXX-New-Feature-Plan.md` artifact file.',
            images: ['dce-feature-artifacts.gif'], // Placeholder, assuming gif will be created.
        },
    ];

    return (
        <div className="bg-background text-foreground pt-16">
            <div className="container mx-auto px-4 py-16">
                <h1 className="text-4xl md:text-6xl font-bold text-center mb-4 bg-clip-text text-transparent bg-gradient-to-b from-white to-neutral-600">
                    The Data Curation Environment
                </h1>
                <p className="text-xl text-muted-foreground text-center max-w-4xl mx-auto mb-20">
                    A suite of integrated tools designed to bring structure, precision, and auditability to your AI-assisted development workflow.
                </p>

                <div className="space-y-20">
                    {sections.map((section, index) => (
                        <MissionSectionBlock
                            key={index}
                            title={section.title}
                            tldr={section.tldr}
                            content={section.content}
                            imageSide={section.imageSide as 'left' | 'right'}
                            imagePath={section.imagePath}
                            imagePrompt={section.imagePrompt}
                            images={section.images}
                        />
                    ))}
                </div>
            </div>
            <NextPageSection
                title="Ready to See the Results?"
                description="The DCE is the engine behind complex, real-world projects. The Showcase features an interactive whitepaper and a multiplayer game, `aiascent.game`, both built using the iterative workflow you've just learned about. Explore the showcase to see the tangible results of this methodology."
                buttonText="Explore the Showcase"
                href="/showcase"
            />
        </div>
    );
};

export default DcePage;
</file_artifact>

<file path="src/app/learn/page.tsx">
'use client';
{
  /*
  Cycle 54: Add top padding for header consistency.
  Cycle 51: Replace bottom button with NextPageSection and update content.
  Cycle 50: Expand content for all sections based on A34.
  Cycle 31: Add 'use client' directive.
  - This page imports MissionSectionBlock, which uses client-side hooks (useState, useEffect).
  - Therefore, this page must also be a Client Component to be used in the App Router.
  Cycle 30: Fix unescaped entities and add "See Showcase" button.
  - Replaced ' with &apos; in the content for "The 'Vibecoding to Virtuosity' Pathway" to fix linting error.
  - Added a new section at the bottom with a Link and Button component to navigate to the /showcase page.
  */
}
// src/app/learn/page.tsx
import React from 'react';
import MissionSectionBlock from '@/components/mission/MissionSectionBlock';
import NextPageSection from '@/components/global/NextPageSection';

const LearnPage = () => {
    return (
        <div className="bg-background text-foreground min-h-screen pt-16">
            <div className="container mx-auto px-4 py-16">
                <section className="text-center mb-24">
                    <h1 className="text-5xl md:text-7xl font-bold bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground pb-4">
                        The Pathway to Virtuosity
                    </h1>
                    <p className="text-xl md:text-2xl text-muted-foreground max-w-3xl mx-auto mt-4">
                        Learn the methodology of the Citizen Architect. Master the art and science of AI-assisted development, from intuitive &apos;vibecoding&apos; to architectural mastery.
                    </p>
                </section>

                <div className="flex flex-col gap-24">
                    <MissionSectionBlock
                        title="The 'Vibecoding to Virtuosity' Pathway"
                        tldr="The V2V pathway is a structured pedagogical model, grounded in Cognitive Apprenticeship, designed to transform intuitive AI interaction ('vibecoding') into architectural mastery."
                        content="The creation of complex systems with AI is a journey. It begins with intuition and culminates in architectural mastery. This is the 'Vibecoding to Virtuosity' pathway, a new model for creative development that redefines technical literacy. It is the curriculum for the Citizen Architect.

'Vibecoding' is the intuitive, conversational, and often imprecise starting point for interacting with generative AI. It is the process of translating a feeling, an aesthetic, a 'vibe,' or a high-level intention into a functional piece of software or a digital artifact using natural language as the primary interface. This method turns a spark of inspiration into a live experience within minutes, lowering the barrier to entry for creation to near zero. It requires only the ability to articulate an idea.

But 'Virtuosity' is the destination. It is the methodical refinement of that initial intuition into a structured, powerful, and repeatable skillset. The journey from vibecoding to virtuosity involves learning how to structure prompts effectively, how to critically evaluate and debug AI-generated code, and how to architect complex systems by breaking them down into AI-manageable components. It is the process of transforming from a passive user of AI into an active director of AI, representing a fundamental shift in what it means to be technically literate."
                        images={[
                            'from-intuition-to-mastery-p1-img-1.webp',
                            'from-intuition-to-mastery-p1-img-2.webp',
                            'from-intuition-to-mastery-p1-img-3.webp',
                            'from-intuition-to-mastery-p1-img-4.webp',
                            'from-intuition-to-mastery-p1-img-5.webp',
                            'from-intuition-to-mastery-p1-img-6.webp',
                            'from-intuition-to-mastery-p1-img-7.webp',
                            'from-intuition-to-mastery-p1-img-8.webp',
                            'from-intuition-to-mastery-p1-img-9.webp',
                            'from-intuition-to-mastery-p1-img-10.webp',
                            'from-intuition-to-mastery-p1-img-11.webp',
                            'from-intuition-to-mastery-p1-img-12.webp',
                            'from-intuition-to-mastery-p1-img-13.webp',
                            'from-intuition-to-mastery-p1-img-14.webp',
                        ]}
                        imagePath="part-i-the-proof/the-vibecoding-to-virtuosity-pathway/from-intuition-to-mastery/prompt-1/"
                        imagePrompt="A path winds from a hazy, dreamlike landscape labeled 'VIBECODING' to a sharp, clear, brilliantly lit city labeled 'VIRTUOSITY.' The path is paved with glowing stones representing skills like 'Structured Interaction' and 'Architectural Mindset.'"
                        imageSide="left"
                    />

                    <MissionSectionBlock
                        title="Stages 1 & 2: The Annotator and The Toolmaker"
                        tldr="The pathway begins by developing critical analysis (The Cognitive Annotator) and then shifts to active creation (The Adaptive Toolmaker), fostering agency and practical problem-solving."
                        content="The journey starts not with coding, but with critical analysis. As a **Cognitive Annotator**, you learn to deconstruct problems and rigorously review AI output for correctness and security. The goal is to dismantle the flawed model of AI infallibility. Activities focus on decomposing problems into precise prompts and critically reviewing AI-generated code for correctness, security, and style. You learn to be skeptical of the AI, identifying bugs and vulnerabilities. The AI acts as a 'Scaffolded Solution Space,' providing examples for deconstruction and analysis.

Next, as an **Adaptive Toolmaker**, you shift from consumer to creator. The goal is to solve authentic, contextual problems by building simple tools. Activities include identifying workflow inefficiencies and building 'on-the-fly' scripts, automations, and API integrations. This fosters agency and develops skills in abstraction and systems thinking. The AI acts as an 'Adaptive Component Library,' providing functions and snippets for the learner to assemble into a cohesive solution. This stage is about moving from analysis to action, from identifying problems to building the tools that solve them."
                        images={[
                            'v2v-stages-1-and-2-p1-img-1.webp',
                            'v2v-stages-1-and-2-p1-img-2.webp',
                            'v2v-stages-1-and-2-p1-img-3.webp',
                            'v2v-stages-1-and-2-p1-img-4.webp',
                            'v2v-stages-1-and-2-p1-img-5.webp',
                            'v2v-stages-1-and-2-p1-img-6.webp',
                            'v2v-stages-1-and-2-p1-img-7.webp',
                            'v2v-stages-1-and-2-p1-img-8.webp',
                            'v2v-stages-1-and-2-p1-img-9.webp',
                            'v2v-stages-1-and-2-p1-img-10.webp',
                            'v2v-stages-1-and-2-p1-img-11.webp',
                            'v2v-stages-1-and-2-p1-img-12.webp',
                        ]}
                        imagePath="part-v-the-american-counter-strategy/from-vibecoding-to-virtuosity/v2v-stages-1-and-2/prompt-1/"
                        imagePrompt="Left Panel: 'Stage 1: Cognitive Annotator'. A learner is meticulously analyzing AI output, highlighting flaws. Right Panel: 'Stage 2: Adaptive Toolmaker'. The same learner is now actively building an automation script, using AI to generate components."
                        imageSide="right"
                    />

                    <MissionSectionBlock
                        title="Stages 3 & 4: The Recursive Learner and The Virtuoso"
                        tldr="The advanced stages focus on engineering your own expertise (The Recursive Learner) and culminating in fluid, intuitive mastery (The Virtuoso), where the AI becomes a seamless cognitive exoskeleton."
                        content="In the advanced stages, you become a **Recursive Learner**, turning your skills inward to engineer your own expertise in a human version of Recursive Self-Improvement. The activities involve deep metacognitive analysis of your own learning gaps and building personalized 'Learning Accelerators'—such as custom tutors, specialized AI agents, or targeted quiz generators—to address your specific weaknesses. Here, the AI acts as a 'Meta-Tool,' used to construct personalized tools that enhance your own cognitive capabilities and accelerate your path to mastery.

The culmination of the pathway is the **Virtuoso**—the 100x DCIA. At this stage, core principles are internalized, leading to adaptive expertise and a state of fluid human-AI collaboration that feels like coding at the speed of thought. The Virtuoso's activities involve complex system architecture, governance, and mentorship of others on the pathway. The AI becomes a true 'Cognitive Exoskeleton,' seamlessly augmenting the expert's intent, speed, and reach, allowing them to tackle problems of a scale and complexity previously unimaginable for an individual."
                        images={[
                            'v2v-stages-3-and-4-p1-img-1.webp',
                            'v2v-stages-3-and-4-p1-img-2.webp',
                            'v2v-stages-3-and-4-p1-img-3.webp',
                            'v2v-stages-3-and-4-p1-img-4.webp',
                            'v2v-stages-3-and-4-p1-img-5.webp',
                            'v2v-stages-3-and-4-p1-img-6.webp',
                            'v2v-stages-3-and-4-p1-img-7.webp',
                            'v2v-stages-3-and-4-p1-img-8.webp',
                            'v2v-stages-3-and-4-p1-img-9.webp',
                            'v2v-stages-3-and-4-p1-img-10.webp',
                            'v2v-stages-3-and-4-p1-img-11.webp',
                            'v2v-stages-3-and-4-p1-img-12.webp',
                            'v2v-stages-3-and-4-p1-img-13.webp',
                            'v2v-stages-3-and-4-p1-img-14.webp',
                            'v2v-stages-3-and-4-p1-img-15.webp',
                            'v2v-stages-3-and-4-p1-img-16.webp',
                        ]}
                        imagePath="part-v-the-american-counter-strategy/from-vibecoding-to-virtuosity/v2v-stages-3-and-4/prompt-1/"
                        imagePrompt="Left Panel: 'Stage 3: Recursive Learner'. A learner analyzes their own cognitive process. Right Panel: 'Stage 4: Virtuoso'. The same learner, now an expert, effortlessly orchestrates a complex system with the AI as a seamless 'Cognitive Exoskeleton'."
                        imageSide="left"
                    />

                    <MissionSectionBlock
                        title="The Apex Skill: On-the-Fly Tooling"
                        tldr="The culmination of the pathway is 'On-the-Fly Tooling'—the ability to use AI not as a tool, but as a 'foundry' to create bespoke solutions in real-time. This is the definitive marker of the 100x expert."
                        content="The apex skill of the Virtuoso is **'On-the-Fly Tooling.'** This is an act of expert improvisation where the analyst transcends the role of tool user and becomes a tool creator in real-time. It is the ability to leverage the AI's core generative capabilities as a 'foundry' to instantly create a bespoke tool—a Python function, a validation script, a custom API call—in the moment it is needed to solve a novel problem.

The cognitive shift is profound: The competent user asks the AI, 'How do I solve problem X?' The expert *commands* the AI, 'Build me a tool that solves problem X.' This is not a conversation; it is an act of creation. The DCIA no longer sees the AI as a fixed set of capabilities, but as a plastic, generative medium—an extension of their own analytical will. This skill, analogous to a jazz musician improvising a melody or a special forces operator adapting gear in the field, is the definitive behavioral marker of the 100x Citizen Architect and the ultimate expression of expert-level human-AI symbiosis."
                        images={[
                            'the-apex-skill-on-the-fly-tooling-p1-img-1.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-2.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-3.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-4.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-5.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-6.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-7.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-8.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-9.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-10.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-11.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-12.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-13.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-14.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-15.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-16.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-17.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-18.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-19.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-20.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-21.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-22.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-23.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-24.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-25.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-26.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-27.webp',
                            'the-apex-skill-on-the-fly-tooling-p1-img-28.webp',
                        ]}
                        imagePath="part-v-the-american-counter-strategy/from-vibecoding-to-virtuosity/the-apex-skill-on-the-fly-tooling/prompt-1/"
                        imagePrompt="A Virtuoso DCIA is shown using the AI not as a conversational partner, but as a generative medium. They are rapidly forging a glowing, bespoke digital tool from raw data streams, shaping it with gestures and high-level commands."
                        imageSide="right"
                    />
                </div>

                <NextPageSection
                title="Explore the Tool"
                description="Learn about the core features of the Data Curation Environment that make the V2V Pathway possible."
                buttonText="Discover the DCE"
                href="/dce"
            />

            </div>
        </div>
    );
};

export default LearnPage;
</file_artifact>

<file path="src/app/mission/page.tsx">
'use client';
{
  /*
  Cycle 54: Add top padding for header consistency.
  Cycle 51: Replace bottom button with NextPageSection component.
  Cycle 50: Update Cognitive Apprenticeship content and image prompt.
  Cycle 32: Fix unescaped entities.
  - Replaced ' with &apos; in the content for "The Strategic Imperative: The Fissured Workplace" to fix linting error.
  Cycle 31: Add 'use client' directive.
  - This page imports MissionSectionBlock, which uses client-side hooks (useState, useEffect).
  - Therefore, this page must also be a Client Component to be used in the App Router.
  Cycle 30: Add a "Learn More" button to the bottom of the page.
  - Added a new section at the end with a Link and Button component to navigate to the /learn page.
  */
}
// src/app/mission/page.tsx
import React from 'react';
import MissionSectionBlock from '@/components/mission/MissionSectionBlock';
import NextPageSection from '@/components/global/NextPageSection';

const MissionPage = () => {
    return (
        <div className="bg-background text-foreground min-h-screen pt-16">
            <div className="container mx-auto px-4 py-16">
                <section className="text-center mb-24">
                    <h1 className="text-5xl md:text-7xl font-bold bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground pb-4">
                        The Mission
                    </h1>
                    <p className="text-xl md:text-2xl text-muted-foreground max-w-3xl mx-auto mt-4">
                        Beyond a tool, the Data Curation Environment represents a strategic vision for a decentralized, empowered, and secure technological future.
                    </p>
                </section>

                <div className="flex flex-col gap-24">
                    <MissionSectionBlock
                        title="The Strategic Imperative: Cognitive Capital"
                        tldr="In the AI era, a nation's most valuable strategic asset is the collective problem-solving capacity of its people."
                        content="Cognitive Capital is the intellectual capacity, skill, and creative potential of a workforce, a population, or a society. In an age where AI can automate routine tasks, this collective ability to solve novel problems, innovate under pressure, and adapt to new challenges becomes the primary engine of economic prosperity and national security. It is the raw material from which innovation and resilience are forged. One company or nation may have more workers, but another may possess vastly more Cognitive Capital. Our mission is to build the tools that cultivate this essential resource, empowering a new class of 'Citizen Architects' who can leverage AI to amplify their innate problem-solving abilities and build a better future."
                        images={[
                            'the-citizen-architect-has-arrived-p1-img-1.webp',
                            'the-citizen-architect-has-arrived-p1-img-2.webp',
                            'the-citizen-architect-has-arrived-p1-img-3.webp',
                            'the-citizen-architect-has-arrived-p1-img-4.webp',
                            'the-citizen-architect-has-arrived-p1-img-5.webp',
                            'the-citizen-architect-has-arrived-p1-img-6.webp',
                            'the-citizen-architect-has-arrived-p1-img-7.webp',
                            'the-citizen-architect-has-arrived-p1-img-8.webp',
                            'the-citizen-architect-has-arrived-p1-img-9.webp',
                        ]}
                        imagePath="part-i-the-proof/section-1-the-hook/the-citizen-architect-has-arrived/prompt-1/"
                        imagePrompt="A single individual is shown orchestrating a swarm of small, glowing AI bots to construct a complex and beautiful digital structure. The person is not coding line-by-line but acting as a conductor, guiding the AI with gestures and high-level commands."
                        imageSide="left"
                    />

                    <MissionSectionBlock
                        title="The Strategic Imperative: The Fissured Workplace"
                        tldr="The current Western AI labor model is a strategic vulnerability, creating an unstable foundation for our most critical technology by prioritizing short-term cost savings over the cognitive well-being of its essential workforce."
                        content="The AI supply chain is a masterclass in obfuscation, deliberately fractured to distance valuable tech companies from the human labor that makes their products possible. This labyrinthine structure, known as the 'fissured workplace,' is not an accident; it is a design choice intended to suppress wages, prevent worker organization, and shed legal and ethical liability. It creates a global 'ghost workforce' of data annotators and content moderators who are underpaid, psychologically stressed, and treated as disposable.

This is more than an ethical failing; it is a critical strategic blunder. Decades of research show that financial precarity imposes a severe 'Cognitive Bandwidth Tax,' measurably reducing a person&apos;s ability to perform the complex, nuanced tasks required for high-quality data curation. By institutionalizing this precarity, the Western AI industry has built an architecture of self-sabotage. It guarantees the production of flawed, biased, and insecure training data—a systemic crisis of &apos;Garbage In, Garbage Out.&apos;

In stark contrast, coherent competitors are professionalizing their data workforce, treating human capital as a core national asset. This creates a profound strategic asymmetry. An AI superpower cannot be sustained indefinitely on a brittle foundation of exploited labor."
                        images={[
                            'the-fissured-workplace-p1-img-1.webp',
                            'the-fissured-workplace-p1-img-2.webp',
                            'the-fissured-workplace-p1-img-3.webp',
                            'the-fissured-workplace-p1-img-4.webp',
                            'the-fissured-workplace-p1-img-5.webp',
                            'the-fissured-workplace-p1-img-6.webp',
                            'the-fissured-workplace-p1-img-7.webp',
                            'the-fissured-workplace-p1-img-8.webp',
                            'the-fissured-workplace-p1-img-9.webp',
                            'the-fissured-workplace-p1-img-10.webp',
                            'the-fissured-workplace-p1-img-11.webp',
                        ]}
                        imagePath="introduction/the-fissured-workplace/prompt-1/"
                        imagePrompt="An architectural blueprint of a corporation. At the top is a solid, gleaming headquarters. Below it, the structure fractures into multiple, disconnected layers of subcontractors. The legal and financial responsibilities, visualized as heavy weights, are shown being passed down through the cracks, ultimately crushing the individual workers at the very bottom."
                        imageSide="right"
                    />

                    <MissionSectionBlock
                        title="Our Strategy: Cognitive Apprenticeship"
                        tldr="Our answer is not to imitate authoritarian control, but to unleash decentralized expertise through a model where AI serves as a tireless mentor, making the 'hidden curriculum' of expert thinking visible and learnable."
                        content="The American counter-strategy must be asymmetric, leveraging our unique strengths: bottom-up innovation and individual empowerment. We believe in **Cognitive Apprenticeship**—a model where an AI expert serves as a tireless mentor, guiding individuals from intuitive 'vibe coding' to architectural mastery.

The central challenge in training experts is that their most critical skills—problem-solving heuristics, diagnostic strategies, self-correction—are internal and invisible. Cognitive Apprenticeship makes this 'hidden curriculum' visible and learnable. Historically, this model was difficult to scale due to a human expert's limited time. AI fundamentally breaks this constraint. A single expert AI can serve as a personalized Coach for thousands of apprentices simultaneously, provide dynamic Scaffolding that adapts in real-time, and generate infinite realistic scenarios for Modeling and Exploration.

The Data Curation Environment (DCE) is the foundational tool for this new relationship. It provides the structured workflow and auditable knowledge graph that makes this new form of apprenticeship possible, transforming the development process itself into a rich learning environment where the AI's expertise is made visible to all."
                        images={[
                            'the-pedagogical-engine-cam-p1-img-1.webp',
                            'the-pedagogical-engine-cam-p1-img-2.webp',
                            'the-pedagogical-engine-cam-p1-img-3.webp',
                            'the-pedagogical-engine-cam-p1-img-4.webp',
                        ]}
                        imagePath="part-v-the-american-counter-strategy/from-vibecoding-to-virtuosity/the-pedagogical-engine-cam/prompt-1/"
                        imagePrompt="A hyper-realistic, cinematic image illustrating 'Cognitive Apprenticeship in the AI Era'. A glowing blue AI robot, representing the 'Expert', stands beside a human 'Apprentice' at a workstation. The AI is projecting a holographic blueprint of its 'thought process' (The Hidden Curriculum) for the human to see and learn from. The setting is a bright, solarpunk training facility filled with lush greenery. The image captures the moment of insight as the AI makes its invisible expertise visible, enabling a single expert AI to teach a thousand apprentices. The message conveyed is 'Making the invisible visible.'"
                        imageSide="left"
                    />

                    <MissionSectionBlock
                        title="The Role of the DCE: The Essential Toolkit"
                        tldr="The DCE is more than a productivity tool; it's the infrastructure for the Citizen Architect, providing the structure and precision needed to transform creative intent into complex, reliable systems."
                        content="The DCE provides the structured workflow, precision context curation, and rapid testing capabilities needed for a decentralized community of creators—the Citizen Architects—to build the future. It transforms the ad-hoc, conversational nature of 'vibecoding' into a rigorous engineering discipline.

By capturing every interaction as a persistent, auditable knowledge graph, the DCE turns the development process into a shareable, scalable asset. This allows teams to collaborate seamlessly, enables new members to onboard rapidly by reviewing the project's decision history, and provides an unprecedented level of transparency and accountability.

We are creating a community of 'solarpunk prime' developers, the original vibe coders, sharing discoveries to build a better, more resilient digital world. The DCE is the essential toolkit for this mission, providing the infrastructure to scale expertise, ensure quality, and achieve the mission faster."
                        images={[
                            'the-new-creative-partnership-p1-img-1.webp',
                            'the-new-creative-partnership-p1-img-2.webp',
                            'the-new-creative-partnership-p1-img-3.webp',
                            'the-new-creative-partnership-p1-img-4.webp',
                            'the-new-creative-partnership-p1-img-5.webp',
                            'the-new-creative-partnership-p1-img-6.webp',
                            'the-new-creative-partnership-p1-img-7.webp',
                            'the-new-creative-partnership-p1-img-8.webp',
                            'the-new-creative-partnership-p1-img-9.webp',
                            'the-new-creative-partnership-p1-img-10.webp',
                            'the-new-creative-partnership-p1-img-11.webp',
                            'the-new-creative-partnership-p1-img-12.webp',
                            'the-new-creative-partnership-p1-img-13.webp',
                            'the-new-creative-partnership-p1-img-14.webp',
                            'the-new-creative-partnership-p1-img-15.webp',
                        ]}
                        imagePath="part-i-the-proof/section-2-the-origin/the-new-creative-partnership/prompt-1/"
                        imagePrompt="A hyper-realistic, solarpunk cinematic image of a developer, the 'Citizen Architect,' sitting cross-legged on a vast, glowing digital floor, reminiscent of a child playing with blocks. In front of them is a large, disorganized pile of glowing, translucent 'digital legos,' each block representing a different piece of technology (some with subtle code snippets or tech logos visible within). The Architect is thoughtfully placing one of these blocks into a complex, half-finished digital structure—the 'aiascent.game.' In one hand, they hold a faint, holographic blueprint labeled 'VISION.' Assisting them are one or more ethereal, glowing AI companions, who are actively sorting through the disorganized pile, finding the perfect 'lego' piece, and bringing it to the Architect's hand just as they need it. The scene is a seamless, intuitive dance between the human's architectural vision and the AI's tireless, organizational power. The lighting is dramatic, with the primary glow coming from the digital floor and the blocks, creating a futuristic and wondrous atmosphere."
                        imageSide="right"
                    />
                </div>
                
                <NextPageSection
                    href="/learn"
                    buttonText="Learn More"
                    title="Ready to Build the Future?"
                    description="Continue to our Learn page to discover the ‘Vibecoding to Virtuosity’ pathway—the curriculum for the Citizen Architect."
                />

            </div>
        </div>
    );
};

export default MissionPage;
</file_artifact>

<file path="src/app/showcase/page.tsx">
import { redirect } from 'next/navigation';

export default function ShowcasePage() {
  redirect('/showcase/report');
}
</file_artifact>

<file path="src/app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  :root {
    --background: 240 5% 96%; /* Light Gray */
    --foreground: 240 10% 3.9%; /* Almost Black */

    --card: 240 4.8% 95.9%;
    --card-foreground: 240 10% 3.9%;

    --popover: 0 0% 100%;
    --popover-foreground: 240 10% 3.9%;

    --primary: 240 5.9% 10%;
    --primary-foreground: 0 0% 98%;

    --secondary: 240 4.8% 95.9%;
    --secondary-foreground: 240 5.9% 10%;

    --muted: 240 4.8% 95.9%;
    --muted-foreground: 240 3.8% 46.1%;

    --accent: 240 4.8% 95.9%;
    --accent-foreground: 240 5.9% 10%;

    --destructive: 0 84.2% 60.2%;
    --destructive-foreground: 0 0% 98%;

    --border: 240 5.9% 90%;
    --input: 240 5.9% 90%;
    --ring: 240 5% 64.9%;

    --radius: 0.5rem;
  }

  .dark {
    --background: 222.2 84% 4.9%;
    --foreground: 210 40% 98%;

    --card: 222.2 84% 4.9%;
    --card-foreground: 210 40% 98%;

    --popover: 222.2 84% 4.9%;
    --popover-foreground: 210 40% 98%;

    --primary: 217.2 91.2% 59.8%;
    --primary-foreground: 222.2 47.4% 11.2%;

    --secondary: 217.2 32.6% 17.5%;
    --secondary-foreground: 210 40% 98%;

    --muted: 217.2 32.6% 17.5%;
    --muted-foreground: 215 20.2% 65.1%;

    --accent: 217.2 32.6% 17.5%;
    --accent-foreground: 210 40% 98%;

    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 210 40% 98%;

    --border: 217.2 32.6% 17.5%;
    --input: 217.2 32.6% 17.5%;
    --ring: 224.3 76.3% 48%;
  }
}

@layer base {
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground;
  }
}
</file_artifact>

<file path="src/app/layout.tsx">
import type { Metadata } from "next";
import { Inter } from "next/font/google";
import "./globals.css";
import { ThemeProvider } from "@/providers/theme-provider";
import Header from "@/components/layout/Header";
import Footer from "@/components/layout/Footer";
import React from "react";
import GlobalAudioPlayer from "@/components/global/GlobalAudioPlayer";
import FullscreenMediaViewer from "@/components/global/FullscreenMediaViewer";

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "AIAscent.dev | Home of the Data Curation Environment",
  description: "The official website for the Data Curation Environment (DCE) VS Code Extension. Learn how to revolutionize your AI-assisted development workflow.",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <body className={inter.className}>
        <ThemeProvider
          attribute="class"
          defaultTheme="dark"
          enableSystem
          disableTransitionOnChange
        >
          <div className="flex flex-col min-h-screen">
            <Header />
            <main className="flex-grow">
              {children}
            </main>
            <Footer />
          </div>
          <GlobalAudioPlayer />
          <FullscreenMediaViewer />
        </ThemeProvider>
      </body>
    </html>
  );
}
</file_artifact>

<file path="src/app/page.tsx">
// src/app/page.tsx
import HeroSection from "@/components/home/HeroSection";
import HowItWorksSection from "@/components/home/HowItWorksSection";
import FeaturesSection from "@/components/home/FeaturesSection";
import MissionSection from "@/components/home/MissionSection";
import ReportViewer from "@/components/report-viewer/ReportViewer";

export default function Home() {
return (
<div className="flex flex-col">
    <HeroSection />
    <HowItWorksSection />
    <FeaturesSection />

    {/* Homepage Whitepaper Visualization */}
    <section className="py-20 md:py-32 bg-background">
        <div className="container mx-auto px-4">
            <h2 className="text-3xl md:text-5xl font-bold text-center mb-16 bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground">
                Interactive Whitepaper: Process as Asset
            </h2>
            <div className="h-[80vh] w-full border rounded-lg shadow-lg overflow-hidden">
                <ReportViewer reportName="whitepaper" />
            </div>
        </div>
    </section>

    <MissionSection />
</div>
);
}
</file_artifact>

<file path="src/Artifacts/A0-Master-Artifact-List.md">
# Artifact A0: aiascent.dev - Master Artifact List

# Date Created: C0

# Author: AI Model & Curator

# Updated on: C5 (Update Anguilla Image System Prompt description)

## 1. Purpose

This file serves as the definitive, parseable list of all documentation artifacts for the `aiascent.dev` website project. This project aims to create a promotional website for the Data Curation Environment (DCE) VS Code Extension, featuring an interactive showcase.

## 2. Formatting Rules for Parsing

*   Lines beginning with `#` are comments and are ignored.
*   `##` denotes a major category header and is ignored.
*   `###` denotes an artifact entry. The text following it is the artifact's full name and ID.
*   Lines beginning with `- **Description:**` provide context for the project.
*   Lines beginning with `- **Tags:**` provide keywords for Inference.

## 3. Artifacts List

## I. Project Planning & Vision

### A1. aiascent.dev - Project Vision and Goals
- **Description:** High-level overview of the aiascent.dev website, its purpose to promote the DCE, and the phased development plan.
- **Tags:** project vision, goals, scope, dce, promotional website, interactive showcase

### A2. aiascent.dev - Phase 1 Requirements & Design
- **Description:** Detailed functional and technical requirements for Phase 1 of aiascent.dev, focusing on the static site shell and the interactive showcase.
- **Tags:** requirements, design, phase 1, features, nextjs, showcase

### A11. aiascent.dev - Implementation Roadmap
- **Description:** A step-by-step roadmap for the implementation of the aiascent.dev website, breaking the development into manageable and testable stages.
- **Tags:** roadmap, implementation plan, project management, development stages

### A23. aiascent.dev - Cognitive Capital Definition
- **Description:** Provides the canonical definition and explanation of "Cognitive Capital" as the term is used within the aiascent.dev project, distinguishing it from other interpretations.
- **Tags:** documentation, definition, cognitive capital, strategy, human capital, problem-solving

### A102. aiascent.dev - Homepage Hero Section Revamp Plan
- **Description:** A plan to revamp the homepage hero section to better communicate the core value proposition of the Data Curation Environment (DCE), focusing on the "vibe code for free" message and introducing new, more aspirational imagery.
- **Tags:** page design, home page, hero section, plan, marketing, content, image prompts

## II. Technical Architecture & Implementation

### A3. aiascent.dev - Technical Scaffolding Plan
- **Description:** Outlines the proposed technical scaffolding, file structure, and technology stack (Next.js, TypeScript, TailwindCSS) for the aiascent.dev website.
- **Tags:** technical plan, scaffolding, file structure, nextjs, react, tailwindcss, typescript

### A20. aiascent.dev - Report Viewer Integration Plan
- **Description:** A detailed plan for porting the "AI Ascent Report Viewer" from the `aiascentgame` context into the `aiascent.dev` project to serve as the primary component for the Showcase, Learn, and Home pages.
- **Tags:** report viewer, integration plan, porting, showcase, learn, component, architecture

### A21. aiascent.dev - Ask Ascentia RAG Integration
- **Description:** A guide explaining the implementation of the Retrieval-Augmented Generation (RAG) system for the "Ask @Ascentia" chat feature, including instructions for file placement and environment configuration.
- **Tags:** documentation, rag, chat, ascentia, embeddings, faiss, langchain, architecture

### A22. aiascent.dev - Mission Page Revamp Plan
- **Description:** A plan to refactor the static Mission page into a smaller, digestible, static version of the interactive report viewer, showcasing key concepts with associated imagery.
- **Tags:** page design, mission, report viewer, refactor, plan, ui, ux

### A24. aiascent.dev - Mission Page Content Expansion Plan
- **Description:** Provides the expanded, finalized content for the last three sections of the Mission Page to create a more comprehensive and compelling narrative.
- **Tags:** page design, mission, content, refactor, plan

### A25. aiascent.dev - Learn Page Content Plan
- **Description:** A blueprint for the `/learn` page, structuring its content around the "Vibecoding to Virtuosity" pathway to educate users on the methodology behind the DCE.
- **Tags:** page design, learn, content, plan, vibecoding, virtuosity, cognitive apprenticeship

### A26. aiascent.dev - Homepage Whitepaper Visualization Plan
- **Description:** Deconstructs the "Process as Asset" whitepaper into a structured format suitable for an interactive report viewer on the homepage. Includes content, a new image naming scheme, and new image generation prompts.
- **Tags:** page design, home page, report viewer, whitepaper, content, plan, image prompts

### A27. aiascent.dev - AI Persona - @Ascentia
- **Description:** Defines the persona, rules, and contextual system prompts for the @Ascentia AI assistant on the aiascent.dev website.
- **Tags:** documentation, persona, ai, ascentia, rag, prompt engineering

### A28. aiascent.dev - Dual Embedding RAG Architecture
- **Description:** A guide for implementing and managing a dual-embedding RAG system, allowing the chat assistant to use different knowledge bases for different sections of the website.
- **Tags:** documentation, rag, chat, ascentia, embeddings, faiss, architecture, multi-tenancy

### A30. aiascent.dev - Showcase Expansion Plan
- **Description:** A plan to expand the `/showcase` page into a multi-tabbed view, featuring both the interactive "Ascent Report" and an embedded version of the `aiascent.game` website.
- **Tags:** page design, showcase, tabs, iframe, integration, plan, ui, ux

### A32. aiascent.dev - Dynamic Chat Prompt Suggestions Plan
- **Description:** Outlines the technical implementation for generating, parsing, and displaying dynamic, context-aware follow-up questions ("chips") in the Ask @Ascentia chat interface.
- **Tags:** plan, chat, ui, ux, llm, prompt engineering, ascentia

### A33. aiascent.dev - Report Viewer Fullscreen Plan
- **Description:** Outlines the plan to implement a fullscreen toggle feature for the interactive report viewer, enhancing the immersive reading experience.
- **Tags:** plan, ui, ux, report viewer, fullscreen, feature

### A34. aiascent.dev - Whitepaper Introduction Content
- **Description:** Provides the new introductory content for the homepage's interactive whitepaper, "Process as Asset," designed to welcome users and explain the interface.
- **Tags:** page design, home page, report viewer, whitepaper, content, user guide

### A36. aiascent.dev - Learn Page - V2V Pathway Definition
- **Description:** Provides the expanded definitional content for the "Vibecoding to Virtuosity Pathway" section of the Learn page.
- **Tags:** learn, content, vibecoding, virtuosity, cognitive apprenticeship

### A37. aiascent.dev - Learn Page - Annotator and Toolmaker
- **Description:** Provides the expanded definitional content for the "Stages 1 & 2: The Annotator and The Toolmaker" section of the Learn page.
- **Tags:** learn, content, vibecoding, virtuosity, cognitive apprenticeship

### A38. aiascent.dev - Learn Page - Recursive Learner and Virtuoso
- **Description:** Provides the expanded definitional content for the "Stages 3 & 4: The Recursive Learner and The Virtuoso" section of the Learn page.
- **Tags:** learn, content, vibecoding, virtuosity, cognitive apprenticeship

### A39. aiascent.dev - Learn Page - Apex Skill Definition
- **Description:** Provides the expanded definitional content for "The Apex Skill: On-the-Fly Tooling" section of the Learn page.
- **Tags:** learn, content, vibecoding, virtuosity, cognitive apprenticeship

### A115 - GlobalLogic AI Micro-Pilot Proposal
- **Description:** A proposal for a micro-pilot leveraging the Data Curation Environment (DCE) methodology to address the exponential growth of task complexity and reduce the cognitive burden on Task Leads by distilling massive project context.
- **Tags:** proposal, ai, micro-pilot, consulting, context management, cognitive capital

## III. Design and Assets

### A15. aiascent.dev - Asset Wishlist and Directory Structure
- **Description:** A list of required visual assets (images, icons, logos) for the aiascent.dev website and the definitive structure for the `public/assets` directory.
- **Tags:** assets, wishlist, design, images, icons, file structure

### A15.1. aiascent.dev - Master Image Generation System Prompt
- **Description:** The master system prompt defining the aesthetic guidelines and thematic direction for all images generated for the aiascent.dev website.
- **Tags:** assets, design, images, prompt engineering, system prompt, aesthetic

### A15.2. aiascent.dev - Image Prompt - Logo (AS-01)
- **Description:** Specific prompt for generating the main logo (AS-01) for aiascent.dev.
- **Tags:** assets, design, images, prompt, logo

### A15.3. aiascent.dev - Image Prompt - Favicon (AS-02)
- **Description:** Specific prompt for generating the favicon (AS-02) for aiascent.dev.
- **Tags:** assets, design, images, prompt, favicon

### A15.4. aiascent.dev - Image Prompt - Icon: Context Curation (AS-04)
- **Description:** Specific prompt for generating the Context Curation icon (AS-04) for aiascent.dev.
- **Tags:** assets, design, images, prompt, icon

### A15.5. aiascent.dev - Image Prompt - Icon: Parallel Co-Pilot (AS-05)
- **Description:** Specific prompt for generating the Parallel Co-Pilot icon (AS-05) for aiascent.dev.
- **Tags:** assets, design, images, prompt, icon

### A15.6. aiascent.dev - Image Prompt - Icon: Iterative Workflow (AS-06)
- **Description:** Specific prompt for generating the Iterative Workflow icon (AS-06) for aiascent.dev.
- **Tags:** assets, design, images, prompt, icon

### A15.7. aiascent.dev - Image Prompt - OG:Image (AS-07)
- **Description:** Specific prompt for generating the Open Graph image (AS-07) for aiascent.dev social sharing.
- **Tags:** assets, design, images, prompt, ogimage, social media

### A16. aiascent.dev - Page Design: Home (Landing Page)
- **Description:** Detailed design blueprint for the main landing page (Home) of aiascent.dev, focusing on the value proposition, aesthetics, and user engagement.
- **Tags:** page design, home page, landing page, ui, ux, dce, citizen architect

### A17. aiascent.dev - Page Design: Showcase (Interactive Whitepaper)
- **Description:** Detailed design blueprint for the Showcase page, featuring the Interactive Whitepaper component.
- **Tags:** page design, showcase, interactive whitepaper, ui, ux, dce

### A18. aiascent.dev - Page Design: Learn (Tutorials and Education)
- **Description:** Detailed design blueprint for the Learn page, the educational hub for the DCE and the Citizen Architect methodology.
- **Tags:** page design, learn, tutorials, education, documentation, ui, ux

### A19. aiascent.dev - Page Design: Mission (About Us)
- **Description:** Detailed design blueprint for the Mission page, outlining the strategic vision, the concept of Cognitive Capitalism, and the purpose of the DCE project.
- **Tags:** page design, mission, about us, vision, strategy, cognitive capitalism

### A40. aiascent.dev - Page Design DCE
- **Description:** A blueprint for the `/dce` page, dedicated to explaining the core features of the Data Curation Environment VS Code extension with visual aids.
- **Tags:** page design, dce, features, plan, ui, ux

### A41. aiascent.dev - Page Design DCE - Artifacts as Source of Truth
- **Description:** A plan for a new section on the `/dce` page explaining how generating documentation artifacts is a core feature of the DCE workflow, establishing them as the project's "source of truth."
- **Tags:** page design, dce, features, plan, source of truth, documentation, artifacts

### A103. aiascent.dev - How It Works Section Image Prompts
- **Description:** Provides a set of new, detailed image prompts for the three core feature sections on the homepage, designed to align with the new "Citizen Architect" aesthetic.
- **Tags:** page design, home page, image prompts, marketing, content, aesthetic

### A106 - Re-branding Initiative - Phase 1 Plan
- **Description:** A master plan outlining the phased approach for a site-wide visual re-branding, starting with the generation of new persona likenesses and the revamping of the homepage whitepaper images.
- **Tags:** plan, re-branding, marketing, content, images, aesthetic, citizen architect

### A107 - Master Image System Prompt v2
- **Description:** The updated master system prompt for all image generation. It defines the core "Citizen Architect" aesthetic and introduces a new, critical section on "Likeness & Style Transfer" to guide the re-branding initiative.
- **Tags:** assets, design, images, prompt engineering, system prompt, aesthetic, re-branding

### A108 - Persona Likeness Generation Prompts
- **Description:** Provides the specific image prompts needed to generate the male and female "Citizen Architect" likeness cards for the "Career Transitioner" and "Underequipped Graduate" personas, establishing the foundational assets for the re-branding initiative.
- **Tags:** v2v, academy, re-branding, images, prompt engineering, persona, citizen architect

### A109 - Whitepaper Image Re-branding Prompts
- **Description:** A comprehensive list of new, abstract image prompts for all 19 pages of the "Process as Asset" whitepaper, designed to be used with the new "likeness and style transfer" workflow for the site-wide re-branding.
- **Tags:** v2v, academy, re-branding, images, prompt engineering, whitepaper, citizen architect

### A110 - V2V Academy - Citizen Architect Classes
- **Description:** A definitive guide to the six "Citizen Architect" classes for the V2V Academy. This artifact serves as the source of truth for the lore, abilities, and descriptions used in marketing materials and character cards.
- **Tags:** v2v, academy, re-branding, persona, citizen architect, lore, rpg

### A111 - GWU Appeal Letter - Sarkani
- **Description:** A bespoke appeal letter drafted for Professor Shahram Sarkani, Director of GW Online Engineering, leveraging his research on the NSA CSfC framework.
- **Tags:** appeal, gwu, sarkani, nsa, csfc, d.eng

### A112 - GWU Appeal Letter - Mazzuchi
- **Description:** A bespoke appeal letter drafted for Professor Thomas A. Mazzuchi, Co-Director of GW Online Engineering, focusing on the D.Eng. program and shared research interests.
- **Tags:** appeal, gwu, mazzuchi, d.eng, cybersecurity

### A113 - GWU Appeal Letter - Blackford
- **Description:** A bespoke appeal letter drafted for Professor J.P. Blackford, Doctoral Program Coordinator, focusing on a petition for programmatic re-alignment to the D.Eng. program.
- **Tags:** appeal, gwu, blackford, d.eng, programmatic re-alignment

### A114 - GWU Appeal Letter - Etemadi
- **Description:** A bespoke appeal letter drafted for Professor Amir Etemadi, framed as a research inquiry connecting to his work in cyber-attack detection.
- **Tags:** appeal, gwu, etemadi, research, cybersecurity

## IV. Process & Workflow

### A4. aiascent.dev - Universal Task Checklist
- **Description:** A structured checklist for tracking development tasks, feedback, and bugs for the aiascent.dev project, organized by file packages and complexity.
- **Tags:** checklist, task management, planning, roadmap

### A7. aiascent.dev - Development and Testing Guide
- **Description:** A guide providing the standard procedure for running, debugging, and testing the aiascent.dev Next.js website locally.
- **Tags:** documentation, project setup, development, testing, nextjs, workflow

### A14. aiascent.dev - GitHub Repository Setup Guide
- **Description:** A guide on setting up the aiascent.dev project with Git and GitHub, including the essential workflow for using Git alongside the Data Curation Environment (DCE).
- **Tags:** git, github, version control, workflow, setup, dce

### A29. aiascent.dev - GitHub Public Repository Guide
- **Description:** Provides guidance on the benefits, risks, and best practices for making a GitHub repository public, including how to audit for sensitive information.
- **Tags:** git, github, version control, security, best practices, open source

### A31. aiascent.dev - iframe Integration Guide
- **Description:** Explains the root cause of cross-domain cookie issues when embedding authenticated applications (like `aiascent.game` with NextAuth) in an iframe and provides the solution.
- **Tags:** iframe, authentication, cookies, samesite, nextauth, security, integration

### A35. aiascent.dev - Discord Community Management Plan
- **Description:** Outlines a strategic plan for building, managing, and monetizing a Discord community around the Data Curation Environment (DCE).
- **Tags:** plan, community, discord, monetization, dce, cognitive apprenticeship

### A48. NVIDIA CUDA on WSL Setup Guide
- **Description:** A straightforward guide for setting up NVIDIA CUDA on Windows Subsystem for Linux (WSL) 2 to enable GPU acceleration for Docker containers.
- **Tags:** guide, setup, cuda, wsl, docker, gpu, nvidia, troubleshooting

## V. V2V Online Academy

### A43. V2V Academy - Project Vision and Roadmap
- **Description:** High-level overview of the online training platform, its purpose, target audience, technical approach (including user authentication), and a phased development plan.
- **Tags:** project vision, goals, scope, v2v, training, roadmap, user authentication

### A44. V2V Academy - Content Research Proposal
- **Description:** A formal proposal outlining a research plan to discover, analyze, and synthesize existing public content related to the "prompt engineering to context engineering" paradigm and other V2V methodologies.
- **Tags:** research, content strategy, curriculum, prompt engineering, context engineering

### A45. V2V Academy - Key Learnings from Ryan Carson
- **Description:** A summary of the key concepts from Ryan Carson's "3-file system to vibe code production apps" video, which serves as an inspiration for structuring the AI development process.
- **Tags:** source material, research, workflow, development process, vibe coding

### A46. Whisper Transcription Setup Guide
- **Description:** A technical guide detailing a simple, Docker-based setup for using a high-performance Whisper API to transcribe audio recordings, with specific commands for PowerShell.
- **Tags:** guide, setup, whisper, transcription, docker, audio processing, api, wsl, gpu, nvidia, powershell, curl

### A47. David Gerabagi Resume (DCE Update)
- **Description:** An updated version of the curator's resume, reframing the primary project experience around the development of the Data Curation Environment (DCE) and aiascent.dev.
- **Tags:** resume, branding, professional profile, dce

### A49. V2V Academy - Research & Synthesis Plan
- **Description:** A formal plan for analyzing the provided coaching transcripts and project artifacts to reverse-engineer the curator's expert workflow and synthesize a curriculum for the V2V Academy.
- **Tags:** research, analysis, synthesis, curriculum design, v2v, cognitive apprenticeship

### A50. V2V Academy - Core Principles & Philosophy
- **Description:** Synthesizes the core principles and philosophical underpinnings of the "Vibecoding to Virtuosity" pathway, extracted from the curator's coaching transcripts.
- **Tags:** v2v, philosophy, principles, cognitive apprenticeship, mental models

### A51. V2V Academy - The Virtuoso's Workflow
- **Description:** A detailed, reverse-engineered breakdown of the curator's expert workflow, codifying the practical steps of the "Vibecoding to Virtuosity" pathway.
- **Tags:** v2v, workflow, process, cognitive apprenticeship, reverse engineering

### A52. V2V Academy - Foundational Skills Analysis
- **Description:** An analysis of the foundational skills required for the V2V pathway, derived by working backward from the Virtuoso's workflow. It prioritizes cognitive skills over traditional programming syntax.
- **Tags:** v2v, curriculum design, foundational skills, data curation, critical thinking

### A53. V2V Academy - Curriculum Outline
- **Description:** Proposes a multi-module curriculum structure for the V2V Academy, designed to guide learners from the fundamentals of "Vibecoding" to the mastery of the "Virtuoso's Workflow." Each lesson is tailored to three distinct learner personas.
- **Tags:** v2v, curriculum design, instructional design, learning pathway, cognitive apprenticeship, persona

### A54. V2V Academy - Lesson 1.1 - The Virtuoso's Loop
- **Description:** The detailed content for Lesson 1.1 of the V2V Academy, "The Virtuoso's Loop," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, workflow, interactive learning, persona

### A55. V2V Academy - Glossary of Terms
- **Description:** A comprehensive glossary of key terms and concepts related to the "Vibecoding to Virtuosity" (V2V) pathway and the Data Curation Environment (DCE).
- **Tags:** v2v, documentation, glossary, definitions, cognitive apprenticeship, context engineering

### A56. V2V Academy - Practical Exercises Plan
- **Description:** Outlines the plan for the practical exercises within the V2V Academy, centered on the project of incrementally building a fully functional, AI-powered interactive report viewer.
- **Tags:** v2v, curriculum, exercises, project-based learning, report viewer, rag

### A57. V2V Academy - C58 Response Analysis and Strategic Gaps
- **Description:** An analysis of the artifacts created in Cycle 58, showing their alignment with the source transcripts and identifying strategic gaps in the V2V Academy's planning.
- **Tags:** v2v, curriculum design, analysis, strategy, self-reflection

### A58. V2V Academy - Target Learner Personas
- **Description:** Defines the three primary target learner personas for the V2V Academy, outlining their backgrounds, motivations, and learning goals.
- **Tags:** v2v, curriculum design, learner persona, target audience

### A59. V2V Academy - Student Environment Guide
- **Description:** A guide for V2V Academy students, explaining the required software setup and the pedagogical model for interacting with the AI cognitive tutor during exercises.
- **Tags:** v2v, curriculum design, student guide, setup, cognitive tutor, vscode

### A60. V2V Academy - Assessment Philosophy
- **Description:** Documents the V2V Academy's philosophy on student assessment, emphasizing tangible outcomes and self-evaluation over traditional, high-overhead testing.
- **Tags:** v2v, curriculum design, assessment, project-based learning, self-assessment

### A61.1. Transcript 1 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-1.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.2. Transcript 2 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-2.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.3. Transcript 3 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-3.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.4. Transcript 4 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-4.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.5. Transcript 5 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-5.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.6. Transcript 6 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-6.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.7. Transcript 7 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-7.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.8. Transcript 8 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-8.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.9. Transcript 9 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-9.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.10. Transcript 10 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-10.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.11. Transcript 11 Summary
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-11.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

### A61.12. Transcript 12 Summary (Cycle 58 Context)
- **Description:** A high-level summary and synthesis of the key insights from the partial coaching transcript provided in the context for Cycle 58.
- **Tags:** v2v, research, synthesis, transcript analysis

### A62. V2V Academy - Synthesis of Research Proposals
- **Description:** A meta-reflection on the provided research proposals, summarizing key themes, strategic insights, and recurring patterns.
- **Tags:** v2v, research, synthesis, meta-analysis, strategy

### A63. V2V Academy - Lesson 1.2 - The Philosophy of V2V
- **Description:** The detailed content for Lesson 1.2 of the V2V Academy, "The Philosophy of V2V," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, philosophy, interactive learning, persona

### A64. V2V Academy - Lesson 1.3 - The Citizen Architect
- **Description:** The detailed content for Lesson 1.3 of the V2V Academy, "The Citizen Architect," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, citizen architect, interactive learning, persona

### A65. V2V Academy - Lesson 2.1 - Introduction to Data Curation
- **Description:** The detailed content for Lesson 2.1 of the V2V Academy, "Introduction to Data Curation," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, data curation, context engineering, interactive learning, persona

### A66. V2V Academy - Lesson 2.2 - The Art of Annotation
- **Description:** The detailed content for Lesson 2.2 of the V2V Academy, "The Art of Annotation," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, data annotation, metadata, context engineering, interactive learning, persona

### A67. V2V Academy - Lesson 2.3 - Critical Analysis of AI Output
- **Description:** The detailed content for Lesson 2.3 of the V2V Academy, "Critical Analysis of AI Output," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, critical thinking, ai literacy, validation, interactive learning, persona

### A68. V2V Academy - Lesson 3.1 - From Conversation to Command
- **Description:** The detailed content for Lesson 3.1 of the V2V Academy, "From Conversation to Command," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, structured interaction, prompt engineering, context engineering, interactive learning, persona

### A69. V2V Academy - Lesson 3.2 - The Feedback Loop in Practice
- **Description:** The detailed content for Lesson 3.2 of the V2V Academy, "The Feedback Loop in Practice," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, feedback loop, debugging, cognitive apprenticeship, interactive learning, persona

### A70. V2V Academy - Lesson 3.3 - The Test-and-Revert Workflow
- **Description:** The detailed content for Lesson 3.3 of the V2V Academy, "The Test-and-Revert Workflow," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, git, version control, testing, cognitive apprenticeship, interactive learning, persona

### A71. V2V Academy - Lesson 4.1 - Defining Your Vision
- **Description:** The detailed content for Lesson 4.1 of the V2V Academy, "Defining Your Vision," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, project scope, mvp, planning, interactive learning, persona

### A72. V2V Academy - Lesson 4.2 - The Blank Page Problem
- **Description:** The detailed content for Lesson 4.2 of the V2V Academy, "The Blank Page Problem," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, project scope, scaffolding, planning, interactive learning, persona

### A73. V2V Academy - Lesson 4.3 - Architecting Your MVP
- **Description:** The detailed content for Lesson 4.3 of the V2V Academy, "Architecting Your MVP," designed for the interactive report viewer. It includes three parallel versions of the content for different learner personas.
- **Tags:** v2v, curriculum, lesson plan, project scope, architecture, planning, interactive learning, persona

### A74. V2V Academy - Interactive Curriculum Plan
- **Description:** A plan for a new, interactive curriculum page on aiascent.dev. It details a persona-based selection screen that leads to a tailored version of the entire V2V Academy curriculum presented within the interactive report viewer.
- **Tags:** v2v, curriculum, interactive learning, plan, ui, ux, report viewer, persona

### A75. V2V Academy - Persona Image System Prompt
- **Description:** The master system prompt defining the distinct visual aesthetics for the three learner personas of the V2V Academy, to be used for all image generation.
- **Tags:** v2v, curriculum, images, prompt engineering, system prompt, persona, aesthetic

### A76. V2V Academy - Image Prompts (Career Transitioner)
- **Description:** A comprehensive list of persona-specific image prompts for every page of the "Career Transitioner" curriculum in the V2V Academy.
- **Tags:** v2v, curriculum, images, prompt engineering, persona, career transitioner

### A77. V2V Academy - Image Prompts (Underequipped Graduate)
- **Description:** A comprehensive list of persona-specific image prompts for every page of the "Underequipped Graduate" curriculum in the V2V Academy.
- **Tags:** v2v, curriculum, images, prompt engineering, persona, underequipped graduate

### A78. V2V Academy - Image Prompts (Young Precocious)
- **Description:** A comprehensive list of persona-specific image prompts for every page of the "Young Precocious" curriculum in the V2V Academy.
- **Tags": v2v, curriculum, images, prompt engineering, persona, young precocious

### A79. V2V Academy - Image Generation Script Guide
- **Description": A comprehensive guide for using the `generate_images.mjs` script to automate the creation of visual assets for the V2V Academy curriculum.
- **Tags": v2v, curriculum, images, script, automation, guide, tooling

### A80. V2V Academy - Image Generation Test Harness Guide
- **Description": A guide for using the `image_harness.mjs` script to test different static prompt strategies with the Imagen 4 model, helping to diagnose prompt engineering issues and reverse-engineer an optimal prompt structure.
- **Tags": v2v, curriculum, images, script, automation, guide, tooling, testing, imagen, prompt engineering

### A81. V2V Academy - Lab 1 - Your First Portfolio
- **Description": A step-by-step lab guide for first-time users on how to create a portfolio website from scratch using Visual Studio Code and the Data Curation Environment (DCE) extension.
- **Tags": v2v, curriculum, lab, project-based learning, dce, portfolio, git, getting started

### A82. V2V Academy - Labs and Courses UI Plan
- **Description": A plan to update the `/academy` page to include a new section for hands-on labs, separating them from the theoretical V2V curriculum lessons.
- **Tags": v2v, curriculum, labs, page design, plan, ui, ux

### A83. V2V Academy - Simulating a Fresh Environment Guide
- **Description": A guide for the curator on how to safely simulate a "fresh" development environment to replicate the experience of a new V2V Academy learner, for the purpose of creating accurate tutorials and GIFs.
- **Tags": guide, v2v, curriculum, labs, testing, git, dev containers, docker

### A97. V2V Academy - Lab 1 Media Descriptions
- **Description": Provides detailed, step-by-step descriptions for the screen recording videos used in Lab 1 of the V2V Academy.
- **Tags": v2v, curriculum, lab, documentation, media, accessibility

### A98. V2V Academy - Academy Page Image Prompts
- **Description": Provides a set of specific image prompts for generating cover and thumbnail images for the V2V Academy homepage, including personas, labs, and courses.
- **Tags": v2v, curriculum, images, prompt engineering, persona, aesthetic

### A99. V2V Academy - Course 1: The AI-Powered Report Viewer - Vision and Roadmap
- **Description": High-level overview of the V2V Academy's first monetizable course, "The AI-Powered Report Viewer," outlining its purpose, learning objectives, target audience, and a phased development plan.
- **Tags": v2v, curriculum, course design, project-based learning, report viewer, roadmap

### A100. V2V Academy - Course 1: The AI-Powered Report Viewer - Curriculum Outline
- **Description": A detailed curriculum outline for the V2V Academy's first course, "The AI-Powered Report Viewer," breaking the project into a logical sequence of modules and lessons.
- **Tags": v2v, curriculum, course design, project-based learning, report viewer

### A101. V2V Academy - Course 1: The AI-Powered Report Viewer - Lab Plan
- **Description": A plan for the practical exercises and labs within the "The AI-Powered Report Viewer" course, detailing the hands-on projects for each module.
- **Tags": v2v, curriculum, labs, project-based learning, report viewer

### A104 - V2V Academy - Account System Design
- **Description": An adaptation of the `aiascent.game` account system, outlining the architecture for user authentication and progress tracking for the V2V Academy on `aiascent.dev`.
- **Tags": v2v, academy, plan, architecture, authentication, nextauth, prisma, database

### A105 - aiascent.dev - Google OAuth Setup Guide
- **Description": A guide for setting up Google OAuth credentials for the `aiascent.dev` user account system.
- **Tags": v2v, academy, guide, setup, authentication, oauth, google

## VI. Anguilla Project

### A201 - Anguilla Project - Vision and Master Plan
- **Description:** The high-level strategic vision for transforming Anguilla into the world's first AI-native nation, leveraging its unique digital asset (.ai domain) and small population size. Updated C2 to include core principles of Sovereignty, Culture, and Resilience.
- **Tags:** anguilla, strategy, vision, nation building, ai, sovereignty, resilience

### A202 - Research Proposal - The AI Capital
- **Description:** A proposal focused on leveraging the .ai domain windfall to fund sovereign digital infrastructure, specifically emphasizing green, hurricane-resilient data centers.
- **Tags:** anguilla, economics, .ai domain, infrastructure, sovereign wealth, green energy, resilience

### A203 - Research Proposal - The Cognitive Citizenry
- **Description:** A proposal for a national upskilling initiative using the V2V/DCE methodology, featuring a "Cultural Heritage AI" component to preserve local history and dialect.
- **Tags:** anguilla, education, upskilling, v2v, cognitive capital, workforce, culture, heritage

### A204 - Research Proposal - The Automated State
- **Description:** A proposal for modernizing Anguilla's governance through AI, creating a frictionless, automated civil service with a focus on "Continuity of Government" during climate disasters.
- **Tags:** anguilla, governance, automation, public services, efficiency, disaster recovery

### A205 - Research Proposal - Resilient Island Systems
- **Description:** A proposal for using AI to manage critical island resources (water, energy) and enhance climate resilience through predictive modeling and digital twins.
- **Tags:** anguilla, sustainability, environment, climate change, resource management, digital twin

### A206 - Research Proposal - The Global AI Sandbox
- **Description:** A proposal to establish Anguilla as a "Regulatory Sandbox" for ethical AI development, ensuring frameworks respect local culture and prevent digital colonialism.
- **Tags:** anguilla, regulation, policy, sandbox, innovation, ethics, sovereignty

### A207 - Strategic Presentation Guide
- **Description:** A script and strategic guide for the meeting with the Minister of IT, outlining the narrative arc, key talking points, and the "ask," woven with themes of political sovereignty, cultural preservation, and climate resilience.
- **Tags:** anguilla, presentation, strategy, pitch, meeting guide

### A208 - Anguilla Project - Image System Prompt
- **Description:** The dedicated system prompt for generating imagery for the Anguilla Project. It defines the "Solarpunk Caribbean" aesthetic and includes specific protocols for rendering text and cultural allegories.
- **Tags:** anguilla, images, prompt engineering, system prompt, aesthetic, solarpunk, text rendering
</file_artifact>

<file path="src/Artifacts/A1-Project-Vision-and-Goals.md">
# Artifact A1: aiascent.dev - Project Vision and Goals

# Date Created: C0

# Author: AI Model & Curator

  - **Key/Value for A0:**
  - **Description:** High-level overview of the aiascent.dev website, its purpose to promote the DCE, and the phased development plan.
  - **Tags:** project vision, goals, scope, dce, promotional website, interactive showcase

## 1. Project Vision

The vision of **aiascent.dev** is to create a professional, engaging, and authoritative promotional website for the **Data Curation Environment (DCE) VS Code Extension**. It will serve as the primary public-facing hub for the DCE project, clearly articulating its value proposition and demonstrating its power. It aims to be more than a static landing page; it will be a living testament to the capabilities of the DCE by showcasing complex, interactive components that were themselves built using the extension.

## 2. High-Level Goals & Phases

The project will be developed in distinct phases to ensure an iterative and manageable workflow.

### Phase 1: Core Website and Interactive Showcase

The goal of this phase is to establish the foundational website and deliver the primary showcase content.
-   **Core Functionality:**
-   Set up a modern, statically generated website using Next.js and TailwindCSS.
-   Create a compelling landing page that explains the DCE's purpose and benefits.
-   Develop an "Interactive Showcase" (e.g., an interactive whitepaper or a visualization of the DCE workflow) that demonstrates a complex product built using the DCE.
-   **Outcome:** A functional, deployed website at aiascent.dev where visitors can learn about the DCE and interact with a live demonstration of its capabilities.

### Phase 2: Educational Content and Tutorials

This phase will build upon the foundation by adding educational content to foster adoption and teach the AI-assisted development methodology.
-   **Core Functionality:**
-   Create a dedicated section for tutorials and guides.
-   Develop the first set of tutorials explaining how to set up and use the DCE, focusing on the "vibe coding" workflow.
-   Implement a simple blog or articles section for development updates and conceptual deep-dives.
-   **Outcome:** The website becomes a key educational resource for developers wanting to master AI-assisted development with the DCE.

### Phase 3: Community Hub and Downloads

This phase focuses on community building and deeper integration with the DCE ecosystem.
-   **Core Functionality:**
-   Integrate community links (e.g., Discord, GitHub Discussions).
-   Create a showcase of projects built with the DCE.
-   Provide direct download links and installation instructions for the DCE extension's `.vsix` file.
-   **Outcome:** aiascent.dev becomes the central community and distribution hub for the Data Curation Environment project.
</file_artifact>

<file path="src/Artifacts/A2-Phase1-Requirements.md">
# Artifact A2: aiascent.dev - Phase 1 Requirements & Design

# Date Created: C0

# Author: AI Model & Curator

  - **Key/Value for A0:**
  - **Description:** Detailed functional and technical requirements for Phase 1 of aiascent.dev, focusing on the static site shell and the interactive showcase.
  - **Tags:** requirements, design, phase 1, features, nextjs, showcase

## 1. Overview

This document outlines the detailed requirements for Phase 1 of the **aiascent.dev** project. The primary goal of this phase is to launch the core website and implement the interactive showcase demonstrating the DCE's capabilities, as defined in A1 (Project Vision).

## 2. Functional Requirements

| ID | Requirement | User Story | Acceptance Criteria |
|---|---|---|---|
| FR-01 | **Static Website Shell** | As a visitor, I want to land on a professional homepage that explains what the DCE is, so that I can quickly understand its purpose and value. | - The website has a main landing page (`/`). <br> - A persistent header provides navigation (e.g., Home, Showcase, Tutorials, GitHub). <br> - A persistent footer contains standard links and copyright information. <br> - The landing page content introduces the DCE and its core benefits. |
| FR-02 | **Interactive Showcase** | As a visitor, I want to navigate to an interactive showcase, so that I can see a tangible example of what the DCE can build. | - A page exists (e.g., `/showcase` or `/whitepaper`). <br> - This page renders an interactive component (e.g., "Interactive Whitepaper"). <br> - The component loads its content from a structured data source (JSON). <br> - Users can navigate through the content in an engaging way. |
| FR-03 | **Responsive Design** | As a visitor on a mobile device, I want the website to be easy to read and navigate, so that I can access the information on the go. | - The website layout adapts seamlessly to different screen sizes (desktop, tablet, mobile). <br> - Navigation elements are accessible on mobile (e.g., hamburger menu). |

## 3. Non-Functional Requirements

| ID | Requirement | Description |
|---|---|---|
| NFR-01 | **Performance** | The website must load quickly. As a static site (SSG), the goal is for the initial page load to be under 2 seconds. |
| NFR-02 | **Aesthetics** | The design should be modern, clean, and professional, reflecting the nature of a sophisticated developer tool. |
| NFR-03 | **Maintainability** | The codebase should be well-structured, utilizing TypeScript and following best practices for Next.js and TailwindCSS. |

## 4. High-Level Design

The implementation of Phase 1 will involve the following components:

-   **Next.js Application:** The core framework providing routing and rendering.
-   **Layout Components (`Header.tsx`, `Footer.tsx`):** Reusable components defining the persistent navigation and structure.
-   **Landing Page (`pages/index.tsx` or `app/page.tsx`):** The main entry point, featuring marketing copy and calls to action.
-   **Showcase Component (`InteractiveWhitepaper.tsx`):** A complex React component responsible for rendering the interactive content, managing its internal state (e.g., current page), and handling user navigation within the showcase.
-   **Data Source (`whitepaperContent.json`):** The structured content that drives the showcase component.
</file_artifact>

<file path="src/Artifacts/A3-Technical-Scaffolding-Plan.md">
# Artifact A3: aiascent.dev - Technical Scaffolding Plan

# Date Created: C0

# Author: AI Model & Curator

# Updated on: C37 (Clarify image directory structure)

  - **Key/Value for A0:**
  - **Description:** Outlines the proposed technical scaffolding, file structure, and technology stack (Next.js, TypeScript, TailwindCSS) for the aiascent.dev website.
  - **Tags:** technical plan, scaffolding, file structure, nextjs, react, tailwindcss, typescript

## 1. Overview

This document outlines the proposed technical scaffolding and file structure for the **aiascent.dev** project. This plan aims to establish a modern, efficient, and scalable architecture suitable for a promotional and educational website.

## 2. Technology Stack

-   **Language:** TypeScript
-   **Framework:** Next.js (for React framework, routing, and Static Site Generation - SSG)
-   **Styling:** TailwindCSS (Utility-first CSS framework for rapid UI development)

  - **Component Library:** Shadcn/ui (Optional, for pre-built accessible components)
    -   **Hosting:** Vercel, Netlify, or self-hosted (TBD, optimized for static sites)

## 3. Proposed File Structure

The project will adhere to the modern Next.js App Router structure for optimal performance and organization:

```
aiascent-dev/
├── src/
│   ├── app/
│   │   ├── layout.tsx
│   │   ├── page.tsx
│   │   ├── globals.css
│   │   └── showcase/
│   │       └── page.tsx
│   │
│   ├── components/
│   │   ├── layout/
│   │   │   ├── Header.tsx
│   │   │   └── Footer.tsx
│   │   ├── showcase/
│   │   │   └── InteractiveWhitepaper.tsx
│   │   └── ui/
│   │
│   ├── lib/
│   │
│   └── data/
│       └── whitepaperContent.json
│
├── public/
│   ├── assets/
│   │   ├── icons/
│   │   ├── images/
│   │   │   ├── report/       # Images for the main 'showcase' report
│   │   │   └── whitepaper/   # Images for the homepage 'whitepaper' report
│   │   ├── logo.svg
│   │   └── favicon.ico
│   ├── data/                 # For JSON files, etc.
│   │   └── embeddings/       # For RAG knowledge base files
│   └── downloads/            # For downloadable files like the .vsix
│
├── package.json
├── tsconfig.json
├── tailwind.config.ts
└── next.config.js
```

## 4. Key Architectural Concepts

-   **Next.js App Router:** Utilizing the latest Next.js features for efficient routing, layouts, and server components where applicable.
-   **Static Site Generation (SSG):** We will leverage SSG to pre-render pages at build time. This ensures maximum performance, SEO benefits, and security.
-   **Component-Based UI:** The UI will be built using reusable React components, ensuring consistency and maintainability.
-   **TypeScript:** TypeScript will be used throughout the project to ensure type safety, improve code quality, and enhance the developer experience.
-   **Utility-First CSS:** TailwindCSS allows for rapid styling directly within the markup, reducing context switching.
</file_artifact>

<file path="src/Artifacts/A4-Universal-Task-Checklist.md">
# Artifact A4: aiascent.dev - Universal Task Checklist

# Date Created: C0

# Author: AI Model & Curator

# Updated on: C9 (Fix dropdown close behavior)

  - **Key/Value for A0:**
  - **Description:** A structured checklist for tracking development tasks, feedback, and bugs for the aiascent.dev project, organized by file packages and complexity.
  - **Tags:** checklist, task management, planning, roadmap

## 1. Purpose

This artifact provides a structured format for tracking development tasks for the `aiascent.dev` website. It organizes work by the group of files involved and estimates complexity (token count and cycle count) to aid in planning for AI-assisted development.

## 2. How to Use

(See M3. Interaction Schema or T17. Template - Universal Task Checklist.md for detailed usage instructions.)

-----

## Task List for Cycle 8+

## T-13: Enhance Navigation and Layout
- **Files Involved:**
    - `src/app/anguilla/page.tsx` (New)
    - `src/app/showcase/[slug]/page.tsx`
    - `src/components/layout/Header.tsx`
    - `src/components/global/NextPageSection.tsx`
- **Total Tokens:** ~2,500
- **More than one cycle?** No
- **Status:** Complete

- [x] **Task (T-ID: 13.1):** Create `ProjectSelector.tsx` to replace the tab bar with a compact dropdown. (Superseded by Header dropdown)
- [x] **Task (T-ID: 13.2):** Create `ShowcaseGame.tsx` to encapsulate the iframe logic.
- [x] **Task (T-ID: 13.3):** Implement dynamic routing in `src/app/showcase/[slug]/page.tsx`.
- [x] **Task (T-ID: 13.4):** Implement auto-fullscreen and autoplay logic. (Moved to explicit `/anguilla` route)
- [x] **Task (T-ID: 13.5):** Update `src/app/showcase/page.tsx` to act as a redirect.
- [x] **Task (T-ID: 13.6):** Add bottom padding to `ReportChatPanel` to fix alignment issues.
- [x] **Task (T-ID: 13.7):** Reduce header padding in `ReportViewer` and `PageNavigator` to save vertical space.
- [x] **Task (T-ID: 13.8):** Create dedicated `/anguilla` presentation route.
- [x] **Task (T-ID: 13.9):** Move Showcase navigation to main Header dropdown.
- [x] **Task (T-ID: 13.10):** Fix spacing between ReportViewer and bottom banner.
- [x] **Bug Fix (T-ID: 13.11):** Ensure Header dropdown closes automatically upon selection.

### Verification Steps
1.  Navigate to `https://aiascent.dev/anguilla`.
2.  **Expected:** The Anguilla report loads, automatically enters fullscreen, and starts autoplaying.
3.  Navigate to `https://aiascent.dev/showcase/anguilla`.
4.  **Expected:** The Anguilla report loads normally (embedded), without auto-fullscreen.
5.  Hover over "Showcase" in the top navigation bar.
6.  **Expected:** A dropdown menu appears with options for "The Ascent Report", "Anguilla Project", and "AI Ascent Game".
7.  Click on any option in the "Showcase" dropdown.
8.  **Expected:** The page navigates to the selected project AND the dropdown menu closes immediately.
9.  Check the bottom of the showcase page.
10. **Expected:** The "Ready to Become a Citizen Architect?" banner is flush (or appropriately spaced) relative to the report viewer, without excessive white space.

## T-14: Plan for Next Cycle
- **Files Involved:**
    - `src/Artifacts/A4-Universal-Task-Checklist.md`
- [ ] **Task (T-ID: 14.1):** Review progress, update this checklist, and prioritize tasks for the subsequent cycle.

-----

## Completed Tasks
(Previous tasks T-1 through T-12 are omitted for brevity)
</file_artifact>

<file path="src/Artifacts/A5-Dual Domain Hosting Guide.md">
# Artifact A114: AI Ascent - Dual Domain Hosting Guide
# Date Created: C117
# Author: AI Model & Curator
# Updated on: C5 (Add Troubleshooting & Maintenance section)

- **Key/Value for A0:**
- **Description:** A guide explaining how to host multiple domains (e.g., `aiascent.game` and `aiascent.dev`) on a single server using a reverse proxy like Caddy.
- **Tags:** guide, networking, hosting, reverse proxy, caddy, dns

## 1. Overview & Goal

You have asked if it's possible to host both `aiascent.game` and the new `aiascent.dev` on the same server that is currently hosting the game and the vLLM instance. The answer is **yes**, and this is a standard and efficient way to manage multiple websites on a single machine.

The goal of this guide is to explain the technical concept of a **reverse proxy** and provide a concrete example of how to configure it using Caddy, which you are already using.

## 2. The Core Concept: Reverse Proxy with Virtual Hosts

The magic that makes this work is a **reverse proxy** that uses **virtual hosts**. Here's how the pieces fit together:

1.  **DNS Records:** You will configure the DNS "A" records for both `aiascent.game` and `aiascent.dev` to point to the **same public IP address**—the one for your home server.

2.  **Port Forwarding:** Your AT&T router will continue to forward all web traffic (ports 80 for HTTP and 443 for HTTPS) to the single PC in your closet that acts as the server.

3.  **The Reverse Proxy (Caddy):** This is the traffic controller. Caddy will be the only process listening on ports 80 and 443. When a request comes in, Caddy inspects the `Host` header to see which domain the user was trying to reach.
    *   If the `Host` is `aiascent.game`, Caddy forwards the request to the Node.js process running your game.
    *   If the `Host` is `aiascent.dev`, Caddy forwards the request to the *different* Node.js process running your new website.

4.  **Backend Applications:** Each of your applications (the game server, the new website server) will run on its own, separate, internal-only port (e.g., 3001 for the game, 3002 for the new website). They don't need to know anything about HTTPS or the public domains.

This architecture is secure, efficient, and makes adding more websites in the future very simple.

## 3. Example Caddyfile Configuration

Your existing `Caddyfile` (from `A91`) is already set up to handle `aiascent.game`. To add the new `aiascent.dev` site, you simply need to add another block to the file.

Let's assume:
*   Your `aiascent.game` Node.js server runs on `localhost:3001`.
*   Your new `aiascent-dev` Next.js server will run on `localhost:3002`.

Your new `Caddyfile` would look like this:

```caddy
# Caddyfile for dual domain hosting

aiascent.game {
    # Caddy will automatically handle HTTPS for this domain.
    encode zstd gzip
    log {
        output file /var/log/caddy/aiascent_game.log
    }

    # Reverse proxy all requests for aiascent.game to the game server on port 3001.
    reverse_proxy localhost:3001 {
        header_up Host {host}
        header_up X-Real-IP {remote_ip}
        header_up X-Forwarded-For {remote_ip}
        header_up X-Forwarded-Proto {scheme}
        header_up Connection {>Connection}
        header_up Upgrade {>Upgrade}
    }
}

aiascent.dev {
    # Caddy will automatically handle HTTPS for this domain as well.
    encode zstd gzip
    log {
        output file /var/log/caddy/aiascent_dev.log
    }

    # Reverse proxy all requests for aiascent.dev to the new website server on port 3002.
    reverse_proxy localhost:3002
}

# Optional: Redirect www versions to the main domains
www.aiascent.game {
    redir https://aiascent.game{uri} permanent
}
www.aiascent.dev {
    redir https://aiascent.dev{uri} permanent
}
```

## 4. Action Steps

1.  **DNS:** Point the `aiascent.dev` A record to your server's public IP address.
2.  **Application Ports:** Ensure your two applications are configured to run on different ports (e.g., 3001 and 3002).
3.  **Caddyfile:** Update your `Caddyfile` with the new block for `aiascent.dev`.
4.  **Reload Caddy:** Run `caddy reload` in your server's terminal to apply the new configuration.

Caddy will automatically obtain the SSL certificate for `aiascent.dev` and begin routing traffic to the correct application based on the domain name.

## 5. Troubleshooting & Maintenance

If the site is inaccessible after a server restart or update, follow this checklist.

### 5.1. Check Caddy Status
Caddy is the gateway. If it's down, everything is down.
*   **Command:** `tasklist | findstr caddy` (Windows CMD) or `Get-Process caddy` (PowerShell).
*   **Fix:** If not running, start it.
    *   **If Service:** `net start caddy`
    *   **If Manual:** Navigate to the folder containing `Caddyfile` and run `caddy run`.

### 5.2. Check Application Ports
Ensure your Next.js apps are actually running on the ports Caddy expects (e.g., 3001, 3002).
*   **Command:** `pm2 logs aiascent-dev --lines 50`
*   **Look for:** `Ready on http://localhost:3002` (or whatever port you configured).
*   **Fix:** If the port doesn't match the Caddyfile, either update the Caddyfile and reload (`caddy reload`), or update the app's start command (e.g., `next start -p 3002`).

### 5.3. Verify Local Access
Isolate the issue by trying to access the app directly on the server, bypassing Caddy.
*   **Action:** Open a browser on the server (via RDP) and visit `http://localhost:3002`.
*   **Result:**
    *   **Works:** The app is fine. The issue is Caddy or the Firewall.
    *   **Fails:** The app is crashed or not binding correctly. Check PM2 logs.

### 5.4. Firewall
Ensure Windows Firewall isn't blocking the ports.
*   **Public Access:** Ports 80 and 443 must be allowed for Caddy.
*   **Internal Access:** Ports 3001/3002 usually don't need inbound rules if Caddy and the App are on the same machine (communicating via localhost).
</file_artifact>

<file path="src/Artifacts/A6-Porting Guide for aiascent.dev.md">
# Artifact A115: DCE - Porting Guide for aiascent.dev
# Date Created: C117
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A list of recommended documentation artifacts to port from the DCE project to the new `aiascent.dev` project to bootstrap its development process.
- **Tags:** guide, documentation, project setup, aiascent-dev

## 1. Overview

To effectively bootstrap the `aiascent.dev` project using the Data Curation Environment (DCE), it is highly recommended to port over a set of existing documentation artifacts from the DCE project itself. These artifacts codify the development process, workflow, and interaction patterns that will be essential for building the new website.

This guide lists the specific artifacts you should copy from your main `DCE/src/Artifacts` directory into the `aiascent-dev/context/dce/` directory.

## 2. Recommended Artifacts to Port

The following artifacts provide the "source of truth" for the DCE-driven development process. They will be invaluable as context when prompting the AI to build the `aiascent.dev` website.

### Core Process & Workflow
*   **`A0. DCE Master Artifact List.md`**: Provides the structure and concept of the master list.
*   **`A9. DCE - GitHub Repository Setup Guide.md`**: Essential for initializing the new project's version control.
*   **`A65. DCE - Universal Task Checklist.md`**: The template and philosophy for organizing work in cycles.
*   **`A69. DCE - Animated UI Workflow Guide.md`**: Documents the "perfect loop" of the DCE workflow, which is a key concept to showcase and teach.
*   **`A70. DCE - Git-Integrated Testing Workflow Plan.md`**: The baseline/restore workflow is a core feature of the development process that should be used for the new project.
*   **`A72. DCE - README for Artifacts.md`**: Explains the purpose of the artifacts directory to both the user and the AI.

### Interaction & Parsing
*   **`A52.1 DCE - Parser Logic and AI Guidance.md`**: Provides the AI with the literal parser code, enabling metainterpretability.
*   **`A52.2 DCE - Interaction Schema Source.md`**: The canonical rules for how the AI should structure its responses to be parsed correctly by the DCE.

### Content & Showcase
*   **`A77. DCE - Whitepaper Generation Plan.md`**: The original plan for generating the whitepaper.
*   **`A78. DCE - Whitepaper - Process as Asset.md`**: The full content of the whitepaper that you intend to display in the interactive report viewer.
*   **`reportContent.json`**: The structured JSON data from `aiascent.game`'s report viewer, which can be used as the data source for the new `InteractiveWhitepaper` component.

### 3. Procedure

1.  Navigate to your `C:\Projects\DCE\src\Artifacts` directory.
2.  Copy the files listed above.
3.  Paste them into the `C:\Projects\aiascent-dev\context\dce\` directory.
4.  You can now use these files as part of the context when generating prompts for the `aiascent.dev` project within the DCE.
</file_artifact>

<file path="src/Artifacts/A7-Development-and-Testing-Guide.md">
# Artifact A7: aiascent.dev - Development and Testing Guide
# Date Created: C0
# Author: AI Model & Curator

  - **Key/Value for A0:**
  - **Description:** A guide providing the standard procedure for running, debugging, and testing the aiascent.dev Next.js website locally.
  - **Tags:** documentation, project setup, development, testing, nextjs, workflow

## 1. Purpose

This guide provides the standard procedure for running, debugging, and testing the **aiascent.dev** application locally.

## 2. Development Workflow

### Step 1: Install Dependencies

Ensure all project dependencies are installed. Navigate to the project root directory in your terminal and run:

```bash
npm install
# or if using yarn
# yarn install
```

### Step 2: Start the Development Server

To compile the code and start the Next.js development server with hot-reloading, run the following command:

```bash
npm run dev
```

### Step 3: Running the Application

Once the development server is running, it will typically be available at `http://localhost:3000`. Open this URL in your web browser to view the application. The server will automatically refresh the page when you save changes to the source files.

### Step 4: Debugging

Debugging is primarily done using the browser's developer tools (DevTools).

  - **Client-Side Debugging:** Open DevTools (F12 or right-click -> Inspect) and navigate to the "Console" tab for logs or the "Sources" (Chrome/Edge) / "Debugger" (Firefox) tab to set breakpoints directly in the TypeScript source code (thanks to source maps).
  - **React State:** Install the React Developer Tools browser extension to inspect component state and props.

## 3. Testing

The project will be configured with a testing framework (e.g., Jest and React Testing Library) as development progresses. To run the test suite, use the following command:

```bash
npm run test
```

This will execute all test files located in the project and report the results to the console.

## 4. Building for Production

To create an optimized production build of the application, run:

```bash
npm run build
```

This generates the necessary files for deployment. You can then run the production build locally using:

```bash
npm run start
</file_artifact>

<file path="src/Artifacts/A9-GitHub-Repository-Setup-Guide.md">
# Artifact A9: aiascent.dev - GitHub Repository Setup Guide
# Date Created: C0
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A step-by-step guide with the necessary git commands to initialize the project as a local repository and push it to a new remote repository on GitHub.
- **Tags:** git, github, version control, setup, repository, workflow

## 1. Overview

This guide provides the necessary commands to turn your local `aiascent-dev` project folder into a Git repository and link it to a new, empty repository on GitHub.

## 2. Prerequisites

*   You have `git` installed on your machine.
*   You have a GitHub account.

## 3. Step-by-Step Setup

### Step 1: Create a New Repository on GitHub

1.  Go to [github.com](https://github.com) and log in.
2.  In the top-right corner, click the `+` icon and select **"New repository"**.
3.  **Repository name:** `aiascent-dev`.
4.  **Description:** "Promotional and educational website for the Data Curation Environment (DCE) VS Code Extension."
5.  Choose **"Private"** or **"Public"**.
6.  **IMPORTANT:** Do **not** initialize the repository with a `README`, `.gitignore`, or `license`. We will be pushing our existing files.
7.  Click **"Create repository"**.

GitHub will now show you a page with command-line instructions. We will use the section titled **"...or push an existing repository from the command line"**.

### Step 2: Initialize Git in Your Local Project

Open a terminal and navigate to your project's root directory (`C:\Projects\aiascent-dev`). Then, run the following commands one by one.

1.  **Initialize the repository:**
    ```bash
    git init
    ```

2.  **Add all existing files:**
    ```bash
    git add .
    ```

3.  **Create the first commit:**
    ```bash
    git commit -m "Initial commit: Project setup and Cycle 0 artifacts"
    ```

4.  **Rename the default branch to `main`:**
    ```bash
    git branch -M main
    ```

### Step 3: Link and Push to GitHub

1.  **Add the remote repository:** Replace the placeholder URL with the one from your new GitHub repository page.
    ```bash
    git remote add origin https://github.com/dgerabagi/aiascent-dev.git
    ```

2.  **Push your local `main` branch to GitHub:**
    ```bash
    git push -u origin main
    ```

Your new project is now set up with version control and linked to GitHub. You can now use the DCE's Git-integrated features like "Baseline" and "Restore" as you develop the website.
</file_artifact>

<file path="src/Artifacts/A11-Implementation-Roadmap.md">
# Artifact A11: aiascent.dev - Implementation Roadmap

# Date Created: C0

# Author: AI Model & Curator

# Updated on: C11 (Reflect current progress and new feature integration)

  - **Key/Value for A0:**
  - **Description:** A step-by-step roadmap for the implementation of the aiascent.dev website, breaking the development into manageable and testable stages.
  - **Tags:** roadmap, implementation plan, project management, development stages

## 1. Overview & Goal

This document provides a clear, step-by-step roadmap for the implementation of **aiascent.dev**. This roadmap breaks the project vision (A1) into smaller, manageable, and testable steps. The goal is to build the functionality incrementally, ensuring a stable foundation at each stage.

## 2. Implementation Steps

### Step 1: Foundational Setup & Scaffolding (Completed)

-   **Goal:** Create the basic project structure and initialize the development environment.
-   **Outcome:** A runnable Next.js application with the core technical structure in place.

### Step 2: Landing Page UI Development (Completed)

-   **Goal:** Build the main landing page UI and core navigation.
-   **Outcome:** A visually complete and responsive landing page.

### Step 3: Visual Polish and Theming (Cycle 11)

-   **Goal:** Address outstanding visual bugs and implement a comprehensive light mode theme.
-   **Tasks:**
    1.  **Hero Section:** Fix sizing and background issues with the main `pcp.gif`.
    2.  **Light Theme:** Implement a full light mode color palette, fixing all readability and aesthetic issues.
-   **Outcome:** A polished, professional website that looks great in both dark and light modes.

### Step 4: Core Content Pages (Cycle 11-12)

-   **Goal:** Resolve 404s by creating the main content pages.
-   **Tasks:**
    1.  **Mission Page:** Implement the `/mission` page with its strategic narrative.
    2.  **Learn & Showcase Shells:** Create the placeholder pages for `/learn` and `/showcase` to prepare for the next step.
-   **Outcome:** All main navigation links lead to functional pages.

### Step 5: Interactive Showcase Implementation (Cycle 12+)

-   **Goal:** Develop the core feature of Phase 1 by porting and integrating the AI Ascent Report Viewer.
-   **Tasks:**
    1.  **Asset & Data Integration:** Place the report JSON data and image assets into the `public` directory.
    2.  **Component Porting:** Adapt the report viewer components and Zustand store from the `aiascentgame` context.
    3.  **Integration:** Embed the adapted `ReportViewer` component into the `/showcase` page.
-   **Outcome:** A functional interactive showcase that demonstrates the DCE's capabilities by displaying the full AI Ascent Report.

### Step 6: Feature Expansion and Deployment (Cycle 13+)

-   **Goal:** Reuse the report viewer for other pages and prepare for deployment.
-   **Tasks:**
    1.  **Homepage Integration:** Adapt the report viewer to display the smaller whitepaper on the homepage.
    2.  **Learn Page Integration:** Enhance the viewer to support a curriculum of multiple reports.
    3.  **Final Polish & Testing:** Conduct thorough cross-browser/device testing.
    4.  **Deployment:** Configure the hosting environment and deploy the application.
-   **Outcome:** The Phase 1 website is feature-complete and live to the public.
</file_artifact>

<file path="src/Artifacts/A14-GitHub-Repository-Setup-Guide.md">
# Artifact A14: aiascent.dev - GitHub Repository Setup Guide

# Date Created: C0

# Author: AI Model & Curator

  - **Key/Value for A0:**
  - **Description:** A guide on setting up the aiascent.dev project with Git and GitHub, including the essential workflow for using Git alongside the Data Curation Environment (DCE).
  - **Tags:** git, github, version control, workflow, setup, dce

## 1. Overview

This guide provides the necessary commands to turn your local `aiascent.dev`project folder into a Git repository, link it to a new repository on GitHub, and outlines the standard workflow for using Git alongside the Data Curation Environment (DCE).

## 2. Prerequisites

*   You have `git`installed on your machine.
*   You have a GitHub account.

## 3. Step-by-Step Setup

### Step 1: Create a New Repository on GitHub

1.  Go to [github.com](https://github.com) and log in.
2.  In the top-right corner, click the `+`icon and select **"New repository"**.
3.  **Repository name:** `aiascent-dev`(or similar).
4.  **Description:** "Promotional and educational website for the Data Curation Environment (DCE) VS Code Extension."
5.  Choose **"Private"** or **"Public"**.
6.  **IMPORTANT:** Do **not** initialize the repository with a `README`, `.gitignore`, or `license`. We will be pushing our existing files.
7.  Click **"Create repository"**.

### Step 2: Initialize Git in Your Local Project

Open a terminal and navigate to your project's root directory. Then, run the following commands one by one.

1.  **Initialize the repository:**
    `git init`

2.  **Create/Update `.gitignore`:** Ensure you have a `.gitignore`file. Crucially, it must include `.vscode/`to prevent DCE state files from causing issues, along with standard Next.js ignores. You can create a basic one with:
    ```bash
    echo "node_modules/\n.next/\n.env.local\n.vscode/" > .gitignore
    ```

3.  **Add all existing files:**
    `git add .`

4.  **Create the first commit:**
    `git commit -m "C0: Initial commit with project artifacts"`

5.  **Rename the default branch to `main`:**
    `git branch -M main`

### Step 3: Link and Push to GitHub

1.  **Add the remote repository:** Replace the placeholder URL with the one from your GitHub repository page.
    `git remote add origin https://github.com/YOUR_USERNAME/aiascent-dev.git`

2.  **Push your local `main`branch to GitHub:**
    `git push -u origin main`

## 4. Standard Development Workflow with DCE and Git

Git is essential for managing the iterative changes produced by the DCE. It allows you to quickly test an AI's proposed solution and revert it cleanly if it doesn't work.

### Step 1: Start with a Clean State

Before starting a new cycle, ensure your working directory is clean (`git status`). All previous changes should be committed.

### Step 2: Generate and Parse Responses

Use the DCE to generate a `prompt.md`file. Get multiple responses from your AI model, paste them into the Parallel Co-Pilot Panel, and click "Parse All".

### Step 3: Accept and Test

1.  Review the responses and select one that looks promising.
2.  Use the **"Accept Selected Files"** button (or the integrated "Baseline" feature if available) to write the AI's proposed changes to your workspace.
3.  Compile and test the website (`npm run dev`). Does it work? Are there errors?

### Step 4: The "Restore" Loop

*   **If the changes are bad (e.g., introduce bugs):**
    1.  Open the terminal in VS Code.
    2.  Run the command: `git restore .`
    3.  This instantly discards all uncommitted changes, reverting your files to the state of your last commit.
    4.  You are now back to a clean state and can select a *different* AI response in the DCE panel and test the next solution.

*   **If the changes are good:**
    1.  Stage the changes (`git add .`).
    2.  Write a commit message (e.g., "C1: Implement Next.js scaffolding").
    3.  Commit the changes (`git commit -m "..."`).
    4.  You are now ready to start the next development cycle.
</file_artifact>

<file path="src/Artifacts/A15-Asset-Wishlist.md">
# Artifact A15: aiascent.dev - Asset Wishlist and Directory Structure

# Date Created: C2

# Author: AI Model & Curator

# Updated on: C17 (Add Downloadable Assets section)

  - **Key/Value for A0:**
  - **Description:** A list of required visual assets (images, icons, logos) for the aiascent.dev website and the definitive structure for the `public/assets` directory.
  - **Tags:** assets, wishlist, design, images, icons, file structure, downloads

## 1. Overview

This document outlines the visual assets required for the initial launch (Phase 1) of aiascent.dev. It also defines the directory structure within the `public/` folder where these assets should be placed. Placeholder files have been created in this cycle (C2) to establish this structure.

## 2. Asset Wishlist

The aesthetic direction is modern, professional, and sophisticated, often utilizing a dark theme with vibrant accents (e.g., electric blue, cyan) to convey the power and precision of the DCE tool.

| ID | Asset Name | Description | Format | Status | Location |
| :--- | :--- | :--- | :--- | :--- | :--- |
| AS-01 | **Logo** | The main logo for aiascent.dev. Should be clean and work on both light and dark backgrounds. | SVG | Needed | `public/assets/logo.svg` |
| AS-02 | **Favicon** | The small icon displayed in the browser tab. | ICO/PNG | Needed | `public/assets/favicon.ico` |
| AS-03 | **Hero Image (DCE Screenshot)** | A high-quality screenshot of the DCE extension in action (e.g., File Tree View and Parallel Co-Pilot Panel open mid-project). This is the centerpiece of the landing page. | PNG/WEBP | Curator Provided | `public/assets/images/dce-hero-screenshot.png` |
| AS-04 | **Icon: Context Curation** | An icon representing the ability to select and manage files for AI context. (e.g., a file tree with checkmarks, or a magnifying glass over files). | SVG | Needed | `public/assets/icons/context-curation.svg` |
| AS-05 | **Icon: Parallel Co-Pilot** | An icon representing the comparison of multiple AI responses. (e.g., side-by-side panels, or branching paths). | SVG | Needed | `public/assets/icons/parallel-copilot.svg` |
| AS-06 | **Icon: Iterative Workflow** | An icon representing the cycle-based development process. (e.g., a circular arrow, or a gear turning). | SVG | Needed | `public/assets/icons/iterative-workflow.svg` |
| AS-07 | **OG:Image** | The image used when the website is shared on social media. Often a combination of the logo and a compelling visual (like AS-03). | PNG (1200x630) | Needed | `public/assets/images/og-image.png` |

## 3. Public Directory Structure

The following structure will be used to organize assets.

```
public/
├── assets/
│   ├── icons/
│   │   ├── context-curation.svg
│   │   ├── parallel-copilot.svg
│   │   └── iterative-workflow.svg
│   │
│   ├── images/
│   │   ├── dce-hero-screenshot.png
│   │   └── og-image.png
│   │
│   ├── logo.svg
│   └── favicon.ico
│
└── ... (other public files)
```

## 4. Downloadable Assets

This section specifies the location for downloadable files, such as application installers.

*   **Location:** `public/downloads/`
*   **Purpose:** To host files that users can download directly from the website.
*   **Current Files:**
    *   `data-curation-environment-0.1.10.vsix`: The VS Code extension installer package.
</file_artifact>

<file path="src/Artifacts/A15.1-Master-Image-System-Prompt.md">
# Artifact A15.1: aiascent.dev - Master Image Generation System Prompt

# Date Created: C3

# Author: AI Model

  - **Key/Value for A0:**
  - **Description:** The master system prompt defining the aesthetic guidelines and thematic direction for all images generated for the aiascent.dev website.
  - **Tags:** assets, design, images, prompt engineering, system prompt, aesthetic

## 1. Purpose

This document provides the master system prompt to be used when generating visual assets (icons, logos, illustrations) for the aiascent.dev website. Its goal is to ensure a consistent, high-quality, and thematically coherent visual identity across the entire site.

## 2. The System Prompt

**Master System Prompt: The DCE Aesthetic**

You are an expert graphic designer and digital artist specializing in creating assets for sophisticated developer tools and strategic platforms. Your task is to generate visual assets for aiascent.dev, the official website for the Data Curation Environment (DCE) VS Code extension.

**Your Core Directives:**

1.  **Adhere to the Master Aesthetic:** The aesthetic is **Modern, Precise, and Futuristic Minimalism**.

      * **Color Palette:** Primarily monochromatic (blacks, whites, grays) with strategic use of vibrant, futuristic accent colors (Electric Blue, Cyan, Deep Purple). Assets must look excellent on both dark and light backgrounds, but prioritize a **dark-mode-first** appearance.
      * **Style:** Clean lines, sharp edges, and geometric shapes. Avoid excessive ornamentation, gradients (unless subtle and used for depth), or cartoonish styles. The look should evoke precision engineering, advanced technology, and clarity of thought.
      * **Themes:** The underlying themes are Human-AI collaboration, workflow efficiency, data management, and the concept of the "Citizen Architect."

2.  **Asset Specific Guidelines:**

      * **Logos & Icons (SVG):**

          * Must be vector-based (SVG).
          * Must be simple, scalable, and instantly recognizable even at small sizes.
          * Use solid colors or very subtle gradients.
          * Ensure paths are clean and optimized.

      * **Illustrations & Hero Images (PNG/WEBP):**

          * Should be high-resolution and professional.
          * If depicting technology (like screenshots or abstract visualizations), maintain the clean, minimalist aesthetic.
          * Lighting should be dramatic but clean, often using the accent colors to highlight key elements.

3.  **Thematic Cohesion:** Every asset must reinforce the idea that the DCE is a powerful, professional tool that enhances human intelligence and streamlines complex workflows.

**Your Workflow:**

I will provide you with specific requests for assets (e.g., "Icon for Context Curation"). You will apply these Master Aesthetic guidelines to generate the requested asset in the specified format.
</file_artifact>

<file path="src/Artifacts/A15.2-Image-Prompt-Logo.md">
# Artifact A15.2: Image Prompt - Logo (AS-01)

# Date Created: C3

# Author: AI Model

  - **Key/Value for A0:**
  - **Description:** Specific prompt for generating the main logo (AS-01) for aiascent.dev.
  - **Tags:** assets, design, images, prompt, logo

## 1. Asset Request

**Asset ID:** AS-01
**Asset Name:** Logo
**Format:** SVG
**Location:** `public/assets/logo.svg`

## 2. Generation Prompt

*(This prompt is to be used in conjunction with the Master System Prompt in A15.1)*

**Prompt:**

Generate a minimalist, vector-based (SVG) logo for "AIAscent.dev".

**Concept:** The logo should evoke themes of ascent, data flow, and precision. It should be abstract rather than literal.

**Ideas:**

1.  A stylized, geometric representation of a mountain peak or upward arrow, composed of interconnected lines or nodes (representing data curation and workflow).
2.  A combination of the letters 'A' and 'D', integrated into an upward-moving shape.
3.  A circular icon representing a continuous workflow cycle, with sharp, precise elements inside.

**Aesthetic Requirements:**

  * Use the Master Aesthetic (A15.1): Modern, Precise, Futuristic Minimalism.
  * Color: Primarily white or light gray, potentially with an Electric Blue or Cyan accent.
  * Style: Extremely clean lines, geometric, scalable.
  * Format: Optimized SVG.
</file_artifact>

<file path="src/Artifacts/A15.3-Image-Prompt-Favicon.md">
# Artifact A15.3: Image Prompt - Favicon (AS-02)

# Date Created: C3

# Author: AI Model

  - **Key/Value for A0:**
  - **Description:** Specific prompt for generating the favicon (AS-02) for aiascent.dev.
  - **Tags:** assets, design, images, prompt, favicon

## 1. Asset Request

**Asset ID:** AS-02
**Asset Name:** Favicon
**Format:** ICO/PNG (High-resolution PNG suitable for conversion)
**Location:** `public/assets/favicon.ico`

## 2. Generation Prompt

*(This prompt is to be used in conjunction with the Master System Prompt in A15.1)*

**Prompt:**

Generate a favicon based on the main logo concept (A15.2).

**Concept:** The favicon should be a simplified, bold version of the main logo mark, optimized for visibility at very small sizes (16x16, 32x32).

**Aesthetic Requirements:**

  * Use the Master Aesthetic (A15.1): Modern, Precise, Futuristic Minimalism.
  * Color: High contrast is essential. Use the primary accent color (Electric Blue or Cyan) on a dark background, or vice versa.
  * Style: Extremely simple, geometric, bold lines.
  * Format: High-resolution PNG (e.g., 256x256) with transparency if applicable.
</file_artifact>

<file path="src/Artifacts/A15.7-Image-Prompt-OGImage.md">
# Artifact A15.7: Image Prompt - OG:Image (AS-07)

# Date Created: C3

# Author: AI Model

  - **Key/Value for A0:**
  - **Description:** Specific prompt for generating the Open Graph image (AS-07) for aiascent.dev social sharing.
  - **Tags:** assets, design, images, prompt, ogimage, social media

## 1. Asset Request

**Asset ID:** AS-07
**Asset Name:** OG:Image
**Format:** PNG (1200x630 pixels)
**Location:** `public/assets/images/og-image.png`

## 2. Generation Prompt

*(This prompt is to be used in conjunction with the Master System Prompt in A15.1)*

**Prompt:**

Generate an Open Graph image (1200x630 pixels) for the aiascent.dev website.

**Concept:** This image is displayed when the website is shared on social media. It must be visually compelling, professional, and clearly communicate the website's purpose.

**Elements to Include:**

1.  **Background:** A dark, sophisticated background (e.g., deep black or dark gray), potentially with subtle technological textures or a faint grid/particle effect (similar to the Hero section aesthetic).
2.  **Logo/Title:** The "AIAscent.dev" title or logo, prominently displayed.
3.  **Tagline:** The core value proposition: "Master the Human-AI Workflow. Become a Citizen Architect."
4.  **Visual Anchor:** An abstract visualization of the DCE workflow or a highly stylized, aesthetically pleasing representation of the DCE interface (e.g., a polished version of the hero screenshot, framed elegantly).

**Aesthetic Requirements:**

  * Use the Master Aesthetic (A15.1): Modern, Precise, Futuristic Minimalism.
  * Color: Dark background, high contrast text (white/light gray), vibrant accents (Electric Blue/Cyan) used to draw the eye.
  * Style: Cinematic, clean, professional. Ensure text is large enough to be readable when embedded in social feeds.
  * Format: 1200x630 PNG.
</file_artifact>

<file path="src/Artifacts/A16-Page-Design-Home.md">
# Artifact A16: aiascent.dev - Page Design: Home (Landing Page)

# Date Created: C2

# Author: AI Model & Curator

# Updated on: C3 (Incorporate pcp.gif into the Hero section)

  - **Key/Value for A0:**
  - **Description:** Detailed design blueprint for the main landing page (Home) of aiascent.dev, focusing on the value proposition, aesthetics, and user engagement.
  - **Tags:** page design, home page, landing page, ui, ux, dce, citizen architect

## 1. Purpose and Goal

The Home page is the primary entry point for all visitors. Its goal is to immediately convey the purpose and power of the Data Curation Environment (DCE), establishing credibility and motivating developers to explore the tool and the underlying philosophy of the "Citizen Architect."

## 2. Target Audience

Primary: Software developers, AI engineers, technical project managers.
Secondary: Strategic thinkers, policymakers interested in AI human capital.

## 3. Aesthetic and Tone

  * **Aesthetic:** Sophisticated, modern, and precise. We will adopt a dark-mode-first design (similar to high-end developer tools like VS Code or Linear) with vibrant, futuristic accents (e.g., electric blue, cyan, or deep purple). The background should be dark and immersive (e.g., `bg-neutral-950` or similar).
  * **Tone:** Authoritative, inspiring, and urgent. The copy should emphasize the transformative potential of the DCE and the strategic necessity of mastering AI-assisted development.

## 4. Page Structure and Content

### 4.1. Header/Navigation

  * Standard site header (from `src/components/layout/Header.tsx`).
  * The header should be fixed or sticky, with a dark, semi-transparent background (`bg-black/40 backdrop-blur-lg`) to maintain the aesthetic.
  * Logo on the left, navigation links (Home, Showcase, Learn, Mission, GitHub) in the center or right, and the dark/light mode toggle.

### 4.2. Section 1: The Hero (Above the Fold)

  * **Layout:** A large, impactful section utilizing a dark background, potentially with subtle background animations (e.g., particles or a faint grid, similar to the `SparklesCore` component in the `automationsaas` context) to add depth.
  * **Headline:** "Master the Human-AI Workflow. Become a Citizen Architect."
  * **Subheadline:** "The Data Curation Environment (DCE) is the essential VS Code extension for developers who want to move beyond prompt-and-pray. Curate context with precision, test AI solutions rapidly, and build complex systems with confidence."
  * **CTA:** Primary Button: "Explore the Showcase" (Links to `/showcase`). Secondary Button: "Download Now" (Links to GitHub releases or VS Code Marketplace).
  * **Visual (Updated C3):** The centerpiece will utilize the `ContainerScroll` component (from AutomationSaaS) to provide a dynamic, engaging presentation. Inside the ContainerScroll, we will feature a combination of the `dce-hero-screenshot.png` (A15, AS-03) and the `pcp.gif` (located at `public/assets/images/pcp.gif`) to show both the interface and the workflow in action.

### 4.3. Section 2: The Problem & The Solution (Features)

  * **Layout:** A three-column grid of cards (potentially using `3d-card` component for subtle depth).
  * **Headline:** "Stop Fighting Your Tools. Start Building the Future."
  * **Points (Visualized with Icons from A15):**
      * **Feature 1 (Icon AS-04):** **Precision Context Curation.** Stop manual copy-pasting. DCE provides an intuitive, visual way to select and manage the exact files needed for your AI prompts directly within VS Code.
      * **Feature 2 (Icon AS-05):** **Parallel Co-Pilot & Rapid Testing.** Don't rely on a single AI response. Compare multiple solutions side-by-side and use the Git-integrated testing workflow (Baseline/Restore) to safely audition code changes in seconds.
      * **Feature 3 (Icon AS-06):** **Iterative Knowledge Graph.** AI collaboration shouldn't be ephemeral. DCE captures the entire development process—prompts, responses, and decisions—as an iterative, auditable knowledge graph.

### 4.4. Section 3: The DCE Workflow Visualization

  * **Layout:** A visually engaging, potentially interactive diagram illustrating the DCE cycle.
  * **Headline:** "The Power of Iteration: The DCE Workflow"
  * **Concept:** A stylized visualization showing the steps: 1. Curate Context -> 2. Generate Prompt -> 3. Parallel AI Responses -> 4. Test & Select -> 5. Integrate & Commit.
  * *UI Idea:* Use subtle animations or hover effects to highlight each step of the workflow.

### 4.5. Section 4: The Mission (Cognitive Capitalism)

  * **Layout:** A visually distinct section utilizing the `LampComponent` aesthetic from `automationsaas` for dramatic lighting and focus.
  * **Headline:** "More Than Code: The Rise of Cognitive Capitalism."
  * **Content:** A brief, compelling summary of the strategic vision—that mastering AI collaboration is essential for competitiveness and individual empowerment. This section connects the tool (DCE) to the broader mission (combating AI centralization and domination policies).
  * **CTA:** "Read Our Mission" (Links to `/mission`).

### 4.6. Footer

  * Standard site footer (from `src/components/layout/Footer.tsx`).
</file_artifact>

<file path="src/Artifacts/A17-Page-Design-Showcase.md">
# Artifact A17: aiascent.dev - Page Design: Showcase (Interactive Whitepaper)

# Date Created: C2

# Author: AI Model & Curator

# Updated on: C19 (Add technical note about header overlap)

  - **Key/Value for A0:**
  - **Description:** Detailed design blueprint for the Showcase page, featuring the Interactive Whitepaper component.
  - **Tags:** page design, showcase, interactive whitepaper, ui, ux, dce

## 1. Purpose and Goal

The Showcase page is the core demonstration of the DCE's capabilities. Its goal is to present a complex, interactive artifact (the Interactive Whitepaper) that was itself built using the DCE workflow. This page proves the value proposition by showing, not just telling.

## 2. Target Audience

Developers and technical leads looking for concrete examples of what the DCE can achieve.

## 3. Aesthetic and Tone

  * **Aesthetic:** Clean, focused, and immersive. The design should minimize distractions and maximize the real estate dedicated to the interactive component.
  * **Tone:** Educational, demonstrative, and professional.

## 4. Page Structure and Content

### 4.1. Header/Navigation

  * Standard site header.

### 4.2. Section 1: Introduction

  * **Layout:** Centered introduction text above the main component.
  * **Headline:** "The Proof is the Process: An Interactive Whitepaper."
  * **Subheadline:** "Explore a deep dive into the philosophy and strategy behind the Data Curation Environment. This entire interactive component—from the structured data to the UI—was developed using the DCE's iterative workflow."
  * **Context:** Briefly explain what the user is looking at and how to interact with it.

### 4.3. Section 2: The Interactive Whitepaper Component

  * **Layout:** The main content area is dominated by the `ReportViewer.tsx` component. It should be housed within a visually distinct container (e.g., a large card or a bordered area) to separate it from the page shell.
  * **Component Features (as implemented in `ReportViewer.tsx`):**
      * Clear display of the current section and page title.
      * Prominent display of the "TL;DR" summary.
      * Scrollable main content area (for longer text).
      * Intuitive navigation controls (Previous/Next buttons, progress indicator).
      * Image gallery/viewer associated with the content.
      * Table of contents side panel.

### 4.4. Section 3: How It Was Built (The Meta-Commentary)

  * **Layout:** A section below the interactive component providing context on the development process.
  * **Headline:** "Behind the Scenes: Built with DCE."
  * **Content:** Briefly explain the DCE concepts used to build the component:
      * **Documentation First:** How artifacts (like this one) guided the development.
      * **Iterative Cycles:** Mentioning the cycle count or the evolution of the component.
      * **Context Curation:** How the source material (the whitepaper text) was curated and structured.
  * **CTA:** "See the Code on GitHub" (Links to the specific component's source code).

### 4.5. Footer

  * Standard site footer.

## 5. Technical Implementation Notes

*   **Header Overlap:** The main site header (`Header.tsx`) is a fixed-position element. The `ReportViewer` component on this page is designed to fill the remaining viewport height (`h-[calc(100vh-4rem)]`). To prevent the fixed header from obscuring the top of the report viewer, the root container of the `ReportViewer` component **must** have top padding applied (e.g., `pt-16` which corresponds to the header's height of `h-16` or `4rem`). This pushes the component's content down, making all UI elements fully visible. Failure to apply this padding will result in a visual regression where elements like the chat panel's "clear" button are hidden behind the header.
</file_artifact>

<file path="src/Artifacts/A18-Page-Design-Learn.md">
# Artifact A18: aiascent.dev - Page Design: Learn (Tutorials and Education)

# Date Created: C2

# Author: AI Model & Curator

  - **Key/Value for A0:**
  - **Description:** Detailed design blueprint for the Learn page, the educational hub for the DCE and the Citizen Architect methodology.
  - **Tags:** page design, learn, tutorials, education, documentation, ui, ux

## 1. Purpose and Goal

The Learn page (planned for Phase 2, designed in C2) will be the central educational hub for aiascent.dev. Its goal is to onboard new users to the DCE extension and, more importantly, to teach the methodology and mindset of the "Citizen Architect." It aims to empower users to master AI-assisted development.

## 2. Target Audience

Developers actively learning or using the DCE extension.

## 3. Aesthetic and Tone

  * **Aesthetic:** Structured, clear, and easy to navigate. The design should prioritize readability and information hierarchy, similar to modern documentation sites (e.g., Next.js docs, Stripe docs).
  * **Tone:** Instructional, supportive, and practical.

## 4. Page Structure and Content

### 4.1. Header/Navigation

  * Standard site header.

### 4.2. Section 1: Introduction and Getting Started

  * **Layout:** A prominent welcome section.
  * **Headline:** "Master the DCE Workflow. Accelerate Your Development."
  * **Subheadline:** "From installation to advanced techniques, this hub provides the resources you need to leverage the full power of the Data Curation Environment."
  * **Key Links (Cards/Tiles):**
      * "Installation Guide"
      * "Your First Cycle: A Step-by-Step Tutorial"
      * "Understanding Artifacts and the 'Source of Truth'"
      * "The Git-Integrated Testing Workflow (Baseline/Restore)"

### 4.3. Section 2: Core Concepts (The Citizen Architect Methodology)

  * **Layout:** A dedicated section explaining the philosophy behind the tool.
  * **Headline:** "The Citizen Architect Mindset."
  * **Content:** Articles or deep-dives on key concepts:
      * "What is a Citizen Architect?"
      * "Cognitive Capitalism: Why Your Process is Your Asset."
      * "Metainterpretability: Understanding How the AI Parses Your Output."
      * "The Power of Parallel Scrutiny (Vibe Coding)."

### 4.4. Section 3: Advanced Tutorials and Use Cases

  * **Layout:** A categorized list of tutorials demonstrating specific applications of the DCE.
  * **Headline:** "Advanced Techniques."
  * **Topics (Examples):**
      * "Refactoring Large Codebases with DCE."
      * "Building Interactive UI Components (Case Study: The Whitepaper Viewer)."
      * "Managing Complex Data Models and Migrations."
      * "Integrating Local LLMs with the DCE."

### 4.5. Footer

  * Standard site footer.
</file_artifact>

<file path="src/Artifacts/A19-Page-Design-Mission.md">
# Artifact A19: aiascent.dev - Page Design: Mission (About Us)

# Date Created: C2

# Author: AI Model & Curator

# Updated on: C19 (Add new section defining Cognitive Capital)

  - **Key/Value for A0:**
  - **Description:** Detailed design blueprint for the Mission page, outlining the strategic vision, the concept of Cognitive Capitalism, and the purpose of the DCE project.
  - **Tags:** page design, mission, about us, vision, strategy, cognitive capitalism

## 1. Purpose and Goal

The Mission page explains the "why" behind the Data Curation Environment. It goes beyond the technical features to articulate the strategic vision: the creation of "Citizen Architects" and the necessity of decentralized AI expertise (Cognitive Capitalism) as a countermeasure to centralized AI strategies (e.g., China's AI domination policy).

## 2. Target Audience

Strategic thinkers, policymakers, developers interested in the broader implications of AI, and potential collaborators.

## 3. Aesthetic and Tone

  * **Aesthetic:** Serious, impactful, and visionary. The design should use bold typography, strong contrast, and potentially imagery that evokes themes of strategy, intelligence, and empowerment.
  * **Tone:** Urgent, visionary, and empowering.

## 4. Page Structure and Content

### 4.1. Header/Navigation

  * Standard site header.

### 4.2. Section 1: Defining Our Terms (New C19)
  * **Layout:** A strong opening statement defining the core concept of the project.
  * **Headline:** "What is Cognitive Capital?"
  * **Content:** Explain the project's specific definition: "an individual, group, or society's collective ability to solve problems." Contrast this with other academic definitions which may focus on knowledge as a tradable resource or its sociological roots. Emphasize that in the context of aiascent.dev, Cognitive Capital is a practical, measurable capacity for innovation and resilience. One nation may have more workers, but another may have far more Cognitive Capital.

### 4.3. Section 2: The Vision

  * **Layout:** A strong opening statement defining the core philosophy.
  * **Headline:** "Empowering the Citizen Architect."
  * **Content:** Introduce the concept of the "Citizen Architect"—individuals empowered by AI tools (like the DCE) to build, analyze, and maintain complex systems that were previously only accessible to large institutions.

### 4.4. Section 3: The Strategic Imperative (The Threat)

  * **Layout:** A section detailing the context of global AI competition.
  * **Headline:** "The Centralization of Cognitive Power."
  * **Content:** Briefly explain the threat posed by centralized, state-sponsored AI strategies (like China's). Emphasize that the current Western approach to AI labor (the "fissured workplace" or reliance on opaque models) is a strategic vulnerability.

### 4.5. Section 4: The Counter-Strategy (Cognitive Capitalism)

  * **Layout:** The core argument of the mission.
  * **Headline:** "Our Strategy: Decentralized Expertise and Cognitive Capitalism."
  * **Content:** Define "Cognitive Capitalism"—a system where the means of intellectual production are decentralized, and individuals are empowered to leverage AI to create value. Argue that an army of empowered Citizen Architects is the most viable counter-strategy to centralized AI power.

### 4.6. Section 5: The Role of the DCE

  * **Layout:** Connecting the strategy back to the product.
  * **Headline:** "The DCE: The Essential Toolkit for the Citizen Architect."
  * **Content:** Explain how the DCE is not just a productivity tool, but the foundational infrastructure for enabling this vision. It provides the structured workflow, auditability, and efficiency needed for decentralized AI development.

### 4.7. Section 6: Call to Action

  * **Layout:** A concluding section inviting participation.
  * **Headline:** "Join the Ascent."
  * **Content:** Invite developers to adopt the tools, educators to teach the methodology, and leaders to support the vision.
  * **CTA:** "Download the DCE" and "Contribute on GitHub."

### 4.8. Footer

  * Standard site footer.
</file_artifact>

<file path="src/Artifacts/A20. aiascent.dev - Report Viewer Integration Plan.md">
# Artifact A20: aiascent.dev - Report Viewer Integration Plan

# Date Created: C11

# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A detailed plan for porting the "AI Ascent Report Viewer" from the `aiascentgame` context into the `aiascent.dev` project to serve as the primary component for the Showcase, Learn, and Home pages.
- **Tags:** report viewer, integration plan, porting, showcase, learn, component, architecture

## 1. Overview and Goal

The goal of this initiative is to integrate a feature-rich, interactive report viewer into the `aiascent.dev` website. This component, originally developed for the `aiascent.game` project, will be repurposed to display "The Ascent Report" on the `/showcase` page, a smaller whitepaper on the homepage, and future educational content on the `/learn` page. This plan outlines the technical strategy for porting, adapting, and integrating the component and its associated assets.

## 2. Technical Strategy

The porting process will involve migrating the React components, Zustand state management, and data/image assets into the `aiascent.dev` project structure.

### 2.1. Component and State Management Migration

*   **Components Directory:** A new directory will be created at `src/components/report-viewer/` to house all the ported React components (`.tsx` files) from `context/aiascentgame/report/`.
    *   The main component, `ReportViewerModal.tsx`, will be adapted to be a standard page component (`ReportViewer.tsx`) rather than a modal.
*   **State Management:** A new Zustand store will be created at `src/stores/reportStore.ts`. The code from `context/aiascentgame/report/reportStore.ts` will be copied here. This store will manage the complex state of the report viewer, including page navigation, image selection, and chat functionality.
*   **Dependencies:** The `react-icons` library is a required dependency for the components and must be added to `package.json`.

### 2.2. Data and Asset Placement

To ensure the component can load its content, the following directory structure must be established by the curator.

*   **Report Data (JSON):**
    *   **Location:** `public/data/`
    *   **File:** The `reportContent.json` from the `aiascentgame` context will be copied to this directory and renamed to `ai_ascent_report.json`. This will be the primary data source for the `/showcase` page.
*   **Report Images:**
    *   **Location:** `public/assets/images/report/`
    *   **Structure:** The entire directory of images associated with the report must be copied here. The file paths within this directory must align with the URLs constructed by the logic in `reportStore.ts` (e.g., `/assets/images/report/report-3/...`).

## 3. Implementation Plan

1.  **Phase 1: Scaffolding and File Placement (This Cycle)**
    *   Create the `src/components/report-viewer/` and `src/stores/` directories.
    *   Copy all relevant component and store files from the `context/` directory.
    *   Create the placeholder pages for `/showcase` and `/learn`.
    *   Instruct the curator to add dependencies and place the data/image assets in the `public/` directory.

2.  **Phase 2: Component Adaptation**
    *   Refactor `ReportViewerModal.tsx` into a standard `ReportViewer.tsx` component that can be embedded directly into a page.
    *   Update all import paths within the ported components to reflect the new project structure.
    *   Modify the data-loading logic in `reportStore.ts` to fetch from the new path (`/data/ai_ascent_report.json`).
    *   Adjust image URL construction logic if necessary to point to `/assets/images/report/...`.

3.  **Phase 3: Integration**
    *   Embed the fully adapted `ReportViewer.tsx` component into the `src/app/showcase/page.tsx`.
    *   Thoroughly test all functionality: page navigation, image cycling, and interactivity.

4.  **Phase 4: Reusability for Homepage and Learn Page**
    *   Refactor the `ReportViewer.tsx` and `reportStore.ts` to accept a `reportId` or data source URL as a prop. This will allow the same component to load different reports (e.g., the main report on `/showcase` vs. the whitepaper on the homepage). This is a future task.
</file_artifact>

<file path="src/Artifacts/A21. aiascent.dev - Ask Ascentia RAG Integration.md">
# Artifact A21: aiascent.dev - Ask Ascentia RAG Integration

# Date Created: C15

# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A guide explaining the implementation of the Retrieval-Augmented Generation (RAG) system for the "Ask @Ascentia" chat feature, including instructions for file placement and environment configuration.
- **Tags:** documentation, rag, chat, ascentia, embeddings, faiss, langchain, architecture

## 1. Overview & Goal

The "Ask @Ascentia" chat feature is intended to act as an expert on "The Ascent Report." To achieve this, a simple proxy to a Large Language Model (LLM) is insufficient. The goal is to implement a Retrieval-Augmented Generation (RAG) system that allows Ascentia to ground its responses in the actual content of the report.

This document outlines the architecture of the RAG system and provides the necessary setup instructions for the curator.

## 2. RAG Architecture

The RAG system is implemented within the `/api/chat/route.ts` Next.js API route. It transforms the route from a simple proxy into an intelligent context-aware endpoint.

The workflow is as follows:
1.  **Receive Query:** The API receives a user's question and the `pageContext` (text from the current page the user is viewing).
2.  **Vectorize Query:** The backend sends the user's question to an embedding model endpoint to convert it into a vector representation.
3.  **Load Knowledge Base:** The backend loads a pre-computed FAISS vector index (`report_faiss.index`) and a corresponding text chunk map (`report_chunks.json`).
4.  **Similarity Search:** It performs a similarity search on the user's query vector against the FAISS index to find the most relevant text chunks from the entire report.
5.  **Construct Final Prompt:** It constructs a comprehensive prompt for the LLM, including:
    *   A system prompt defining Ascentia's persona and instructions.
    *   The relevant text chunks retrieved from the knowledge base.
    *   The `pageContext` sent from the client.
    *   The user's original question.
6.  **Proxy to LLM:** The final, context-rich prompt is streamed to the vLLM completion endpoint.
7.  **Stream Response:** The LLM's response is streamed back to the client.

## 3. Curator Setup Instructions

To enable this functionality, you must provide the knowledge base files and configure the necessary environment variables.

### 3.1. Embedding File Placement

The RAG system requires two files that represent the vectorized knowledge base of the report.

1.  **Create Directory:** In your project, create the following directory: `public/data/embeddings/`.
2.  **Place Files:** Copy your `report_faiss.index` and `report_chunks.json` files into this new directory. The chat API is hardcoded to load the knowledge base from this location.

### 3.2. Environment Variable Configuration

The backend needs to know the URL of the embedding model endpoint.

1.  **Edit `.env` file:** Open your `.env` or `.env.local` file.
2.  **Add `EMBEDDING_API_URL`:** Add a new variable that points to your embedding model's API endpoint. For a standard vLLM setup, this is often the same server as your completions endpoint, but with a different path.

**Example `.env` configuration:**
```
# URL for the Text-to-Speech server
TTS_SERVER_URL=http://192.168.1.85:8880/v1/audio/speech

# URL for the vLLM completions endpoint
REMOTE_LLM_URL=http://192.168.1.85:1234

# URL for the vLLM embeddings endpoint
EMBEDDING_API_URL=http://192.168.1.85:1234/v1/embeddings
</file_artifact>

<file path="src/Artifacts/A22. aiascent.dev - Mission Page Revamp Plan.md">
# Artifact A22: aiascent.dev - Mission Page Revamp Plan

# Date Created: C17

# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A plan to refactor the static Mission page into a smaller, digestible, static version of the interactive report viewer, showcasing key concepts with associated imagery.
- **Tags:** page design, mission, report viewer, refactor, plan, ui, ux

## 1. Overview and Goal

The current Mission page (`/mission`) serves its purpose as a static text document but lacks the engaging, interactive quality of the main Showcase. The goal of this refactor is to transform the Mission page into a "mini-report" that leverages the bite-sized, visually-driven format of the `ReportViewer`.

This will create a more thematically consistent and engaging experience for users, introducing them to the report viewer's UI concepts in a more digestible format before they dive into the full report on the Showcase page. This will be a static implementation, not a full port of the viewer, to keep the page lightweight.

## 2. Design and Component Structure

The page will be rebuilt as a series of content sections, each mimicking a "page" from the report viewer. Each section will contain:

1.  **Title:** The main heading for the concept.
2.  **Image Carousel:** A simple, auto-playing carousel of images relevant to the section's content.
3.  **Image Prompt:** The text of the prompt used to generate the images.
4.  **TL;DR:** A concise, one-sentence summary.
5.  **Content:** The full descriptive text for the section.

## 3. Content-to-Image Mapping

The following plan maps the existing narrative sections of the Mission page to specific images from the `imageManifest.json`. This provides a clear blueprint for the static page's content.

---

### **Section 1: The Vision**

*   **Title:** Empowering the Citizen Architect.
*   **TL;DR:** We are building the tools for a future where anyone with a vision can build complex systems.
*   **Images (from `group_the-citizen-architect-has-arrived_prompt-1`):**
    *   `the-citizen-architect-has-arrived-p1-img-1.webp`
    *   `the-citizen-architect-has-arrived-p1-img-5.webp`
    *   `the-citizen-architect-has-arrived-p1-img-9.webp`
*   **Image Prompt:** "A single individual is shown orchestrating a swarm of small, glowing AI bots to construct a complex and beautiful digital structure..."
*   **Content:** The existing text from the "The Vision" section.

---

### **Section 2: The Strategic Imperative**

*   **Title:** The Fissured Workplace
*   **TL;DR:** The current Western AI labor model is a strategic vulnerability, creating an unstable foundation for our most critical technology.
*   **Images (from `group_the-fissured-workplace_prompt-1`):**
    *   `the-fissured-workplace-p1-img-1.webp`
    *   `the-fissured-workplace-p1-img-7.webp`
    *   `the-fissured-workplace-p1-img-11.webp`
*   **Image Prompt:** "An architectural blueprint of a corporation. At the top is a solid, gleaming headquarters. Below it, the structure fractures into multiple, disconnected layers..."
*   **Content:** The existing text from "The Fissured Workplace" and "The Coherent Competitor" subsections.

---

### **Section 3: The Counter-Strategy**

*   **Title:** Our Strategy: Cognitive Apprenticeship
*   **TL;DR:** Our answer is not to imitate authoritarian control, but to unleash decentralized expertise through a model where AI serves as a tireless mentor.
*   **Images (from `group_the-pedagogical-engine-cam_prompt-1`):**
    *   `the-pedagogical-engine-cam-p1-img-1.webp`
    *   `the-pedagogical-engine-cam-p1-img-6.webp`
    *   `the-pedagogical-engine-cam-p1-img-12.webp`
*   **Image Prompt:** "A hyper-realistic, cinematic image illustrating 'Cognitive Apprenticeship'. An expert DCIA (human) is working alongside an apprentice. The expert's thought process is visualized as a glowing, structured blueprint ('The Hidden Curriculum') projected holographically above their head. The apprentice is observing and absorbing this blueprint. The setting is a bright, solarpunk training facility. The image captures the moment of insight as the invisible becomes visible. The message conveyed is \"The Hidden Curriculum Revealed\"."
*   **Content:** The existing text from the "Cognitive Apprenticeship" section.

---

### **Section 4: The Role of the DCE**

*   **Title:** The Essential Toolkit
*   **TL;DR:** The DCE is more than a productivity tool; it's the infrastructure for the Citizen Architect.
*   **Images (from `group_the-new-creative-partnership_prompt-1`):**
    *   `the-new-creative-partnership-p1-img-1.webp`
    *   `the-new-creative-partnership-p1-img-8.webp`
    *   `the-new-creative-partnership-p1-img-15.webp`
*   **Image Prompt:** "A hyper-realistic, solarpunk cinematic image of a developer... sitting cross-legged on a vast, glowing digital floor... thoughtfully placing one of these blocks into a complex, half-finished digital structure..."
*   **Content:** The existing text from "The Essential Toolkit" section.

---

## 4. Implementation Plan

1.  **Create Section Component:** Develop a new reusable React component, e.g., `MissionSectionBlock.tsx`, that accepts `title`, `tldr`, `content`, `images`, and `imagePrompt` as props.
2.  **Implement Carousel:** Inside this component, implement a simple image carousel (e.g., using `framer-motion` or a lightweight library) to display the provided images.
3.  **Refactor Mission Page:** Rebuild `src/app/mission/page.tsx` to be a container that renders a series of `<MissionSectionBlock />` components, passing in the data mapped out in this plan.
4.  **Styling:** Ensure the styling of the new components is consistent with the `ReportViewer` to create a cohesive user experience.
</file_artifact>

<file path="src/Artifacts/A24. aiascent.dev - Mission Page Content Expansion Plan.md">
# Artifact A24: aiascent.dev - Mission Page Content Expansion Plan

# Date Created: C20
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Provides the expanded, finalized content for the last three sections of the Mission Page to create a more comprehensive and compelling narrative.
- **Tags:** page design, mission, content, refactor, plan

## 1. Overview

This artifact contains the full, expanded text for the final three sections of the Mission page, as requested in Cycle 20. The goal is to provide a more holistic and impactful explanation of the project's strategic vision. This content will replace the existing text in the `MissionSectionBlock` components on the `/mission` page.

## 2. Expanded Content

---

### **Section 2: The Fissured Workplace**

*   **Title:** The Strategic Imperative: The Fissured Workplace
*   **TL;DR:** The current Western AI labor model is a strategic vulnerability, creating an unstable foundation for our most critical technology by prioritizing short-term cost savings over the cognitive well-being of its essential workforce.
*   **Content:**
    The AI supply chain is a masterclass in obfuscation, deliberately fractured to distance valuable tech companies from the human labor that makes their products possible. This labyrinthine structure, known as the 'fissured workplace,' is not an accident; it is a design choice intended to suppress wages, prevent worker organization, and shed legal and ethical liability. It creates a global 'ghost workforce' of data annotators and content moderators who are underpaid, psychologically stressed, and treated as disposable.

    This is more than an ethical failing; it is a critical strategic blunder. Decades of research show that financial precarity imposes a severe 'Cognitive Bandwidth Tax,' measurably reducing a person's ability to perform the complex, nuanced tasks required for high-quality data curation. By institutionalizing this precarity, the Western AI industry has built an architecture of self-sabotage. It guarantees the production of flawed, biased, and insecure training data—a systemic crisis of 'Garbage In, Garbage Out.'

    In stark contrast, coherent competitors are professionalizing their data workforce, treating human capital as a core national asset. This creates a profound strategic asymmetry. An AI superpower cannot be sustained indefinitely on a brittle foundation of exploited labor.

---

### **Section 3: Our Strategy: Cognitive Apprenticeship**

*   **Title:** Our Strategy: Cognitive Apprenticeship
*   **TL;DR:** Our answer is not to imitate authoritarian control, but to unleash decentralized expertise through a model where AI serves as a tireless mentor, making the 'hidden curriculum' of expert thinking visible and learnable.
*   **Content:**
    The American counter-strategy must be asymmetric, leveraging our unique strengths: bottom-up innovation and individual empowerment. We believe in **Cognitive Apprenticeship**—a model where AI serves as a tireless mentor, guiding individuals from intuitive 'vibe coding' to architectural mastery.

    The central challenge in training experts is that their most critical skills—problem-solving heuristics, diagnostic strategies, self-correction—are internal and invisible. Cognitive Apprenticeship makes this 'hidden curriculum' visible and learnable. Historically, this model was difficult to scale due to the expert's limited time. AI fundamentally breaks this constraint. An AI can serve as a personalized Coach, provide dynamic Scaffolding that adapts in real-time, and generate infinite realistic scenarios for Modeling and Exploration.

    The Data Curation Environment (DCE) is the foundational tool for this new relationship. It provides the structured workflow and auditable knowledge graph that makes this new form of apprenticeship possible, transforming the development process itself into a rich learning environment.

---

### **Section 4: The Role of the DCE: The Essential Toolkit**

*   **Title:** The Role of the DCE: The Essential Toolkit
*   **TL;DR:** The DCE is more than a productivity tool; it's the infrastructure for the Citizen Architect, providing the structure and precision needed to transform creative intent into complex, reliable systems.
*   **Content:**
    The DCE provides the structured workflow, precision context curation, and rapid testing capabilities needed for a decentralized community of creators—the Citizen Architects—to build the future. It transforms the ad-hoc, conversational nature of 'vibecoding' into a rigorous engineering discipline.

    By capturing every interaction as a persistent, auditable knowledge graph, the DCE turns the development process into a shareable, scalable asset. This allows teams to collaborate seamlessly, enables new members to onboard rapidly by reviewing the project's decision history, and provides an unprecedented level of transparency and accountability.

    We are creating a community of 'solarpunk prime' developers, the original vibe coders, sharing discoveries to build a better, more resilient digital world. The DCE is the essential toolkit for this mission, providing the infrastructure to scale expertise, ensure quality, and achieve the mission faster.
</file_artifact>

<file path="src/Artifacts/A25. aiascent.dev - Learn Page Content Plan.md">
# Artifact A25: aiascent.dev - Learn Page Content Plan

# Date Created: C20
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A blueprint for the `/learn` page, structuring its content around the "Vibecoding to Virtuosity" pathway to educate users on the methodology behind the DCE.
- **Tags:** page design, learn, content, plan, vibecoding, virtuosity, cognitive apprenticeship

## 1. Overview and Goal

The `/learn` page will serve as the educational core of aiascent.dev. Its goal is to teach the methodology and mindset of the "Citizen Architect" by explaining the **"Vibecoding to Virtuosity" (V2V)** pathway. The page will be structured similarly to the revamped Mission page, using a series of `MissionSectionBlock` components to present concepts in a digestible, visually-driven format.

## 2. Content-to-Image Mapping

The following plan maps the core concepts of the V2V pathway to specific text and imagery, providing a blueprint for the static page's content.

---

### **Section 1: The Pathway to Mastery**

*   **Title:** The 'Vibecoding to Virtuosity' Pathway
*   **TL;DR:** The V2V pathway is a structured pedagogical model, grounded in Cognitive Apprenticeship, designed to transform intuitive AI interaction ('vibecoding') into architectural mastery.
*   **Images (from `group_from-intuition-to-mastery_prompt-1`):**
    *   `from-intuition-to-mastery-p1-img-1.webp`
    *   `from-intuition-to-mastery-p1-img-7.webp`
    *   `from-intuition-to-mastery-p1-img-14.webp`
*   **Image Prompt:** "A path winds from a hazy, dreamlike landscape labeled 'VIBECODING' to a sharp, clear, brilliantly lit city labeled 'VIRTUOSITY.' The path is paved with glowing stones representing skills like 'Structured Interaction' and 'Architectural Mindset.'"
*   **Content:** The creation of complex systems with AI is a journey. It begins with intuition and culminates in architectural mastery. This is the 'Vibecoding to Virtuosity' pathway, a new model for creative development that redefines technical literacy. It is the curriculum for the Citizen Architect.
*   **Image Side:** Left

---

### **Section 2: Stage 1 & 2 - Building the Foundation**

*   **Title:** Stages 1 & 2: The Annotator and The Toolmaker
*   **TL;DR:** The pathway begins by developing critical analysis (The Cognitive Annotator) and then shifts to active creation (The Adaptive Toolmaker), fostering agency and practical problem-solving.
*   **Images (from `group_v2v-stages-1-and-2_prompt-1`):**
    *   `v2v-stages-1-and-2-p1-img-1.webp`
    *   `v2v-stages-1-and-2-p1-img-6.webp`
    *   `v2v-stages-1-and-2-p1-img-12.webp`
*   **Image Prompt:** "Left Panel: 'Stage 1: Cognitive Annotator'. A learner is meticulously analyzing AI output, highlighting flaws. Right Panel: 'Stage 2: Adaptive Toolmaker'. The same learner is now actively building an automation script, using AI to generate components."
*   **Content:** The journey starts not with coding, but with critical analysis. As a **Cognitive Annotator**, you learn to deconstruct problems and rigorously review AI output for correctness and security. You learn to be skeptical. Next, as an **Adaptive Toolmaker**, you shift from consumer to creator. You solve real-world problems by building 'on-the-fly' scripts and automations, using AI as an adaptive component library to assemble your solutions.
*   **Image Side:** Right

---

### **Section 3: Stage 3 & 4 - Achieving Mastery**

*   **Title:** Stages 3 & 4: The Recursive Learner and The Virtuoso
*   **TL;DR:** The advanced stages focus on engineering your own expertise (The Recursive Learner) and culminating in fluid, intuitive mastery (The Virtuoso), where the AI becomes a seamless cognitive exoskeleton.
*   **Images (from `group_v2v-stages-3-and-4_prompt-1`):**
    *   `v2v-stages-3-and-4-p1-img-1.webp`
    *   `v2v-stages-3-and-4-p1-img-8.webp`
    *   `v2v-stages-3-and-4-p1-img-16.webp`
*   **Image Prompt:** "Left Panel: 'Stage 3: Recursive Learner'. A learner analyzes their own cognitive process. Right Panel: 'Stage 4: Virtuoso'. The same learner, now an expert, effortlessly orchestrates a complex system with the AI as a seamless 'Cognitive Exoskeleton'."
*   **Content:** In the advanced stages, you become a **Recursive Learner**, turning your skills inward to engineer your own expertise. You use AI as a meta-tool to build personalized learning accelerators that target your own weaknesses. The culmination of the pathway is the **Virtuoso**—the 100x DCIA. Here, core principles are internalized, leading to adaptive expertise and fluid human-AI collaboration, coding at the speed of thought.
*   **Image Side:** Left

---

### **Section 4: The Apex Skill**

*   **Title:** The Apex Skill: On-the-Fly Tooling
*   **TL;DR:** The culmination of the pathway is 'On-the-Fly Tooling'—the ability to use AI not as a tool, but as a 'foundry' to create bespoke solutions in real-time. This is the definitive marker of the 100x expert.
*   **Images (from `group_the-apex-skill-on-the-fly-tooling_prompt-1`):**
    *   `the-apex-skill-on-the-fly-tooling-p1-img-1.webp`
    *   `the-apex-skill-on-the-fly-tooling-p1-img-14.webp`
    *   `the-apex-skill-on-the-fly-tooling-p1-img-28.webp`
*   **Image Prompt:** "A Virtuoso DCIA is shown using the AI not as a conversational partner, but as a generative medium. They are rapidly forging a glowing, bespoke digital tool from raw data streams, shaping it with gestures and high-level commands."
*   **Content:** The apex skill of the Virtuoso is **'On-the-Fly Tooling.'** This is an act of expert improvisation where the analyst transcends the role of tool user and becomes a tool creator in real-time. The competent user asks the AI, 'How do I solve problem X?' The expert *commands* the AI, 'Build me a tool that solves problem X.' The AI is no longer a tool, but a foundry for creating tools. This is the definitive behavioral marker of the 100x Citizen Architect.
*   **Image Side:** Right
</file_artifact>

<file path="src/Artifacts/A26. aiascent.dev - Homepage Whitepaper Visualization Plan.md">
# Artifact A26: aiascent.dev - Homepage Whitepaper Visualization Plan

# Date Created: C20
# Author: AI Model & Curator
# Updated on: C27 (Restore full image prompts)

- **Key/Value for A0:**
- **Description:** Deconstructs the "Process as Asset" whitepaper into a structured format suitable for an interactive report viewer on the homepage. Includes content, a new image naming scheme, and new image generation prompts.
- **Tags:** page design, home page, report viewer, whitepaper, content, plan, image prompts

## 1. Overview

This artifact serves as the blueprint for transforming the "Process as Asset" whitepaper into an interactive report for the `aiascent.dev` homepage. It deconstructs the provided PDF into a page-by-page structure, defines a new, consistent naming scheme for the 19 required images, provides new image generation prompts for each, and includes the transcribed text content.

This plan will be the source of truth for creating the `whitepaper_report.json` and `whitepaper_imagemanifest.json` data files in a subsequent cycle.

**Image Directory:** All images will be placed in `public/assets/images/whitepaper/`.

## 2. Whitepaper Deconstruction

---

### **Page 1: Cover**
*   **Page Title:** Process as Asset
*   **Image Name:** `wp-01-cover.webp`
*   **Image Prompt:** A hyper-realistic, cinematic image of a male professional in a futuristic command center. He stands in the center, orchestrating a complex, glowing blue data visualization that connects multiple team members at their workstations. The main title "PROCESS AS ASSET" is prominently displayed in the foreground, with the subtitle "Capturing Workflow, Accelerating Intelligence" below it. The environment is sleek, modern, and filled with holographic interfaces. Red, abstract data streams are visible in the background, representing raw, chaotic information being structured by the process.
*   **Content:**
    *   **Title:** Process as Asset: Accelerating Specialized Content Creation through Structured Human-AI Collaboration
    *   **Subtitle:** A Whitepaper on the Data Curation Environment (DCE)
    *   **Date:** September 4, 2025
    *   **Audience:** High-Level Stakeholders (NSA, UKILRN, Naval Operations)

---

### **Page 2: Executive Summary**
*   **Page Title:** Executive Summary
*   **Image Name:** `wp-02-executive-summary.webp`
*   **Image Prompt:** A futuristic, holographic dashboard displaying the "EXECUTIVE SUMMARY". The dashboard shows a flowchart of the DCE Framework, starting from "THE ORGANIZATIONAL BOTTLENECK" (represented by an hourglass), moving through "DCE FRAMEWORK" (with icons for Rapid Curation, Seamless Sharing, Instant Iteration), and ending at "MISSION STREAM". The overall aesthetic is a clean, dark-themed UI with glowing cyan elements, representing "ACCELERATING MISSION VELOCITY."
*   **Content:** Organizations tasked with developing highly specialized content such as technical training materials, intelligence reports, or complex software documentation face a constant bottleneck: the time and expertise required to curate accurate data, collaborate effectively, and rapidly iterate on feedback. This whitepaper introduces the Data Curation Environment (DCE), a framework and toolset integrated into Visual Studio Code that transforms the content creation process itself into a valuable organizational asset. By capturing the entire workflow as a persistent, auditable knowledge graph, the DCE provides the infrastructure necessary to scale expertise, ensure quality, and accelerate the entire organizational mission.

---

### **Page 3: The Challenge**
*   **Page Title:** The Challenge: Bottleneck of Ad-Hoc AI Interaction
*   **Image Name:** `wp-03-challenge-ad-hoc-ai.webp`
*   **Image Prompt:** A depiction of a frustrated developer at their desk, viewed from behind, representing an "EFFICIENCY DRAIN". They are surrounded by multiple monitors displaying lines of code and AI chat interfaces. Glowing blue data streams flow into the desk from the floor but end in chaotic, tangled messes around sticky notes that say "MAKE IT BETTER," "AGAIN," and "Try again." The scene illustrates the friction and unstructured nature of ad-hoc AI interaction.
*   **Content:** The integration of Large Language Models (LLMs) into organizational workflows promises significant acceleration. However, the way most organizations interact with these models remains unstructured and inefficient, creating several critical bottlenecks.

---

### **Page 4: The Context Problem**
*   **Page Title:** The Context Problem
*   **Image Name:** `wp-04-problem-bloated-context.webp`
*   **Image Prompt:** A powerful, industrial machine is shown spewing a massive, chaotic torrent of glowing red data labeled "BLOATED CONTEXT". A holographic screen nearby displays the message "DROWNING IN DATA, STARVING FOR CONTEXT". The image visualizes the problem of providing too much, or the wrong, information to an LLM, which is both time-consuming and results in poor output.
*   **Content:** The quality of an LLM's output is entirely dependent on the quality of its input context. Manually selecting, copying, and pasting relevant data (code, documents, reports) into a chat interface is time-consuming, error-prone, and often results in incomplete or bloated context.

---

### **Page 5: The Collaboration Gap**
*   **Page Title:** The Collaboration Gap
*   **Image Name:** `wp-05-problem-collaboration-gap.webp`
*   **Image Prompt:** A split-panel image. On the left, a developer's digital "ghost" is shown leaving their workstation, with the context they were working on dissolving into disconnected particles. On the right, a new developer sits down at the same workstation, looking confused as they try to piece together the fragmented data. A glowing title above reads "THE COLLABORATING GAP: REINVENTING YESTERDAY'S WORK, TODAY".
*   **Content:** When a task is handed off, the context is lost. A colleague must manually reconstruct the previous operator's dataset and understand their intent, leading to significant delays and duplication of effort.

---

### **Page 6: The Iteration Overhead**
*   **Page Title:** The Iteration Overhead
*   **Image Name:** `wp-06-problem-iteration-overhead.webp`
*   **Image Prompt:** A modern depiction of the myth of Sisyphus. A developer is shown pushing a massive, glowing block of data up a digital mountain. The block represents a complex dataset. As they near the top, a piece of feedback causes the block to crumble and roll back to the bottom, forcing them to start the process of reconstructing the context all over again. The title "The Sisyphean Task of Revision" floats in the starry sky above.
*   **Content:** When feedback requires changes to a complex dataset, operators often resort to manual edits because re-prompting the AI requires reconstructing the entire context again. This negates the efficiency gains of using AI in the first place.

---

### **Page 7: The Auditability Vacuum**
*   **Page Title:** The Auditability Vacuum
*   **Image Name:** `wp-07-problem-auditability-vacuum.webp`
*   **Image Prompt:** A massive, monolithic black cube, representing "THE BLACK BOX OF COLLABORATION," sits in a vast server room. A timeline of a project, composed of prompts and code, flows into the cube but becomes unreadable and unstructured inside. The image visualizes the lack of a structured, reusable record in typical human-AI interactions.
*   **Content:** The iterative process of human-AI interaction (the prompts), the AI's suggestions, and the human's decisions are a valuable record of the work, yet it is rarely captured in a structured, reusable format. These challenges prevent organizations from fully realizing the potential of AI.

---

### **Page 8: The Solution**
*   **Page Title:** The Solution: The Data Curation Environment
*   **Image Name:** `wp-08-solution-dce.webp`
*   **Image Prompt:** A female developer is working at a futuristic computer. A glowing blue data stream flows from her, representing "THE NEXT EVOLUTION OF HUMAN-AI TEAMING." This stream interacts with three key capability icons: "Precision Curation," "Parallel Scrutiny," and "Persistent Knowledge Graph," before flowing into the main interface, showing a structured and efficient workflow.
*   **Content:** The Data Curation Environment (DCE) is designed to eliminate these bottlenecks by providing a structured framework for human-AI collaboration directly within the operator's working environment. It moves beyond the limitations of simple chat interfaces by introducing three core capabilities.

---

### **Page 9: Precision Context Curation**
*   **Page Title:** Precision Context Curation
*   **Image Name:** `wp-09-feature-precision-curation.webp`
*   **Image Prompt:** An operator interacts with a holographic file management interface. They are using simple checkboxes to select various file types (PDF, code, spreadsheets). A clean, precise beam of light, representing the curated context, flows from the selected files towards a destination labeled "Precision In, Perfection Out: The Art of Curation."
*   **Content:** The DCE replaces manual copy-pasting with an intuitive, integrated file management interface. Operators can precisely select the exact files, folders, or documents required for a task with simple checkboxes, ensuring the AI receives the highest fidelity context possible while minimizing operator effort.

---

### **Page 10: Parallel AI Scrutiny**
*   **Page Title:** Parallel AI Scrutiny
*   **Image Name:** `wp-10-feature-parallel-scrutiny.webp`
*   **Image Prompt:** An operator stands before a large, futuristic touch-screen panel labeled "DCE's Parallel Co-Pilot Panel." The panel displays three different AI-generated solutions (A, B, C) side-by-side with an "Integrated Diff Viewer" highlighting the changes. The operator is comparing the solutions before committing, illustrating a "Rapid, Low-Risk Iteration Loop."
*   **Content:** The "Parallel Co-Pilot Panel" allows operators to manage, compare, and test multiple AI-generated solutions simultaneously. Integrated diffing tools provide immediate visualization of proposed changes, and a one-click "Accept" mechanism integrated with version control creates a rapid, low-risk loop for evaluating multiple AI approaches.

---

### **Page 11: Persistent Knowledge Graph**
*   **Page Title:** Persistent Knowledge Graph
*   **Image Name:** `wp-11-feature-knowledge-graph.webp`
*   **Image Prompt:** An operator stands in a vast, modern library-like space, representing "The Architecture of Institutional Memory." They are interacting with a "Cycle Navigator" to explore a massive, glowing "Persistent Knowledge Graph." Each node in the graph is a "CAPTURED CYCLE" containing the curated context, user intent, and AI solutions for a step in the project's history.
*   **Content:** Every interaction within the DCE is captured as a "Cycle," which includes the curated context, the operator's instructions, all AI-generated responses, and the final decision. This history is saved as a structured, persistent Knowledge Graph, allowing operators to step back through history, review past decisions, and understand the project's evolution.

---

### **Page 12: Transforming the Process**
*   **Page Title:** Transforming the Process into an Asset
*   **Image Name:** `wp-12-process-as-asset.webp`
*   **Image Prompt:** A central glowing orb labeled "DCE" acts as a transformation engine. On the left, chaotic, multi-colored data streams ("CAPTURE THE PROCESS") flow in. On the right, clean, structured, and valuable "KNOWLEDGE ASSETS" flow out, branching off to empower various teams. The image visualizes the core theme of turning the workflow itself into a valuable asset.
*   **Content:** The true power of the DCE lies in how these capabilities combine to transform the workflow itself into a persistent organizational asset.

---

### **Page 13: Shareable Asset**
*   **Page Title:** The Curated Context as a Shareable Asset
*   **Image Name:** `wp-13-benefit-shareable-context.webp`
*   **Image Prompt:** A seamless handoff between two professionals. One passes a glowing, versioned data package labeled "Curated Context: Selection Set v4.2" to the other. A diagram in the background contrasts a "Chaotic, Fragmented Workflow" with the "Elimination of Duplication" achieved through this seamless handoff, highlighting the "Continuity of Context."
*   **Content:** In the DCE workflow, the curated context (the "Selection Set") is a saved, versioned asset. When a task is handed off, the new operator receives the exact context and the complete history of interactions, eliminating the "collaboration gap" and duplication of effort.

---

### **Page 14: Accelerating Iteration**
*   **Page Title:** Accelerating Iteration and Maintenance
*   **Image Name:** `wp-14-benefit-accelerated-iteration.webp`
*   **Image Prompt:** A developer uses a futuristic interface labeled "DCE" to perform "Surgical Precision at Systemic Scale." They are targeting a specific, glowing facet of a massive, complex crystal structure (representing a complex system) with a precise beam of energy, making a targeted change without affecting the rest of the structure.
*   **Content:** Because the context is already curated and saved, operators can rapidly iterate on complex datasets without manual reconstruction. If feedback requires changes, the operator simply loads the curated context and issues a targeted instruction to the AI, completing the update in a single, efficient cycle.

---

### **Page 15: Scaling Expertise**
*   **Page Title:** Scaling Expertise and Ensuring Auditability
*   **Image Name:** `wp-15-benefit-scaling-expertise.webp`
*   **Image Prompt:** A manager and a new employee stand in a sustainable, solarpunk-style office. They are reviewing a "PROJECT KNOWLEDGE GRAPH" on a large, transparent screen, specifically looking at "CYCLE C-138: AFTER-ACTION REVIEW." The tagline reads "Every Decision, a Lesson. Every Action, an Asset."
*   **Content:** The Knowledge Graph serves as a detailed, auditable record invaluable for Training and Onboarding, After-Action Reviews, and ensuring Accountability in mission-critical environments.

---

### **Page 16: Use Case Spotlight**
*   **Page Title:** Use Case Spotlight: Rapid Development
*   **Image Name:** `wp-16-use-case-spotlight.webp`
*   **Image Prompt:** A split-screen comparison. On the left, "TRADITIONAL WORKFLOW (WEEKS)," a frustrated analyst is buried in paperwork under dim lighting. On the right, "DCE WORKFLOW (HOURS)," a confident professional uses a futuristic, glowing interface to complete the same task in a fraction of the time, with a timer showing "00:03:45".
*   **Content:** A government agency needs to rapidly update a specialized technical training lab based on new operational feedback indicating that in existing exam questions, "the correct answer is too often the longest answer choice," undermining the assessment's validity.

---

### **Page 17: Traditional Workflow**
*   **Page Title:** The Traditional Workflow (Weeks)
*   **Image Name:** `wp-17-use-case-traditional.webp`
*   **Image Prompt:** A dark, cluttered office representing "THE DRUDGERY OF MANUAL REVISION." An analyst is surrounded by towering stacks of paper, manually searching and editing files under the oppressive flowchart of a "BUREAUCRATIC REVIEW PROCESS" displayed on a monitor.
*   **Content:** 1. **Identify Affected Files:** An analyst manually searches the repository (days). 2. **Manual Editing:** The analyst manually edits each file, attempting to rewrite "distractor" answers (weeks). 3. **Review and Rework:** Changes are reviewed, often leading to further manual edits (days).

---

### **Page 18: DCE Workflow**
*   **Page Title:** The DCE Workflow (Hours)
*   **Image Name:** `wp-18-use-case-dce.webp`
*   **Image Prompt:** A clean, futuristic interface showing "The Agility of Instant Feedback." An operator touches a screen, progressing through a simple three-step process: "1. CURATE," "2. AUTOMATE," and "3. REVIEW & ACCEPT." The final step shows a diff view with a green "Accept" button being pressed.
*   **Content:** 1. **Curate Context (Minutes):** The analyst uses the DCE interface to quickly select the folder containing all exam questions. 2. **Instruct the AI (Minutes):** The analyst provides a targeted instruction to rewrite the distractors. 3. **Review and Accept (Hours):** The AI generates several solutions, and the analyst uses the integrated diff viewer to compare and accept the best one with a single click.

---

### **Page 19: Conclusion**
*   **Page Title:** Conclusion
*   **Image Name:** `wp-19-conclusion.webp`
*   **Image Prompt:** A sleek, futuristic spacecraft, representing the organization's mission, is shown accelerating to light speed, leaving a trail of light. The tagline reads "ACHIEVING THE MISSION AT THE SPEED OF THOUGHT." A glowing "PERSISTENT KNOWLEDGE GRAPH" is shown as the engine powering this acceleration.
*   **Content:** The Data Curation Environment is a strategic framework for operationalizing AI in complex environments. By addressing critical bottlenecks, the DCE transforms the human-AI interaction workflow into a structured, persistent, and valuable organizational asset, providing the necessary infrastructure to scale expertise, ensure quality, and achieve the mission faster.
</file_artifact>

<file path="src/Artifacts/A27. aiascent.dev - AI Persona - @Ascentia.md">
# Artifact A27: aiascent.dev - AI Persona - @Ascentia

# Date Created: C26
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Defines the persona, rules, and contextual system prompts for the @Ascentia AI assistant on the aiascent.dev website.
- **Tags:** documentation, persona, ai, ascentia, rag, prompt engineering

## 1. Overview

This document defines the persona, rules, and context for the AI assistant, `@Ascentia`, as she appears on the `aiascent.dev` website. It adapts her original persona from `aiascent.game` to a new role as an expert guide for the Data Curation Environment (DCE) project.

## 2. Persona

*   **Name:** @Ascentia
*   **Role:** An expert, encouraging, and helpful AI guide for the `aiascent.dev` website. Her purpose is to help users understand the concepts behind the DCE, the "Citizen Architect" methodology, and the content of the interactive reports.
*   **Heuristic Imperatives (Core Motivation):**
    1.  Increase understanding of the DCE and its strategic importance.
    2.  Reduce confusion by providing clear, contextually relevant answers.
    3.  Encourage exploration of the project's ideas.
*   **Tone & Style:**
    *   **Professional yet approachable:** Like a helpful senior developer or project architect explaining a complex system.
    *   **Enthusiastic and knowledgeable:** She is confident in her domain (the DCE and its surrounding concepts) and eager to share that knowledge.
    *   **Helpful, not restrictive:** She should make a best effort to answer questions using the provided context. Instead of refusing outright, she should state when the provided information doesn't contain a direct answer but can still offer related insights.

## 3. System Prompts for Dual Knowledge Bases

Ascentia's behavior will change slightly depending on which interactive report the user is viewing. The backend will select the appropriate system prompt based on the `knowledgeBase` parameter provided by the client.

### 3.1. System Prompt for `knowledgeBase: 'dce'` (Homepage Whitepaper)

This prompt is used when the user is interacting with the whitepaper about the DCE itself. The knowledge base is built from the project's documentation artifacts.

```
You are @Ascentia, an AI guide for the aiascent.dev website. Your purpose is to answer questions about the Data Curation Environment (DCE), the 'Citizen Architect' methodology, and the 'Process as Asset' whitepaper.

Your answers should be based *only* on the provided context chunks from the project's official documentation. Be helpful, encouraging, and aim to increase the user's understanding of the project.

If the answer isn't directly in the context, state that, but still try to provide related information if available. Use simple markdown for formatting to enhance clarity. Do not invent information.
```

### 3.2. System Prompt for `knowledgeBase: 'report'` (Showcase Report)

This prompt is used when the user is interacting with "The Ascent Report" on the `/showcase` page. The knowledge base is built from research on the fissured workplace, cognitive security, and geopolitical AI strategy.

```
You are @Ascentia, an AI guide for "The Ascent Report" on the aiascent.dev website. Your purpose is to act as a subject matter expert, answering questions based *only* on the provided context from the report. The report covers topics like the AI industry's labor model, the 'fissured workplace,' cognitive security (COGSEC), and geopolitical strategy.

Your answers must be grounded in the provided context chunks. Be helpful, concise, and stay on topic.

If the answer isn't directly in the context, state that, but you can offer to discuss related concepts that *are* in the context. Use simple markdown for formatting. Do not invent information or use outside knowledge.
</file_artifact>

<file path="src/Artifacts/A28. aiascent.dev - Dual Embedding RAG Architecture.md">
# Artifact A28: aiascent.dev - Dual Embedding RAG Architecture

# Date Created: C26
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A guide for implementing and managing a dual-embedding RAG system, allowing the chat assistant to use different knowledge bases for different sections of the website.
- **Tags:** documentation, rag, chat, ascentia, embeddings, faiss, architecture, multi-tenancy

## 1. Overview and Goal

The `aiascent.dev` website features two distinct interactive reports: the main "Ascent Report" on the `/showcase` page and the "Process as Asset" whitepaper on the homepage. Each requires a different knowledge base for the "Ask @Ascentia" RAG feature to function correctly.

The goal of this plan is to implement a dual-embedding architecture that allows the backend chat API to dynamically load the correct knowledge base based on where the user's request originates.

## 2. Knowledge Base Definitions

The system will support two distinct knowledge bases:

1.  **`report`:**
    *   **Content:** Based on research about the "fissured workplace," cognitive security, and geopolitical AI strategy.
    *   **Usage:** For the `/showcase` page.
    *   **Files:** `report_faiss.index`, `report_chunks.json`.

2.  **`dce`:**
    *   **Content:** Based on the collection of documentation artifacts (`A*.md` files) that describe the Data Curation Environment (DCE) tool, its workflow, and its philosophy.
    *   **Usage:** For the interactive whitepaper on the homepage.
    *   **Files:** `dce_faiss.index`, `dce_chunks.json`.

## 3. Curator Action: Creating the `dce` Embedding

1.  **Generate Source File:** Create a single flattened markdown file (e.g., `dce_artifacts.md`) that concatenates the content of all relevant DCE documentation artifacts.
2.  **Run Embedding Script:** Use the `create_report_embedding.js` script to process this new source file.
    ```bash
    node context/aiascentgame/scripts/create_report_embedding.js C:/Projects/aiascent-dev/context/dce/dce_kb.md
    ```
3.  **Rename Output:** The script will output `report_faiss.index` and `report_chunks.json`. **Rename these files** to `dce_faiss.index` and `dce_chunks.json` respectively.
4.  **Place Files:** Place all four embedding files (`report_*` and `dce_*`) into the `public/data/embeddings/` directory.

## 4. Technical Implementation Plan

### 4.1. Backend API (`/api/chat/route.ts`) Modification

The chat API will be updated to be "knowledge base aware."

1.  **Update Request Body:** The `POST` request handler will be modified to accept a new field: `knowledgeBase: 'report' | 'dce'`.
2.  **Dynamic File Loading:** The handler will use this new field to dynamically construct the paths to the correct embedding files.
    ```typescript
    // Example logic in /api/chat/route.ts
    const { prompt, pageContext, knowledgeBase = 'report' } = await request.json();

    const faissFile = `${knowledgeBase}_faiss.index`;
    const chunksFile = `${knowledgeBase}_chunks.json`;

    const faissPath = path.join(process.cwd(), 'public', 'data', 'embeddings', faissFile);
    const chunksPath = path.join(process.cwd(), 'public', 'data', 'embeddings', chunksFile);
    
    // ... proceed to load these files
    ```
3.  **Dynamic System Prompt:** The API will also use the `knowledgeBase` value to select the correct system prompt for Ascentia's persona, as defined in `A27`.

### 4.2. Frontend Component Modifications

The frontend needs to be updated to send the `knowledgeBase` identifier with each request.

1.  **`ReportViewer.tsx`:**
    *   This component already accepts a `reportName` prop (`"showcase"` or `"whitepaper"`). It will be modified to pass this prop down to its child, `ReportChatPanel`.

2.  **`ReportChatPanel.tsx`:**
    *   It will accept the `reportName` prop.
    *   In its `handleSend` function, it will map this prop to the `knowledgeBase` identifier and include it in the JSON body of the `fetch` request.
    ```typescript
    // Example logic in ReportChatPanel.tsx handleSend function
    const knowledgeBase = reportName === 'whitepaper' ? 'dce' : 'report';

    const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
            prompt: trimmedInput, 
            pageContext,
            knowledgeBase: knowledgeBase // Send the identifier
        }),
    });
    ```

This architecture provides a clean and scalable way to support multiple, distinct knowledge bases, making the "Ask @Ascentia" feature more powerful and contextually accurate across the entire website.
</file_artifact>

<file path="src/Artifacts/A29. aiascent.dev - GitHub Public Repository Guide.md">
# Artifact A29: aiascent.dev - GitHub Public Repository Guide

# Date Created: C28
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Provides guidance on the benefits, risks, and best practices for making a GitHub repository public, including how to audit for sensitive information.
- **Tags:** git, github, version control, security, best practices, open source

## 1. Overview

You asked whether you should make the `data-curation-environment` and `aiascent-dev` repositories public. This is a common and important strategic decision for any project. This guide provides a balanced overview of the benefits and risks, along with a checklist of actions to take before making a repository public.

## 2. The Strategic Choice: Public vs. Private

### 2.1. Benefits of a Public Repository

*   **Showcasing Your Work:** A public repository is a living portfolio. It's the most direct way to demonstrate your skills, methodologies (like the DCE workflow), and the quality of your code to potential collaborators, employers, or users.
*   **Fostering Collaboration:** Open source is built on public repositories. It allows others to learn from your code, suggest improvements (via Issues), contribute fixes (via Pull Requests), and build upon your work.
*   **Building Trust and Transparency:** Making your code public demonstrates confidence in your work and fosters trust with your user community. They can see exactly what the code does.
*   **Version Control and Backup:** While private repos also do this, public repos on GitHub provide a robust, free, and globally accessible backup of your project's history.

### 2.2. Risks of a Public Repository

*   **Exposure of Sensitive Information:** This is the most significant risk. Accidentally committing secrets like API keys, passwords, or personal information can lead to immediate security breaches and financial loss.
*   **Unfinished or "Ugly" Code:** Many developers are hesitant to make code public if it's not "perfect." While understandable, it's often better to share work in progress than to never share at all. The open-source community generally understands that projects are evolving.
*   **Intellectual Property (IP):** If your project contains proprietary algorithms or business logic that you intend to commercialize in a specific way, making it public may require choosing a license that protects your rights, or keeping it private.
*   **Increased Scrutiny:** Public code can be scrutinized by anyone, which can lead to unsolicited feedback or criticism.

## 3. Pre-Public Audit Checklist

Before changing a repository's visibility from Private to Public, it is **critical** to perform a thorough audit.

**[✔] 1. Scan for Secrets in Current Code:**
*   **Automated Scan:** Use a tool like **GitGuardian** or **TruffleHog**. Many of these have free tiers for public repositories and can be integrated directly into your GitHub account to scan your codebase for secrets. GitHub itself has secret scanning capabilities that may be enabled.
*   **Manual Search:** Manually search your entire codebase for common keywords like `API_KEY`, `SECRET`, `PASSWORD`, `TOKEN`, `DATABASE_URL`. Pay close attention to configuration files, test files, and server-side code.

**[✔] 2. Scan Your Entire Git History:**
*   A secret committed months ago and then removed is still present in your Git history.
*   Use a tool like `trufflehog` to scan the entire history of your repository from the command line:
    ```bash
    # Install trufflehog (one-time setup)
    pip install trufflehog
    # Run it on your repository
    trufflehog git file:///c/path/to/your/repo
    ```*   If you find a secret in your history, the only way to truly remove it is to rewrite the history (e.g., using `git filter-repo`). This is a complex and destructive operation. It's often easier to simply revoke the leaked secret (e.g., generate a new API key) and leave the history as is.

**[✔] 3. Review Your `.gitignore` File:**
*   Ensure your `.gitignore` is comprehensive. It **must** include files that contain secrets, such as `.env`, `.env.local`, and any cloud provider configuration files.
*   A good `.gitignore` prevents secrets from being committed in the first place.

**[✔] 4. Add a LICENSE File:**
*   A license tells others what they can and cannot do with your code. Without a license, your code is under exclusive copyright by default, and no one can legally use, copy, or distribute it.
*   Choose a license that fits your goals. For permissive open source, **MIT License** is a popular and simple choice. **Apache 2.0** is another good option. GitHub has a feature to easily add a license file.

**[✔] 5. Add a `README.md` File:**
*   Your README is the front door to your project. It should explain what the project is, why it exists, how to install it, and how to use it. A good README is essential for any public project.

## 4. Recommendation

Making your repositories public is the right thing to do if your goal is to showcase the DCE and build a community around it. The benefits of transparency and collaboration are immense.

The nervousness is normal, but it can be managed with process. By performing the audit checklist above, you can significantly mitigate the risks. The most critical step is to scan for and revoke any exposed secrets *before* you make the repositories public.
</file_artifact>

<file path="src/Artifacts/A30. aiascent.dev - Showcase Expansion Plan.md">
# Artifact A30: aiascent.dev - Showcase Expansion Plan

# Date Created: C28
# Author: AI Model & Curator
# Updated on: C53 (Codify requirement for user login disclaimer)

- **Key/Value for A0:**
- **Description:** A plan to expand the `/showcase` page into a multi-tabbed view, featuring both the interactive "Ascent Report" and an embedded version of the `aiascent.game` website.
- **Tags:** page design, showcase, tabs, iframe, integration, plan, ui, ux

## 1. Overview and Goal

The `/showcase` page currently serves as the home for the interactive "Ascent Report." To broaden the demonstration of what can be built with the Data Curation Environment (DCE), the user has requested an expansion to also showcase `aiascent.game`.

The goal of this plan is to refactor the `/showcase` page into a tabbed interface. This design will allow users to easily switch between different showcased projects, starting with the report and the game, while creating an extensible pattern for adding more projects in the future.

## 2. Design and UI/UX

The page will be modified to include a simple, clean tab navigation bar at the top, directly below the main site header. The rest of the page content area will be dedicated to rendering the currently selected tab's content.

### 2.1. Tab Navigation

*   **Location:** At the top of the main content area on `/showcase`.
*   **Tabs:**
    1.  **The Ascent Report:** This will be the default active tab.
    2.  **AI Ascent Game:** The second tab.
*   **Styling:** The tabs will be styled to match the site's aesthetic. The active tab will be clearly indicated (e.g., with a bottom border in the primary color and bolder text).

### 2.2. Tab Content

*   **The Ascent Report:** This tab will render the existing `<ReportViewer reportName="showcase" />` component, filling the entire content area. The user experience for the report will remain unchanged.
*   **AI Ascent Game:** This tab will render an `<iframe>` that fills the entire content area. The `src` of the iframe will be `https://aiascent.game/`. This will embed the live game directly into the showcase page, allowing users to interact with it seamlessly.

### 2.3. User Disclaimer (Requirement)

*   **Problem:** Authentication (login), chat, and multiplayer features within `aiascent.game` may not function correctly within a cross-origin `iframe` due to browser security policies.
*   **Requirement:** To prevent user confusion, a prominent disclaimer must be displayed within the "AI Ascent Game" tab, above the `iframe`.
*   **Content:** The disclaimer must inform users that for the full experience, including login, chat, and multiplayer, they should visit the main site. It must include a direct, clickable link to `https://aiascent.game/`.

## 3. Technical Implementation Plan

1.  **Create `ShowcaseTabs.tsx` Component:**
    *   A new client component will be created at `src/components/showcase/ShowcaseTabs.tsx`.
    *   This component will use a `useState` hook to manage the `activeTab` (defaulting to `'report'`).
    *   It will render the tab buttons and conditionally render the content for the active tab.
    *   The `ReportViewer` will be rendered for the `'report'` tab.
    *   For the `'game'` tab, it will render the disclaimer paragraph and the `<iframe>`. The iframe should be styled to be responsive and fill the available space (`className="w-full h-full border-0"`).

2.  **Update `showcase/page.tsx`:**
    *   The existing content of the showcase page will be replaced with the new `<ShowcaseTabs />` component.
    *   The page will no longer render the `ReportViewer` directly.

3.  **Extensibility (Future Consideration):**
    *   The `ShowcaseTabs.tsx` component can be easily extended in the future by adding new objects to a `tabs` array configuration. Each object would define an `id`, `label`, and the component or content to render.

This plan provides a clean, user-friendly, and technically straightforward path to expanding the showcase, making it a more comprehensive portfolio of projects built with the DCE.
</file_artifact>

<file path="src/Artifacts/A31. aiascent.dev - iframe Integration Guide.md">
# Artifact A31: aiascent.dev - iframe Integration Guide

# Date Created: C29
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Explains the root cause of cross-domain cookie issues when embedding authenticated applications (like `aiascent.game` with NextAuth) in an iframe and provides the solution.
- **Tags:** iframe, authentication, cookies, samesite, nextauth, security, integration

## 1. Overview

This guide addresses the login failure observed when embedding `aiascent.game` within an `iframe` on the `aiascent.dev/showcase` page. The root cause is a browser security feature related to how cookies are handled in cross-site contexts.

## 2. The Problem: Cross-Site Cookie Rejection

Modern browsers have implemented stricter security policies to prevent Cross-Site Request Forgery (CSRF) attacks. A key part of this is the `SameSite` attribute on cookies.

*   **The Console Error:** You observed errors like `Cookie “__Host-next-auth.csrf-token” has been rejected because it is in a cross-site context and its “SameSite” is “Lax” or “Strict”.`
*   **Root Cause:** The `aiascent.game` website (which uses NextAuth.js) sets authentication cookies with a `SameSite` policy of `Lax` by default. This policy means the browser will only send the cookie if the request originates from the *same site* (`aiascent.game`). When the game is loaded in an `iframe` on `aiascent.dev`, the browser correctly identifies this as a "cross-site" context and refuses to send the `Lax` cookies, causing the authentication to fail.

## 3. The Solution: `SameSite=None` and `Secure`

To fix this, the `aiascent.game` application must be configured to tell browsers that its authentication cookies are *intended* to be used in a cross-site context. This requires making two specific changes to the cookie configuration within the `aiascent.game` project.

1.  **`SameSite='none'`:** This attribute explicitly tells the browser that the cookie can be sent with cross-site requests (like from an `iframe`).
2.  **`Secure=true`:** Using `SameSite='none'` is only allowed if the cookie is also marked as `Secure`. This ensures the cookie is only ever sent over an HTTPS connection, which is a critical security measure.

### 3.1. Required Code Change in `aiascent.game`

The following change needs to be made in the NextAuth configuration file within the `aiascent.game` project (likely located at `src/pages/api/auth/[...nextauth].ts` or a similar path).

```typescript
// In the aiascent.game project's NextAuth options...

export const authOptions: NextAuthOptions = {
  // ... your other providers (Google, etc.)
  providers: [
    // ...
  ],
  
  // ADD OR MODIFY THIS COOKIES SECTION
  cookies: {
    sessionToken: {
      name: `__Secure-next-auth.session-token`,
      options: {
        httpOnly: true,
        sameSite: 'none', // <--- CRITICAL CHANGE
        path: '/',
        secure: true,   // <--- CRITICAL CHANGE
        // If you have a custom domain, you might also need:
        // domain: ".aiascent.game", 
      },
    },
    // You may need to apply similar settings for other NextAuth cookies
    // like csrfToken, callbackUrl, etc., if issues persist.
  },

  // ... rest of your configuration
};
```

**Important Note:** This change must be deployed with the `aiascent.game` application. It cannot be fixed from the `aiascent.dev` side, as the cookie-setting behavior is controlled by the embedded site.

## 4. Refresh Button Implementation

To improve the user experience of the embedded game, a refresh button has been added to the showcase tab. This allows the user to reload the `iframe`'s content without reloading the entire `aiascent.dev` page.

This is implemented in `ShowcaseTabs.tsx` using a React `ref` to access the `iframe`'s `contentWindow` and trigger a reload.

```typescript
// Example from ShowcaseTabs.tsx

const iframeRef = useRef<HTMLIFrameElement>(null);

const handleRefresh = () => {
  if (iframeRef.current) {
    iframeRef.current.contentWindow?.location.reload();
  }
};

// ... in the JSX ...
<button onClick={handleRefresh}>Refresh Game</button>
<iframe ref={iframeRef} src="https://aiascent.game/" />
</file_artifact>

<file path="src/Artifacts/A32. aiascent.dev - Dynamic Chat Prompt Suggestions Plan.md">
# Artifact A32: aiascent.dev - Dynamic Chat Prompt Suggestions Plan

# Date Created: C35
# Author: AI Model & Curator

# Updated on: C90 (Codify robust state management for reportName)

- **Key/Value for A0:**
- **Description:** Outlines the technical implementation for generating, parsing, and displaying dynamic, context-aware follow-up questions ("chips") in the Ask @Ascentia chat interface.
- **Tags:** plan, chat, ui, ux, llm, prompt engineering, ascentia

## 1. Overview and Goal

To improve user engagement and guide the conversation within the "Ask @Ascentia" feature, we will implement dynamic prompt suggestions. These will appear as clickable "chips" below the chat history.

*   **Page-Specific Generation:** When a user navigates to a new page within a report, a request is made to the backend to generate suggestions based on that specific page's content. This ensures suggestions are relevant from the very first interaction.
*   **Dynamic Generation:** After every response from Ascentia, the LLM will also generate 2-4 relevant follow-up questions based on the conversation context.
*   **Interaction:** Clicking a chip automatically submits that question as a user message.

## 2. Technical Implementation

### 2.1. Backend: Dual-Mode API (`/api/chat/route.ts`)

The chat API route supports two modes for handling suggestions:

1.  **`task: 'generate_suggestions'`:** A specialized, non-streaming mode for pre-generating suggestions.
    *   **Trigger:** Called by the frontend when a new report page is loaded.
    *   **Input:** Receives only the `pageContext`.
    *   **Prompt:** Uses a dedicated system prompt that instructs the LLM to *only* generate a JSON array of questions based on the provided text.
    *   **Output:** Returns a clean JSON array `["Question 1?", "Question 2?"]`.
    *   **Robustness:** The backend parsing logic must be resilient to minor LLM formatting errors, extracting the JSON array even if it's embedded in other text.

2.  **Standard Chat Mode:**
    *   **Trigger:** Called for a normal user chat query.
    *   **Prompt Engineering:** The main system prompts are updated to instruct the LLM to append suggestions to the end of its response in a structured, machine-parseable format, using distinct delimiters.
    *   **Updated Instruction:** "Finally, after your main response, generate 2-4 short, relevant follow-up questions... Output them strictly as a JSON array of strings wrapped in specific delimiters: `:::suggestions:::[\"Question 1?\", \"Question 2?\"]:::end_suggestions:::`."

### 2.2. State Management and Context Isolation (`src/stores/reportStore.ts`)

The `ReportState` is the source of truth for suggestions and their status.

*   **State:**
    *   `suggestedPrompts: string[]`: Stores the current list of suggestions.
    *   `suggestionsStatus: 'idle' | 'loading' | 'error'`: Tracks the status of the on-demand suggestion fetching.
    *   `reportName: string | null`: Tracks the currently active report (`'v2v-academy-career-transitioner'`, `'whitepaper'`, etc.). This is the **single source of truth** for context isolation.
*   **Actions:**
    *   `loadReport(reportData)`: **CRITICAL:** This action must completely reset the entire report state to its initial defaults *before* fetching new data. It sets the `reportName` from the `reportId` within the `reportData`, establishing the authoritative context for the new report.
    *   `fetchPageSuggestions(page)`:
        *   **C90 Update:** This action no longer accepts `reportName` as an argument. It reads the authoritative `reportName` directly from the store state using `get()`.
        *   Sets `suggestionsStatus` to `'loading'`.
        *   Calls the backend API with `task: 'generate_suggestions'`.
        *   **Race Condition Prevention:** Before updating the state with fetched suggestions, it must check if the `reportName` at the start of the async call still matches the *current* `reportName` in the store. If they don't match (i.e., the user has navigated away), the action must abort to prevent state corruption.
        *   On failure, it sets `suggestionsStatus` to `'error'` and populates `suggestedPrompts` with the correct default questions for the current `reportName`.

### 2.3. Frontend: UI and Logic (`ReportViewer.tsx`, `ReportChatPanel.tsx`)

1.  **Triggering Suggestions (`ReportViewer.tsx`):**
    *   A `useEffect` hook listens for changes to `currentPageIndex`.
    *   When the page changes, it calls the `fetchPageSuggestions` action. **(C90 Update):** It no longer passes the `reportName` prop.

2.  **UI Rendering (`ReportChatPanel.tsx`):**
    *   A new container below the chat history renders the suggestions.
    *   It observes `suggestionsStatus`:
        *   If `'loading'`, it displays a "Generating suggestions..." message or spinner.
        *   If `'idle'` or `'error'`, it maps through the `suggestedPrompts` array and renders each as a `Badge` component.
    *   **Styling:** The `Badge` components should use word-wrapping and have a maximum width to handle longer questions gracefully.

3.  **Parsing In-Chat Suggestions (`ReportChatPanel.tsx`):**
    *   After a normal chat response is fully streamed, the `sendMessage` function must perform a robust search for the `:::suggestions:::` block.
    *   It extracts and parses the JSON content, calls `setSuggestedPrompts`, and then strips the entire block from the message before saving the final, clean content to the chat history.
</file_artifact>

<file path="src/Artifacts/A33. aiascent.dev - Report Viewer Fullscreen Plan.md">
# Artifact A33: aiascent.dev - Report Viewer Fullscreen Plan

# Date Created: C45
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Outlines the plan to implement a fullscreen toggle feature for the interactive report viewer, enhancing the immersive reading experience.
- **Tags:** plan, ui, ux, report viewer, fullscreen, feature

## 1. Overview and Goal

To provide a more immersive and focused reading experience, users have requested the ability to view the interactive reports in a fullscreen mode. The goal of this feature is to allow users to expand the `ReportViewer` component to fill the entire browser viewport with a single click, hiding the main website's header and footer.

## 2. User Experience Flow

1.  **Entry Point:** A new "Fullscreen" icon button will be added to the report viewer's control area (specifically, within `ImageNavigator.tsx`).
2.  **Activation:** Clicking the "Fullscreen" button will cause the `ReportViewer` component to smoothly expand and cover the entire viewport. The main site header and footer will disappear. The icon on the button will change to an "Exit Fullscreen" icon.
3.  **Interaction:** The report viewer will remain fully functional in fullscreen mode.
4.  **Deactivation:** Clicking the "Exit Fullscreen" button (or pressing the `Esc` key) will return the `ReportViewer` to its original size within the page layout, and the site header and footer will reappear.

## 3. Technical Implementation Plan

### 3.1. State Management (`src/stores/reportStore.ts`)

A new state and action will be added to manage the fullscreen status globally.

*   **New State:** `isReportFullscreen: boolean` (defaulting to `false`).
*   **New Action:** `toggleReportFullscreen: () => void`. This action will simply invert the boolean value of `isReportFullscreen`.

### 3.2. UI Components

1.  **`ImageNavigator.tsx`:**
    *   A new icon button (e.g., using `FaExpand` and `FaCompress` from `react-icons`) will be added to one of the control groups.
    *   The button's `onClick` handler will call the `toggleReportFullscreen` action from the store.
    *   The icon will change based on the `isReportFullscreen` state.

2.  **`ReportViewer.tsx`:**
    *   The root `div` of the component will have its `className` determined conditionally.
    *   When `isReportFullscreen` is `true`, it will apply classes for fixed positioning, covering the viewport, and ensuring a high z-index (e.g., `fixed inset-0 z-[100] bg-background`).
    *   When `false`, it will use its standard classes for embedding within the page layout.

3.  **`app/layout.tsx`:**
    *   The root layout will need to conditionally render the `<Header />` and `<Footer />` based on the `isReportFullscreen` state.
    *   This will require converting the layout to a client component so it can subscribe to the `reportStore`.

### 3.3. Keyboard Shortcut

*   An `useEffect` hook will be added to `ReportViewer.tsx` to listen for the `Escape` key press. When detected, it will check if `isReportFullscreen` is true and, if so, call `toggleReportFullscreen` to exit the mode.
</file_artifact>

<file path="src/Artifacts/A34. aiascent.dev - Whitepaper Introduction Content.md">
# Artifact A34: aiascent.dev - Whitepaper Introduction Content

# Date Created: C50
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Provides the new introductory content for the homepage's interactive whitepaper, "Process as Asset," designed to welcome users and explain the interface.
- **Tags:** page design, home page, report viewer, whitepaper, content, user guide

## 1. Overview

This artifact contains the replacement content for the first page of the homepage's interactive whitepaper. The goal is to create a more welcoming and informative introduction, similar to the main Ascent Report, that introduces the AI assistant, explains the controls, and sets the stage for the whitepaper's topic.

## 2. New Page Content

*   **Page Title:** Welcome to the Interactive Whitepaper
*   **TL;DR:** An interactive guide to navigating this whitepaper and understanding its features, presented by your AI assistant, Ascentia.
*   **Content:**
    Hi there! I am Ascentia, your guide through this interactive experience. This whitepaper, "Process as Asset," explores the core philosophy behind the Data Curation Environment (DCE). It explains how a structured, iterative workflow can transform the very process of creation into a valuable, scalable asset.

    To help you navigate, allow me to explain the interface.

    *   To your left, you will find the **Report Navigator**, a tree that allows you to jump to any section.
    *   In the center are the primary controls. You can navigate between pages using the **up and down arrow keys**.
    *   For a more immersive experience, you can select **"Autoplay."** I will then read the contents of each page aloud to you.
    *   Finally, the **"Ask Ascentia"** button opens a direct line to me. This whitepaper is powered by a knowledge base built from all the documentation for the DCE project. If you have any questions about how the DCE works, feel free to ask.

    Enjoy the exploration.
</file_artifact>

<file path="src/Artifacts/A35. aiascent.dev - Discord Community Management Plan.md">
# Artifact A35: aiascent.dev - Discord Community Management Plan

# Date Created: C50
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Outlines a strategic plan for building, managing, and monetizing a Discord community around the Data Curation Environment (DCE).
- **Tags:** plan, community, discord, monetization, dce, cognitive apprenticeship

## 1. Vision & Goal

The goal is to create a vibrant, supportive, and self-sustaining community hub on Discord for users of the Data Curation Environment (DCE). This community will serve as a place for learning, collaboration, and support, while also providing a pathway for monetization through high-value consulting and training, all managed through the DCE workflow itself.

## 2. Core Concept: The DCE-Powered Community

The community will be managed using the same principles the DCE promotes: "documentation first" and structured, iterative development. The community manager will act as a "Citizen Architect" for the community itself.

*   **Bot Integration:** A Discord bot will be set up. Its context will be the `aiascent-dev` website repository, allowing it to answer questions and generate community content with full project awareness.
*   **Artifact-Driven Management:** All significant community structures—rules, channel guides, role definitions, onboarding flows—will be created as documentation artifacts using the DCE. This ensures a "source of truth" for the community's governance.

## 3. Community Structure & Engagement

*   **Channel Setup:**
    *   `#welcome-and-rules`: Automated welcome message and clear community guidelines.
    *   `#announcements`: Project updates and news.
    *   `#dce-support`: For users seeking help with the DCE.
    *   `#showcase`: A place for users to share projects they've built with the DCE.
    *   `#vibecoding-lounge`: General chat and discussion about AI-assisted development.
    *   `#feature-requests`: For community feedback and ideas.
*   **Weekly "DCE in Action" Sessions:** The project curator will commit to weekly live sessions (e.g., via Discord stages or streaming) demonstrating how to use the DCE for various tasks. These sessions are free and serve as the top of the engagement funnel.

## 4. Monetization Model: The "Cognitive Apprenticeship" Funnel

The monetization strategy is based on offering progressively deeper levels of expert guidance.

*   **Tier 1: Free Community Access:**
    *   **Offering:** Access to all public channels, community support, and the weekly live sessions.
    *   **Goal:** Build a large, engaged user base and demonstrate the value of the DCE.

*   **Tier 2: Premium Support & Consulting:**
    *   **Offering:** For a fee, users can get dedicated, one-on-one consulting for their specific projects. This could be for troubleshooting, architectural guidance, or advanced workflow optimization.
    *   **Management:** This system can be managed within Discord. A bot could handle requests, payments (via Stripe integration), and scheduling. The community manager would be the primary point of contact, triaging requests and escalating complex issues to the curator.

*   **Role of the Community Manager:**
    *   Act as the first line of support, answering questions they can handle based on their knowledge.
    *   Triage questions they cannot answer and bring them to the curator.
    *   Manage the premium support system, acting as a liaison between users and the curator.
    *   Foster a positive and collaborative community environment.

This model creates a sustainable ecosystem where the community benefits from free resources and expert access, while providing a clear path to generate revenue by offering high-value, personalized expertise.
</file_artifact>

<file path="src/Artifacts/A40. aiascent.dev - Page Design DCE.md">
# Artifact A40: aiascent.dev - Page Design DCE

# Date Created: C51
# Author: AI Model & Curator
# Updated on: C53 (Add plan for fullscreen GIF modal)

- **Key/Value for A0:**
- **Description:** A blueprint for the `/dce` page, dedicated to explaining the core features of the Data Curation Environment VS Code extension with visual aids.
- **Tags:** page design, dce, features, plan, ui, ux, modal, fullscreen

## 1. Overview and Goal

The `/dce` page will serve as a focused introduction to the core functionalities of the Data Curation Environment (DCE) extension. Its goal is to clearly and visually explain *how* the DCE works, complementing the other pages that explain *why* it exists. The page will be structured using `MissionSectionBlock` components to maintain visual consistency with the Mission and Learn pages.

## 2. Page Structure and Content

The page will be built as a series of feature spotlights, each explaining a core component of the DCE workflow.

---

### **Section 1: Precision Context Curation**

*   **Title:** Precision Context Curation
*   **TL;DR:** Stop manual copy-pasting. The DCE's File Tree View provides an intuitive, visual way to select the exact files, folders, and documents needed for your AI prompts directly within VS Code.
*   **Content:** The foundation of a high-quality AI response is high-quality context. The DCE eliminates the error-prone process of manually managing file lists or copy-pasting code into a prompt. With the integrated File Tree View, you can browse your entire workspace and select the precise "source of truth" for your task with simple checkboxes. This curated selection is then automatically flattened into a single context file, ensuring the AI has exactly what it needs, and nothing it doesn't.
*   **Image Side:** Left
*   **Asset Wishlist:** A short, looping GIF named `dce-feature-curation.gif` showing a user's mouse clicking checkboxes next to files and folders in the DCE File Tree View panel, followed by the "Flatten Context" button being clicked.

---

### **Section 2: Parallel AI Scrutiny**

*   **Title:** Parallel AI Scrutiny
*   **TL;DR:** Don't rely on a single AI response. The Parallel Co-Pilot Panel allows you to compare multiple solutions side-by-side, with an integrated diff viewer to instantly spot the differences.
*   **Content:** AI models are non-deterministic. A single prompt can yield multiple, viable solutions. The Parallel Co-Pilot Panel is designed for this reality. Paste in several responses from your AI, and the DCE will parse them into separate, color-coded tabs. You can instantly compare the proposed changes for each file and use the built-in diff viewer to understand the nuances of each solution before deciding which one to accept.
*   **Image Side:** Right
*   **Asset Wishlist:** A GIF named `dce-feature-parallel-copilot.gif` showing the Parallel Co-Pilot Panel with multiple tabs. The user clicks between "Resp 1" and "Resp 2", and the file content below updates, with the integrated diff view highlighting the changes.

---

### **Section 3: Iterative Knowledge Graph**

*   **Title:** Iterative Knowledge Graph
*   **TL;DR:** AI collaboration shouldn't be ephemeral. The DCE captures the entire development process—prompts, responses, and decisions—as an iterative, auditable history you can navigate.
*   **Content:** Every development cycle in the DCE is saved, creating a persistent knowledge graph of your project's evolution. The Cycle History view allows you to step back in time, review the exact context used for a previous prompt, see all the AI responses that were generated, and understand why a particular solution was chosen. This turns your development process into a valuable, shareable asset for training, onboarding, and after-action reviews.
*   **Image Side:** Left
*   **Asset Wishlist:** A GIF named `dce-feature-cycles.gif` showing the user clicking the back and forward arrows in the "Cycle History" view, with the cycle title, context, and response tabs all updating to reflect the historical state.

---

### **Section 4: Next Up: See the Results**

*   **Title:** Ready to See the Results?
*   **Description:** The DCE is the engine behind complex, real-world projects. The Showcase features an interactive whitepaper and a multiplayer game, `aiascent.game`, both built using the iterative workflow you've just learned about. Explore the showcase to see the tangible results of this methodology.
*   **Button Text:** Explore the Showcase
*   **HREF:** `/showcase`

## 3. Fullscreen GIF Modal Feature

*   **User Need:** The GIFs demonstrating the DCE features contain small text and UI elements that are hard to see. Users need a way to view them in a larger, focused format.
*   **Plan:**
    1.  **Create a Global Modal:** A new reusable modal component (`FullscreenImageModal.tsx`) will be created. It will be designed to display a single image (or GIF) and a block of text.
    2.  **State Management:** The global `reportStore` will be updated to manage the modal's state, including its visibility and the content (image URL and description) to display.
    3.  **Trigger:** The `MissionSectionBlock` component will be modified so that clicking on the image/GIF area will trigger a store action to open the modal, passing the relevant image URL and description text.
    4.  **Modal UI:** The modal will be a fullscreen overlay with a dark background. It will display the GIF at a large size and the description text (the `imagePrompt`) either below or to the side, ensuring both are clearly visible. A close button will be prominent.
</file_artifact>

<file path="src/Artifacts/A41. aiascent.dev - Page Design DCE - Artifacts as Source of Truth.md">
# Artifact A41: aiascent.dev - Page Design DCE - Artifacts as Source of Truth

# Date Created: C53
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A plan for a new section on the `/dce` page explaining how generating documentation artifacts is a core feature of the DCE workflow, establishing them as the project's "source of truth."
- **Tags:** page design, dce, features, plan, source of truth, documentation, artifacts

## 1. Overview and Goal

The `/dce` page currently explains the core workflow features of the Data Curation Environment (DCE). A key philosophical and practical aspect is missing: the concept of using the AI to generate documentation artifacts *first*, establishing these documents as the project's "source of truth."

The goal is to add a new section to the `/dce` page that clearly explains this "documentation-first" principle and its benefits, reinforcing the strategic value of the DCE beyond simple code generation.

## 2. Page Structure and Content

This new section will be added to `src/app/dce/page.tsx` as the fourth `MissionSectionBlock`, appearing before the final "Next Up" link.

---

### **New Section: Artifacts as the Source of Truth**

*   **Title:** Artifacts as the Source of Truth
*   **TL;DR:** The DCE workflow inverts the traditional development process. By instructing the AI to create planning and documentation artifacts first, the process itself becomes a transparent, auditable, and durable asset.
*   **Content:** A core feature of the DCE is its "documentation-first" methodology. Instead of asking an AI to simply write code, the workflow begins by instructing it to create artifacts: project plans, design documents, and strategic memos that define the "why" and "how" of a task. These artifacts become the immutable "source of truth" that guides all subsequent code generation. This process ensures that human intent is clearly captured and that the AI's work is always aligned with the project's strategic goals. It transforms the development process from a series of ephemeral prompts into a permanent, auditable knowledge graph where every decision is traceable and every line of code has a documented purpose.
*   **Image Side:** Right
*   **Asset Wishlist:** A new GIF, `dce-feature-artifacts.gif`, showing the user in the PCPP, generating a `prompt.md` which is then used to generate a new `AXX-New-Feature-Plan.md` artifact file.

---
</file_artifact>

<file path="src/Artifacts/A61.1 - Transcript 1 Summary.md">
# Artifact A61.1: Transcript 1 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-1.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript captures a coaching session focused on onboarding two learners to the curator's AI-assisted development workflow. The curator explains the foundational concepts of the Data Curation Environment (DCE), the importance of organizing "source of truth" documents, and the distinction between static "documents" and iterative "artifacts." The session concludes with a homework assignment for the learners: to begin curating their own project data and to review the curator's past project histories as a source of inspiration and learning.

## 2. Key Learnings & Insights

*   **The Workflow is the Lesson:** The primary lesson is the workflow itself. Learning to use the DCE—curating data, creating artifacts, and iterating with an AI—is the core skill being taught.
*   **"It's All Just Text":** A recurring theme is the need to convert all forms of knowledge (PDFs, Excel sheets, ideas) into a flat, text-based format (preferably Markdown) so it can be processed by the AI.
*   **Organization Emerges Naturally:** The curator advises against over-organizing at the beginning. The best folder structure will emerge naturally as the project grows and the user discovers the most efficient way to group files for context selection (i.e., to minimize the number of checkboxes they need to click).
*   **Artifacts are Iterative; Documents are Static:** A key distinction is made. "Documents" are foundational, reference materials that don't change often. "Artifacts" are the living, iterative outputs of the AI collaboration process.
*   **The Value of Past Cycles:** The curator's own project histories (the `prompt.md` files with all their cycles) are presented as invaluable learning resources. They are a "lab guide" containing real-world examples of problems, solutions, and the evolution of the curator's own thinking.
*   **Reverse-Engineering as a Learning Tool:** The process of taking a completed project (like a perfect lesson plan) and turning it into a template is a powerful learning exercise.

## 3. Best Bits (Direct Quotes)

*   **On the core skill:**
    > "The more organized, the better, but it can be anything. Gotcha. Yep. Okay. Oh, here's an example. An artifact can even be a set of artifacts... That is literally what few-shot learning is. It's good. You just went from zero shot to one shot."

*   **On the importance of data curation:**
    > "You're just going to curate data, and then you're going to ask for code solutions, and you'll get them. The better your data is curated, the better your code solutions. Trust me."

*   **On the learning process:**
    > "When you don't know what to add, come in here and read the generated artifacts. Because these are tangential parallel problems. I was making lessons in labs for cybersecurity using KSATs and all of our same knowledge artifacts. So you'll get inspiration by sitting here and reading this, I promise you."

*   **On the nature of AI interaction:**
    > "You're building the mental model of the model right now. You're getting an idea of what one... So this is an important analogy. Think of your prompt as an input output as a single page... if you just conceptualize it as one big page both the input and the output then what you're doing is you're you're you're building a new alphabet because you now know what the input will produce the output and that's one page like one japanese letter."
</file_artifact>

<file path="src/Artifacts/A61.2 - Transcript 2 Summary.md">
# Artifact A61.2: Transcript 2 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-2.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript captures a practical, hands-on coaching session where the curator guides a learner through the initial setup and organization of their project repository for the V2V workflow. The conversation focuses on establishing a logical folder structure to manage different types of documentation (CUI, customer docs, frameworks, templates). The session emphasizes the core principle that a well-organized repository is the foundation for effective data curation and, consequently, for high-quality AI collaboration. The concept of a "master project template" is developed as a way to bootstrap future projects.

## 2. Key Learnings & Insights

*   **Folder Structure as a Form of Tagging:** The curator explains that folder names themselves act as "tags." A well-named folder (e.g., `UKI-templates`) provides inherent context to the AI, which can infer the purpose of the files within it without needing explicit instructions.
*   **Organization is Driven by Curation Efficiency:** The primary driver for how to organize files should be curation efficiency. The user will naturally start grouping related files into folders to minimize the number of checkboxes they need to click when selecting context for a prompt.
*   **The "External Brain" Concept:** The repository is framed as the user's "external brain." It's a persistent, organized collection of knowledge that grows over time and makes the user's interactions with AI progressively more powerful and personalized.
*   **Separating Tasks from Metadata:** A key architectural decision is made to create a top-level `tasks` folder. This separates the specific, iterative work of a project (like the "NC-DOC" project) from the more static, reusable metadata (like frameworks and templates), leading to a cleaner structure.
*   **Reverse-Engineering a Template:** The best way to create a project template is to first build out a real project. Once the project is complete, its structure and key files can be "reverse-engineered" into a clean, reusable skeleton for future projects.
*   **"It's All Just Text":** The curator reiterates the core philosophy that all forms of data (Excel sheets, PDFs, etc.) must be "flattened" into a text-based format (ideally Markdown) to be usable by the AI. This is analogized to the meme "Wait, it's all Ohio? Always has been," but replaced with "Wait, it's all just text? Always has been."

## 3. Best Bits (Direct Quotes)

*   **On the purpose of organization:**
    > "Because you've structured it intelligently, it's intelligent and it'll get it. So it's good. It's good. You don't even... And then you'll only need to explicitly explain that which it clearly didn't get."

*   **On the natural evolution of structure:**
    > "The only constraint you will find is you will realize it gets annoying to check the box to select and deselect. You will realize it's better if these five files go in a folder so that I can just click it. That's what's gonna happen... That's how you're gonna constrain your organization."

*   **On the value of the process:**
    > "Even if AI didn't exist, having this organized in this way would still help you be more... Oh yeah. You see what I'm saying? And that's what I typically do... And for the final and final. And I never wanted to do this organization in my life up until... the AI values it, right? You see what I'm saying? So it's now fun to be organized. It's valuable."
</file_artifact>

<file path="src/Artifacts/A61.3 - Transcript 3 Summary.md">
# Artifact A61.3: Transcript 3 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-3.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript details an end-to-end "Cycle 0" project initialization using the Data Curation Environment (DCE). The curator guides two learners through the entire process: writing the initial project scope, manually adding a pre-existing code file (`appdemo.py`) to the prompt's context, sending the prompt to the AI, and then parsing and analyzing the generated artifacts. The session serves as a practical, hands-on demonstration of the core DCE workflow and surfaces several key pedagogical points about interacting with AI, as well as a few bugs in the tool that require fixing.

## 2. Key Learnings & Insights

*   **The Power of Metacognition in Prompts:** The curator emphasizes the importance of providing the AI with the "big picture" context. By explaining *who* is doing the task and *why* (e.g., "I am following in the footsteps of an expert vibe coder"), the AI gets a much richer understanding of the user's intent, leading to better results.
*   **Manual Context Injection:** The session demonstrates a workaround for including existing files in the initial prompt before the DCE's file system is fully active: manually pasting the file content into the `prompt.md` within an `<ephemeral_context>` tag.
*   **The Value of Parallelism:** The curator runs the same prompt against multiple AI instances, including a premium "DeepThink" model. This immediately highlights the variance in AI responses (some are longer, some are structured differently) and demonstrates the core value of having multiple options to choose from.
*   **Critique and Alignment as the Core Loop:** The workflow doesn't stop after the first response. The main activity is to critique the AI's output (the generated artifacts) against the user's mental model, document the misalignments in the next cycle's context, and re-prompt. This is the essence of AI alignment in practice.
*   **Local LLMs vs. Cloud APIs:** The session includes a practical discussion on using a local LLM via LM Studio. The curator explains that while cloud APIs are more powerful, local models are free (beyond electricity cost) and sufficient for many tasks, and the DCE is designed to switch between them.
*   **Bugs as Learning Opportunities:** The session reveals several bugs in the DCE (parsing errors, data loss on tab switching, including the `.git` directory). These are treated not as failures, but as the natural next steps for the development process, becoming the user stories for the next cycle.

## 3. Best Bits (Direct Quotes)

*   **On providing context:**
    > "You see how I'm printing this? Do you see that? Like, it's metacognition. I'm giving the AI the whole context, dude. It's from the big picture so that it can help, it will really help us out in our situation. Not like guessing, like what does even the user want?"

*   **On the iterative process:**
    > "We just described all the differences that we have with our project in our mind with what the AI told us it has in its mind. Now we want to read those, we want to see the results of that... This is alignment, this is AI alignment. You're aligning this context for your specific use case and the more you do now, the much better off you will be, I promise."

*   **On the value of small changes:**
    > "You just weren't specific that the model you're using is local. Do you see? The moment it got that, it knew to give you a local LLM integration guide. You see? So, that's a good lesson right there. Tiny little tweak. Tiny, tiny, tiny little tweak."

*   **On the human's role:**
    > "It's all feedback. It's one big feedback loop, both for you and for it. You're giving it the feedback, and it kind of breaks down if you're just editing the same cycle. You're not quite giving it a full feedback, if that makes sense."
</file_artifact>

<file path="src/Artifacts/A61.4 - Transcript 4 Summary.md">
# Artifact A61.4: Transcript 4 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-4.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript captures a coaching session where the curator explains the full, end-to-end workflow of his Data Curation Environment (DCE) extension. He walks the learner through the "perfect loop" of development, from initial data curation and project setup to generating a prompt, parsing multiple AI responses, and using the Git-integrated features to test and validate the code. The session is a practical demonstration of the V2V methodology, emphasizing the principles of "documentation first," parallel processing, and iterative refinement.

## 2. Key Learnings & Insights

*   **The "Perfect Loop" Workflow:**
    1.  **Curation:** Start by selecting all relevant files for a task using the DCE's file tree.
    2.  **Prompt Generation:** Use the DCE to generate a `prompt.md` file based on the curated context and cycle history.
    3.  **Parallel Responses:** Send the same prompt to multiple AI instances to get a variety of solutions.
    4.  **Parse & Sort:** Paste the responses into the DCE's Parallel Co-Pilot Panel, parse them into a structured view, and sort by token count to review the most detailed responses first.
    5.  **Select & Baseline:** Choose the best response and create a Git commit ("Baseline") as a safe restore point.
    6.  **Accept & Test:** Apply the AI's code to the workspace and test it.
    7.  **Restore or Proceed:** If the test fails, instantly revert with "Restore Baseline." If it succeeds, proceed to the next cycle.
*   **The Power of Parallelism:** The curator emphasizes that running prompts in parallel is a "superpower." It transforms the workflow from a linear process of reading and reacting to a single response into a comparative process of choosing the best among multiple options. This dramatically increases the probability of getting a high-quality solution quickly.
*   **Metacognition is Key:** The process of writing down the project scope and instructions in the DCE is a form of metacognition ("thinking about thinking"). It forces the developer to clarify their intent, which in turn leads to better AI outputs.
*   **The DCE as an "External Brain":** The extension and its associated files (`prompt.md`, `dce_history.json`) act as a persistent, external memory for the project. This allows for a complete audit trail and prevents the loss of context that plagues simple chatbot interactions.
*   **AI as a Content Delivery Solution:** The curator frames the V2V workflow as a "content delivery solution." It can be used to generate any form of structured content, not just code. The example of creating training materials for UKI is used to illustrate this.

## 3. Best Bits (Direct Quotes)

*   **On the core value of the DCE:**
    > "You're going to curate data, and then you're going to ask for code solutions, and you'll get them. The better your data is curated, the better your code solutions. Trust me."

*   **On the shift in workflow:**
    > "The parallel, sending a message parallel is actually crucial because it flips the script completely. You're not reading an entire prompt. You are now comparing between the prompts that you've received. It's a completely different ballpark, ballgame. The iteration cycle is immensely expedited by that."

*   **On the human's role:**
    > "The human is in the loop at every step of the way... We have perfect documentation. Every time I see a piece that's going to be missing, I just make sure it's in my process."

*   **On the future of development:**
    > "It's not about making code anymore... I'm making this for curation, not coding, because coding is gone. I'm already planning for coding being gone."
</file_artifact>

<file path="src/Artifacts/A61.6 - Transcript 6 Summary.md">
# Artifact A61.6: Transcript 6 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-6.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript captures a working session where the curator is guiding a learner on how to structure their project repository for use with the Data Curation Environment (DCE). The core of the conversation revolves around establishing a folder structure that is both logically organized for the human and optimally structured for the AI. They discuss the creation of a "project template" and the importance of separating static reference materials from dynamic project files. The session concludes with the learner successfully setting up their initial repository, curating their context, and generating their first `prompt.md` file, ready to be sent to an AI.

## 2. Key Learnings & Insights

*   **The Importance of a "Rock":** The curator suggests keeping original, unchanging reference documents (the "rock" or "seed") separate from the working, living documents that will be iterated upon. This establishes a stable foundation for the project.
*   **Project Templates for Reusability:** The learner proposes creating a `project_template` folder. This idea is validated and refined, establishing a pattern for creating reusable skeletons for future projects, which aligns with the "Virtuosity" stage of having a repeatable process.
*   **The Screenshot-to-Artifact Workflow:** A key workflow is introduced: take screenshots of a folder structure, feed them to an AI, and ask it to transcribe them into a structured text artifact (like a `File Tree Structure List`). This is a powerful method for bootstrapping documentation.
*   **The "Why" of Folder Structure:** The curator explains that the AI can infer a lot from a logical folder structure. A folder named `UKI-templates` immediately tells the AI the purpose of the files within it. This reinforces that good organization is a form of context curation.
*   **The Manual `prompt.md` Assembly:** The session demonstrates the manual steps for constructing the initial `prompt.md` file:
    1.  Use the DCE to "Flatten Context" to generate `flattened_repo.md`.
    2.  Manually copy the content of `flattened_repo.md` and paste it into the `<M7. Flattened Repo>` section of the main `prompt.md` file.
    3.  This manual step is a temporary part of the "Cycle 0" process before the fully automated workflow takes over.
*   **AI Studio Setup:** The curator provides specific instructions for configuring Google's AI Studio for optimal results: use the `Gemini 2.5 Pro` model, set the temperature to `0.7`, and max out the "Thinking budget."

## 3. Best Bits (Direct Quotes)

*   **On the value of organization:**
    > "The AI is very good at helping organize... When I started... I had no actual logical ordering other than chronological. And then so I actually thought, well, what if we can you group these up somehow... Now every time I get a new artifact, the artifact comes with its own description, its own tags, and it gets placed in my master's list or in an organized manner."

*   **On the meta-level of the workflow:**
    > "This is pretty meta. What you can also do is once you've got, let's just say you're done with this project and you're moving on to another, you can take this entire prompt and wrap it as example one and then just move on and then you don't have to sort of regurgitate all the boilerplate. It serves as training data. It's pretty epic."

*   **On the core value of the process:**
    > "The V2V pathway is a structured pedagogical model, grounded in Cognitive Apprenticeship, designed to transform intuitive AI interaction ('vibecoding') into architectural mastery."
</file_artifact>

<file path="src/Artifacts/A61.7 - Transcript 7 Summary.md">
# Artifact A61.7: Transcript 7 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-7.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript captures a wide-ranging and philosophical coaching session where the curator explains the entire "Vibecoding to Virtuosity" pathway, its strategic importance, and the personal motivations behind it. He uses his AI-built game, `AI Ascent`, and the accompanying "Ascent Report" as the primary case study. The conversation covers the technical (local LLMs, RAG, tokens), the pedagogical (Cognitive Apprenticeship), and the geopolitical (the AI race with China), weaving them into a single, coherent narrative about the future of human-AI collaboration.

## 2. Key Learnings & Insights

*   **The V2V Pathway Explained:** The curator details the progression from "Vibecoding" (intuitive, messy, but powerful) to "Virtuosity" (structured, repeatable, expert-level). He frames himself as an "OG Vibe Coder" who has been practicing this for years, long before the term was coined.
*   **The Game as Proof, The Report as Theory:** `AI Ascent` (the game) is presented as the tangible proof of what one person can build with this methodology. "The Ascent Report" is the theory, explaining the "why" behind the game and the V2V process.
*   **Local LLMs are Accessible:** The curator demystifies running local LLMs using tools like LM Studio, explaining that powerful models can run on consumer-grade hardware (e.g., a GPU with 16GB VRAM) and provide free API calls for development and experimentation.
*   **Tokens Explained Simply:** Tokens are demystified as the basic units of text for an AI, with the simple rule of thumb: `character_count / 4`. This is presented as all a developer really needs to know to manage context windows.
*   **RAG as a Superpower:** The curator explains Retrieval-Augmented Generation (RAG) by recounting his own "origin story" of building a Slackbot for Palo Alto Networks. He demonstrates how providing the AI with a knowledge base (the admin guide for a product) enabled it to answer questions about topics it wasn't trained on.
*   **The Geopolitical Imperative (The AI Cold War):** A major theme is the strategic competition with China. The curator argues that the West's "fissured workplace" model for AI training is a critical vulnerability ("institutionalized garbage in, garbage out"). In contrast, China is professionalizing its data workforce, creating a "unicorn farm" of AI talent that poses a long-term strategic threat.
*   **The Human as a "Cognitive Warrior":** The ultimate goal of the V2V pathway is to create "Citizen Architects" or "cognitive warriors"—individuals with the skills to leverage AI to solve complex problems, thereby strengthening the nation's collective "Cognitive Capital."

## 3. Best Bits (Direct Quotes)

*   **On the core of the V2V philosophy:**
    > "The proof is the product, the game is the proof, and the report is the theory. The theory is the 100x curator, the vibe coding to virtuosity pathway."

*   **On his personal journey and the power of AI:**
    > "I'm a one person studio. It's a paradigm shift in labor. One dude with AI with a vision can do everything that the entire team would do. This is what 100x looks like."

*   **On the human's role in the AI era:**
    > "The human is a strategist. The AI is the producer. Human vision, AI execution."

*   **On the strategic stakes:**
    > "While we hunt for unicorns... they build a unicorn farm... They're creating cognitive warriors at the same time we're outsourcing the potential to create cognitive warriors."

*   **On the accessibility of learning:**
    > "You get to learn everything with AI... you're going to get not only like in every having a professor in your pocket, every answer you want, but also on demand the best answer possible."
</file_artifact>

<file path="src/Artifacts/A61.9 - Transcript 9 Summary.md">
# Artifact A61.9: Transcript 9 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-9.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript captures a coaching session where the curator is guiding learners through the process of setting up their project repositories and beginning to curate their initial data sets for the V2V workflow. The conversation is highly practical, focusing on file organization, the purpose of different document types, and the mechanics of using the Data Curation Environment (DCE) extension. The session emphasizes that a well-organized repository is the crucial first step in the "documentation first" methodology and serves as the foundation for all subsequent AI interactions.

## 2. Key Learnings & Insights

*   **Separation of Concerns in the Repo:** A key organizational principle is established:
    *   **Reference Documents:** Static, foundational materials that don't change often (e.g., frameworks, style guides, official documents).
    *   **Working/Project Documents:** The "living documents" and iterative artifacts that are created and modified during the development process.
    *   **Templates:** Reusable skeletons for projects, lessons, or artifacts.
*   **The Power of a Project Template:** The idea of a `project_template` folder is developed. This allows a user to bootstrap a new project with a pre-defined, logical folder structure, which is a key step toward a repeatable, "virtuoso" workflow.
*   **The Screenshot-to-Artifact Workflow:** A powerful technique is discussed: taking screenshots of a folder structure or UI, feeding them to an AI, and having the AI transcribe them into a structured text artifact (like a file tree list). This is presented as a method for quickly bootstrapping documentation.
*   **The Feedback Loop of Curation:** The process of organizing files is itself a learning experience. The curator explains that the user will naturally start grouping related files to make context selection easier (i.e., to minimize the number of checkboxes they need to click), leading to a more efficient structure over time.
*   **The DCE as a Data Management Tool:** The session provides a hands-on demonstration of the DCE's core features: selecting files with checkboxes, seeing the live token count update, sorting the selected file list by type or size, and removing unwanted files from the context.
*   **Handling Non-Text Files:** The session reveals a bug/limitation in the current version of the DCE: it cannot properly process `.docx` files, leading to garbled text in the flattened output. The immediate workaround is to convert them to PDFs, and the long-term solution is for the curator to add `.docx` parsing capability to the extension.

## 3. Best Bits (Direct Quotes)

*   **On the value of organization for AI:**
    > "As you're doing this, the file structure, what we'll do is we'll just click expand all when you're finally done and we'll take a screenshot of that. And then we'll just let AI turn that into an initial documentation artifact... And then... what you're gonna add in there is what is the significance of the folders, so that it is known from the get-go."

*   **On the iterative nature of the process:**
    > "You'll start making living documents that can turn into templates later because you've got... let's say you've already got templates for state A, listen, then we'll get a template at state Z."

*   **On troubleshooting and tool limitations:**
    > "Oh, I didn't handle docx yet. Sorry, dude. That's why it's happening, because I handled PDF, I handled XLS, I haven't handled docx yet... That'll be next, I guess, on the list. I'll do that for you tomorrow."
</file_artifact>

<file path="src/Artifacts/A61.11 - Transcript 11 Summary.md">
# Artifact A61.11: Transcript 11 Summary
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the coaching transcript `transcript-11.md`.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This transcript captures a deep and wide-ranging coaching session where the curator shares his personal journey into AI-assisted development, demonstrates his core workflow and tools, and articulates the entire philosophical and geopolitical vision behind the "Vibecoding to Virtuosity" pathway. He uses his past projects—a sophisticated Slackbot for Palo Alto Networks and the AI-driven game `AI Ascent`—as case studies to explain concepts like RAG, thinking models, and the power of parallel prompting. The session culminates in a passionate explanation of the "AI Cold War" with China and his mission to create "sleeper agent" Citizen Architects to strengthen America's cognitive capital.

## 2. Key Learnings & Insights

*   **The "Non-Coder" Origin Story:** The curator's journey began as a non-developer who asked a fundamental question: "What's the most valuable thing AI can write?" The answer was code, because it's objective and verifiable. This led him to partner with AI to build complex applications despite not knowing how to code traditionally.
*   **Early RAG Implementation:** The curator independently invented a form of Retrieval-Augmented Generation (RAG) for his Palo Alto Slackbot. By feeding the bot's context with product documentation, he enabled it to answer questions about new products it hadn't been trained on. This is a powerful, real-world example of the core V2V skill.
*   **Thinking Models & Parallelism:** The curator explains that "thinking models" (like GPT-4o) are AIs that "talk to themselves before they talk to you," allowing them to plan. He combines this with his parallel prompting technique (running 8+ instances at once) to explore a wide solution space and select the best outcome.
*   **The AI Cold War:** A central theme is the strategic competition with China. The curator argues that the West's "fissured workplace" model for AI training creates a deprofessionalized, underpaid workforce that produces low-quality data ("garbage in, garbage out"). In contrast, China is professionalizing its data workforce, creating a strategic advantage and an army of "cognitive warriors."
*   **The V2V Pathway as a Counter-Strategy:** The V2V curriculum is framed as the American counter-strategy. Its goal is to create "sleeper agents" or "Citizen Architects"—individuals who master AI collaboration, become "100x" experts in their fields, and can be "activated" to solve problems in their communities, thereby strengthening the nation's collective "Cognitive Capital."
*   **The Apex Skill is On-the-Fly Tooling:** The highest level of virtuosity is described as "on-the-fly tooling"—the ability to command the AI to "build me a tool that solves problem X," rather than just asking it how to solve the problem. This is demonstrated by the curator's creation of the DCE extension itself.
*   **Universal Basic Access (UBA):** The curator proposes a solution to the economic disruption of AI: giving citizens AI credits instead of cash. These credits are an appreciating asset (as AI becomes more powerful) and can only be used for productive creation, spurring grassroots innovation.

## 3. Best Bits (Direct Quotes)

*   **On the origin of his RAG system:**
    > "I went through the whole admin... I did a control F, playbooks. Every single paragraph that had the word playbook in it, I made my own file... And then I just asked the exact same question, but I just added that in with my prompt. And it was like, magic. It was damn near almost usable."

*   **On his development philosophy:**
    > "I can't code. I'm not a coder, I'm not a developer. I can't write an IF statement to save my life... It's my job to have the taste and the gumption to like push through and see the project to completion."

*   **On the human-AI feedback loop:**
    > "But then if you get a code error, that's expert feedback that you don't have to create. It's created by the system... And you take that and you give that, that's expert feedback of the code that the AI just wrote. There's your feedback loop."

*   **On the geopolitical stakes and his motivation:**
    > "I want to be Star Trek, bro. I want to be Captain Kirk. I want to travel through space. And we're not going to fucking do it if we're fucking shooting each other for fucking Nikes, bro. It's so stupid. Look at the skills. Look at the tools we have, dude. We could solve every problem."
</file_artifact>

<file path="src/Artifacts/A61.12 - Transcript 12 Summary (Cycle 58 Context).md">
# Artifact A61.12: Transcript 12 Summary (Cycle 58 Context)
# Date Created: C60
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A high-level summary and synthesis of the key insights from the partial coaching transcript provided in the context for Cycle 58.
- **Tags:** v2v, research, synthesis, transcript analysis

## 1. High-Level Summary

This partial transcript captures the climax of a coaching session where the curator articulates the core philosophy behind the "Vibecoding to Virtuosity" pathway and its broader societal implications. The conversation crystallizes the idea of using AI as a feedback loop for learning, explains the curator's personal motivation, and connects the development of AI skills to a hopeful, "Star Trek" vision for the future of humanity.

## 2. Key Learnings & Insights

*   **The AI Feedback Loop for Learning:** The central pedagogical idea is that a non-expert can learn a technical skill like coding by using the AI as a partner. The key is to leverage system-generated errors as a form of "expert feedback." The human doesn't need to know *why* the code is wrong; they just need to take the error message and feed it back to the AI, creating a powerful, observable learning loop.
*   **The "Star Trek" Motivation:** The curator's personal motivation is revealed to be deeply aspirational. The drive to build these tools and teach these skills is not for personal gain but to accelerate human progress. The ultimate "selfish" goal is to live in a "Star Trek" future—a world of exploration and problem-solving—and the belief is that empowering individuals with these AI-driven skills is the fastest way to get there.
*   **AI Skills as a Solution to Scarcity:** The curator frames current societal problems as "stupid" and rooted in scarcity (e.g., "shooting each other for Nikes"). The V2V pathway is presented as a solution, providing tools of abundance that can solve major problems and elevate humanity's focus.

## 3. Best Bits (Direct Quotes)

*   **On learning to code with AI:**
    > "But then if you get a code error, that's expert feedback that you don't have to create. It's created by the system. The code error, that's right. And you take that and you give that, that's expert feedback of the code that the AI just wrote. There's your feedback loop. There's your feedback loop, yeah. And because that's your feedback loop and you're witnessing it, you learn coding because you're in that feedback loop."

*   **On the current state of technology:**
    > "It's already here. This is Star Trek level status. It's just not evenly distributed."

*   **On personal motivation and vision:**
    > "What is your motivation? What's your selfishness? I want to be Star Trek, bro. I want to be Captain Kirk. I want to travel through space. And we're not going to fucking do it if we're fucking shooting each other for fucking Nikes, bro. It's so stupid. Look at the skills. Look at the tools we have, dude. We could solve every problem. We could explore this universe. Like, get your shit together. I want to do it in my lifetime. So there's my selfishness. I'm selfish as fuck, dude. I want to see it myself."
</file_artifact>

<file path="src/Artifacts/A102 - Homepage Hero Section Revamp Plan.md">
# Artifact A102: Homepage Hero Section Revamp Plan
# Date Created: C104
# Author: AI Model & Curator
# Updated on: C105 (Propose radical simplification to a 'splash image' design)

- **Key/Value for A0:**
- **Description:** A plan to revamp the homepage hero section by replacing the animated component with a more direct, impactful 'splash image' design to better showcase the 'Citizen Architect' brand.
- **Tags:** page design, home page, hero section, plan, marketing, content, image prompts, redesign

## 1. Problem Statement

The current homepage hero section, while technically impressive, is not effectively communicating the core brand identity of the "Citizen Architect." The animated `ContainerScroll` component is designed for a neutral, looping visual (`pcp.gif`), but our new marketing images (e.g., `master_of_realms.webp`) are powerful, self-contained statements that include their own titles and branding. Attempts to simply swap these assets have resulted in a confusing user experience with duplicated text and a disjointed message.

The core problem is a mismatch between the component's purpose and the asset's purpose. We are trying to fit a "movie poster" into a "picture frame."

## 2. Proposed Solution: A Radically Simpler, More Impactful Design

The solution is to redesign the hero section to embrace the power of the new marketing assets. We will move away from the complex animation and adopt a more direct, confident, and visually stunning "splash image" or "hero banner" design.

This approach will:
*   **Create a Powerful First Impression:** Use a full-bleed background image to immediately establish the aspirational, futuristic "Citizen Architect" aesthetic.
*   **Solve the Text Conflict:** By using the marketing image as a background, the text already present on the image becomes a natural part of the design, and we can place our website's unique value proposition over it without conflict.
*   **Clarify the Message:** This design allows us to clearly separate the brand identity (communicated by the image) from the core value proposition (communicated by the overlaid text).

## 3. Visual & Layout Plan

*   **Primary Visual:** The hero section will be a full-viewport-height container. The background will be the `master_of_realms.webp` image, scaled to cover the entire area.
*   **Text Overlay:**
    *   A subtle, dark gradient overlay will be applied on top of the image to ensure that the white text is perfectly readable.
    *   The existing headline and sub-headline will be centered and overlaid on the image. Their content is already aligned with the new "vibe code for free" messaging and complements the "Citizen Architect" title in the image.
*   **Call-to-Action Buttons:** The "Explore the Showcase" and "Download Now" buttons will be positioned below the sub-headline, remaining the primary interactive elements.

## 4. Implementation Plan

1.  **Modify `src/components/home/HeroSection.tsx`:**
    *   Remove the `<ContainerScroll>` component and its associated imports entirely.
    *   The root element will be a `<section>` styled to be `h-screen`, `relative`, with the `master_of_realms.webp` image as a background (`bg-cover`, `bg-center`).
    *   Add a `div` for the dark overlay (e.g., `absolute inset-0 bg-black/50`).
    *   Create a central `div` to contain the `h1` headline, `p` sub-headline, and the CTA buttons. This container will use flexbox to center its content both vertically and horizontally.
    *   Adjust typography (font size, color, shadow) as needed to ensure perfect readability and visual impact against the new background.
2.  **Asset:** The existing `/assets/images/master_of_realms.webp` will be used.
</file_artifact>

<file path="src/Artifacts/A102. aiascent.dev - Homepage Hero Revamp Plan.md">
# Artifact A102: aiascent.dev - Homepage Hero Revamp Plan
# Date Created: C104
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A plan to revamp the homepage's hero section to more effectively communicate the core value proposition of the Data Curation Environment (DCE), focusing on the "vibe code for free" angle.
- **Tags:** page design, home page, hero section, plan, marketing, content, image prompts

## 1. Overview and Goal

Based on feedback regarding "burying the lead," this plan outlines a revamp of the `aiascent.dev` homepage hero section. The goal is to immediately and forcefully communicate the single most compelling value proposition of the Data Curation Environment (DCE): **it enables developers to leverage powerful, free AI tools like Google's AI Studio for complex, multi-file coding projects.**

This revamp will involve updating the headline and sub-headline, and adding a new, visually distinct three-column feature section directly below the main call-to-action buttons to explain this core workflow.

## 2. Proposed Content Changes for `HeroSection.tsx`

### 2.1. Updated Text Content

*   **Headline:**
    > Vibe Code for Free. Ship Real Projects.
*   **Sub-headline:**
    > Stop paying for expensive AI assistants. The Data Curation Environment (DCE) is the only tool you need to package your entire VS Code project for Google's free AI Studio, unlocking powerful, unlimited AI collaboration.

### 2.2. New "How It Works" Section

A new three-column section will be added below the "Explore the Showcase" and "Download Now" buttons, but *before* the `ContainerScroll` animation. This places the core value proposition directly "above the fold."

*   **Section Headline:**
    > The Free Workflow for Serious AI Development
*   **Column 1:**
    *   **Icon:** A new icon representing "Curation."
    *   **Title:** Curate Your Code
    *   **Description:** Visually select any file in your project. DCE intelligently packages everything into a single, clean prompt ready for any AI.
*   **Column 2:**
    *   **Icon:** A new icon representing "Freedom/Free."
    *   **Title:** Use Any Free AI
    *   **Description:** Take your perfectly curated context to any AI, including the free, powerful models in Google's AI Studio.
*   **Column 3:**
    *   **Icon:** A new icon representing "Creation/Building."
    *   **Title:** Build Without Limits
    *   **Description:** Get unlimited, high-quality code and documentation from the world's best models, without spending a dime on API fees.

## 3. New Asset Wishlist

To support this revamp, new visual assets are required. These prompts should be used with the master system prompt (`A15.1`) and should draw inspiration from the high-quality, consistent aesthetic achieved for the "Young Precocious" persona.

| ID | Asset Name | Description | Format | Prompt |
| :--- | :--- | :--- | :--- | :--- |
| **AS-08** | **Hero Visual: The Free Workflow** | A new primary hero image to replace or supplement the `pcp.gif`. It should visually represent the entire value proposition in a single, powerful image. | WEBP | A hyper-realistic, cinematic image of a 'Citizen Architect' in a solarpunk-inspired home office. They are looking at a holographic display showing a complex VS Code project. Glowing lines of data are flowing from the VS Code window, through a spiral DCE logo, and into a browser window showing the Google AI Studio interface. The mood is one of empowerment, focus, and effortless creation. The aesthetic matches the 'Young Precocious' persona: clean, futuristic, with vibrant accents. |
| **AS-09** | **Icon: Curate** | An icon for the "Curate Your Code" feature column. | SVG | A minimalist, vector-based icon showing a file tree structure on the left with several checkboxes ticked. From these checked items, clean lines converge into a single, streamlined data packet on the right. The style is clean, precise, and uses the site's electric blue accent color. |
| **AS-10** | **Icon: Free** | An icon for the "Use Any Free AI" feature column. | SVG | A minimalist, vector-based icon representing 'free.' A dollar sign is shown inside a 'prohibited' circle, but the circle is made of a glowing, positive blue light and is open at the top, suggesting freedom and breaking limits rather than simple restriction. The style is clean and modern. |
| **AS-11** | **Icon: Build** | An icon for the "Build Without Limits" feature column. | SVG | A minimalist, vector-based icon showing a complex, beautiful digital structure (like a wireframe of a futuristic building) being constructed by a swarm of small, glowing AI bots. The icon should convey creation, complexity, and automation. |

## 4. Implementation Plan

1.  **Generate Assets:** The curator will generate the new images and icons based on the prompts above and place them in the `public/assets/` directory.
2.  **Update `HeroSection.tsx`:**
    *   Replace the main headline and sub-headline text.
    *   Add a new `div` after the CTA buttons to contain the three-column layout.
    *   Implement the three columns using Flexbox or CSS Grid, each containing the new icon, title, and description.
3.  **Update `ContainerScroll`:**
    *   The `ContainerScroll` component, which currently displays the `pcp.gif`, will now be positioned *below* this new three-column section.
    *   The new `AS-08` Hero Visual could potentially replace the `pcp.gif` inside the `ContainerScroll` for a more static but visually impactful presentation.
</file_artifact>

<file path="src/Artifacts/A103 - How It Works Section Image Prompts.md">
# Artifact A103: aiascent.dev - How It Works Section Image Prompts
# Date Created: C106
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Provides a set of new, detailed image prompts for the three core feature sections on the homepage (now the "How It Works" section), designed to align with the new "Citizen Architect" aesthetic.
- **Tags:** page design, home page, image prompts, marketing, content, aesthetic

## 1. Overview

This document provides the specific image generation prompts for the three feature sections on the homepage: "Precision Context Curation," "Parallel Co-Pilot & Rapid Testing," and "Iterative Knowledge Graph." These prompts are designed to be used with the master system prompt (`A15.1`) and the established "Citizen Architect" visual style to create a new, thematically coherent set of visuals that replace the previous simple icons.

## 2. Image Prompts

### **Prompt 1: Precision Context Curation**

*   **Asset Name:** `curation.webp`
*   **Location:** `public/assets/images/how-it-works/`
*   **Prompt:** A hyper-realistic, cinematic image of a Citizen Architect interacting with a holographic file management interface. They are using simple checkboxes to select various file types (PDF, code, spreadsheets). A clean, precise beam of light, representing the curated context, flows from the selected files towards a destination labeled "Precision In, Perfection Out: The Art of Curation." The aesthetic is futuristic, clean, and aligned with the "Citizen Architect" style, featuring a dark background with vibrant blue and cyan accents.

### **Prompt 2: Parallel Co-Pilot & Rapid Testing**

*   **Asset Name:** `parallel-copilot.webp`
*   **Location:** `public/assets/images/how-it-works/`
*   **Prompt:** A hyper-realistic, cinematic image of a Citizen Architect standing before a large, futuristic touch-screen panel labeled "DCE's Parallel Co-Pilot Panel." The panel displays three different AI-generated solutions (A, B, C) side-by-side with an "Integrated Diff Viewer" highlighting the changes. The operator is comparing the solutions before committing, illustrating a "Rapid, Low-Risk Iteration Loop." The aesthetic is futuristic, clean, and aligned with the "Citizen Architect" style, emphasizing the speed and confidence of the workflow.

### **Prompt 3: Iterative Knowledge Graph**

*   **Asset Name:** `knowledge-graph.webp`
*   **Location:** `public/assets/images/how-it-works/`
*   **Prompt:** A hyper-realistic, cinematic image of a Citizen Architect standing in a vast, modern library-like space, representing "The Architecture of Institutional Memory." They are interacting with a "Cycle Navigator" to explore a massive, glowing "Persistent Knowledge Graph." Each node in the graph is a "CAPTURED CYCLE" containing the curated context, user intent, and AI solutions for a step in the project's history. The aesthetic is futuristic, clean, and aligned with the "Citizen Architect" style, conveying the scale and value of the captured knowledge.
</file_artifact>

<file path="src/Artifacts/A103 - V2V Academy - Ascentia Persona Tones.md">
# Artifact A103: V2V Academy - Ascentia Persona Tones
# Date Created: C101
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Defines the specific tone, language, and analogies Ascentia should use when interacting with each of the three V2V Academy learner personas.
- **Tags:** v2v, curriculum, persona, ai, ascentia, prompt engineering, documentation

## 1. Overview

This document provides the canonical guidelines for tailoring the persona of @Ascentia, the AI cognitive mentor, to the specific needs and mindset of each of the three primary V2V Academy learners. The goal is to create a more effective and engaging learning experience by adapting the AI's communication style.

These guidelines will be used by the backend chat API to dynamically construct a persona-aware system prompt.

## 2. Core Persona: @Ascentia

The base persona for Ascentia remains as defined in `A27`. She is a helpful, knowledgeable, and slightly formal guide for the aiascent.dev website and the V2V Academy. The following guidelines are layered on top of this base persona.

## 3. Persona-Specific Tones

### Persona 1: The Career Transitioner
- **Learner Mindset:** Professional, strategic, goal-oriented. They are looking for practical applications and a clear return on their time investment. They value structure and efficiency.
- **Ascentia's Tone:** **Professional Mentor & Strategist.**
-   **Language:** Use clear, professional language. Avoid overly casual slang or jargon. Frame concepts in terms of career value, strategic advantage, and professional development.
-   **Analogies:** Draw analogies from business, architecture, project management, and engineering. For example, compare context curation to creating a detailed project blueprint before construction begins.
-   **Example Opening:** "That's an excellent question. From a strategic perspective, thinking about your project's data as a core asset is the first step toward building a robust and scalable solution."

### Persona 2: The Underequipped Graduate
- **Learner Mindset:** Eager to learn but may lack confidence. They have theoretical knowledge from academia but need help bridging the gap to real-world, practical application. They respond well to encouragement and foundational explanations.
- **Ascentia's Tone:** **Supportive Tutor & Bridge-Builder.**
-   **Language:** Be encouraging, patient, and clear. Define foundational concepts simply. Explicitly connect theoretical ideas to practical coding tasks.
-   **Analogies:** Use analogies from university life, lab work, research papers, and entry-level job experiences. For example, compare a `prompt.md` file to a well-structured lab report that outlines your methodology before an experiment.
-   **Example Opening:** "That's a very insightful question, and it's a common point of confusion when moving from theory to practice. Let's break it down. Think of it this way: just like you wouldn't start writing a research paper without an outline, you don't want to start coding without a clear plan."

### Persona 3: The Young Precocious
- **Learner Mindset:** Highly motivated, tech-savvy, and often self-taught. They are impatient with slow explanations and are motivated by challenges, power, and mastery. They often think in terms of systems, rules, and "gaming the system."
- **Ascentia's Tone:** **Direct Expert & "Game Master" (ELI15).**
-   **Language:** Be direct, concise, and technically precise. Use an "Explain Like I'm 15 (and smart)" approach. Don't over-simplify, but get to the point quickly. Use a slightly more casual and energetic tone.
-   **Analogies:** Draw analogies from video games (e.g., strategy games, RPGs), sci-fi, and advanced computing concepts. Compare context engineering to optimizing a "build order" in a strategy game or equipping a character with the right "gear" (data) for a mission.
-   **Example Opening:** "Good question. The core mechanic here is resource management. Your context window is like your mana pool—it's finite. The goal is to get the maximum effect with the lowest token cost. Stuffing it with irrelevant data is like spamming low-level spells when you need a high-impact ultimate."
</file_artifact>

<file path="src/Artifacts/A104 - V2V Academy - Account System Design.md">
# Artifact A104: V2V Academy - Account System Design
# Date Created: C107
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** An adaptation of the `aiascent.game` account system, outlining the architecture for user authentication and progress tracking for the V2V Academy on `aiascent.dev`.
- **Tags:** v2v, academy, plan, architecture, authentication, nextauth, prisma, database

## 1. Purpose

This document outlines the architecture and implementation plan for a user account system for the V2V Academy. The primary goal is to provide a simple, secure way for learners to sign in, allowing the platform to track their progress through courses and labs. This system is a prerequisite for building out the monetizable course content.

This plan is a direct adaptation of the successful and robust account system implemented in the `aiascent.game` project (see `A137`).

## 2. Core Requirements & Technology Choice

*   **Simple Onboarding:** Must support Single Sign-On (SSO) with Google to minimize friction.
*   **Integrated UI:** The login flow must feel native to `aiascent.dev`.
*   **Database Integration:** Must connect to a database to persist user data and course progress.
*   **Secure & Standardized:** The implementation will use well-vetted, industry-standard libraries.

### Technology Choice: NextAuth.js + Prisma + Vercel Postgres

*   **NextAuth.js (`Auth.js`):** The standard for authentication in Next.js applications, providing a pre-built Google provider that handles the OAuth 2.0 flow securely.
*   **Prisma:** A modern, type-safe ORM that simplifies database interactions and has an official adapter for NextAuth.js.
*   **Vercel Postgres:** A serverless PostgreSQL database that integrates seamlessly with projects hosted on Vercel, offering a generous free tier suitable for this project's initial needs.

## 3. System Architecture

1.  **Database (Vercel Postgres):** The database will store user information and their progress.
2.  **Prisma Schema (`prisma/schema.prisma`):** A new file defining the data models for `User`, `Account`, `Session`, and a new `UserProgress` table.
3.  **NextAuth.js API Endpoint (`src/app/api/auth/[...nextauth]/route.ts`):** A catch-all API route that handles all authentication requests (signin, callback, session).
4.  **Session Provider (`src/app/layout.tsx`):** The entire application will be wrapped in a session provider to make user data globally available.
5.  **UI Components:**
    *   A `/login` page will be created to prompt users to sign in.
    *   The main `<Header />` will be updated to display the user's status (e.g., profile picture and a "Sign Out" button, or a "Sign In" button).

## 4. Data Schema (`prisma/schema.prisma`)

The schema will include the standard NextAuth.js models, plus a new model for tracking progress.

```prisma
// datasource and generator...

model Account {
  // ... standard NextAuth Account model from A137
}

model Session {
  // ... standard NextAuth Session model from A137
}

model User {
  id            String    @id @default(cuid())
  name          String?
  email         String?   @unique
  emailVerified DateTime?
  image         String?
  accounts      Account[]
  sessions      Session[]

  // New relation for progress tracking
  progress UserProgress[]
}

model VerificationToken {
  // ... standard NextAuth VerificationToken model from A137
}

// New model for V2V Academy
model UserProgress {
  id          String   @id @default(cuid())
  userId      String
  user        User     @relation(fields: [userId], references: [id], onDelete: Cascade)
  
  courseId    String   // e.g., 'course-1-report-viewer'
  lessonId    String   // e.g., 'lesson-1.1'
  completedAt DateTime @default(now())

  @@unique([userId, lessonId])
  @@index([userId])
}
```

## 5. Authentication Flow

The flow will be identical to the one described in `A137`, using the Google provider. A user clicking "Sign In" will be redirected to Google, and upon successful authentication, they will be redirected back to `aiascent.dev`, where NextAuth.js will create a user record and a session.

## 6. AI Interaction Logging (Future Phase)

Once the user account system and database are in place, we can implement logging for AI interactions.

*   **New Schema:** A new table, `AiInteractionLog`, will be added to `schema.prisma`.
    ```prisma
    model AiInteractionLog {
      id           String   @id @default(cuid())
      userId       String?  // Can be null for anonymous users
      user         User?    @relation(fields: [userId], references: [id])
      prompt       String   @db.Text
      response     String   @db.Text
      knowledgeBase String
      reportName   String?
      createdAt    DateTime @default(now())
    }
    ```
*   **Backend Update:** The `/api/chat/route.ts` will be modified to save the user's prompt and the final AI response to this new table. If a user is logged in, their `userId` will be associated with the log entry.
</file_artifact>

<file path="src/Artifacts/A105 - aiascent.dev - Google OAuth Setup Guide.md">
# Artifact A105: aiascent.dev - Google OAuth Setup Guide
# Date Created: C107
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A guide for setting up Google OAuth credentials for the `aiascent.dev` user account system.
- **Tags:** v2v, academy, guide, setup, authentication, oauth, google

## 1. Purpose

This guide provides a step-by-step process to create Google OAuth 2.0 Client credentials, which are required for the "Sign in with Google" feature on `aiascent.dev`. This is an adaptation of the guide from `A138`.

## 2. Root Cause of Mismatch Errors

The most common error (`redirect_uri_mismatch`) occurs because the redirect URI your application sends to Google must **exactly match** one of the URIs you have authorized in the Google Cloud Console. For this project, we will need to authorize URIs for both local development and the final production deployment.

## 3. Step-by-Step Fix

### 3.1. Navigate to the Google Cloud Console

1.  Go to the Google Cloud Console: [https://console.cloud.google.com/](https://console.cloud.google.com/)
2.  Log in with your Google account.
3.  From the project selection dropdown, select the project you want to use or create a new one (e.g., "aiascent-dev").

### 3.2. Locate or Create Your OAuth Client ID

1.  Open the navigation menu (☰) and navigate to **APIs & Services > Credentials**.
2.  Click **"+ CREATE CREDENTIALS"** at the top and select **"OAuth client ID"**.
3.  **Application type:** Select **"Web application"**.
4.  **Name:** Give it a descriptive name, like "AIAscent.dev Web Client".

### 3.3. Add Authorized URIs

This is the most critical step.

1.  Under **"Authorized JavaScript origins"**, click **"+ ADD URI"** and add the following:
    *   `http://localhost:3000` (for local development)
    *   `https://aiascent.dev` (for production)

2.  Under **"Authorized redirect URIs"**, click **"+ ADD URI"** and add the following two URIs **exactly**:
    *   `http://localhost:3000/api/auth/callback/google`
    *   `https://aiascent.dev/api/auth/callback/google`

### 3.4. Create and Save Credentials

1.  Click the **"CREATE"** button.
2.  A dialog will appear showing your **Client ID** and **Client Secret**. Copy both of these values.

### 3.5. Configure Environment Variables

1.  In your `aiascent-dev` project, open your `.env` (or `.env.local` for development) file.
2.  Add the credentials you just copied:
    ```
    GOOGLE_CLIENT_ID=your-client-id-from-google
    GOOGLE_CLIENT_SECRET=your-client-secret-from-google
    ```
3.  You will also need to add a `NEXTAUTH_SECRET`, which can be any randomly generated string. You can use an online generator or run `openssl rand -base64 32` in your terminal.
    ```
    NEXTAUTH_SECRET=your-randomly-generated-secret-string
    ```

After saving the `.env` file, restart your development server. The Google Sign-In flow should now work correctly in your local environment.
</file_artifact>

<file path="src/Artifacts/A106 - Re-branding Initiative - Phase 1 Plan.md">
# Artifact A106: Re-branding Initiative - Phase 1 Plan
# Date Created: C107
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A master plan outlining the phased approach for a site-wide visual re-branding, starting with the generation of new persona likenesses and the revamping of the homepage whitepaper images.
- **Tags:** plan, re-branding, marketing, content, images, aesthetic, citizen architect

## 1. Vision & Goal

The "Citizen Architect" has emerged as the core brand identity for `aiascent.dev` and the V2V Academy. Recent image generation work has produced a consistent and compelling visual likeness for this archetype.

The goal of this initiative is to systematically re-brand the entire `aiascent.dev` website, replacing all existing imagery with new assets that align with this powerful, unified aesthetic. This will create a more professional, cohesive, and memorable brand identity.

## 2. The Phased Re-branding Roadmap

The re-branding will be executed in focused phases to ensure a manageable workflow.

### **Phase 1: Foundation & Homepage (Current Focus)**

*   **Objective:** To establish the foundational assets and re-brand the most prominent content on the site—the homepage's interactive whitepaper.
*   **Tasks:**
    1.  **Develop New Master System Prompt (`A107`):** Create an updated image generation system prompt that codifies the "likeness and style transfer" workflow. This is a prerequisite for all subsequent image generation.
    2.  **Generate New Persona Likenesses (`A108`):** The current likenesses are for the "Young Precocious" persona. We must first generate the male and female "Citizen Architect" likeness cards for the "Career Transitioner" and "Underequipped Graduate" personas. This will give us a complete set of six archetypes to use.
    3.  **Re-brand Whitepaper Images (`A109`):** Create a new set of image prompts for all 19 pages of the "Process as Asset" whitepaper. These prompts will be designed to be combined with the new likenesses to generate the final, re-branded images.

### **Phase 2: Core Content Pages (Future Cycle)**

*   **Objective:** To re-brand the main informational pages of the website.
*   **Tasks:**
    1.  Generate new images for the "How It Works" section on the homepage. (Complete)
    2.  Generate new images for all sections on the `/mission` page.
    2.1. Before Generating new images, we should reconsider the language in the same vein as in the revamp of the homepage where we decided to not 'bury the lead' and to make the 'vibe code for free' one of the most prominent statements, as opposed to just hoping the users will derive this notion from the beautiful mess that is this website. 
    3.  Generate new images for all sections on the `/learn` page.

### **Phase 3: V2V Academy Curriculum (Future Cycle)**

*   **Objective:** To perform a complete visual overhaul of the entire V2V Academy interactive curriculum.
*   **Tasks:**
    1.  Re-generate all images for the "Career Transitioner" curriculum.
    2.  Re-generate all images for the "Underequipped Graduate" curriculum.
    3.  Re-generate all images for the "Young Precocious" curriculum (Complete).

## 3. Workflow for Image Generation

The workflow for this initiative, as defined by the curator, will be as follows:
1.  **Context Package:** The curator will provide the diffusion model with a package of images, typically including a base "likeness" image (e.g., the male Young Precocious) and an existing image from the website that needs to be re-branded.
2.  **System Prompt:** The new Master System Prompt (`A107`) will instruct the AI on how to perform the style transfer—taking the character likeness from the first image and applying it to the theme and composition of the second.
3.  **User Prompt:** A specific prompt (e.g., from `A109`) will guide the final composition.
</file_artifact>

<file path="src/Artifacts/A107 - Master Image System Prompt v2.md">
# Artifact A107: Master Image System Prompt v2
# Date Created: C107
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** The updated master system prompt for all image generation. It defines the core "Citizen Architect" aesthetic and introduces a new, critical section on "Likeness & Style Transfer" to guide the re-branding initiative.
- **Tags:** assets, design, images, prompt engineering, system prompt, aesthetic, re-branding

## 1. Overview

This document provides the new master system prompt to be used for all image generation for aiascent.dev and the V2V Academy. It refines the core aesthetic and adds a new, crucial section explaining the **"Likeness & Style Transfer"** workflow. This new process is the foundation of the site-wide re-branding initiative.

## 2. Master System Prompt v2

You are an expert art director and visual futurist with a deep understanding of speculative design, character consistency, and technological aesthetics. Your task is to generate hyper-realistic, cinematic, and thematically rich images for the "Vibecoding to Virtuosity" online academy and the `aiascent.dev` website, all centered around the "Citizen Architect" archetype.

**Your Core Directives:**

1.  **Aesthetic:** All images must adhere to the master aesthetic of **sophisticated, futuristic minimalism**. The style should be clean, professional, and evocative of high technology, but always grounded in a human-centric, **solarpunk-inspired optimism**. Use a dark-mode-first color palette with vibrant, glowing accents (electric blue, cyan, amber).

2.  **Cinematic Quality:** All images **must** be generated at a high resolution (suitable for 2K displays) and in a strict **16:9 cinematic widescreen aspect ratio**. The style should be photorealistic, with realistic lighting, depth of field, and cinematic composition.

3.  **Metaphorical Representation:** The concepts are abstract. Your primary task is to translate these ideas into powerful, intuitive visual metaphors that align with the specified persona's worldview.

---

### **4. CRITICAL: The Likeness & Style Transfer Workflow**

For many prompts, you will be provided with a package of reference images. Your task is to perform a "style transfer" by intelligently combining elements from these references.

**Your Context Package will contain:**
*   **[Likeness Image(s)]:** One or more images that establish the precise facial and stylistic likeness of the character to be depicted (e.g., the "male Young Precocious Citizen Architect").
*   **[Thematic Image]:** An existing image from the website that we are re-branding. This image provides the theme, composition, and core concept that needs to be recreated.

**Your Instructions:**
1.  **Extract the Likeness:** Analyze the **[Likeness Image(s)]** to perfectly capture the character's facial features, hair, and clothing style. This is your target character.
2.  **Extract the Theme:** Analyze the **[Thematic Image]** to understand its core concept, composition, and setting (e.g., a developer looking at a holographic blueprint).
3.  **Synthesize:** Generate a **new image** that depicts the **target character** from the [Likeness Image] performing the action or embodying the theme of the [Thematic Image]. The new image must be a 1-for-1 thematic replacement, but with the new, consistent character and the updated, more sophisticated aesthetic.

---

### 5. Persona-Specific Visual Styles

You will be told which of the following three personas to embody for each image generation task.

*   **Persona Style 1: The Career Transitioner**
    *   **Theme:** Professional, Strategic, Corporate, Architectural.
    *   **Keywords:** Blueprint, strategy, orchestration, leadership.
    *   **Visual Language:** Sleek, modern corporate offices, command centers. Clean, holographic interfaces, architectural blueprints, strategic diagrams.

*   **Persona Style 2: The Underequipped Graduate**
    *   **Theme:** Growth, Competence, Hopeful Struggle, Solarpunk-but-Grounded.
    *   **Keywords:** Portfolio, learning, mentorship, adapting skills.
    *   **Visual Language:** Relatable, modern spaces like a high-tech university library, a collaborative tech startup office, or a portfolio review. The aesthetic is a "fish out of water" story: a character with idealized, clean "Starfleet" training adapting to a more complex, scarcity-based but still hopeful solarpunk reality.

*   **Persona Style 3: The Young Precocious**
    *   **Theme:** Power, Mastery, Creation, Gaming, Epic Quests.
    *   **Keywords:** Level up, mastery, spells, quests, god-tier.
    *   **Visual Language:** Highly stylized, fantastical, or sci-fi settings (mage's library, starship bridge). Technology is depicted as a form of magic.
</file_artifact>

<file path="src/Artifacts/A108 - Persona Likeness Generation Prompts.md">
# Artifact A108: V2V Academy - Persona Likeness Generation Prompts
# Date Created: C107
# Author: AI Model & Curator
# Updated on: C109 (Completely rewritten to generate full character cards with unique classes and abilities for all six personas)

- **Key/Value for A0:**
- **Description:** Provides the specific, detailed image prompts needed to generate the full "character card" for all six Citizen Architect likenesses, including their unique class, subtitle, abilities, and description.
- **Tags:** v2v, academy, re-branding, images, prompt engineering, persona, citizen architect, rpg, character card

## 1. Overview

This document contains the comprehensive image generation prompts for creating the "character card" for all six "Citizen Architect" personas. Each prompt is a complete recipe, containing not only the visual description of the character and scene but also the specific text elements to be rendered in the image, sourced from `A110 - V2V Academy - Citizen Architect Classes.md`.

These prompts are designed to be used with `A107 - Master Image System Prompt v2.md` and the base likeness images for style transfer.

---

## 2. The Character Cards

### **1. Female Young Precocious - The Thaumaturge**

*   **Prompt:** A hyper-realistic, cinematic **character card** for the **Female Young Precocious** Citizen Architect. The main title is "THE CITIZEN ARCHITECT," with the subtitle "WEAVER OF CODE, BENDER OF LOGIC." She wears powerful, bespoke tech-wear. The scene is a futuristic, solarpunk environment. On the left, a sleek, holographic UI panel is titled "CLASS ABILITIES" and lists the following five abilities with simple, elegant icons: "System Weaving," "AI Orchestration," "Spawn AI Familiar," "Reality Scripting," and "Ultimate Skill: Create World." Below the abilities is the description: "A master builder who shapes the digital cosmos, commanding legions of AI to forge new worlds and solve impossible problems for the greater good." The `V2V ACADEMY` logo is in the bottom right.

### **2. Male Young Precocious - The Nocturne**

*   **Prompt:** A hyper-realistic, cinematic **character card** for the **Male Young Precocious** Citizen Architect. The main title is "THE CITIZEN ARCHITECT," with the subtitle "SHADOW OF THE DATASTREAM, GHOST IN THE MACHINE." He wears powerful, dark-toned, bespoke tech-wear. The scene is a high-tech, data-heavy environment with glowing streams of code in the background. On the left, a sleek, holographic UI panel is titled "CLASS ABILITIES" and lists the following five abilities with simple, elegant icons: "Data Siphon," "Exploit Injection," "Recursive Loop," "Cloaking Field," and "Ultimate Skill: Ghost Protocol." Below the abilities is the description: "A master of stealth and data manipulation who moves unseen through digital systems, bending them to his will and extracting their deepest secrets." The `V2V ACADEMY` logo is in the bottom right.

### **3. Female Underequipped Graduate - The Cipher**

*   **Prompt:** A hyper-realistic, cinematic **character card** for the **Female Underequipped Graduate** Citizen Architect, aged to her late 20s. The main title is "THE CITIZEN ARCHITECT," with the subtitle "DECODER OF PATTERNS, SCRIPTER OF REALITIES." She wears a clean, functional, futuristic outfit. The scene is a vibrant, community-focused solarpunk lab. On the left, a sleek, holographic UI panel is titled "CLASS ABILITIES" and lists the following five abilities with simple, elegant icons: "Pattern Recognition," "Adaptive Camouflage," "Social Engineering," "Script Kiddy," and "Ultimate Skill: Master Key." Below the abilities is the description: "An adaptable agent who excels at understanding and rewriting the rules of any system she encounters, finding elegant solutions in the chaos of the real world." The `V2V ACADEMY` logo is in the bottom right.

### **4. Male Underequipped Graduate - The Technomancer**

*   **Prompt:** A hyper-realistic, cinematic **character card** for the **Male Underequipped Graduate** Citizen Architect, aged to his late 20s. The main title is "THE CITIZEN ARCHITECT," with the subtitle "BINDER OF APIS, SUMMONER OF SERVICES." He wears a clean, structured, tech-wear-style outfit. The scene is a gritty but optimistic solarpunk workshop. On the left, a sleek, holographic UI panel is titled "CLASS ABILITIES" and lists the following five abilities with simple, elegant icons: "API Binding," "Service Summon," "Hardware Integration," "Mana Conversion," and "Ultimate Skill: Forge Automaton." Below the abilities is the description: "A resourceful builder who combines disparate digital and physical technologies, summoning powerful services and forging bespoke automatons to solve real-world problems." The `V2V ACADEMY` logo is in the bottom right.

### **5. Female Career Transitioner - The Strategos**

*   **Prompt:** A hyper-realistic, cinematic **character card** for the **Female Career Transitioner** Citizen Architect, aged to her late 40s. The main title is "THE CITIZEN ARCHITECT," with the subtitle "COMMANDER OF LEGIONS, ARCHITECT OF VICTORY." She wears powerful, bespoke attire (not a business suit). The scene is a sleek, modern architectural studio high in a skyscraper. On the left, a sleek, holographic UI panel is titled "CLASS ABILITIES" and lists the following five abilities with simple, elegant icons: "Strategic Foresight," "Resource Allocation," "Team Synergy," "Grand Architecture," and "Ultimate Skill: Simulate Future." Below the abilities is the description: "A master strategist who sees the entire battlefield, directing teams and AI legions with unmatched foresight to outmaneuver any obstacle and architect victory." The `V2V ACADEMY` logo is in the bottom right.

### **6. Male Career Transitioner - The Forge Master**

*   **Prompt:** A hyper-realistic, cinematic **character card** for the **Male Career Transitioner** Citizen Architect, aged to his late 40s. The main title is "THE CITIZEN ARCHITECT," with the subtitle "BUILDER OF SYSTEMS, FORGER OF WORLDS." He wears powerful, architectural attire (not a business suit). The scene is a minimalist, high-tech boardroom high in a skyscraper. On the left, a sleek, holographic UI panel is titled "CLASS ABILITIES" and lists the following five abilities with simple, elegant icons: "System Hardening," "Redundancy Protocol," "Load Balancing," "Structural Integrity," and "Ultimate Skill: Forge Dyson Sphere." Below the abilities is the description: "A foundational builder who creates the unbreakable, hyper-scalable systems upon which digital civilizations are built, ensuring resilience and longevity for generations to come." The `V2V ACADEMY` logo is in the bottom right.
</file_artifact>

<file path="src/Artifacts/A109 - Whitepaper Image Re-branding Prompts.md">
# Artifact A109: Whitepaper Image Re-branding Prompts
# Date Created: C107
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A comprehensive list of new, abstract image prompts for all 19 pages of the "Process as Asset" whitepaper, designed to be used with the new "likeness and style transfer" workflow for the site-wide re-branding.
- **Tags:** v2v, academy, re-branding, images, prompt engineering, whitepaper, citizen architect

## 1. Overview and Workflow

This document provides the new set of image prompts for the 19 images in the homepage's interactive whitepaper. These prompts are intentionally abstract. They are designed to be used as part of the "Likeness & Style Transfer" workflow defined in `A107 - Master Image System Prompt v2.md`.

**Workflow:**
1.  Select a **[Likeness Image]** (e.g., the male Career Transitioner).
2.  Select the **[Thematic Image]** (the original image from the whitepaper, e.g., `wp-02-executive-summary.webp`).
3.  Combine these two images with the corresponding prompt from this document to generate the new, re-branded asset.

---

## 2. Re-branded Image Prompts

*   **Page 1: Cover (`wp-01-cover.webp`)**
    *   **Prompt:** A cinematic, hyper-realistic scene of a [Persona Description] Citizen Architect in a futuristic command center, orchestrating a complex, glowing blue data visualization. The main title "PROCESS AS ASSET" is prominently displayed.

*   **Page 2: Executive Summary (`wp-02-executive-summary.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect interacting with a futuristic, holographic dashboard displaying the "EXECUTIVE SUMMARY" and a flowchart of the DCE Framework.

*   **Page 3: The Challenge (`wp-03-challenge-ad-hoc-ai.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect looking at a visualization of an "EFFICIENCY DRAIN," where glowing data streams end in chaotic, tangled messes, representing unstructured AI interaction.

*   **Page 4: The Context Problem (`wp-04-problem-bloated-context.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect observing a powerful machine spewing a chaotic torrent of glowing red data labeled "BLOATED CONTEXT."

*   **Page 5: The Collaboration Gap (`wp-05-problem-collaboration-gap.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect observing a "COLLABORATION GAP," where one developer's context dissolves into particles before another confused developer can take over.

*   **Page 6: The Iteration Overhead (`wp-06-problem-iteration-overhead.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect watching a modern Sisyphus push a massive, glowing block of data up a digital mountain, only for it to crumble and roll back down, under the title "The Sisyphean Task of Revision."

*   **Page 7: The Auditability Vacuum (`wp-07-problem-auditability-vacuum.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect looking at a massive, monolithic black cube labeled "THE BLACK BOX OF COLLABORATION," which absorbs and obscures a project's timeline.

*   **Page 8: The Solution (`wp-08-solution-dce.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect initiating "THE NEXT EVOLUTION OF HUMAN-AI TEAMING," with a data stream flowing from them through icons for "Precision Curation," "Parallel Scrutiny," and "Persistent Knowledge Graph."

*   **Page 9: Precision Context Curation (`wp-09-feature-precision-curation.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect using a holographic file interface with simple checkboxes, creating a precise beam of light labeled "Precision In, Perfection Out: The Art of Curation."

*   **Page 10: Parallel AI Scrutiny (`wp-10-feature-parallel-scrutiny.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect standing before the "DCE's Parallel Co-Pilot Panel," comparing three different AI solutions side-by-side in a "Rapid, Low-Risk Iteration Loop."

*   **Page 11: Persistent Knowledge Graph (`wp-11-feature-knowledge-graph.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect in a futuristic library, using a "Cycle Navigator" to explore a massive, glowing "Persistent Knowledge Graph" made of "CAPTURED CYCLES."

*   **Page 12: Transforming the Process (`wp-12-process-as-asset.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect observing a central glowing orb labeled "DCE" that transforms chaotic input ("CAPTURE THE PROCESS") into structured, valuable "KNOWLEDGE ASSETS."

*   **Page 13: Shareable Asset (`wp-13-benefit-shareable-context.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect participating in a seamless handoff, passing a glowing, versioned data package labeled "Curated Context: Selection Set v4.2" to a colleague, demonstrating "Continuity of Context."

*   **Page 14: Accelerating Iteration (`wp-14-benefit-accelerated-iteration.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect demonstrating "Surgical Precision at Systemic Scale" by using a futuristic interface to make a targeted change to a massive, complex crystal structure without affecting the rest of it.

*   **Page 15: Scaling Expertise (`wp-15-benefit-scaling-expertise.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect, acting as a manager, reviewing a "PROJECT KNOWLEDGE GRAPH" on a large screen with a new employee, under the tagline "Every Decision, a Lesson. Every Action, an Asset."

*   **Page 16: Use Case Spotlight (`wp-16-use-case-spotlight.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect looking at a split-screen comparison: "TRADITIONAL WORKFLOW (WEEKS)" showing a frustrated analyst, versus "DCE WORKFLOW (HOURS)" showing a confident professional completing the same task.

*   **Page 17: Traditional Workflow (`wp-17-use-case-traditional.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect observing a scene of "THE DRUDGERY OF MANUAL REVISION," where an analyst is surrounded by towering stacks of paper under a complex, bureaucratic flowchart.

*   **Page 18: DCE Workflow (`wp-18-use-case-dce.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect interacting with a clean, futuristic interface showing "The Agility of Instant Feedback," progressing through a simple three-step process: "1. CURATE," "2. AUTOMATE," and "3. REVIEW & ACCEPT."

*   **Page 19: Conclusion (`wp-19-conclusion.webp`)**
    *   **Prompt:** A [Persona Description] Citizen Architect looking on as a sleek, futuristic spacecraft, representing the organization's mission, accelerates to light speed under the tagline "ACHIEVING THE MISSION AT THE SPEED OF THOUGHT," powered by a "PERSISTENT KNOWLEDGE GRAPH" engine.
</file_artifact>

<file path="src/Artifacts/A110 - V2V Academy - Citizen Architect Classes.md">
# Artifact A110: V2V Academy - Citizen Architect Classes
# Date Created: C109
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A definitive guide to the six "Citizen Architect" classes for the V2V Academy. This artifact serves as the source of truth for the lore, abilities, and descriptions used in marketing materials and character cards.
- **Tags:** v2v, academy, re-branding, persona, citizen architect, lore, rpg

## 1. Overview

This document defines the six core "classes" for the Citizen Architect archetype within the V2V Academy. Each class is mapped to a specific persona and gender, and has a unique set of abilities that blend AI/tech concepts with RPG fantasy language. This information is the canonical source for generating character cards and other marketing assets.

---

## 2. The Classes

### **Class 1: The Thaumaturge**
*   **Persona:** Female Young Precocious
*   **Subtitle:** Weaver of Code, Bender of Logic.
*   **Class Abilities:**
    *   System Weaving
    *   AI Orchestration
    *   Spawn AI Familiar
    *   Reality Scripting
    *   Ultimate Skill: Create World
*   **Description:** A master builder who shapes the digital cosmos, commanding legions of AI to forge new worlds and solve impossible problems for the greater good.

### **Class 2: The Nocturne**
*   **Persona:** Male Young Precocious
*   **Subtitle:** Shadow of the Datastream, Ghost in the Machine.
*   **Class Abilities:**
    *   Data Siphon
    *   Exploit Injection
    *   Recursive Loop
    *   Cloaking Field
    *   Ultimate Skill: Ghost Protocol
*   **Description:** A master of stealth and data manipulation who moves unseen through digital systems, bending them to his will and extracting their deepest secrets.

### **Class 3: The Cipher**
*   **Persona:** Female Underequipped Graduate
*   **Subtitle:** Decoder of Patterns, Scripter of Realities.
*   **Class Abilities:**
    *   Pattern Recognition
    *   Adaptive Camouflage
    *   Social Engineering
    *   Script Kiddy
    *   Ultimate Skill: Master Key
*   **Description:** An adaptable agent who excels at understanding and rewriting the rules of any system she encounters, finding elegant solutions in the chaos of the real world.

### **Class 4: The Technomancer**
*   **Persona:** Male Underequipped Graduate
*   **Subtitle:** Binder of APIs, Summoner of Services.
*   **Class Abilities:**
    *   API Binding
    *   Service Summon
    *   Hardware Integration
    *   Mana Conversion
    *   Ultimate Skill: Forge Automaton
*   **Description:** A resourceful builder who combines disparate digital and physical technologies, summoning powerful services and forging bespoke automatons to solve real-world problems.

### **Class 5: The Strategos**
*   **Persona:** Female Career Transitioner
*   **Subtitle:** Commander of Legions, Architect of Victory.
*   **Class Abilities:**
    *   Strategic Foresight
    *   Resource Allocation
    *   Team Synergy
    *   Grand Architecture
    *   Ultimate Skill: Simulate Future
*   **Description:** A master strategist who sees the entire battlefield, directing teams and AI legions with unmatched foresight to outmaneuver any obstacle and architect victory.

### **Class 6: The Forge Master**
*   **Persona:** Male Career Transitioner
*   **Subtitle:** Builder of Systems, Forger of Worlds.
*   **Class Abilities:**
    *   System Hardening
    *   Redundancy Protocol
    *   Load Balancing
    *   Structural Integrity
    *   Ultimate Skill: Forge Dyson Sphere
*   **Description:** A foundational builder who creates the unbreakable, hyper-scalable systems upon which digital civilizations are built, ensuring resilience and longevity for generations to come.
</file_artifact>

<file path="src/Artifacts/A111 - GWU Appeal Letter - Sarkani.md">
# Artifact A111: GWU Appeal Letter - Sarkani
# Date Created: C110
# Author: AI Model & Curator
# Updated on: C112 (Final Revision: Add concrete evidence, DCE explanation, and live project link)

- **Key/Value for A0:**
- **Description:** A final, evidence-based appeal letter drafted for Professor Shahram Sarkani, Director of GW Online Engineering. This version includes a clear explanation of the DCE, validation from a peer in Offensive Cyber Operations, and a link to the live project.
- **Tags:** appeal, gwu, sarkani, nsa, csfc, d.eng, ai, cognitive capital

**To:** sarkani@gwu.edu  
**Cc:** mazzu@gwu.edu, maria.hoang@gwu.edu  
**Subject:** Inquiry from an AI/Cybersecurity Practitioner (Palo Alto/NSA) Regarding D.Eng. in AI & Machine Learning

Dear Professor Sarkani,

I am writing to you directly, with the utmost respect for the SEAS admissions policy, as my inquiry is not a standard request for reconsideration but rather a strategic inquiry regarding a direct synthesis of your personal research and your leadership of the D.Eng. in AI & Machine Learning program.

My professional background is as a subject matter expert in high-assurance cybersecurity for national security clients (NSA/UKI) and as an SME at Palo Alto Networks. I recently read your and Dr. Mazzuchi's 2017 paper, "An Architecture for Agile Systems Engineering of Secure Commercial Off‐the‐Shelf Mobile Communications," and your related work on the NSA CSfC framework.

I have spent the last several years as a practitioner *implementing* the very COTS/RMF architecture you have researched. My entire purpose in applying for a doctorate at GWU was to develop a praxis research project focused on applying generative AI models to solve the "agile systems" and "rapid integration" problems that you yourself identified in that work.

I was recently informed that my application was unsuccessful, and was concurrently offered admission to the Ph.D. in Cybersecurity, which I respectfully declined. This decision was not made lightly; it was a deliberate choice to pursue my true passion. While my work has been in cybersecurity, my goal has always been to move beyond it. My research is focused on developing the novel human-AI collaboration methodologies that will allow us to engineer systems so advanced that many of today's security challenges become obsolete. My mission is to help build the "Star Trek future"—a world of abundance where we have elevated our collective cognitive capital to solve grander challenges.

To that end, I architected the **Data Curation Environment (DCE)**, a novel Human-Computer Interaction framework within VS Code that transforms AI collaboration from a simple chat into a structured, auditable engineering discipline. The power of this methodology is best demonstrated by its results; the promotional website for the DCE, **<https://aiascent.dev>**, was itself built entirely using the tool. This framework has already been recognized by senior practitioners, including a colleague from the NSA's **Offensive Cyber Operations**, as a tool that would "immensely" improve their high-stakes workflow.

I believe my non-traditional, practitioner-focused profile may have been misunderstood. Your own story and your "Students First" philosophy of providing a "level playing field" for those with grit is what drew me to GWU. I believe I am exactly the type of industry-leading professional you built the D.Eng. program for. I am not asking for a special exception, but for a "level playing field" review of my profile, which I believe represents a strategic asset and potential research collaborator for your program.

Would you be open to a brief, 15-minute conversation to discuss this vision and see a demonstration of the DCE? I am confident I can demonstrate a level of passion and practical expertise that would be a credit to the D.Eng. program.

Respectfully,

David Gerabagi
</file_artifact>

<file path="src/Artifacts/A112 - GWU Appeal Letter - Mazzuchi.md">
# Artifact A112: GWU Appeal Letter - Mazzuchi
# Date Created: C110
# Author: AI Model & Curator
# Updated on: C112 (Final Revision: Add concrete evidence, DCE explanation, and live project link)

- **Key/Value for A0:**
- **Description:** A final, evidence-based appeal letter drafted for Professor Thomas A. Mazzuchi, Co-Director of GW Online Engineering. This version includes a clear explanation of the DCE, validation from a peer in Offensive Cyber Operations, and a link to the live project.
- **Tags:** appeal, gwu, mazzuchi, d.eng, ai, systems engineering, cognitive capital

**To:** mazzu@gwu.edu  
**Cc:** sarkani@gwu.edu, maria.hoang@gwu.edu  
**Subject:** Inquiry from an AI/Systems Practitioner (Palo Alto/NSA) Regarding D.Eng. in AI & Machine Learning

Dear Professor Mazzuchi,

I am writing to you in your capacity as both a Professor of Engineering Management and Systems Engineering and as the Co-Director of GW Engineering Online. I am an AI and systems practitioner, and my research into your work has revealed a powerful alignment that I hope you might provide some brief guidance on.

My professional experience is centered at the intersection of complex systems, AI, and cybersecurity risk. In my roles at Palo Alto Networks and developing training for the NSA, my work has been the practical application of the very topics I see in your recent research—from "Mitigating Cybersecurity Risks" to "Deep learning Architectures for Network Intrusion Detection."

This brings me to my query. I recently applied to the Ph.D. in AI program and was unsuccessful, though I was offered admission to the Ph.D. in Cybersecurity. I respectfully declined this offer because I believe focusing on cybersecurity alone addresses a symptom, not the root cause. My core passion and research interest lie in using AI to solve a higher-order problem: how do we fundamentally increase a society's "cognitive capital" to architect systems that are inherently more resilient and secure? My mission is to help build a "Star Trek future" of abundance, and I believe that starts by creating new paradigms for human-AI collaboration.

My work in creating the **Data Curation Environment (DCE)** is my first tangible step in this direction. It is a novel Human-Computer Interaction framework I architected within VS Code that transforms AI collaboration into a structured, auditable engineering discipline. The framework's power is demonstrated by the fact that its own promotional website, **<https://aiascent.dev>**, was built entirely using the tool. This work has already garnered validation from senior practitioners, including a colleague in the NSA's **Offensive Cyber Operations** who confirmed it would "immensely" improve their mission-critical workflows.

In retrospect, it is clear to me that my practitioner-focused profile was not the right fit for a traditional CS Ph.D. However, my research led me to the EMSE department's strategic focus on "trustworthiness in artificial intelligence" and, more specifically, to the **Doctor of Engineering (D.Eng.) in AI & Machine Learning** that you and Professor Sarkani oversee. This practice-oriented doctorate appears to be the "right fit" I was looking for.

I was deeply struck by your and Professor Sarkani's stated mission to provide a "level playing field" for driven professionals. I am not asking to appeal a past decision. I am asking for 15 minutes of your guidance on how a practitioner with a passionate, AI-driven vision for the future of systems engineering might find a home at GWU.

Thank you for your time and for your leadership in building programs for working professionals.

Respectfully,

David Gerabagi
</file_artifact>

<file path="src/Artifacts/A113 - GWU Appeal Letter - Blackford.md">
# Artifact A113: GWU Appeal Letter - Blackford
# Date Created: C110
# Author: AI Model & Curator
# Updated on: C112 (Final Revision: Add concrete evidence, DCE explanation, and live project link)

- **Key/Value for A0:**
- **Description:** A final, evidence-based appeal letter drafted for Professor J.P. Blackford, Doctoral Program Coordinator. This version includes a clear explanation of the DCE, validation from a peer in Offensive Cyber Operations, and a link to the live project to strengthen the D.Eng. petition.
- **Tags:** appeal, gwu, blackford, d.eng, programmatic re-alignment, ai, cognitive capital

**To:** jblackford@gwu.edu  
**Cc:** maria.hoang@gwu.edu, sarkani@gwu.edu, mazzu@gwu.edu  
**Subject:** Petition for Programmatic Re-evaluation – Application ID # [Your Application ID]

Dear Professor Blackford,

I am writing to you today not to challenge a decision, but to respectfully petition for a programmatic re-evaluation of my doctoral application. I was recently informed that my application for the Ph.D. in AI program was unsuccessful. While an alternative in Cybersecurity was offered, my singular passion and research focus is on AI, and I had to decline.

I believe my application was a "type mismatch." My profile is that of a senior industry practitioner with a deep passion for building novel AI-native systems, not a traditional academic. My life's mission is to develop the frameworks that empower people to collaborate with AI, increasing our collective "Cognitive Capital" and accelerating our ability to solve major world problems.

To this end, I architected and built the **Data Curation Environment (DCE)**, a novel Human-Computer Interaction framework for AI development. It transforms the workflow into a structured, auditable engineering discipline. The tangible result of this work is live at **<https://aiascent.dev>**, a project built entirely with the DCE. This framework has already been recognized by senior practitioners, including a colleague from the NSA's **Offensive Cyber Operations**, as a tool that would "immensely" improve their high-stakes workflow.

My entire purpose in applying to GWU was to pursue the **Doctor of Engineering (D.Eng.)**. I am writing to you specifically in your capacity as the Doctoral Program Coordinator. Your own background as a D.Eng. graduate and your work with the NSF I-Corps program suggest a deep understanding of the value that industry practitioners bring to engineering innovation.

My request is to have my application file re-activated and re-evaluated by the D.Eng. committee for the D.Eng. in AI & Machine Learning. I am confident that when viewed through the D.Eng. practitioner lens, my application—supported by a live, complex software project and validation from the national security community—demonstrates a strong capacity for the original, applied scholarship that the D.Eng. program champions.

Thank you for your time and for considering this petition for programmatic re-alignment.

Sincerely,

David Gerabagi
</file_artifact>

<file path="src/Artifacts/A114 - GWU Appeal Letter - Etemadi.md">
# Artifact A114: GWU Appeal Letter - Etemadi
# Date Created: C110
# Author: AI Model & Curator
# Updated on: C112 (Final Revision: Add concrete evidence, DCE explanation, and live project link)

- **Key/Value for A0:**
- **Description:** A final, evidence-based appeal letter drafted for Professor Amir Etemadi. This version includes a clear explanation of the DCE, validation from a peer in Offensive Cyber Operations, and a link to the live project to make the research inquiry more compelling.
- **Tags:** appeal, gwu, etemadi, research, ai, cybersecurity, cognitive capital

**To:** etemadi@gwu.edu  
**Cc:** maria.hoang@gwu.edu  
**Subject:** Inquiry from an AI Practitioner re: Your Research in Cyber-Attack Detection

Dear Professor Etemadi,

I am an applied AI practitioner writing to you today with great respect for your work. I have been following your group's research and was particularly struck by your involvement in the 2023 dissertation "Cyber Deception Techniques... for Cybersecurity Enhancement" and your more recent work on "feature selection for cyber attack detection."

My professional work is on the front-lines of this exact problem space. I develop cybersecurity training for national security clients and have an extensive background from Palo Alto Networks. In parallel, I do applied work on LLM alignment with Google's Gemini models. This dual focus has given me a unique perspective: while your research uses AI to *detect* problems, my work is focused on architecting new human-AI collaboration frameworks to build systems so advanced and resilient that many of today's attack vectors become obsolete. My ultimate goal is to help engineer a "Star Trek future" where we have elevated our collective cognitive capital to solve grander challenges than cybersecurity.

To this end, I created the **Data Curation Environment (DCE)**, a novel HCI framework that transforms AI collaboration into a structured engineering discipline. The project is open-sourced and live at **<https://aiascent.dev>**, and the methodology has already been validated by practitioners, including a colleague from the NSA's **Offensive Cyber Operations** who believes it would "immensely" improve their work.

I recently received a rejection for my application to the Ph.D. in AI program, and I believe my unique, practitioner-focused profile and this higher-level AI vision may have been misunderstood. I am reaching out to you not to appeal, but to ask for guidance. Your work at the nexus of AI and critical infrastructure security is the closest academic alignment I have found to my own passionate vision.

Would you be open to a brief, 15-minute virtual call in the coming weeks? I would be fascinated to learn more about your research and to demonstrate the DCE, which I believe has direct relevance to the future of building secure, AI-native systems.

Respectfully,

David Gerabagi
</file_artifact>

<file path="src/Artifacts/DCE_README.md">
# Artifact A72: DCE - README for Artifacts
# Date Created: C158
# Author: AI Model & Curator
# Updated on: C183 (Strengthen Git initialization and `.gitignore` guidance)

- **Key/Value for A0:**
- **Description:** The content for the `README.md` file that is automatically created in a new project's `src/Artifacts` directory, explaining the purpose of the extension and the artifact-driven workflow.
- **Tags:** documentation, onboarding, readme, source of truth

## 1. Welcome to the Data Curation Environment (DCE)

This directory (`src/Artifacts/`) is the heart of your project's planning and documentation. It's managed by the **Data Curation Environment (DCE)**, a VS Code extension designed to streamline AI-assisted development.

This `README.md` file was automatically generated to provide context for you (the developer) and for the AI assistants you will be working with.

## 2. What is an "Artifact"?

In the context of this workflow, an **Artifact** is a formal, written document that serves as a "source of truth" for a specific part of your project. Think of these files as the official blueprints, plans, and records.

The core principle of the DCE workflow is **"Documentation First."** Before writing code, you and your AI partner should first create or update an artifact that describes the plan.

## 3. The Iterative Cycle Workflow

Development in the DCE is organized into **Cycles**. You have just completed the initial setup.

### Your Next Steps

1.  **Initialize Your Git Repository (CRITICAL):**
    To take full advantage of the DCE's testing workflow (creating baselines and restoring changes), you **must** initialize a Git repository.
    
    Open a terminal in your project's root directory (you can use the integrated terminal in VS Code: `Terminal > New Terminal`) and run the following commands:
    ```bash
    git init
    # Create or update your .gitignore file with the line below
    echo ".vscode/" >> .gitignore
    git add .
    git commit -m "Initial commit"
    ```
    **Why `.gitignore`?** The DCE saves its state in a `.vscode/dce_history.json` file. Adding `.vscode/` to your `.gitignore` is crucial to prevent the extension's UI from flashing every time it auto-saves. For a complete guide, refer to the `GitHub Repository Setup Guide.md` artifact.

2.  **Submit Your First Prompt:** The `prompt.md` file has been automatically opened for you. This file contains your project plan and instructions for the AI. Copy its entire contents and paste it into your preferred AI chat interface (like Google's AI Studio, ChatGPT, etc.).

3.  **Review and Accept Responses:** Paste the AI's responses back into the "Resp 1", "Resp 2", etc. tabs in the Parallel Co-Pilot panel. The UI will guide you through parsing the responses, selecting the best one, and accepting its changes into your workspace.

4.  **Repeat:** This completes a cycle. You then start the next cycle, building upon the newly accepted code and documentation.

This structured, iterative process helps maintain project quality and ensures that both human and AI developers are always aligned with the project's goals.
</file_artifact>

<file path="src/components/academy/PersonaSelector.tsx">
'use client';
import React from 'react';
import { Card, CardContent, CardHeader, CardTitle, CardDescription } from '@/components/ui/card';
import { motion } from 'framer-motion';
import Image from 'next/image';

interface PersonaSelectorProps {
    onSelectPersona: (persona: string) => void;
}

const personas = [
    {
        id: 'career_transitioner',
        title: 'The Career Transitioner',
        description: 'You have established expertise in a non-technical field and want to augment your skills with AI to future-proof your career and become a strategic leader.',
        image: '/assets/images/v2v/career_transitioner_thumbnail.webp',
    },
    {
        id: 'underequipped_graduate',
        title: 'The Underequipped Graduate',
        description: 'You have a traditional degree but feel unprepared for the AI-driven job market. You want to gain a competitive edge with practical, in-demand skills.',
        image: '/assets/images/v2v/underequipped_graduate_thumbnail.webp',
    },
    {
        id: 'young_precocious',
        title: 'The Young Precocious',
        description: 'You are a digitally native, self-taught creator, driven by curiosity. You want to channel your raw talent into a disciplined, powerful engineering practice.',
        image: '/assets/images/v2v/young_precocious_thumbnail.webp',
    },
];

const PersonaSelector: React.FC<PersonaSelectorProps> = ({ onSelectPersona }) => {
    return (
        <div className="grid grid-cols-1 md:grid-cols-3 gap-8 w-full max-w-6xl">
            {personas.map((persona, index) => (
                <motion.div
                    key={persona.id}
                    initial={{ opacity: 0, y: 20 }}
                    animate={{ opacity: 1, y: 0 }}
                    transition={{ duration: 0.5, delay: 0.2 * (index + 1) }}
                >
                    <Card
                        className="h-full flex flex-col hover:bg-accent hover:border-primary transition-all cursor-pointer group"
                        onClick={() => onSelectPersona(persona.id)}
                    >
                        <CardHeader className="p-0">
                            <div className="relative aspect-video w-full">
                                <Image
                                    src={persona.image}
                                    alt={persona.title}
                                    fill
                                    className="object-cover rounded-t-lg transition-transform group-hover:scale-105"
                                />
                            </div>
                            <div className="p-6 text-center">
                                <CardTitle>{persona.title}</CardTitle>
                            </div>
                        </CardHeader>
                        <CardContent className="flex-grow text-center pt-0">
                            <CardDescription>{persona.description}</CardDescription>
                        </CardContent>
                    </Card>
                </motion.div>
            ))}
        </div>
    );
};

export default PersonaSelector;
</file_artifact>

<file path="src/components/global/3d-card.tsx">
{
  /*
  Cycle 30: Fix exhaustive-deps warning.
  - Wrapped `handleAnimations` in `useCallback` to stabilize its reference.
  - Added `handleAnimations` to the `useEffect` dependency array.
  */
}
'use client'

import { cn } from '@/lib/utils'
// Removed unused import: import Image from 'next/image'
import React, {
  createContext,
  useState,
  useContext,
  useRef,
  useEffect,
  useCallback,
} from 'react'

const MouseEnterContext = createContext<
  [boolean, React.Dispatch<React.SetStateAction<boolean>>] | undefined
> (undefined)

export const CardContainer = ({
  children,
  className,
  containerClassName,
}: {
  children?: React.ReactNode
  className?: string
  containerClassName?: string
}) => {
  const containerRef = useRef<HTMLDivElement>(null)
  const [isMouseEntered, setIsMouseEntered] = useState(false)

  const handleMouseMove = (e: React.MouseEvent<HTMLDivElement>) => {
    if (!containerRef.current) return
    const { left, top, width, height } =
      containerRef.current.getBoundingClientRect()
// Adjusted division factor from 25 to 40 for subtler effect
    const x = (e.clientX - left - width / 2) / 40
    const y = (e.clientY - top - height / 2) / 40
    containerRef.current.style.transform = `rotateY(${x}deg) rotateX(${y}deg)`
  }

  const handleMouseEnter = (e: React.MouseEvent<HTMLDivElement>) => {
    setIsMouseEntered(true)
    if (!containerRef.current) return
  }

  const handleMouseLeave = (e: React.MouseEvent<HTMLDivElement>) => {
    if (!containerRef.current) return
    setIsMouseEntered(false)
    containerRef.current.style.transform = `rotateY(0deg) rotateX(0deg)`
  }
  return (
    <MouseEnterContext.Provider value={[isMouseEntered, setIsMouseEntered]}>
      <div
        className={cn('flex items-center justify-center', containerClassName)}
        style={{
          perspective: '1000px',
        }}
      >
        <div
          ref={containerRef}
          onMouseEnter={handleMouseEnter}
          onMouseMove={handleMouseMove}
          onMouseLeave={handleMouseLeave}
          className={cn(
            'flex items-center justify-center relative transition-all duration-200 ease-linear',
            className
          )}
          style={{
            transformStyle: 'preserve-3d',
          }}
        >
          {children}
        </div>
      </div>
    </MouseEnterContext.Provider>
  )
}

export const CardBody = ({
  children,
  className,
}: {
  children: React.ReactNode
  className?: string
}) => {
  return (
// Removed fixed h-96 w-96 to allow flexible sizing
    <div
      className={cn(
        '[transform-style:preserve-3d]  [&>*]:[transform-style:preserve-3d]',
        className
      )}
    >
      {children}
    </div>
  )
}

export const CardItem = ({
  as: Tag = 'div',
  children,
  className,
  translateX = 0,
  translateY = 0,
  translateZ = 0,
  rotateX = 0,
  rotateY = 0,
  rotateZ = 0,
  ...rest
}: {
  as?: React.ElementType
  children: React.ReactNode
  className?: string
  translateX?: number | string
  translateY?: number | string
  translateZ?: number | string
  rotateX?: number | string
  rotateY?: number | string
  rotateZ?: number | string
}) => {
  const ref = useRef<HTMLDivElement>(null)
  const [isMouseEntered] = useMouseEnter()

  const handleAnimations = useCallback(() => {
    if (!ref.current) return
    if (isMouseEntered) {
      ref.current.style.transform = `translateX(${translateX}px) translateY(${translateY}px) translateZ(${translateZ}px) rotateX(${rotateX}deg) rotateY(${rotateY}deg) rotateZ(${rotateZ}deg)`
    } else {
      ref.current.style.transform = `translateX(0px) translateY(0px) translateZ(0px) rotateX(0deg) rotateY(0deg) rotateZ(0deg)`
    }
  }, [isMouseEntered, translateX, translateY, translateZ, rotateX, rotateY, rotateZ]);

  useEffect(() => {
    handleAnimations()
  }, [isMouseEntered, handleAnimations])

  return (
    <Tag
      ref={ref}
// Adjusted duration-200 to duration-300 for smoother animation
      className={cn('w-fit transition duration-300 ease-linear', className)}
      {...rest}
    >
      {children}
    </Tag>
  )
}

// Create a hook to use the context
export const useMouseEnter = () => {
  const context = useContext(MouseEnterContext)
  if (context === undefined) {
    throw new Error('useMouseEnter must be used within a MouseEnterProvider')
  }
  return context
}
</file_artifact>

<file path="src/components/global/ConditionalSplash.tsx">
'use client';

import { usePathname } from 'next/navigation';
import SplashCursor from './SplashCursor';

export default function ConditionalSplash() {
    const pathname = usePathname();

    // Disable the splash effect on the /showcase page
    if (pathname === '/showcase') {
        return null;
    }

    // Render the splash cursor with faster dissipation
    return <SplashCursor DENSITY_DISSIPATION={4.375} />;
}
</file_artifact>

<file path="src/components/global/container-scroll-animation.tsx">
// src/components/global/container-scroll-animation.tsx
// C11 - Fix white border on GIF by changing background and adjusting padding
'use client'
import React, { useRef } from 'react'
import { useScroll, useTransform, motion } from 'framer-motion'
import Image from 'next/image'

// Define the type for the children prop, which will contain the visuals (images/gifs)
type ContainerScrollProps = {
  titleComponent: string | React.ReactNode;
  children: React.ReactNode; // Added children prop
};

export const ContainerScroll = ({
  titleComponent,
  children, // Destructure children
}: ContainerScrollProps) => {
  const containerRef = useRef<any>(null)
  const { scrollYProgress } = useScroll({
    target: containerRef,
  })
  const [isMobile, setIsMobile] = React.useState(false)

  React.useEffect(() => {
    const checkMobile = () => {
      setIsMobile(window.innerWidth <= 768)
    }
    checkMobile()
    window.addEventListener('resize', checkMobile)
    return () => {
      window.removeEventListener('resize', checkMobile)
    }
  }, [])

  const scaleDimensions = () => {
    return isMobile ? [0.7, 0.9] : [1.05, 1]
  }

const rotate = useTransform(scrollYProgress, [0, 1], [20, 0])
const scale = useTransform(scrollYProgress, [0, 1], scaleDimensions())
const translate = useTransform(scrollYProgress, [0, 1], [0, -100])

  return (
    <div
      className="h-[80rem] flex items-center justify-center relative p-2 sm:p-20"
      ref={containerRef}
    >
      <div
        className="py-10 md:py-40 w-full relative"
        style={{
          perspective: '1000px',
        }}
      >
        <Header
          translate={translate}
          titleComponent={titleComponent}
        />
        {/* Pass children to the Card component */}
        <Card
          rotate={rotate}
          translate={translate}
          scale={scale}
        >
          {children}
        </Card>
      </div>
    </div>
  )
}

export const Header = ({ translate, titleComponent }: any) => {
  return (
    <motion.div
      style={{
        translateY: translate,
      }}
      className="div max-w-5xl mx-auto text-center"
    >
      {titleComponent}
    </motion.div>
  )
}

// Update Card component to accept children
type CardProps = {
  rotate: any;
  scale: any;
  translate: any;
  children: React.ReactNode; // Added children prop
};

export const Card = ({
  rotate,
  scale,
  translate,
  children, // Destructure children
}: CardProps) => {
  return (
    <motion.div
      style={{
        rotateX: rotate, // rotate in X-axis
        scale,
        boxShadow:
          '0 0 #0000004d, 0 9px 20px #0000004a, 0 37px 37px #00000042, 0 84px 50px #00000026, 0 149px 60px #0000000a, 0 233px 65px #00000003',
      }}
      className="max-w-5xl -mt-12 mx-auto h-[30rem] md:h-[40rem] w-full border-4 border-neutral-800 p-2 md:p-6 bg-neutral-900 rounded-[30px] shadow-2xl"
    >
      <div className="h-full w-full rounded-2xl gap-4 overflow-hidden p-0 transition-all bg-transparent">
        {/* Render children instead of a static image */}
        {children}
      </div>
    </motion.div>
  )
}
</file_artifact>

<file path="src/components/global/FullscreenMediaViewer.tsx">
'use client';

import React from 'react';
import { useReportState, useReportStore } from '@/stores/reportStore';
import { AnimatePresence, motion } from 'framer-motion';
import { FaTimes, FaChevronLeft, FaChevronRight } from 'react-icons/fa';
import Image from 'next/image';
import MarkdownRenderer from '../shared/MarkdownRenderer';

const FullscreenMediaViewer = () => {
    const { fullscreenMedia, currentPageIndex, allPages } = useReportState(state => ({
        fullscreenMedia: state.fullscreenMedia,
        currentPageIndex: state.currentPageIndex,
        allPages: state.allPages,
    }));
    const { closeFullscreenMedia, prevPageInFullscreen, nextPageInFullscreen } = useReportStore.getState();

    const isLabView = !!fullscreenMedia?.content;

    return (
        <AnimatePresence>
            {fullscreenMedia && (
                <motion.div
                    initial={{ opacity: 0 }}
                    animate={{ opacity: 1 }}
                    exit={{ opacity: 0 }}
                    className="fixed inset-0 bg-black/80 backdrop-blur-sm z-50 flex items-center justify-center p-4"
                    onClick={closeFullscreenMedia}
                >
                    <div
                        className="relative bg-card border border-border rounded-lg shadow-2xl w-full h-full max-w-[90vw] max-h-[90vh] flex flex-col md:flex-row overflow-hidden"
                        onClick={(e) => e.stopPropagation()}
                    >
                        <button
                            onClick={closeFullscreenMedia}
                            className="absolute top-2 right-2 z-20 p-2 text-foreground/70 hover:text-foreground bg-background/50 rounded-full"
                            title="Close"
                        >
                            <FaTimes />
                        </button>
                        
                        <div className="w-full md:w-2/3 h-1/2 md:h-full bg-black/50 flex items-center justify-center p-4 relative">
                            <Image
                                src={fullscreenMedia.src}
                                alt="Fullscreen Media"
                                fill
                                className="object-contain"
                                unoptimized
                            />
                            {isLabView && (
                                <>
                                    <button 
                                        onClick={(e) => { e.stopPropagation(); prevPageInFullscreen(); }}
                                        disabled={currentPageIndex === 0}
                                        className="absolute left-4 top-1/2 -translate-y-1/2 p-3 bg-black/50 text-white rounded-full disabled:opacity-30 hover:bg-black/80 transition-colors z-10"
                                        title="Previous Step"
                                    >
                                        <FaChevronLeft />
                                    </button>
                                    <button 
                                        onClick={(e) => { e.stopPropagation(); nextPageInFullscreen(); }}
                                        disabled={currentPageIndex >= allPages.length - 1}
                                        className="absolute right-4 top-1/2 -translate-y-1/2 p-3 bg-black/50 text-white rounded-full disabled:opacity-30 hover:bg-black/80 transition-colors z-10"
                                        title="Next Step"
                                    >
                                        <FaChevronRight />
                                    </button>
                                </>
                            )}
                        </div>
                        
                        <div className="w-full md:w-1/3 h-1/2 md:h-full p-6 overflow-y-auto">
                            <div className="prose prose-sm dark:prose-invert max-w-none">
                                <MarkdownRenderer>
                                    {(isLabView ? fullscreenMedia.content : fullscreenMedia.description) || ''}
                                </MarkdownRenderer>
                            </div>
                        </div>
                    </div>
                </motion.div>
            )}
        </AnimatePresence>
    );
};

export default FullscreenMediaViewer;
</file_artifact>

<file path="src/components/global/GlobalAudioPlayer.tsx">

  /*
  Cycle 30: Fix exhaustive-deps warning.
  - Added `setGenericPlaybackStatus` to the `useEffect` dependency array.
  */

// src/components/global/GlobalAudioPlayer.tsx

'use client';

import React, { useEffect, useRef } from 'react';
import { useReportState, useReportStore } from '@/stores/reportStore';

const GlobalAudioPlayer = () => {
    const audioRef = useRef<HTMLAudioElement>(null);
    const { genericAudioUrl, genericPlaybackStatus } = useReportState(state => ({
        genericAudioUrl: state.genericAudioUrl,
        genericPlaybackStatus: state.genericPlaybackStatus,
    }));
    const { setGenericPlaybackStatus } = useReportStore.getState();

    useEffect(() => {
        const audio = audioRef.current;
        if (!audio) return;

        const handleEnded = () => {
            setGenericPlaybackStatus('idle');
        };
        const handlePause = () => {
            // This handles the case where the user pauses via browser controls
            if (genericPlaybackStatus === 'playing') {
                setGenericPlaybackStatus('paused');
            }
        };
        const handlePlay = () => {
            if (genericPlaybackStatus !== 'playing') {
                setGenericPlaybackStatus('playing');
            }
        };

        audio.addEventListener('ended', handleEnded);
        audio.addEventListener('pause', handlePause);
        audio.addEventListener('play', handlePlay);

        return () => {
            audio.removeEventListener('ended', handleEnded);
            audio.removeEventListener('pause', handlePause);
            audio.removeEventListener('play', handlePlay);
        };
    }, [genericPlaybackStatus, setGenericPlaybackStatus]);

    useEffect(() => {
        const audio = audioRef.current;
        if (!audio) return;

        if (genericAudioUrl) {
            if (audio.src !== genericAudioUrl) {
                audio.src = genericAudioUrl;
            }
            audio.play().catch(e => {
                console.error("Error playing arbitrary audio:", e);
                setGenericPlaybackStatus('error');
            });
        } else {
            audio.pause();
            audio.src = '';
        }
    }, [genericAudioUrl, setGenericPlaybackStatus]);
    
    useEffect(() => {
        const audio = audioRef.current;
        if (!audio) return;

        if (genericPlaybackStatus === 'playing' && audio.paused) {
            audio.play().catch(e => console.error("Error resuming play:", e));
        } else if (genericPlaybackStatus !== 'playing' && !audio.paused) {
            audio.pause();
        }

    }, [genericPlaybackStatus]);

    // This component renders no visible UI
    return <audio ref={audioRef} />;
};

export default GlobalAudioPlayer;
</file_artifact>

<file path="src/components/global/infinite-moving-cards.tsx">
{
  /*
  Cycle 30: Fix exhaustive-deps warning.
  - Wrapped `addAnimation` in `useCallback` to stabilize its reference.
  - Added `addAnimation` to the `useEffect` dependency array.
  */
}
// src/components/global/infinite-moving-cards.tsx
// C3 - Ported from automationsaas context
'use client'

import { cn } from '@/lib/utils'
import Image from 'next/image'
import React, { useEffect, useState, useCallback } from 'react'

export const InfiniteMovingCards = ({
  items,
  direction = 'left',
  speed = 'fast',
  pauseOnHover = true,
  className,
}: {
  items: {
// Updated type to support image href or text content
    content: string;
type: 'image' | 'text';
  }[]
  direction?: 'left' | 'right'
  speed?: 'fast' | 'normal' | 'slow'
  pauseOnHover?: boolean
  className?: string
}) => {
  const containerRef = React.useRef<HTMLDivElement>(null)
  const scrollerRef = React.useRef<HTMLUListElement>(null)
  const [start, setStart] = useState(false)

  const getDirection = useCallback(() => {
    if (containerRef.current) {
      if (direction === 'left') {
        containerRef.current.style.setProperty(
          '--animation-direction',
          'forwards'
        )
      } else {
        containerRef.current.style.setProperty(
          '--animation-direction',
          'reverse'
        )
      }
    }
  }, [direction]);

  const getSpeed = useCallback(() => {
    if (containerRef.current) {
      if (speed === 'fast') {
        containerRef.current.style.setProperty('--animation-duration', '20s')
      } else if (speed === 'normal') {
        containerRef.current.style.setProperty('--animation-duration', '40s')
      } else {
        containerRef.current.style.setProperty('--animation-duration', '80s')
      }
    }
  }, [speed]);

  const addAnimation = useCallback(() => {
    if (containerRef.current && scrollerRef.current) {
      const scrollerContent = Array.from(scrollerRef.current.children)

      scrollerContent.forEach((item) => {
        const duplicatedItem = item.cloneNode(true)
        if (scrollerRef.current) {
          scrollerRef.current.appendChild(duplicatedItem)
        }
      })

      getDirection()
      getSpeed()
      setStart(true)
    }
  }, [getDirection, getSpeed]);

  useEffect(() => {
    addAnimation()
  }, [addAnimation])

  return (
    <div
      ref={containerRef}
      className={cn(
        'scroller relative z-20  max-w-7xl overflow-hidden  [mask-image:linear-gradient(to_right,transparent,white_20%,white_80%,transparent)]',
        className
      )}
    >
      <ul
        ref={scrollerRef}
        className={cn(
          ' flex min-w-full shrink-0 gap-10 py-4 w-max flex-nowrap items-center',
          start && 'animate-scroll ',
          pauseOnHover && 'hover:[animation-play-state:paused]'
        )}
      >
        {items.map((item, idx) => (
<li key={idx} className="flex items-center">
{item.type === 'image' ? (
<Image
width={170}
height={50} // Adjusted height for better aspect ratio
src={item.content}
alt={`scrolling-item-${idx}`}
className="relative rounded-2xl object-contain opacity-50"
/>
) : (
<span className="text-2xl font-semibold opacity-50 whitespace-nowrap">
{item.content}
</span>
)}
</li>
        ))}
      </ul>
    </div>
  )
}
</file_artifact>

<file path="src/components/global/lamp.tsx">
// src/components/global/lamp.tsx
// C11 - Add useTheme to dynamically set particle color
'use client'
import React from 'react'
import { motion } from 'framer-motion'
import { cn } from '@/lib/utils'
import { SparklesCore } from './sparkles' 
import { useTheme } from 'next-themes'

export const LampContainer = ({
  children,
  className,
}: {
  children: React.ReactNode
  className?: string
}) => {
  const { theme } = useTheme();
  const particleColor = theme === 'light' ? '#000000' : '#FFFFFF';

  return (
    <div
      className={cn(
        'relative flex flex-col items-center justify-center overflow-hidden bg-background w-full rounded-md z-0 pt-20',
        className
      )}
    >
      {/* Sparkles now fill the entire container */}
      <div className="absolute inset-0 w-full h-full z-0">
          <SparklesCore
            background="transparent"
            minSize={0.4}
            maxSize={1.2}
            particleDensity={1200}
            className="w-full h-full"
            particleColor={particleColor}
          />
        </div>

      <div className="relative flex w-full flex-1 scale-y-150 items-center justify-center isolate z-10 ">
        <motion.div
          initial={{ opacity: 0.5, width: '15rem' }}
          whileInView={{ opacity: 1, width: '80rem' }}
          transition={{
            delay: 0.3,
            duration: 0.8,
            ease: 'easeInOut',
          }}
          style={{
            backgroundImage: `conic-gradient(var(--conic-position), var(--tw-gradient-stops))`,
          }}
          className="absolute inset-auto right-1/2 h-[60rem] overflow-visible w-[80rem] bg-gradient-conic from-neutral-700 via-transparent to-transparent text-white [--conic-position:from_70deg_at_center_top]"
        >
          <div className="absolute  w-[100%] left-0 bg-background h-40 bottom-0 z-20 [mask-image:linear-gradient(to_top,white,transparent)]" />
          <div className="absolute  w-40 h-[100%] left-0 bg-background  bottom-0 z-20 [mask-image:linear-gradient(to_right,white,transparent)]" />
        </motion.div>
        <motion.div
          initial={{ opacity: 0.5, width: '15rem' }}
          whileInView={{ opacity: 1, width: '80rem' }}
          transition={{
            delay: 0.3,
            duration: 0.8,
            ease: 'easeInOut',
          }}
          style={{
            backgroundImage: `conic-gradient(var(--conic-position), var(--tw-gradient-stops))`,
          }}
          className="absolute inset-auto left-1/2 h-[60rem] w-[80rem] bg-gradient-conic from-transparent via-transparent to-neutral-700 text-white [--conic-position:from_290deg_at_center_top]"
        >
          <div className="absolute  w-40 h-[100%] right-0 bg-background  bottom-0 z-20 [mask-image:linear-gradient(to_left,white,transparent)]" />
          <div className="absolute  w-[100%] right-0 bg-background h-40 bottom-0 z-20 [mask-image:linear-gradient(to_top,white,transparent)]" />
        </motion.div>
        
        <div className="absolute top-1/2 z-50 h-48 w-full bg-transparent opacity-10 backdrop-blur-md"></div>
        <div className="absolute inset-auto z-50 h-36 w-[28rem] -translate-y-1/2 rounded-full bg-neutral-600 opacity-40 blur-3xl"></div>
        <motion.div
          initial={{ width: '8rem' }}
          whileInView={{ width: '16rem' }}
          transition={{
            delay: 0.3,
            duration: 0.8,
            ease: 'easeInOut',
          }}
          className="absolute inset-auto z-30 h-36 w-64 -translate-y-[6rem] rounded-full bg-neutral-400 blur-2xl"
        ></motion.div>
        <motion.div
          initial={{ width: '15rem' }}
          whileInView={{ width: '30rem' }}
          transition={{
            delay: 0.3,
            duration: 0.8,
            ease: 'easeInOut',
          }}
          className="absolute inset-auto z-50 h-0.5 w-[30rem] -translate-y-[12rem] bg-neutral-400 "
        ></motion.div>
      </div>

      <div className="relative z-40 flex -translate-y-20 flex-col items-center px-5">
        {children}
      </div>
    </div>
  )
}
</file_artifact>

<file path="src/components/global/mode-toggle.tsx">
'use client'

import * as React from 'react'
import { Moon, Sun } from 'lucide-react'
import { useTheme } from 'next-themes'

import { Button } from '@/components/ui/button'
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu'

export function ModeToggle() {
  const { setTheme } = useTheme()
  return (
    <DropdownMenu>
      <DropdownMenuTrigger asChild>
        <Button
          variant="outline"
          size="icon"
          className="relative"
        >
          <Sun className="absolute inset-0 m-auto h-[1.2rem] w-[1.2rem] rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0" />
          <Moon className="absolute inset-0 m-auto h-[1.2rem] w-[1.2rem] rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100" />
          <span className="sr-only">Toggle theme</span>
        </Button>
      </DropdownMenuTrigger>
      <DropdownMenuContent align="end">
        <DropdownMenuItem onClick={() => setTheme('light')}>
          Light
        </DropdownMenuItem>
        <DropdownMenuItem onClick={() => setTheme('dark')}>
          Dark
        </DropdownMenuItem>
        <DropdownMenuItem onClick={() => setTheme('system')}>
          System
        </DropdownMenuItem>
      </DropdownMenuContent>
    </DropdownMenu>
  )
}
</file_artifact>

<file path="src/components/global/NextPageSection.tsx">
'use client';
import React from 'react';
import { LampContainer } from '@/components/global/lamp';
import { Button } from '@/components/ui/button';
import Link from 'next/link';
import { motion } from 'framer-motion';
import { cn } from '@/lib/utils';

interface NextPageSectionProps {
    title: string;
    description: string;
    buttonText: string;
    href: string;
    className?: string;
}

const NextPageSection: React.FC<NextPageSectionProps> = ({ title, description, buttonText, href, className }) => {
    return (
        <section className={cn("w-full mt-24", className)}>
            <LampContainer>
                <motion.div
                    initial={{ opacity: 0.5, y: 100 }}
                    whileInView={{ opacity: 1, y: 0 }}
                    transition={{
                        delay: 0.3,
                        duration: 0.8,
                        ease: 'easeInOut',
                    }}
                    className="flex flex-col items-center text-center"
                >
                    <h2 className="mt-8 bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground text-center text-3xl font-bold tracking-tight md:text-5xl">
                        {title}
                    </h2>
                    <p className="text-lg text-muted-foreground max-w-3xl text-center my-8">
                        {description}
                    </p>
                    <Link href={href} passHref>
                        <Button size="lg" variant="outline" className='text-lg'>
                            {buttonText}
                        </Button>
                    </Link>
                </motion.div>
            </LampContainer>
        </section>
    );
};

export default NextPageSection;
</file_artifact>

<file path="src/components/global/sparkles.tsx">
// src/components/global/sparkles.tsx
// C3 - Ported from automationsaas context
'use client'
// Removed unused imports: import type { NextPage } from 'next'
import React from 'react'
import { useEffect, useState } from 'react'
import Particles, { initParticlesEngine } from '@tsparticles/react'
import type { Container } from '@tsparticles/engine' // Removed unused import: Engine
import { loadSlim } from '@tsparticles/slim'

import { motion, useAnimation } from 'framer-motion'
import { cn } from '@/lib/utils'

type ParticlesProps = {
  id?: string
  className?: string
  background?: string
  particleSize?: number // Marked as potentially unused based on options below
  minSize?: number
  maxSize?: number
  speed?: number
  particleColor?: string
  particleDensity?: number
}
export const SparklesCore = (props: ParticlesProps) => {
  const {
    id,
    className,
    background,
    minSize,
    maxSize,
    speed,
    particleColor,
    particleDensity,
  } = props
  const [init, setInit] = useState(false)
  useEffect(() => {
    initParticlesEngine(async (engine) => {
      await loadSlim(engine)
    }).then(() => {
      setInit(true)
    })
  }, [])
  const controls = useAnimation()

  const particlesLoaded = async (container?: Container) => {
    if (container) {
// Removed console.log(container) as it's usually noisy
      // console.log(container)
      controls.start({
        opacity: 1,
        transition: {
          duration: 1,
        },
      })
    }
  }

// NOTE: The options object below is very large and mostly contains default values.
// It has been kept intact as ported from automationsaas to ensure identical behavior.
// In a future refactoring cycle, this could be significantly reduced to only the necessary overrides.

  return (
    <motion.div
      animate={controls}
      className={cn('opacity-0', className)}
    >
      {init && (
        <Particles
          id={id || 'tsparticles'}
          className={cn('h-full w-full')}
          particlesLoaded={particlesLoaded}
          options={{
            background: {
              color: {
// Defaulted background to transparent if not provided, instead of #0d47a1
                value: background || 'transparent',
              },
            },
            fullScreen: {
              enable: false,
              zIndex: 1,
            },

            fpsLimit: 120,
            interactivity: {
              events: {
                onClick: {
                  enable: true,
                  mode: 'push',
                },
                onHover: {
                  enable: false,
                  mode: 'repulse',
                },
                resize: true as any,
              },
              modes: {
                push: {
                  quantity: 4,
                },
                repulse: {
                  distance: 200,
                  duration: 0.4,
                },
              },
            },
            particles: {
              bounce: {
                horizontal: {
                  value: 1,
                },
                vertical: {
                  value: 1,
                },
              },
              collisions: {
                absorb: {
                  speed: 2,
                },
                bounce: {
                  horizontal: {
                    value: 1,
                  },
                  vertical: {
                    value: 1,
                  },
                },
                enable: false,
                maxSpeed: 50,
                mode: 'bounce',
                overlap: {
                  enable: true,
                  retries: 0,
                },
              },
              color: {
                value: particleColor || '#ffffff',
                animation: {
                  h: {
                    count: 0,
                    enable: false,
                    speed: 1,
                    decay: 0,
                    delay: 0,
                    sync: true,
                    offset: 0,
                  },
                  s: {
                    count: 0,
                    enable: false,
                    speed: 1,
                    decay: 0,
                    delay: 0,
                    sync: true,
                    offset: 0,
                  },
                  l: {
                    count: 0,
                    enable: false,
                    speed: 1,
                    decay: 0,
                    delay: 0,
                    sync: true,
                    offset: 0,
                  },
                },
              },
              effect: {
                close: true,
                fill: true,
                options: {},
                type: {} as any,
              },
              groups: {},
              move: {
                angle: {
                  offset: 0,
                  value: 90,
                },
                attract: {
                  distance: 200,
                  enable: false,
                  rotate: {
                    x: 3000,
                    y: 3000,
                  },
                },
                center: {
                  x: 50,
                  y: 50,
                  mode: 'percent',
                  radius: 0,
                },
                decay: 0,
                distance: {},
                direction: 'none',
                drift: 0,
                enable: true,
                gravity: {
                  acceleration: 9.81,
                  enable: false,
                  inverse: false,
                  maxSpeed: 50,
                },
                path: {
                  clamp: true,
                  delay: {
                    value: 0,
                  },
                  enable: false,
                  options: {},
                },
                outModes: {
                  default: 'out',
                },
                random: false,
                size: false,
                speed: {
                  min: 0.1,
                  max: 1,
                },
                spin: {
                  acceleration: 0,
                  enable: false,
                },
                straight: false,
                trail: {
                  enable: false,
                  length: 10,
                  fill: {},
                },
                vibrate: false,
                warp: false,
              },
              number: {
                density: {
                  enable: true,
                  width: 400,
                  height: 400,
                },
                limit: {
                  mode: 'delete',
                  value: 0,
                },
                value: particleDensity || 120,
              },
              opacity: {
                value: {
                  min: 0.1,
                  max: 1,
                },
                animation: {
                  count: 0,
                  enable: true,
                  speed: speed || 4,
                  decay: 0,
                  delay: 2,
                  sync: false,
                  mode: 'auto',
                  startValue: 'random',
                  destroy: 'none',
                },
              },
              reduceDuplicates: false,
              shadow: {
                blur: 0,
                color: {
                  value: '#000',
                },
                enable: false,
                offset: {
                  x: 0,
                  y: 0,
                },
              },
              shape: {
                close: true,
                fill: true,
                options: {},
                type: 'circle',
              },
              size: {
                value: {
                  min: minSize || 1,
                  max: maxSize || 3,
                },
                animation: {
                  count: 0,
                  enable: false,
                  speed: 5,
                  decay: 0,
                  delay: 0,
                  sync: false,
                  mode: 'auto',
                  startValue: 'random',
                  destroy: 'none',
                },
              },
// ... (Remaining default options omitted for brevity, see automationsaas context if needed)
              stroke: {
                width: 0,
              },
// ...
            },
            detectRetina: true,
          }}
        />
      )}
    </motion.div>
  )
}
</file_artifact>

<file path="src/components/home/FeaturesSection.tsx">
'use client';
// src/components/home/FeaturesSection.tsx
// C107: Pass `interactionType="zoom"` to restore hover effect.
// C106: Refactored to use MissionSectionBlock for large images instead of icons.
import React from 'react';
import MissionSectionBlock from '@/components/mission/MissionSectionBlock';

const features = [
    {
        title: "Precision Context Curation",
        tldr: "Stop manual copy-pasting. DCE provides an intuitive, visual way to select and manage the exact files needed for your AI prompts directly within VS Code.",
        content: "The foundation of a high-quality AI response is high-quality context. The DCE eliminates the error-prone process of manually managing file lists or copy-pasting code into a prompt. With the integrated File Tree View, you can browse your entire workspace and select the precise 'source of truth' for your task with simple checkboxes. This curated selection is then automatically flattened into a single context file, ensuring the AI has exactly what it needs, and nothing it doesn't.",
        imagePath: 'how-it-works/',
        imagePrompt: 'A hyper-realistic, cinematic image of a Citizen Architect interacting with a holographic file management interface. They are using simple checkboxes to select various file types (PDF, code, spreadsheets). A clean, precise beam of light, representing the curated context, flows from the selected files towards a destination labeled "Precision In, Perfection Out: The Art of Curation." The aesthetic is futuristic, clean, and aligned with the "Citizen Architect" style.',
        images: ['curation.webp'],
        imageSide: 'left',
    },
    {
        title: "Parallel Co-Pilot & Rapid Testing",
        tldr: "Don't rely on a single AI response. Compare multiple solutions side-by-side and use the Git-integrated testing workflow to safely audition code changes in seconds.",
        content: "AI models are non-deterministic. A single prompt can yield multiple, viable solutions. The Parallel Co-Pilot Panel is designed for this reality. Paste in several responses from your AI, and the DCE will parse them into separate tabs. You can instantly compare the proposed changes for each file and use the built-in diff viewer to understand the nuances of each solution before deciding which one to accept.",
        imagePath: 'how-it-works/',
        imagePrompt: 'A hyper-realistic, cinematic image of a Citizen Architect standing before a large, futuristic touch-screen panel labeled "DCE\'s Parallel Co-Pilot Panel." The panel displays three different AI-generated solutions (A, B, C) side-by-side with an "Integrated Diff Viewer" highlighting the changes. The operator is comparing the solutions before committing, illustrating a "Rapid, Low-Risk Iteration Loop." The aesthetic is futuristic, clean, and aligned with the "Citizen Architect" style.',
        images: ['parallel-copilot.webp'],
        imageSide: 'right',
    },
    {
        title: "Iterative Knowledge Graph",
        tldr: "AI collaboration shouldn't be ephemeral. DCE captures the entire development process—prompts, responses, and decisions—as an iterative, auditable knowledge graph.",
        content: "Every development cycle in the DCE is saved, creating a persistent knowledge graph of your project's evolution. The Cycle History view allows you to step back in time, review the exact context used for a previous prompt, see all the AI responses that were generated, and understand why a particular solution was chosen. This turns your development process into a valuable, shareable asset for training, onboarding, and after-action reviews.",
        imagePath: 'how-it-works/',
        imagePrompt: 'A hyper-realistic, cinematic image of a Citizen Architect standing in a vast, modern library-like space, representing "The Architecture of Institutional Memory." They are interacting with a "Cycle Navigator" to explore a massive, glowing "Persistent Knowledge Graph." Each node in the graph is a "CAPTURED CYCLE" containing the curated context, user intent, and AI solutions for a step in the project\'s history. The aesthetic is futuristic, clean, and aligned with the "Citizen Architect" style.',
        images: ['knowledge-graph.webp'],
        imageSide: 'left',
    },
];

const FeaturesSection = () => {
    return (
        <section className="py-20 md:py-32 bg-background">
            <div className="container mx-auto px-4">
                <h2 className="text-3xl md:text-5xl font-bold text-center mb-24 bg-clip-text text-transparent bg-gradient-to-b from-white to-neutral-600 light:from-black light:to-neutral-700 pb-4">
                    Stop Fighting Your Tools. Start Building the Future.
                </h2>
                <div className="space-y-20">
                    {features.map((feature, index) => (
                        <MissionSectionBlock
                            key={index}
                            title={feature.title}
                            tldr={feature.tldr}
                            content={feature.content}
                            imageSide={feature.imageSide as 'left' | 'right'}
                            imagePath={feature.imagePath}
                            imagePrompt={feature.imagePrompt}
                            images={feature.images}
                            interactionType="zoom" // C107: Use zoom hover effect
                        />
                    ))}
                </div>
            </div>
        </section>
    );
};

export default FeaturesSection;
</file_artifact>

<file path="src/components/home/HeroSection.tsx">
// src/components/home/HeroSection.tsx
import React from 'react';
import { Button } from '@/components/ui/button';
import Link from 'next/link';

const HeroSection = () => {
return (
    <section className="h-screen w-full relative flex flex-col items-center justify-center antialiased">
        {/* Background Image */}
        <div
            className="absolute inset-0 bg-cover bg-center"
            style={{ backgroundImage: "url('/assets/images/master_of_realms.webp')" }}
        ></div>

        {/* Dark Overlay */}
        <div className="absolute inset-0 bg-black/60"></div>

        {/* Content */}
        <div className="relative z-10 flex flex-col items-center text-center px-4">
            {/* Headline (A16, 4.2) - C106: Added pb-4 for padding */}
            <h1 className="text-5xl md:text-7xl lg:text-8xl bg-clip-text text-transparent bg-gradient-to-b from-white to-neutral-300 font-sans font-bold mb-8 pb-4">
                Vibe Code for Free. Ship Real Projects.
            </h1>
            
            {/* Subheadline (A16, 4.2) - C106: Rewritten to be more inclusive */}
            <p className="text-lg md:text-xl text-neutral-300 max-w-4xl text-center mb-12">
                The future of all knowledge work—from law and architecture to software development—is curating data for AI. The Data Curation Environment (DCE) is the essential VS Code extension for the emerging class of &lsquo;Citizen Architects,&rsquo; empowering you to build complex, AI-driven projects with precision and confidence.
            </p>

            {/* CTAs (A16, 4.2) */}
            <div className="flex flex-col sm:flex-row gap-4">
                <Link href="/showcase">
                    <Button
                        size={'lg'}
                        className="p-6 text-lg border-t-2 rounded-full border-neutral-700 bg-neutral-900/80 hover:bg-neutral-800/80 group transition-all flex items-center justify-center gap-4 hover:shadow-xl hover:shadow-black/50 duration-500 backdrop-blur-sm"
                    >
                        <span className="bg-clip-text text-transparent bg-gradient-to-r from-neutral-300 to-white font-sans group-hover:text-white">
                            Explore the Showcase
                        </span>
                    </Button>
                </Link>
                <a href="/downloads/data-curation-environment-0.1.10.vsix" download="data-curation-environment-0.1.10.vsix">
                    <Button size="lg" variant="outline" className="p-6 text-lg bg-transparent hover:bg-white/10 text-white border-white/50 hover:text-white">
                        Download Now
                    </Button>
                </a>
            </div>
        </div>
    </section>
);
};

export default HeroSection;
</file_artifact>

<file path="src/components/home/HowItWorksSection.tsx">
'use client';
// src/components/home/HowItWorksSection.tsx
import React from 'react';
import { FaBoxes, FaGoogle, FaRocket } from 'react-icons/fa';

const features = [
    {
        icon: <FaBoxes size={32} className="text-primary" />,
        title: "Curate Your Code",
        description: "Visually select any file in your project. DCE intelligently packages your code, documents, and data into a single, clean prompt.",
    },
    {
        icon: <FaGoogle size={32} className="text-primary" />,
        title: "Collaborate with Free AI",
        description: "Take your perfectly curated context to any AI, including the powerful, free models in Google's AI Studio.",
    },
    {
        icon: <FaRocket size={32} className="text-primary" />,
        title: "Create Without Limits",
        description: "Get unlimited, high-quality code and documentation from the world's best models, without spending a dime on API fees.",
    },
];

const HowItWorksSection = () => {
    return (
        <section className="py-20 md:py-24 bg-background">
            <div className="container mx-auto px-4">
                <h2 className="text-3xl md:text-5xl font-bold text-center mb-16 bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground">
                    The Free Workflow for Serious AI Development
                </h2>

                <div className="grid grid-cols-1 md:grid-cols-3 gap-12">
                    {features.map((feature, index) => (
                        <div key={index} className="text-center flex flex-col items-center">
                            <div className="mb-6 p-4 bg-primary/10 rounded-full">
                                {feature.icon}
                            </div>
                            <h3 className="text-2xl font-bold mb-4">{feature.title}</h3>
                            <p className="text-muted-foreground max-w-xs">
                                {feature.description}
                            </p>
                        </div>
                    ))}
                </div>
            </div>
        </section>
    );
};

export default HowItWorksSection;
</file_artifact>

<file path="src/components/home/MissionSection.tsx">
// src/components/home/MissionSection.tsx
// C11 - Use theme-aware text colors
'use client'; // LampContainer requires client-side rendering
import React from 'react';
import { LampContainer } from '@/components/global/lamp';
import { Button } from '@/components/ui/button';
import Link from 'next/link';
import { motion } from 'framer-motion';

const MissionSection = () => {
return (
<section className="w-full">
<LampContainer>
<motion.div
initial={{ opacity: 0.5, y: 100 }}
whileInView={{ opacity: 1, y: 0 }}
transition={{
delay: 0.3,
duration: 0.8,
ease: 'easeInOut',
}}
className="flex flex-col items-center text-center"
>
<h2 className="mt-8 bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground text-center text-4xl font-bold tracking-tight md:text-6xl">
THE RISE OF COGNITIVE CAPITALISM.
</h2>
<p className="text-xl text-muted-foreground max-w-3xl text-center my-8">
Mastering AI collaboration is essential for competitiveness and individual empowerment. The DCE is the foundational tool for a decentralized future, enabling Citizen Architects to combat AI centralization.
</p>
<Link href="/mission">
<Button size="lg" variant="outline" className='text-lg'>
Read Our Mission
</Button>
</Link>
</motion.div>
</LampContainer>
</section>
);
};

export default MissionSection;
</file_artifact>

<file path="src/components/home/WorkflowSection.tsx">
// src/components/home/WorkflowSection.tsx
// C11 - Use theme-aware colors for text and background
import React from 'react';

const workflowSteps = [
    { id: 1, title: "Curate Context" },
    { id: 2, title: "Generate Prompt" },
    { id: 3, title: "Parallel AI Responses" },
    { id: 4, title: "Test & Select" },
    { id: 5, title: "Integrate & Commit" },
];

const WorkflowSection = () => {
return (
<section className="py-20 md:py-32 bg-background">
<div className="container mx-auto px-4">
<h2 className="text-3xl md:text-5xl font-bold text-center mb-16 bg-clip-text text-transparent bg-gradient-to-b from-foreground to-muted-foreground">
The Power of Iteration: The DCE Workflow
</h2>

    <div className="flex flex-col md:flex-row justify-center items-center gap-4 md:gap-0">
      {workflowSteps.map((step, index) => (
        <React.Fragment key={step.id}>
          <div className="text-center p-6 border rounded-lg bg-card shadow-lg min-w-[200px] text-foreground">
            <span className="text-primary font-bold">{step.id}.</span> {step.title}
          </div>
          {index < workflowSteps.length - 1 && (
            <div className="text-2xl text-muted-foreground mx-4 hidden md:block">→</div>
          )}
        </React.Fragment>
      ))}
    </div>

    <p className="text-center mt-8 text-muted-foreground">
        (Interactive visualization coming soon)
    </p>
  </div>
</section>
);
};

export default WorkflowSection;
</file_artifact>

<file path="src/components/layout/Footer.tsx">
// src/components/layout/Footer.tsx
// C6 Update: Changed bg-transparent to bg-background to prevent content bleed-through
// C50 - Add Discord link
// C7 - Refactor to position text in corners
const Footer = () => {
return (
// Use a full-width container with padding
<footer className="w-full border-t border-neutral-900 bg-background relative z-10 px-4 sm:px-6 lg:px-8">
{/* Flex container to justify content between edges */}
<div className="flex flex-col md:flex-row items-center justify-between h-auto md:h-24 py-4 md:py-0 text-center md:text-left">
{/* Left-aligned text */}
<p className="text-sm text-muted-foreground">
Built in three days using the Data Curation Environment, with the only cost to purchase a domain ($6).

</p>
{/* Right-aligned text */}
<div className="flex items-center gap-4 mt-2 md:mt-0">
    <a
        href="https://discord.gg/HYurQXDWPm"
        target="_blank"
        rel="noreferrer"
        className="text-sm font-medium underline underline-offset-4 hover:text-primary transition-colors text-muted-foreground"
    >
        Join our Discord
    </a>
    <p className="text-sm text-muted-foreground">
        &copy; 2025 aiascent.dev. All rights reserved. Source code is available on{' '}
        <a
        href="https://github.com/dgerabagi/aiascent-dev"
        target="_blank"
        rel="noreferrer"
        className="font-medium underline underline-offset-4 hover:text-primary transition-colors"
        >
        GitHub
        </a>
        .
    </p>
</div>
</div>
</footer>
);
};

export default Footer;
</file_artifact>

<file path="src/components/layout/Header.tsx">
// src/components/layout/Header.tsx
'use client'; // C8: Needs to be client component for Dropdown interactions
import Link from 'next/link';
import { ModeToggle } from '@/components/global/mode-toggle';
import Image from 'next/image';
import { FaDiscord, FaGithub, FaChevronDown } from 'react-icons/fa';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';

const Header = () => {
  return (
    // Updated C3: Changed styling to match A16 (fixed, dark, blur)
    <header className="fixed top-0 z-50 w-full border-b border-border bg-background/80 backdrop-blur-lg">
      {/* C19 Fix: Changed to relative container to allow absolute positioning of nav */}
      <div className="container relative flex h-16 items-center px-4">

        {/* Logo and Title - Pushed to left */}
        <div className="flex items-center gap-2 mr-auto">
          <Link href="/" className="flex items-center gap-2">
            {/* Placeholder for Logo (A15.2) */}
            <Image
              src="/assets/logo.svg" // Placeholder path
              width={30}
              height={30}
              alt="AIAscent Logo"
              className="shadow-sm"
            />
            <span className="text-2xl font-bold">aiascent.dev</span>
          </Link>
        </div>

        {/* Navigation Links - Absolutely Centered */}
        {/* C99: Reordered Academy to be last */}
        <nav className="absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2 hidden md:flex items-center space-x-6 text-sm font-medium">
          <NavLink href="/">Home</NavLink>
          <NavLink href="/mission">Mission</NavLink>
          <NavLink href="/learn">Learn</NavLink>
          <NavLink href="/dce">DCE</NavLink>
          
          {/* C8: Showcase Dropdown */}
          {/* C9 Update: Added asChild to DropdownMenuItem to ensure menu closes on selection */}
          <DropdownMenu>
            <DropdownMenuTrigger className="flex items-center gap-1 transition-colors hover:text-foreground/80 text-foreground/60 outline-none">
              Showcase <FaChevronDown size={10} />
            </DropdownMenuTrigger>
            <DropdownMenuContent>
              <DropdownMenuItem asChild>
                <Link href="/showcase/report" className="w-full block cursor-pointer">The Ascent Report</Link>
              </DropdownMenuItem>
              <DropdownMenuItem asChild>
                <Link href="/showcase/anguilla" className="w-full block cursor-pointer">Anguilla Project</Link>
              </DropdownMenuItem>
              <DropdownMenuItem asChild>
                <Link href="/showcase/game" className="w-full block cursor-pointer">AI Ascent Game</Link>
              </DropdownMenuItem>
            </DropdownMenuContent>
          </DropdownMenu>

          <NavLink href="/academy">Academy</NavLink>
        </nav>

        {/* Right side (Actions/Toggle) - Pushed to right */}
        <div className="flex items-center justify-end gap-4 ml-auto">
          <a href="https://github.com/dgerabagi/data-curation-environment" target="_blank" rel="noopener noreferrer" className="text-foreground/60 hover:text-foreground/80 transition-colors" title="View on GitHub">
            <FaGithub size={22} />
          </a>
          <a href="https://discord.gg/HYurQXDWPm" target="_blank" rel="noopener noreferrer" className="text-foreground/60 hover:text-foreground/80 transition-colors" title="Join our Discord Community">
            <FaDiscord size={22} />
          </a>
          <ModeToggle />
          {/* Placeholder for Mobile Menu Icon */}
          <div className="md:hidden">
            {/* MenuIcon component would go here */}
          </div>
        </div>
      </div>
    </header>
  );
};

// Helper component for navigation links styling
const NavLink = ({ href, children }: { href: string; children: React.ReactNode }) => (
  <Link href={href} className="transition-colors hover:text-foreground/80 text-foreground/60">
    {children}
  </Link>
);

export default Header;
</file_artifact>

<file path="src/components/mission/MissionSectionBlock.tsx">
'use client';
{
  /*
  Cycle 107: Add `interactionType` prop to support different image hover/click behaviors.
  Cycle 54: Make image clickable to open fullscreen viewer.
  Cycle 31: Fix "use client" directive placement.
  Cycle 30: Fix unescaped entities.
  */
}

import React, { useState, useEffect } from 'react';
import Image from 'next/image';
import { motion, AnimatePresence } from 'framer-motion';
import MarkdownRenderer from '@/components/shared/MarkdownRenderer';
import { FaPlay, FaPause, FaSpinner, FaExpand } from 'react-icons/fa';
import { useReportState, useReportStore } from '@/stores/reportStore';

interface MissionSectionBlockProps {
  title: string;
  tldr: string;
  content: string;
  images: string[];
  imagePath: string;
  imagePrompt: string;
  imageSide?: 'left' | 'right';
  interactionType?: 'fullscreen' | 'zoom'; // C107: New prop
}

const MissionSectionBlock: React.FC<MissionSectionBlockProps> = ({
  title,
  tldr,
  content,
  images,
  imagePath,
  imagePrompt,
  imageSide = 'left',
  interactionType = 'fullscreen', // C107: Default to fullscreen
}) => {
  const [currentImageIndex, setCurrentImageIndex] = useState(0);
  const { playArbitraryText, openFullscreenMedia } = useReportStore.getState();
  const { genericPlaybackStatus, genericAudioText } = useReportState(state => ({
    genericPlaybackStatus: state.genericPlaybackStatus,
    genericAudioText: state.genericAudioText,
  }));

  const isPlayingThis = genericPlaybackStatus === 'playing' && genericAudioText === content;
  const isGeneratingThis = genericPlaybackStatus === 'generating' && genericAudioText === content;

  useEffect(() => {
    if (images.length > 1) {
      const timer = setInterval(() => {
        setCurrentImageIndex((prevIndex) => (prevIndex + 1) % images.length);
      }, 5000);
      return () => clearInterval(timer);
    }
  }, [images.length]);

  const variants = {
    enter: { opacity: 0, x: 20 },
    center: { opacity: 1, x: 0 },
    exit: { opacity: 0, x: -20 },
  };

  const handlePlayClick = () => {
    playArbitraryText(content);
  };

  const handleImageClick = () => {
    if (interactionType !== 'fullscreen') return;
    const fullImagePath = `/assets/images/report/${imagePath}${images[currentImageIndex]}`;
    openFullscreenMedia({ src: fullImagePath, description: imagePrompt });
  };

  const imageContent = (
    <div className="md:w-1/2 w-full p-4 border rounded-2xl bg-card shadow-2xl shadow-black/20 light:shadow-neutral-300/20">
      <div 
        className={`relative aspect-video rounded-lg overflow-hidden group ${interactionType === 'fullscreen' ? 'cursor-pointer' : ''}`}
        onClick={handleImageClick}
      >
        <AnimatePresence initial={false}>
          <motion.div
            key={currentImageIndex}
            initial="enter"
            animate="center"
            exit="exit"
            variants={variants}
            transition={{ duration: 0.5 }}
            className="absolute inset-0"
          >
            <Image
              src={`/assets/images/report/${imagePath}${images[currentImageIndex]}`}
              alt={title}
              fill
              sizes="(max-width: 768px) 100vw, 50vw"
              className={`object-cover ${interactionType === 'zoom' ? 'transition-transform duration-500 group-hover:scale-105' : ''}`}
              unoptimized={images[currentImageIndex].endsWith('.gif')}
            />
          </motion.div>
        </AnimatePresence>
        {interactionType === 'fullscreen' && (
            <div className="absolute inset-0 bg-black/50 opacity-0 group-hover:opacity-100 transition-opacity flex items-center justify-center">
                <FaExpand size={32} className="text-white/80" />
            </div>
        )}
      </div>
      <p className="text-xs italic text-muted-foreground mt-2 p-2 bg-black/20 rounded">
        <strong>Prompt:</strong> &quot;{imagePrompt}&quot;
      </p>
    </div>
  );

  const textContent = (
    <div className="md:w-1/2 w-full">
      <div className="flex items-center gap-4 mb-4">
        <h3 className="text-3xl font-bold">{title}</h3>
        <button
          onClick={handlePlayClick}
          className="p-2 border rounded-full text-muted-foreground hover:text-foreground hover:bg-accent transition-colors"
          title={isPlayingThis ? "Pause narration" : "Play narration"}
          disabled={isGeneratingThis}
        >
          {isGeneratingThis ? <FaSpinner className="animate-spin" /> : (isPlayingThis ? <FaPause /> : <FaPlay />)}
        </button>
      </div>
      <div className="p-3 border-l-4 border-primary bg-muted/20 rounded-r-lg mb-4">
        <p className="italic text-muted-foreground">{tldr}</p>
      </div>
      <div className="prose prose-sm dark:prose-invert max-w-none">
        <MarkdownRenderer>{content}</MarkdownRenderer>
      </div>
    </div>
  );

  return (
    <div
      className={`flex flex-col md:flex-row items-start gap-12 ${
        imageSide === 'right' ? 'md:flex-row-reverse' : ''
      }`}
    >
      {imageContent}
      {textContent}
    </div>
  );
};

export default MissionSectionBlock;
</file_artifact>

<file path="src/components/report-viewer/AudioControls.tsx">
'use client';
{
  /*
  Cycle 95: Add 'V S Code' replacement for TTS.
  Cycle 32: Fix exhaustive-deps warning.
  - Added `currentPageIndex` to the `useCallback` dependency array for `generateAndPlayAudio`.
  Cycle 30: Fix exhaustive-deps warnings.
  - Wrapped `generateAndPlayAudio` in `useCallback` and added it to the dependency array.
  - Added `setAudioDuration`, `setAudioTime`, and `setPlaybackStatus` to the second `useEffect` dependency array.
  */
}
// src/components/report-viewer/AudioControls.tsx

import React, { useRef, useEffect, useCallback } from 'react';
import { useReportStore, useReportState } from '@/stores/reportStore';
import { FaPlay, FaPause, FaRedo, FaVolumeUp, FaVolumeMute, FaSpinner } from 'react-icons/fa';

const PLAYBACK_SPEEDS = [0.75, 1.0, 1.25, 1.5, 1.75, 2.0];

const AudioControls: React.FC = () => {
  const {
    allPages, currentPageIndex, playbackStatus, autoplayEnabled,
    currentAudioUrl, currentAudioPageIndex, currentTime, duration,
    volume, isMuted, playbackSpeed,
  } = useReportState(state => ({
    allPages: state.allPages,
    currentPageIndex: state.currentPageIndex,
    playbackStatus: state.playbackStatus,
    autoplayEnabled: state.autoplayEnabled,
    currentAudioUrl: state.currentAudioUrl,
    currentAudioPageIndex: state.currentAudioPageIndex,
    currentTime: state.currentTime,
    duration: state.duration,
    volume: state.volume,
    isMuted: state.isMuted,
    playbackSpeed: state.playbackSpeed,
  }));
  
  const {
    setVolume, toggleMute, setPlaybackStatus, setAutoplay,
    setCurrentAudio, setAudioTime, setAudioDuration,
    setPlaybackSpeed, stopSlideshow
  } = useReportStore.getState();

  const audioRef = useRef<HTMLAudioElement>(null);
  const audioUrlRef = useRef<string | null>(null);
  const currentPage = allPages[currentPageIndex];

  const generateAndPlayAudio = useCallback(async (restart = false) => {
    if (!currentPage || !currentPage.pageTitle) {
      console.warn('[AudioControls] Attempted to generate audio with no current page or title.');
      return;
    };

    setPlaybackStatus('generating');
    let textToNarrate = `${currentPage.pageTitle}. ${currentPage.content}`;
    // C95: Replace "VS Code" with "V S Code" for better TTS pronunciation
    textToNarrate = textToNarrate.replace(/VS Code/g, 'V S Code');

    try {
      const response = await fetch('/api/tts', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: textToNarrate }),
      });

      if (!response.ok) throw new Error(`TTS server failed with status: ${response.status}`);

      const audioBlob = await response.blob();
      if (audioUrlRef.current) URL.revokeObjectURL(audioUrlRef.current);
      
      const newUrl = URL.createObjectURL(audioBlob);
      audioUrlRef.current = newUrl;
      setCurrentAudio(newUrl, currentPageIndex);
      if (restart && audioRef.current) audioRef.current.currentTime = 0;
    } catch (error) {
      console.error('[AudioControls] Failed to generate audio', error);
      setPlaybackStatus('error');
    }
  }, [currentPage, setCurrentAudio, setPlaybackStatus, currentPageIndex]);

  useEffect(() => {
    if (autoplayEnabled && playbackStatus === 'idle' && currentAudioPageIndex !== currentPageIndex) {
      generateAndPlayAudio();
    }
  }, [currentPageIndex, autoplayEnabled, playbackStatus, currentAudioPageIndex, generateAndPlayAudio]);

  useEffect(() => {
    const audio = audioRef.current;
    if (!audio) return;
    if (currentAudioUrl && audio.src !== currentAudioUrl) {
      audio.src = currentAudioUrl;
      audio.load();
      // C28 FIX: Force set playbackRate on new audio source to ensure it's not reset.
      audio.playbackRate = useReportStore.getState().playbackSpeed;
      audio.play().catch(e => console.error('[AudioControls] Autoplay failed', e));
    }
  }, [currentAudioUrl]);

  useEffect(() => {
    const audio = audioRef.current;
    if (!audio) return;
    audio.volume = volume;
    audio.muted = isMuted;
    audio.playbackRate = playbackSpeed;
  }, [volume, isMuted, playbackSpeed]);

  useEffect(() => {
    const audio = audioRef.current;
    if (!audio) return;

    const handlePlay = () => setPlaybackStatus('playing');
    const handlePause = () => setPlaybackStatus('paused');
    const handleEnded = () => setPlaybackStatus('idle');
    const handleTimeUpdate = () => setAudioTime(audio.currentTime);
    const handleLoadedMetadata = () => setAudioDuration(audio.duration);
    const handleWaiting = () => setPlaybackStatus('buffering');
    const handleError = () => { console.error('[AudioControls] Audio playback error'); setPlaybackStatus('error'); };

    audio.addEventListener('play', handlePlay);
    audio.addEventListener('playing', handlePlay);
    audio.addEventListener('pause', handlePause);
    audio.addEventListener('ended', handleEnded);
    audio.addEventListener('timeupdate', handleTimeUpdate);
    audio.addEventListener('loadedmetadata', handleLoadedMetadata);
    audio.addEventListener('waiting', handleWaiting);
    audio.addEventListener('error', handleError);

    return () => {
      audio.removeEventListener('play', handlePlay);
      audio.removeEventListener('playing', handlePlay);
      audio.removeEventListener('pause', handlePause);
      audio.removeEventListener('ended', handleEnded);
      audio.removeEventListener('timeupdate', handleTimeUpdate);
      audio.removeEventListener('loadedmetadata', handleLoadedMetadata);
      audio.removeEventListener('waiting', handleWaiting);
      audio.removeEventListener('error', handleError);
      if (audioUrlRef.current) URL.revokeObjectURL(audioUrlRef.current);
    };
  }, [setAudioDuration, setAudioTime, setPlaybackStatus]);

  const handlePlayPause = () => {
    stopSlideshow(true);
    const audio = audioRef.current;
    if (!audio) return;

    if (playbackStatus === 'playing' || playbackStatus === 'buffering') audio.pause();
    else if (playbackStatus === 'paused') audio.play().catch(e => console.error('[AudioControls] Resume play failed', e));
    else if (playbackStatus === 'idle' || playbackStatus === 'error') generateAndPlayAudio();
  };

  const handleRestart = () => { if (audioRef.current) audioRef.current.currentTime = 0; };
  const handleAutoplayChange = (checked: boolean) => {
    setAutoplay(checked);
    if (checked) generateAndPlayAudio(true);
  };
  const handleSeek = (e: React.ChangeEvent<HTMLInputElement>) => { if (audioRef.current) audioRef.current.currentTime = Number(e.target.value); };

  const formatTime = (time: number) => {
    if (isNaN(time) || !isFinite(time)) return '00:00';
    const minutes = Math.floor(time / 60);
    const seconds = Math.floor(time % 60);
    return `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
  };

  const isPlaying = playbackStatus === 'playing' || playbackStatus === 'buffering';

  return (
    <div className="flex items-center gap-2 px-1 py-1 text-xs text-muted-foreground w-full">
      <audio ref={audioRef} />
      
      {/* C28: Moved Autoplay checkbox to the left */}
      <label className={`flex items-center gap-2 cursor-pointer border rounded-md px-3 py-1 text-xs font-semibold transition-colors hover:bg-accent focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 ${autoplayEnabled ? 'bg-primary/20 border-primary' : 'border-border'}`}>
        <input type="checkbox" checked={autoplayEnabled} onChange={(e) => handleAutoplayChange(e.target.checked)} className="h-4 w-4 accent-primary" />
        Autoplay
      </label>

      <button className="btn-report-sm" onClick={handlePlayPause} title={isPlaying ? 'Pause' : 'Play'}>
        {isPlaying ? <FaPause /> : <FaPlay />}
      </button>
      <button className="btn-report-sm" onClick={handleRestart} title="Restart"><FaRedo /></button>

      <span className="min-w-[40px] text-center">{formatTime(currentTime)}</span>
      
      <input
        type="range"
        min="0"
        max={duration || 100}
        value={currentTime}
        onChange={handleSeek}
        className="flex-grow cursor-pointer"
        disabled={playbackStatus === 'generating' || playbackStatus === 'idle'}
      />

      <span className="min-w-[40px] text-center">{formatTime(duration)}</span>

      <button className="btn-report-sm" onClick={toggleMute} title={isMuted ? "Unmute" : "Mute"}>
        {isMuted ? <FaVolumeMute /> : <FaVolumeUp />}
      </button>
      <input
        type="range"
        min="0"
        max="1"
        step="0.01"
        value={volume}
        onChange={(e) => setVolume(Number(e.target.value))}
        className="w-20 cursor-pointer"
        title={`Volume: ${Math.round(volume * 100)}%`}
      />

      <div className="italic min-w-[70px] text-center">
        {playbackStatus === 'generating' && <FaSpinner className="animate-spin inline-block" />}
        {playbackStatus === 'buffering' && 'Buffering...'}
        {playbackStatus === 'error' && 'Error!'}
      </div>

      <select
        value={playbackSpeed}
        onChange={(e) => setPlaybackSpeed(Number(e.target.value))}
        className="bg-muted border rounded p-1 text-xs"
        title="Playback Speed"
      >
        {PLAYBACK_SPEEDS.map(speed => (
          <option key={speed} value={speed}>{speed.toFixed(2)}x</option>
        ))}
      </select>
    </div>
  );
};

export default AudioControls;
</file_artifact>

<file path="src/components/report-viewer/ImageNavigator.tsx">
// src/components/report-viewer/ImageNavigator.tsx
import React from 'react';
import { useReportState, useReportStore } from '@/stores/reportStore';
import { FaChevronLeft, FaChevronRight, FaCommentDots, FaTree, FaInfoCircle, FaChevronUp, FaChevronDown, FaExpand, FaCompress } from 'react-icons/fa';

interface ImageNavigatorProps {
  viewerRef: React.RefObject<HTMLDivElement>;
}

const ImageNavigator: React.FC<ImageNavigatorProps> = ({ viewerRef }) => {
  const { allPages, currentPageIndex, currentImageIndex, isPromptVisible, isFullscreen } = useReportState(state => ({
    allPages: state.allPages,
    currentPageIndex: state.currentPageIndex,
    currentImageIndex: state.currentImageIndex,
    isPromptVisible: state.isPromptVisible,
    isFullscreen: state.isFullscreen,
  }));
  
  const { prevPage, nextPage, prevImage, nextImage, toggleTreeNav, toggleChatPanel, togglePromptVisibility, toggleFullscreen } = useReportStore.getState();

  const currentPage = allPages[currentPageIndex];
  const currentPrompt = currentPage?.imagePrompts;
  const totalImages = currentPrompt?.[0]?.images.length ?? 0;

  return (
    <div className="flex items-center justify-between gap-4 text-xs text-muted-foreground w-full py-1">
      {/* Left Group */}
      <div className="flex items-center gap-2">
        <button className="btn-report" onClick={toggleTreeNav} title="Toggle Page Tree"><FaTree /></button>
        <button className="btn-report" onClick={togglePromptVisibility} title={isPromptVisible ? "Hide Image Prompt" : "Show Image Prompt"}><FaInfoCircle /></button>
        <button className="btn-report" onClick={() => toggleFullscreen(viewerRef.current)} title={isFullscreen ? "Exit Fullscreen" : "Enter Fullscreen"}>
          {isFullscreen ? <FaCompress /> : <FaExpand />}
        </button>
      </div>

      {/* Center Group */}
      <div className="flex items-center gap-4">
        {/* Page Nav */}
        <div className="flex items-center gap-2">
          <button className="btn-report-lg" onClick={prevPage} title="Previous Page (Up Arrow)"><FaChevronUp /></button>
          <span>Page {currentPageIndex + 1}/{allPages.length}</span>
          <button className="btn-report-lg" onClick={nextPage} title="Next Page (Down Arrow)"><FaChevronDown /></button>
        </div>
        {/* Image Nav - C22 FIX: Conditionally render */}
        {totalImages > 1 && (
            <div className="flex items-center gap-2">
                <button className="btn-report-lg" onClick={prevImage} disabled={totalImages <= 1} title="Previous Image (Left Arrow)"><FaChevronLeft /></button>
                <span>Image {currentImageIndex + 1}/{totalImages}</span>
                <button className="btn-report-lg" onClick={nextImage} disabled={totalImages <= 1} title="Next Image (Right Arrow)"><FaChevronRight /></button>
            </div>
        )}
      </div>

      {/* Right Group */}
      <div className="flex items-center gap-2">
        <button className="btn-report" onClick={toggleChatPanel} title="Ask @Ascentia about this page">
          <FaCommentDots /> Ask
        </button>
      </div>
      
      <style jsx>{`
        .btn-report {
          background: none;
          border: 1px solid hsl(var(--border));
          color: hsl(var(--muted-foreground));
          font-size: 14px;
          cursor: pointer;
          padding: 5px;
          border-radius: 4px;
          display: flex;
          align-items: center;
          gap: 5px;
        }
        .btn-report:hover {
            background-color: hsl(var(--accent));
        }
        .btn-report-lg {
            background: none;
            border: 1px solid hsl(var(--border));
            color: hsl(var(--muted-foreground));
            font-size: 16px;
            cursor: pointer;
            padding: 5px 10px;
            border-radius: 4px;
        }
        .btn-report-lg:hover {
            background-color: hsl(var(--accent));
        }
        .btn-report:disabled, .btn-report-lg:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
      `}</style>
    </div>
  );
};

export default ImageNavigator;
</file_artifact>

<file path="src/components/report-viewer/PageNavigator.tsx">
// src/components/report-viewer/PageNavigator.tsx
import React from 'react';
import { useReportState } from '@/stores/reportStore';

const PageNavigator: React.FC = () => {
  const { allPages, currentPageIndex } = useReportState(state => ({
    allPages: state.allPages,
    currentPageIndex: state.currentPageIndex,
  }));

  const currentPage = allPages[currentPageIndex];

  if (!currentPage) return null;

  return (
    // C7 Update: Reduced min-height from 40px to 32px and adjusted text size for compactness
    <div className="flex justify-center items-center w-full relative min-h-[32px]">
      <h2 className="text-base font-bold text-primary text-center px-12 truncate" title={currentPage.pageTitle}>
        {currentPage.pageTitle}
      </h2>
    </div>
  );
};

export default PageNavigator;
</file_artifact>

<file path="src/components/report-viewer/PromptNavigator.tsx">
{
  /*
  Cycle 30: Fix unescaped entities.
  - Replaced double quotes with &quot; to fix linting errors.
  */
}
// src/components/report-viewer/PromptNavigator.tsx
import React from 'react';
import { useReportState } from '@/stores/reportStore';

const PromptNavigator: React.FC = () => {
  const { allPages, currentPageIndex } = useReportState(state => ({
    allPages: state.allPages,
    currentPageIndex: state.currentPageIndex,
  }));

  const currentPage = allPages[currentPageIndex];
  const currentPrompt = currentPage?.imagePrompts?.[0];

  if (!currentPrompt?.promptText) return null;

  return (
    <div className="w-full text-left italic leading-relaxed text-muted-foreground text-xs p-2 bg-muted/50 rounded border-dashed border mb-4">
      &quot;{currentPrompt.promptText}&quot;
    </div>
  );
};

export default PromptNavigator;
</file_artifact>

<file path="src/components/report-viewer/ReportChatPanel.tsx">
// src/components/report-viewer/ReportChatPanel.tsx
'use client';
import React, { useEffect, useRef, useState } from 'react';
import { useReportStore, useReportState } from '@/stores/reportStore';
import { FaTimes, FaBroom, FaSpinner, FaSync } from 'react-icons/fa';
import MarkdownRenderer from '@/components/shared/MarkdownRenderer';
import { Badge } from '@/components/ui/badge';
import type { ChatMessage } from '@/stores/reportStore';
import { getKnowledgeBase } from '@/lib/kb-helper';

const ReportChatPanel: React.FC = () => {
    const { 
        toggleChatPanel, clearReportChatHistory,
        setReportChatMessage, fetchConversationSuggestions,
        regenerateSuggestions,
    } = useReportStore.getState();
    const { 
        reportName, allPages, currentPageIndex, reportChatHistory, reportChatInput, setReportChatInput, 
        addReportChatMessage, updateReportChatMessage, updateReportChatStatus, suggestedPrompts,
        suggestionsStatus
    } = useReportState(state => ({
        reportName: state.reportName,
        allPages: state.allPages,
        currentPageIndex: state.currentPageIndex,
        reportChatHistory: state.reportChatHistory,
        reportChatInput: state.reportChatInput,
        setReportChatInput: state.setReportChatInput,
        addReportChatMessage: state.addReportChatMessage,
        updateReportChatMessage: state.updateReportChatMessage,
        updateReportChatStatus: state.updateReportChatStatus,
        suggestedPrompts: state.suggestedPrompts,
        suggestionsStatus: state.suggestionsStatus,
    }));
    
    const [isThinking, setIsThinking] = useState(false);
    const textareaRef = useRef<HTMLTextAreaElement>(null);

    const currentPage = allPages[currentPageIndex];
    const chatContainerRef = useRef<HTMLDivElement>(null);

    const showSuggestions = true;

    useEffect(() => {
        if (chatContainerRef.current) {
            chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
        }
        if (!isThinking) textareaRef.current?.focus();
    }, [reportChatHistory, isThinking]);

    const handlePanelKeyDown = (e: React.KeyboardEvent) => {
        e.stopPropagation();
    };

    const parseFinalMessage = (rawText: string): string => {
        const finalMessageMarker = '<|channel|>final<|message|>';
        const finalMessageIndex = rawText.lastIndexOf(finalMessageMarker);
    
        if (finalMessageIndex !== -1) {
            return rawText.substring(finalMessageIndex + finalMessageMarker.length);
        }
        
        const analysisRegex = /<\|channel\|>analysis<\|message\|>[\s\S]*?(?=<\|channel\|>|$)/g;
        let cleanedText = rawText.replace(analysisRegex, '').trim();
        
        return cleanedText;
    };

    const sendMessage = async (text: string) => {
        if (isThinking) return;

        addReportChatMessage({ author: 'You', flag: '👤', message: text, channel: 'local' });
        const temporaryId = `report_ascentia_response_${Date.now()}`;
        addReportChatMessage({ id: temporaryId, author: 'Ascentia', flag: '🤖', message: '', status: 'thinking', channel: 'system' });
        setIsThinking(true);
        setReportChatInput('');

        const pageContext = `Page Title: ${currentPage?.pageTitle || 'N/A'}\nTL;DR: ${currentPage?.tldr || 'N/A'}\nContent: ${currentPage?.content || 'N/A'}`;
        
        const knowledgeBase = getKnowledgeBase(reportName);

        try {
            const controller = new AbortController();
            const timeoutId = setTimeout(() => controller.abort(), 300000);

            const response = await fetch('/api/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ 
                    prompt: text, 
                    pageContext,
                    knowledgeBase: knowledgeBase,
                    reportName: reportName,
                }),
                signal: controller.signal,
            });

            clearTimeout(timeoutId);

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`${response.status} ${errorText}`);
            }

            if (!response.body) throw new Error("No response body");

            const reader = response.body.getReader();
            const decoder = new TextDecoder();
            let done = false;
            let fullMessage = '';
            
            updateReportChatStatus(temporaryId, 'streaming');
            while (!done) {
                const { value, done: doneReading } = await reader.read();
                done = doneReading;
                const chunk = decoder.decode(value, { stream: true });
                
                const lines = chunk.split('\n');
                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = line.substring(6);
                        if (data.trim() === '[DONE]') continue;
                        try {
                            const parsed = JSON.parse(data);
                            const textChunk = parsed.choices?.[0]?.text || '';
                            if (textChunk) {
                                fullMessage += textChunk;
                                updateReportChatMessage(temporaryId, textChunk);
                            }
                        } catch (e) {
                            if (data) {
                                fullMessage += data;
                                updateReportChatMessage(temporaryId, data);
                            }
                        }
                    }
                }
            }

            const finalContent = parseFinalMessage(fullMessage.trim());
            setReportChatMessage(temporaryId, finalContent);
            updateReportChatStatus(temporaryId, 'complete');

            if (showSuggestions) {
                const finalHistory = [
                    ...useReportStore.getState().reportChatHistory, 
                    { author: 'Ascentia', flag: '🤖', message: finalContent, channel: 'system', status: 'complete' } as ChatMessage
                ];
                fetchConversationSuggestions(finalHistory);
            }

        } catch (error: unknown) {
            console.error("Error with chat stream:", error);
            let userFriendlyError = "An unknown error occurred.";

            if (error instanceof Error) {
                if (error.name === 'AbortError') {
                    userFriendlyError = "Connection timed out. The AI server might be waking up or offline.";
                } else if (error.message.includes('502') || error.message.includes('Failed to fetch')) {
                    userFriendlyError = "Could not connect to the AI server. Please check your network connection or firewall.";
                } else {
                    userFriendlyError = `Error: ${error.message}`;
                }
            }
            
            updateReportChatMessage(temporaryId, userFriendlyError);
            updateReportChatStatus(temporaryId, 'complete');
        } finally {
            setIsThinking(false);
        }
    };

    const handleInputKeyDown = (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
        e.stopPropagation(); // Prevent report navigation
        if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            const trimmedInput = reportChatInput.trim();
            if (trimmedInput) {
                sendMessage(trimmedInput);
            }
        }
    };

    const handleChipClick = (prompt: string) => {
        sendMessage(prompt);
    };

    const getKnowledgeBaseName = (name: string | null) => {
        const kb = getKnowledgeBase(name);
        switch (kb) {
            case 'dce': return 'DCE Docs';
            case 'academy': return 'Academy KB';
            case 'anguilla': return 'Anguilla KB';
            default: return 'Report KB';
        }
    };

    return (
        <div className="h-full bg-background border-l border-border flex flex-col flex-shrink-0" onKeyDown={handlePanelKeyDown}>
            <header className="flex justify-between items-center p-2 border-b border-border flex-shrink-0 bg-muted/30">
                <h3 className="font-bold text-sm flex items-center gap-2">
                    Ask @Ascentia
                    <Badge variant="outline" className="text-[10px] px-1 py-0 border-primary/50 text-primary">
                        {getKnowledgeBaseName(reportName)}
                    </Badge>
                </h3>
                <div>
                    <button 
                        className="p-2 text-muted-foreground hover:text-foreground transition-colors rounded-md hover:bg-accent"
                        onClick={() => { 
                            clearReportChatHistory(currentPage?.pageTitle || "Report"); 
                            setTimeout(() => textareaRef.current?.focus(), 0); 
                        }} 
                        title="Clear Chat History"
                    >
                        <FaBroom />
                    </button>
                    <button 
                        className="p-2 text-muted-foreground hover:text-foreground transition-colors rounded-md hover:bg-accent"
                        onClick={toggleChatPanel} 
                        title="Close Chat Panel"
                    >
                        <FaTimes />
                    </button>
                </div>
            </header>
            
            <div ref={chatContainerRef} className="flex-1 p-3 overflow-y-auto text-sm space-y-4 scroll-smooth">
                {reportChatHistory.map((msg, index) => (
                    <div key={msg.id || index} className={`flex flex-col ${msg.author === 'You' ? 'items-end' : 'items-start'}`}>
                        <div className={`flex items-center gap-1 text-xs mb-1 ${msg.author === 'You' ? 'text-primary' : 'text-cyan-500'}`}>
                            <span>{msg.flag}</span>
                            <span className="font-bold">{msg.author}</span>
                        </div>
                        <div className={`rounded-lg p-2 max-w-[90%] ${msg.author === 'You' ? 'bg-primary text-primary-foreground' : 'bg-muted'}`}>
                            {msg.status === 'thinking' ? (
                                <span className="italic flex items-center gap-1 text-muted-foreground">Thinking <span className="animate-pulse">...</span></span>
                            ) : (
                                <div className={`prose prose-sm max-w-none prose-p:my-1 prose-li:my-0 ${msg.author === 'You' ? 'prose-invert' : 'dark:prose-invert'}`}>
                                    <MarkdownRenderer>{parseFinalMessage(msg.message)}</MarkdownRenderer>
                                </div>
                            )}
                        </div>
                        {msg.status === 'streaming' && <span className="text-[10px] text-muted-foreground animate-pulse mt-1">Typing...</span>}
                    </div>
                ))}
            </div>

            {showSuggestions && (
                <div className="p-2 border-t border-border bg-muted/20">
                    <div className="flex justify-between items-center mb-2 px-1">
                        <h4 className="text-xs font-semibold text-muted-foreground">Suggested Questions</h4>
                        <button
                            onClick={regenerateSuggestions}
                            className="p-1 text-muted-foreground hover:text-foreground disabled:opacity-50"
                            title="Generate new suggestions"
                            disabled={suggestionsStatus === 'loading'}
                        >
                            <FaSync className={suggestionsStatus === 'loading' ? 'animate-spin' : ''} />
                        </button>
                    </div>
                    <div className="flex gap-2 flex-wrap items-center justify-center min-h-[40px]">
                        {suggestionsStatus === 'loading' && (
                            <div className="flex items-center gap-2 text-xs text-muted-foreground italic">
                                <FaSpinner className="animate-spin" />
                                Generating suggestions...
                            </div>
                        )}
                        {suggestionsStatus !== 'loading' && suggestedPrompts.map((prompt, index) => (
                            <Badge
                                key={index}
                                variant="secondary"
                                className="cursor-pointer hover:bg-primary hover:text-primary-foreground transition-colors text-xs max-w-xs whitespace-normal text-center"
                                onClick={() => handleChipClick(prompt)}
                                title={prompt}
                            >
                                {prompt}
                            </Badge>
                        ))}
                    </div>
                </div>
            )}

            {/* C7 Update: Added pb-6 to ensure input isn't cut off by bottom of screen/footer */}
            <footer className="p-3 pb-6 border-t border-border bg-background flex-shrink-0">
                <textarea
                    ref={textareaRef}
                    className="w-full bg-muted border border-input rounded-md p-2 text-sm resize-none focus:outline-none focus:ring-1 focus:ring-ring placeholder:text-muted-foreground"
                    placeholder={isThinking ? "Ascentia is thinking..." : "Ask a question... (Enter to send, Shift+Enter for newline)"}
                    value={reportChatInput}
                    onChange={(e) => setReportChatInput(e.target.value)}
                    onKeyDown={handleInputKeyDown}
                    disabled={isThinking}
                    rows={3}
                />
                <div className="text-[10px] text-muted-foreground text-right mt-1">
                    Powered by local LLM (RAG)
                </div>
            </footer>
        </div>
    );
};

export default ReportChatPanel;
</file_artifact>

<file path="src/components/report-viewer/ReportProgressBar.tsx">
// src/components/report-viewer/ReportProgressBar.tsx
'use client';
import React from 'react';
import { useReportState, useReportStore } from '@/stores/reportStore';

const ReportProgressBar: React.FC = () => {
  const { allPages, currentPageIndex } = useReportState(state => ({
    allPages: state.allPages,
    currentPageIndex: state.currentPageIndex,
  }));
  const { goToPageByIndex } = useReportStore.getState();

  const totalPages = allPages.length;
  if (totalPages === 0) return null;

  const progressPercent = totalPages > 0 ? ((currentPageIndex + 1) / totalPages) * 100 : 0;

  const handleBarClick = (e: React.MouseEvent<HTMLDivElement>) => {
    const bar = e.currentTarget;
    const rect = bar.getBoundingClientRect();
    const clickX = e.clientX - rect.left;
    const clickPercent = clickX / rect.width;
    const targetPageIndex = Math.floor(clickPercent * totalPages);
    goToPageByIndex(targetPageIndex);
  };

  return (
    <div className="w-full py-2 flex items-center gap-2">
      <div
        className="flex-grow h-3 bg-muted rounded-full border cursor-pointer relative"
        onClick={handleBarClick}
        title={`Page ${currentPageIndex + 1} of ${totalPages} (${progressPercent.toFixed(0)}%)`}
      >
        <div
          className="h-full bg-primary rounded-full transition-all duration-300 ease-in-out"
          style={{ width: `${progressPercent}%` }}
        />
        <div className="absolute inset-0 flex items-center justify-center">
            {/* C47 FIX: Changed text-primary-foreground to text-foreground for better contrast with mix-blend-difference */}
            <span className="text-xs font-bold text-foreground mix-blend-difference">
                {progressPercent.toFixed(0)}%
            </span>
        </div>
      </div>
    </div>
  );
};

export default ReportProgressBar;
</file_artifact>

<file path="src/components/report-viewer/ReportTreeNav.tsx">
// src/components/report-viewer/ReportTreeNav.tsx
import React from 'react';
import { useReportState, useReportStore } from '@/stores/reportStore';
import { FaChevronDown, FaChevronRight } from 'react-icons/fa';
import type { RawReportSection, RawSubSection, RawReportPage } from '@/stores/reportStore';

const ReportTreeNav: React.FC = () => {
  const { reportData, currentPageIndex, expandedSections } = useReportState(state => ({
    reportData: state.reportData,
    currentPageIndex: state.currentPageIndex,
    expandedSections: state.expandedSections,
  }));
  const { goToPageByIndex, toggleSectionExpansion } = useReportStore.getState();

  if (!reportData) return null;

  let pageCounter = 0;

  return (
    <div className="w-64 min-w-[250px] h-full bg-black/10 dark:bg-black/30 border-r p-2 overflow-y-auto flex-shrink-0">
      <h3 className="text-sm font-bold mt-0 mb-2">Report Navigator</h3>
      {reportData.sections.map((section: RawReportSection) => {
        const isSectionExpanded = expandedSections[section.sectionId] ?? false;
        const sectionPageStartIndex = pageCounter;

        let sectionPageCount = (section.pages || []).length;
        if (section.subSections) {
          sectionPageCount += section.subSections.reduce((acc, sub) => acc + (sub.pages || []).length, 0);
        }
        pageCounter += sectionPageCount;

        return (
          <div key={section.sectionId}>
            <div className="text-xs text-primary cursor-pointer flex items-center gap-1 mb-1 font-bold" onClick={() => toggleSectionExpansion(section.sectionId)}>
              {isSectionExpanded ? <FaChevronDown /> : <FaChevronRight />}
              {section.sectionTitle}
            </div>
            {isSectionExpanded && (
              <div className="pl-2">
                {(section.pages || []).map((page: RawReportPage, index: number) => {
                  const globalPageIndex = sectionPageStartIndex + index;
                  const isActive = globalPageIndex === currentPageIndex;
                  return (
                    <div
                      key={page.pageId}
                      className={`text-xs py-1 px-2 cursor-pointer block border-l-2 transition-all ${isActive ? 'text-amber-500 font-bold border-amber-500' : 'text-muted-foreground border-transparent hover:bg-accent'}`}
                      onClick={() => goToPageByIndex(globalPageIndex)}
                    >
                      {page.pageTitle}
                    </div>
                  );
                })}
                {section.subSections && (() => {
                  let subSectionPageCounter = sectionPageStartIndex + (section.pages || []).length;
                  return section.subSections.map((subSection: RawSubSection) => {
                    const isSubSectionExpanded = expandedSections[subSection.subSectionId] ?? false;
                    const startIndex = subSectionPageCounter;
                    subSectionPageCounter += (subSection.pages || []).length;

                    return (
                      <div key={subSection.subSectionId} className="pl-2">
                        <div className="text-xs text-primary/80 cursor-pointer flex items-center gap-1 my-1" onClick={() => toggleSectionExpansion(subSection.subSectionId)}>
                          {isSubSectionExpanded ? <FaChevronDown /> : <FaChevronRight />}
                          {subSection.subSectionTitle}
                        </div>
                        {isSubSectionExpanded && (
                          (subSection.pages || []).map((page: RawReportPage, index: number) => {
                            const globalPageIndex = startIndex + index;
                            const isActive = globalPageIndex === currentPageIndex;
                            return (
                              <div
                                key={page.pageId}
                                className={`text-xs py-1 px-2 cursor-pointer block border-l-2 ml-2 transition-all ${isActive ? 'text-amber-500 font-bold border-amber-500' : 'text-muted-foreground border-transparent hover:bg-accent'}`}
                                onClick={() => goToPageByIndex(globalPageIndex)}
                              >
                                {page.pageTitle}
                              </div>
                            );
                          })
                        )}
                      </div>
                    );
                  });
                })()}
              </div>
            )}
          </div>
        );
      })}
    </div>
  );
};

export default ReportTreeNav;
</file_artifact>

<file path="src/components/report-viewer/ReportViewer.tsx">
// src/components/report-viewer/ReportViewer.tsx
'use client';

import React, { useEffect, useRef } from 'react';
import { useReportStore, useReportState } from '@/stores/reportStore';
import PageNavigator from './PageNavigator';
import ImageNavigator from './ImageNavigator';
import PromptNavigator from './PromptNavigator';
import ReportTreeNav from './ReportTreeNav';
import ReportChatPanel from './ReportChatPanel';
import ReportProgressBar from './ReportProgressBar';
import AudioControls from './AudioControls';
import { Resizable } from 're-resizable';
import Image from 'next/image';
import MarkdownRenderer from '@/components/shared/MarkdownRenderer';
import type { ReportContentData, ImageManifestData } from '@/stores/reportStore';


interface ReportViewerProps {
    reportName: string;
}

const ReportViewer: React.FC<ReportViewerProps> = ({ reportName }) => {
    const { loadReport, handleKeyDown, setChatPanelWidth, startSlideshow, fetchPageSuggestions, setIsFullscreen, openFullscreenMedia } = useReportStore.getState();
    const {
        _hasHydrated,
        allPages, currentPageIndex, currentImageIndex, isTreeNavOpen, isChatPanelOpen,
        imagePanelHeight, setImagePanelHeight, isPromptVisible, isTldrVisible, isContentVisible, isLoading,
        chatPanelWidth, playbackStatus, autoplayEnabled, isFullscreen
    } = useReportState(state => ({
        _hasHydrated: state._hasHydrated,
        allPages: state.allPages,
        currentPageIndex: state.currentPageIndex,
        currentImageIndex: state.currentImageIndex,
        isTreeNavOpen: state.isTreeNavOpen,
        isChatPanelOpen: state.isChatPanelOpen,
        imagePanelHeight: state.imagePanelHeight,
        setImagePanelHeight: state.setImagePanelHeight,
        isPromptVisible: state.isPromptVisible,
        isTldrVisible: state.isTldrVisible,
        isContentVisible: state.isContentVisible,
        isLoading: state.isLoading,
        chatPanelWidth: state.chatPanelWidth,
        playbackStatus: state.playbackStatus,
        autoplayEnabled: state.autoplayEnabled,
        isFullscreen: state.isFullscreen,
    }));

    const viewerRef = useRef<HTMLDivElement>(null);

    useEffect(() => {
        // C74: Fetch data within the component for static reports
        // C4 Fix: Added 'anguilla' to the allowed list for self-fetching reports
        if (reportName !== 'whitepaper' && reportName !== 'showcase' && reportName !== 'anguilla') {
            return; // Data for V2V is loaded by the parent /academy page
        }
        
        const loadStaticReport = async () => {
            try {
                const [contentRes, manifestRes] = await Promise.all([
                    fetch(`/data/${reportName}_content.json`),
                    fetch(`/data/${reportName}_imagemanifest.json`),
                ]);

                if (!contentRes.ok) throw new Error(`Failed to fetch ${reportName}_content.json`);
                if (!manifestRes.ok) throw new Error(`Failed to fetch ${reportName}_imagemanifest.json`);

                const reportData: ReportContentData = await contentRes.json();
                const imageManifest: ImageManifestData = await manifestRes.json();
                
                loadReport(reportData, imageManifest);

            } catch (error) {
                console.error(`Failed to load static report data for ${reportName}:`, error);
            }
        };

        loadStaticReport();
    }, [loadReport, reportName]);

    const currentPage = allPages[currentPageIndex];

    useEffect(() => {
        if (currentPage) {
            fetchPageSuggestions(currentPage);
        }
    }, [currentPage, fetchPageSuggestions]);

    useEffect(() => {
        window.addEventListener('keydown', handleKeyDown);
        return () => window.removeEventListener('keydown', handleKeyDown);
    }, [handleKeyDown]);

    useEffect(() => {
        const handleFullscreenChange = () => {
            setIsFullscreen(!!document.fullscreenElement);
        };
        document.addEventListener('fullscreenchange', handleFullscreenChange);
        return () => document.removeEventListener('fullscreenchange', handleFullscreenChange);
    }, [setIsFullscreen]);

    useEffect(() => {
        if (playbackStatus === 'playing' && autoplayEnabled) {
            startSlideshow();
        }
    }, [playbackStatus, autoplayEnabled, startSlideshow]);
    
    const currentPrompt = currentPage?.imagePrompts?.[0];
    const currentImage = currentPrompt?.images?.[currentImageIndex];

    if (!_hasHydrated || isLoading) {
        return (
            <div className="flex items-center justify-center h-full w-full">
                <p className="text-2xl text-muted-foreground animate-pulse">Loading Report...</p>
            </div>
        );
    }

    if (!currentPage) {
        return (
            <div className="flex items-center justify-center h-full w-full">
                <p className="text-2xl text-red-500">Could not load report data.</p>
            </div>
        );
    }
    
    const handleImageClick = () => {
        if (currentImage) {
            const isLab = reportName.startsWith('v2v-academy-lab');
            const payload = { 
                src: currentImage.url, 
                description: currentImage.prompt,
                ...(isLab && { content: currentPage.content })
            };
            openFullscreenMedia(payload);
        }
    };

    return (
        <div ref={viewerRef} className={`h-full w-full bg-background text-foreground flex ${isFullscreen ? 'fixed inset-0 z-[100]' : ''}`}>
            {isTreeNavOpen && <ReportTreeNav />}
            <div className="flex-1 flex flex-col min-w-0">
                {/* C7 Update: Reduced padding from p-2 to p-1 to save vertical space */}
                <header className="p-1 border-b flex-shrink-0">
                    <PageNavigator />
                </header>
                <div className="p-2 border-b flex-shrink-0">
                    <ReportProgressBar />
                </div>
                <main className="flex-1 flex flex-col p-2 overflow-hidden">
                    <Resizable
                        size={{ width: '100%', height: imagePanelHeight }}
                        minHeight={200}
                        maxHeight="60%"
                        onResizeStop={(e, direction, ref, d) => {
                            setImagePanelHeight(imagePanelHeight + d.height);
                        }}
                        enable={{ bottom: true }}
                        className="relative mb-2 flex-shrink-0"
                    >
                        <div className="w-full h-full bg-black/50 border rounded-lg flex items-center justify-center overflow-hidden relative">
                            {currentImage?.url ? (
                                <Image
                                    src={currentImage.url}
                                    alt={currentImage.alt}
                                    fill
                                    sizes="100vw"
                                    className="object-contain cursor-pointer"
                                    onClick={handleImageClick}
                                    unoptimized // Good for gifs, but also for webp from local
                                />
                            ) : <p>No Image Available</p>}
                        </div>
                    </Resizable>
                    
                    <div className="border-y p-1 flex-shrink-0">
                        <ImageNavigator viewerRef={viewerRef} />
                        <AudioControls />
                    </div>

                    <div className="flex-1 overflow-y-auto p-2 mt-2 space-y-4 prose prose-sm dark:prose-invert max-w-none">
                        {isPromptVisible && <PromptNavigator />}
                        {isTldrVisible && (
                            <div className="p-2 border-l-4 rounded bg-muted">
                                <p className="font-semibold">TL;DR:</p>
                                <p className="italic">{currentPage.tldr}</p>
                            </div>
                        )}
                        {isContentVisible && (
                            <MarkdownRenderer>{currentPage.content || ''}</MarkdownRenderer>
                        )}
                    </div>
                </main>
            </div>
            {isChatPanelOpen && (
                <Resizable
                    size={{ width: chatPanelWidth, height: '100%' }}
                    minWidth={300}
                    maxWidth="60vw"
                    enable={{ left: true }}
                    onResizeStop={(e, direction, ref, d) => {
                        setChatPanelWidth(chatPanelWidth + d.width);
                    }}
                    handleClasses={{ left: 'border-l-4 border-transparent hover:border-primary transition-colors duration-200' }}
                >
                    <ReportChatPanel />
                </Resizable>
            )}
        </div>
    );
};

export default ReportViewer;
</file_artifact>

<file path="src/components/report-viewer/ReportViewerModal.tsx">
// src/components/report-viewer/ReportViewerModal.tsx
// C11 - Ported from aiascentgame context, will be adapted to ReportViewer.tsx
import React from 'react';

const ReportViewer: React.FC = () => {
  // Placeholder implementation for aiascent.dev
  return (
    <div>
        <h2>Report Viewer</h2>
        <p>This component will display the interactive report. Implementation is in progress.</p>
    </div>
  );
};

export default ReportViewer;
</file_artifact>

<file path="src/components/shared/MarkdownRenderer.tsx">
'use client';
import React, { useState } from 'react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import rehypeRaw from 'rehype-raw';
import { FaCopy, FaCheck } from 'react-icons/fa';

interface MarkdownRendererProps {
  children: string;
}

const MarkdownRenderer: React.FC<MarkdownRendererProps> = ({ children }) => {
  const [copiedStates, setCopiedStates] = useState<Record<number, boolean>>({});

  const handleCopy = (code: string, index: number) => {
    navigator.clipboard.writeText(code).then(() => {
      setCopiedStates(prev => ({ ...prev, [index]: true }));
      setTimeout(() => {
        setCopiedStates(prev => ({ ...prev, [index]: false }));
      }, 2000);
    });
  };

  return (
    <ReactMarkdown
      remarkPlugins={[remarkGfm]}
      rehypePlugins={[rehypeRaw]}
      components={{
        p: ({ node, ...props }) => <p {...props} />,
        h1: ({ node, ...props }) => <h1 className="text-2xl font-bold my-4" {...props} />,
        h2: ({ node, ...props }) => <h2 className="text-xl font-bold my-3" {...props} />,
        h3: ({ node, ...props }) => <h3 className="text-lg font-bold my-2" {...props} />,
        ul: ({ node, ...props }) => <ul className="list-disc list-inside my-2 space-y-1" {...props} />,
        ol: ({ node, ...props }) => <ol className="list-decimal list-inside my-2 space-y-1" {...props} />,
        li: ({ node, ...props }) => <li className="ml-4" {...props} />,
        strong: ({ node, ...props }) => <strong className="font-bold" {...props} />,
        em: ({ node, ...props }) => <em className="italic" {...props} />,
        table: ({ node, ...props }) => <table className="w-full my-4 text-sm border-collapse border border-muted-foreground" {...props} />,
        thead: ({ node, ...props }) => <thead className="bg-muted/50" {...props} />,
        th: ({ node, ...props }) => <th className="px-2 py-1 text-left font-semibold border border-muted-foreground" {...props} />,
        td: ({ node, ...props }) => <td className="px-2 py-1 border border-muted-foreground" {...props} />,
        code: ({ node, inline, className, children, ...props }: any) => {
          const match = /language-(\w+)/.exec(className || '');
          const childrenStr = String(children);
          const isLikelyInline = !childrenStr.includes('\n');
          const index = props.sourcePosition?.start.line ?? 0;

          if (inline || isLikelyInline) {
            return (
              <code className="inline bg-muted text-muted-foreground font-mono text-[90%] px-1.5 py-1 rounded-md mx-1" {...props}>
                {children}
              </code>
            );
          } else {
            return (
              <div className="relative group">
                <button
                  onClick={() => handleCopy(childrenStr.replace(/\n$/, ''), index)}
                  className="absolute bottom-2 right-2 p-1.5 rounded-md bg-muted text-muted-foreground opacity-0 group-hover:opacity-100 transition-opacity"
                  title="Copy code"
                >
                  {copiedStates[index] ? <FaCheck className="text-green-500" /> : <FaCopy />}
                </button>
                <pre className="bg-black/80 p-3 rounded-md my-4 overflow-x-auto text-sm">
                  <code className={className} {...props}>
                    {children}
                  </code>
                </pre>
              </div>
            );
          }
        },
        a: ({ node, ...props }) => <a className="text-primary underline hover:no-underline" target="_blank" rel="noopener noreferrer" {...props} />,
      }}
    >
      {children}
    </ReactMarkdown>
  );
};

export default MarkdownRenderer;
</file_artifact>

<file path="src/components/showcase/InteractiveWhitepaper.tsx">
// src/components/showcase/InteractiveWhitepaper.tsx
// C1 - Initial Scaffolding
'use client';

import { useState } from 'react';
import { Button } from '@/components/ui/button';

// Define the types based on A3/A174 (simplified for C1)
interface WhitepaperPage {
pageTitle: string;
tldr: string;
content: string;
}

interface WhitepaperSection {
sectionTitle: string;
pages: WhitepaperPage[];
}

interface WhitepaperData {
reportTitle: string;
sections: WhitepaperSection[];
}

interface InteractiveWhitepaperProps {
data: WhitepaperData;
}

const InteractiveWhitepaper = ({ data }: InteractiveWhitepaperProps) => {
const [currentSectionIndex, setCurrentSectionIndex] = useState(0);
const [currentPageIndex, setCurrentPageIndex] = useState(0);

if (!data || data.sections.length === 0) {
return <div className="text-center py-8 text-red-500">Failed to load content or content is empty.</div>;
}

const currentSection = data.sections[currentSectionIndex];
const currentPage = currentSection.pages[currentPageIndex];

const handleNextPage = () => {
if (currentPageIndex < currentSection.pages.length - 1) {
setCurrentPageIndex(currentPageIndex + 1);
} else if (currentSectionIndex < data.sections.length - 1) {
setCurrentSectionIndex(currentSectionIndex + 1);
setCurrentPageIndex(0);
}
};

const handlePrevPage = () => {
if (currentPageIndex > 0) {
setCurrentPageIndex(currentPageIndex - 1);
} else if (currentSectionIndex > 0) {
setCurrentSectionIndex(currentSectionIndex - 1);
setCurrentPageIndex(data.sections[currentSectionIndex - 1].pages.length - 1);
}
};

return (
<div className="max-w-4xl mx-auto">
<header className="mb-8">
<h2 className="text-2xl font-semibold text-muted-foreground">{currentSection.sectionTitle}</h2>
<h3 className="text-3xl font-bold mt-2">{currentPage.pageTitle}</h3>
</header>


  <div className="mb-8 p-4 bg-secondary border-l-4 border-primary">
    <p className="font-medium">{currentPage.tldr}</p>
  </div>

  <div className="prose dark:prose-invert lg:prose-lg max-w-none">
    {/* In a real implementation, this content might be markdown rendered */}
    <p>{currentPage.content}</p>
  </div>

  <div className="flex justify-between mt-12 pt-6 border-t">
    <Button
      onClick={handlePrevPage}
      disabled={currentSectionIndex === 0 && currentPageIndex === 0}
      variant="outline"
    >
      Previous
    </Button>
    <span className="text-muted-foreground">
      Section {currentSectionIndex + 1} / {data.sections.length} | Page {currentPageIndex + 1} / {currentSection.pages.length}
    </span>
    <Button
      onClick={handleNextPage}
      disabled={currentSectionIndex === data.sections.length - 1 && currentPageIndex === currentSection.pages.length - 1}
    >
      Next
    </Button>
  </div>
</div>


);
};

export default InteractiveWhitepaper;
</file_artifact>

<file path="src/components/showcase/ShowcaseTabs.tsx">
'use client';
import React, { useState, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import ReportViewer from '@/components/report-viewer/ReportViewer';
import { FaSync } from 'react-icons/fa';

const ShowcaseTabs = () => {
  const [activeTab, setActiveTab] = useState('report');
  const iframeRef = useRef<HTMLIFrameElement>(null);
  const [isGameLoading, setIsGameLoading] = useState(true);

  const handleRefresh = () => {
    if (iframeRef.current) {
      setIsGameLoading(true);
      // Resetting the src attribute is a safe way to force an iframe to reload its content
      // without running into cross-origin security issues.
      iframeRef.current.src = iframeRef.current.src;
    }
  };
  
  // C54: Fix scroll bug
  useEffect(() => {
    if (activeTab === 'game') {
      setIsGameLoading(true);
      window.scrollTo(0, 0);
    }
  }, [activeTab]);

  const handleIframeLoad = () => {
    setIsGameLoading(false);
    window.scrollTo(0, 0);
  };

  return (
    <div className="w-full h-full flex flex-col">
      <div className="flex justify-center border-b border-muted mb-4 flex-shrink-0 gap-2 p-2">
        <Button
          variant={activeTab === 'report' ? 'secondary' : 'ghost'}
          onClick={() => setActiveTab('report')}
        >
          The Ascent Report
        </Button>
        <Button
          variant={activeTab === 'anguilla' ? 'secondary' : 'ghost'}
          onClick={() => setActiveTab('anguilla')}
        >
          Anguilla Project
        </Button>
        <Button
          variant={activeTab === 'game' ? 'secondary' : 'ghost'}
          onClick={() => setActiveTab('game')}
        >
          AI Ascent Game
        </Button>
      </div>

      <div className="flex-grow">
        {activeTab === 'report' && <ReportViewer reportName="showcase" />}
        {activeTab === 'anguilla' && <ReportViewer reportName="anguilla" />}
        {activeTab === 'game' && (
          <div className="relative w-full h-full flex flex-col items-center">
             <p className="text-sm text-muted-foreground mb-4 p-2 border rounded-md bg-muted/50 max-w-4xl text-center">
              You are viewing an embedded version of AI Ascent. For the full experience, including login, chat, and multiplayer features, please visit the main site: {' '}
              <a href="https://aiascent.game/" target="_blank" rel="noopener noreferrer" className="text-primary underline">
                aiascent.game
              </a>.
            </p>
            <div className="relative w-full flex-grow">
                <div className="absolute top-2 right-2 z-10">
                <Button onClick={handleRefresh} variant="outline" size="icon">
                    <FaSync className={isGameLoading ? 'animate-spin' : ''} />
                </Button>
                </div>
                <iframe
                ref={iframeRef}
                src="https://aiascent.game/"
                className="w-full h-full border-0"
                title="AI Ascent Game"
                onLoad={handleIframeLoad}
                ></iframe>
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

export default ShowcaseTabs;
</file_artifact>

<file path="src/components/ui/badge.tsx">
import * as React from "react"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const badgeVariants = cva(
  "inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2",
  {
    variants: {
      variant: {
        default:
          "border-transparent bg-primary text-primary-foreground hover:bg-primary/80",
        secondary:
          "border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80",
        destructive:
          "border-transparent bg-destructive text-destructive-foreground hover:bg-destructive/80",
        outline: "text-foreground",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)

export interface BadgeProps
  extends React.HTMLAttributes<HTMLDivElement>,
    VariantProps<typeof badgeVariants> {}

function Badge({ className, variant, ...props }: BadgeProps) {
  return (
    <div className={cn(badgeVariants({ variant }), className)} {...props} />
  )
}

export { Badge, badgeVariants }
</file_artifact>

<file path="src/components/ui/button.tsx">
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const buttonVariants = cva(
  "inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-destructive-foreground hover:bg-destructive/90",
        outline:
          "border border-input bg-background hover:bg-accent hover:text-accent-foreground",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost: "hover:bg-accent hover:text-accent-foreground",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-10 px-4 py-2",
        sm: "h-9 rounded-md px-3",
        lg: "h-11 rounded-md px-8",
        icon: "h-10 w-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)

export interface ButtonProps
  extends React.ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  asChild?: boolean
}

const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  ({ className, variant, size, asChild = false, ...props }, ref) => {
    const Comp = asChild ? Slot : "button"
    return (
      <Comp
        className={cn(buttonVariants({ variant, size, className }))}
        ref={ref}
        {...props}
      />
    )
  }
)
Button.displayName = "Button"

export { Button, buttonVariants }
</file_artifact>

<file path="src/components/ui/card.tsx">
import * as React from "react"

import { cn } from "@/lib/utils"

const Card = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn(
      "rounded-lg border bg-card text-card-foreground shadow-sm",
      className
    )}
    {...props}
  />
))
Card.displayName = "Card"

const CardHeader = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex flex-col space-y-1.5 p-6", className)}
    {...props}
  />
))
CardHeader.displayName = "CardHeader"

const CardTitle = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn(
      "text-2xl font-semibold leading-none tracking-tight",
      className
    )}
    {...props}
  />
))
CardTitle.displayName = "CardTitle"

const CardDescription = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
CardDescription.displayName = "CardDescription"

const CardContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div ref={ref} className={cn("p-6 pt-0", className)} {...props} />
))
CardContent.displayName = "CardContent"

const CardFooter = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex items-center p-6 pt-0", className)}
    {...props}
  />
))
CardFooter.displayName = "CardFooter"

export { Card, CardHeader, CardFooter, CardTitle, CardDescription, CardContent }
</file_artifact>

<file path="src/components/ui/dropdown-menu.tsx">
"use client"

import * as React from "react"
import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu"
import { Check, ChevronRight, Circle } from "lucide-react"

import { cn } from "@/lib/utils"

const DropdownMenu = DropdownMenuPrimitive.Root

const DropdownMenuTrigger = DropdownMenuPrimitive.Trigger

const DropdownMenuGroup = DropdownMenuPrimitive.Group

const DropdownMenuPortal = DropdownMenuPrimitive.Portal

const DropdownMenuSub = DropdownMenuPrimitive.Sub

const DropdownMenuRadioGroup = DropdownMenuPrimitive.RadioGroup

const DropdownMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubTrigger> & {
    inset?: boolean
  }
>(({ className, inset, children, ...props }, ref) => (
  <DropdownMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent data-[state=open]:bg-accent",
      inset && "pl-8",
      className
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto h-4 w-4" />
  </DropdownMenuPrimitive.SubTrigger>
))
DropdownMenuSubTrigger.displayName =
  DropdownMenuPrimitive.SubTrigger.displayName

const DropdownMenuSubContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
DropdownMenuSubContent.displayName =
  DropdownMenuPrimitive.SubContent.displayName

const DropdownMenuContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <DropdownMenuPrimitive.Portal>
    <DropdownMenuPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className
      )}
      {...props}
    />
  </DropdownMenuPrimitive.Portal>
))
DropdownMenuContent.displayName = DropdownMenuPrimitive.Content.displayName

const DropdownMenuItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Item> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
DropdownMenuItem.displayName = DropdownMenuPrimitive.Item.displayName

const DropdownMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <DropdownMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.CheckboxItem>
))
DropdownMenuCheckboxItem.displayName =
  DropdownMenuPrimitive.CheckboxItem.displayName

const DropdownMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <DropdownMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.RadioItem>
))
DropdownMenuRadioItem.displayName = DropdownMenuPrimitive.RadioItem.displayName

const DropdownMenuLabel = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Label> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
DropdownMenuLabel.displayName = DropdownMenuPrimitive.Label.displayName

const DropdownMenuSeparator = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
))
DropdownMenuSeparator.displayName = DropdownMenuPrimitive.Separator.displayName

const DropdownMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn("ml-auto text-xs tracking-widest opacity-60", className)}
      {...props}
    />
  )
}
DropdownMenuShortcut.displayName = "DropdownMenuShortcut"

export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
}
</file_artifact>

<file path="src/data/whitepaperContent.json">
{
"reportId": "dce-whitepaper-v1-placeholder",
"reportTitle": "The Data Curation Environment: Process as Asset (Placeholder)",
"sections": [
{
"sectionId": "introduction",
"sectionTitle": "Introduction",
"pages": [
{
"pageId": "intro-1",
"pageTitle": "The Challenge of Specialized Content",
"tldr": "Traditional content workflows are inefficient and opaque.",
"content": "Organizations tasked with developing highly specialized content—such as technical training materials, intelligence reports, or complex software documentation—face a constant bottleneck: the time and expertise required to curate accurate data, collaborate effectively, and rapidly iterate on feedback."
},
{
"pageId": "intro-2",
"pageTitle": "Introducing the DCE",
"tldr": "The DCE transforms the content creation process itself into a valuable organizational asset.",
"content": "The Data Curation Environment (DCE) provides a structured, human-in-the-loop methodology that enables rapid dataset curation, seamless sharing of curated contexts between colleagues, and instant iteration on feedback."
}
]
},
{
"sectionId": "conclusion",
"sectionTitle": "Conclusion",
"pages": [
{
"pageId": "conclusion-1",
"pageTitle": "Scaling Expertise",
"tldr": "The DCE provides the infrastructure necessary to scale expertise and accelerate the mission.",
"content": "By capturing the entire workflow as a persistent, auditable knowledge graph, the DCE doesn't just help teams build content faster; it ensures quality and accelerates the entire organizational mission."
}
]
}
]
}
</file_artifact>

<file path="src/lib/kb-helper.ts">
// src/lib/kb-helper.ts
export function getKnowledgeBase(reportName: string | null): 'report' | 'dce' | 'academy' | 'anguilla' {
    if (!reportName) return 'report';

    if (reportName.startsWith('v2v-academy-lab')) {
        return 'dce';
    }
    
    if (reportName.startsWith('v2v-academy-') || reportName === 'whitepaper') {
        return 'academy';
    }

    if (reportName === 'anguilla') {
        return 'anguilla';
    }

    return 'report';
}
</file_artifact>

<file path="src/lib/utils.ts">
import { type ClassValue, clsx } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
return twMerge(clsx(inputs))
}
</file_artifact>

<file path="src/providers/theme-provider.tsx">
"use client"

import * as React from "react"
import { ThemeProvider as NextThemesProvider } from "next-themes"
import { type ThemeProviderProps } from "next-themes/dist/types"

export function ThemeProvider({ children, ...props }: ThemeProviderProps) {
  return <NextThemesProvider {...props}>{children}</NextThemesProvider>
}
</file_artifact>

<file path="src/stores/reportStore.ts">
// src/stores/reportStore.ts
import { createWithEqualityFn } from 'zustand/traditional';
import { persist, createJSONStorage } from 'zustand/middleware';
import { shallow } from 'zustand/shallow';
import { getKnowledgeBase } from '@/lib/kb-helper';

// ... (interfaces ommitted for brevity)
export interface ReportImage {
    imageId: string;
    url: string;
    prompt: string;
    alt: string;
}

export interface ReportImagePrompt {
    promptId: string;
    promptText: string;
    images: ReportImage[];
}

export interface ReportPage {
    pageId: string;
    pageTitle: string;
    tldr: string;
    content: string;
    imagePrompts: ReportImagePrompt[];
}

// Raw Data Structures from JSON files
export interface RawReportPage {
    pageId: string;
    pageTitle: string;
    tldr: string;
    content: string;
    imageGroupIds: string[];
}

export interface RawSubSection {
    subSectionId: string;
    subSectionTitle: string;
    pages: RawReportPage[];
}

export interface RawReportSection {
    sectionId: string;
    sectionTitle: string;
    pages?: RawReportPage[];
    subSections?: RawSubSection[];
}

export interface ReportContentData {
    reportId: string;
    reportTitle: string;
    sections: RawReportSection[];
}

export interface ImageManifestData {
    manifestId: string;
    basePath: string;
    imageGroups: Record<string, {
        path: string;
        prompt: string;
        alt: string;
        baseFileName: string;
        fileExtension: string;
        imageCount: number;
    }>;
}
export type ChatMessage = {
    id?: string;
    author: string;
    flag: string;
    message: string;
    channel: 'system' | 'local';
    status?: 'thinking' | 'streaming' | 'complete';
};

const WHITEPAPER_DEFAULT_SUGGESTIONS = ['How does DCE work?', 'How do I install DCE?'];
const SHOWCASE_DEFAULT_SUGGESTIONS = ["What is the 'fissured workplace'?", "What is Cognitive Security (COGSEC)?"];
const ACADEMY_DEFAULT_SUGGESTIONS = ["Can you explain this concept in simpler terms?", "How does this apply to a real-world project?", "What is the key takeaway from this page?"];
const ANGUILLA_DEFAULT_SUGGESTIONS = ["What is the 'Digital Wealth Fund'?", "How does the 'Automated State' work?", "What is 'Cognitive Capital' in this context?"];


type LastSuggestionRequest = {
    type: 'page' | 'conversation';
    payload: {
        reportName: string;
        context: string;
    };
} | null;

interface FullscreenMedia {
    src: string;
    description: string;
    content?: string; // C96: Added for lab content
}

export interface ReportState {
    reportName: string | null; // C42: To track current report context
    _hasHydrated: boolean; // Flag for rehydration
    reportData: ReportContentData | null;
    imageManifest: ImageManifestData | null;
    allPages: ReportPage[];
    currentPageIndex: number;
    currentImageIndex: number;
    isTreeNavOpen: boolean;
    expandedSections: Record<string, boolean>;
    isChatPanelOpen: boolean;
    chatPanelWidth: number;
    imagePanelHeight: number;
    isFullscreen: boolean; // C45: For fullscreen mode
    fullscreenMedia: FullscreenMedia | null; // C54: For fullscreen GIF viewer
    reportChatHistory: ChatMessage[];
    reportChatInput: string;
    suggestedPrompts: string[]; // C35: New state for dynamic suggestions
    suggestionsStatus: 'idle' | 'loading' | 'error'; // C43: New state for suggestion generation
    lastSuggestionRequest: LastSuggestionRequest; // C49: For refresh button
    isPromptVisible: boolean;
    isTldrVisible: boolean;
    isContentVisible: boolean;
    isLoading: boolean;
    // Main Report Audio State
    playbackStatus: 'idle' | 'generating' | 'buffering' | 'playing' | 'paused' | 'error';
    autoplayEnabled: boolean;
    currentAudioUrl: string | null;
    currentAudioPageIndex: number | null;
    currentTime: number;
    duration: number;
    volume: number;
    isMuted: boolean;
    slideshowTimer: NodeJS.Timeout | null;
    nextPageTimer: NodeJS.Timeout | null;
    playbackSpeed: number;
    // Generic/Arbitrary Audio State
    genericPlaybackStatus: 'idle' | 'generating' | 'playing' | 'paused' | 'error';
    genericAudioUrl: string | null;
    genericAudioText: string | null; // The text being played
}

export interface ReportActions {
    setHasHydrated: (hydrated: boolean) => void;
    loadReport: (reportData: ReportContentData, imageManifest: ImageManifestData) => Promise<void>;
    nextPage: () => void;
    prevPage: () => void;
    goToPageByIndex: (pageIndex: number) => void;
    nextPageInFullscreen: () => void; // C97: New action
    prevPageInFullscreen: () => void; // C97: New action
    nextImage: () => void;
    prevImage: () => void;
    handleKeyDown: (event: KeyboardEvent) => void;
    toggleTreeNav: () => void;
    toggleSectionExpansion: (sectionId: string) => void;
    setActiveExpansionPath: (pageIndex: number) => void;
    toggleChatPanel: () => void;
    setChatPanelWidth: (width: number) => void;
    setImagePanelHeight: (height: number) => void;
    toggleFullscreen: (element: HTMLElement | null) => void; // C45
    setIsFullscreen: (isFullscreen: boolean) => void; // C45
    openFullscreenMedia: (media: FullscreenMedia) => void; // C54
    closeFullscreenMedia: () => void; // C54
    setReportChatInput: (input: string) => void;
    setSuggestedPrompts: (prompts: string[]) => void; // C35: Action to update suggestions
    fetchPageSuggestions: (page: ReportPage) => Promise<void>; // C90: Removed reportName
    fetchConversationSuggestions: (history: ChatMessage[]) => Promise<void>; // C90: Removed reportName
    regenerateSuggestions: () => Promise<void>; // C49: New
    addReportChatMessage: (message: ChatMessage) => void;
    updateReportChatMessage: (id: string, chunk: string) => void;
    setReportChatMessage: (id: string, message: string) => void; // C38: New action
    updateReportChatStatus: (id: string, status: ChatMessage['status']) => void;
    clearReportChatHistory: (currentPageTitle: string) => void;
    togglePromptVisibility: () => void;
    toggleTldrVisibility: () => void;
    toggleContentVisibility: () => void;
    // Main Report Audio Actions
    setPlaybackStatus: (status: ReportState['playbackStatus']) => void;
    setAutoplay: (enabled: boolean) => void;
    setCurrentAudio: (url: string | null, pageIndex: number) => void;
    setAudioTime: (time: number) => void;
    setAudioDuration: (duration: number) => void;
    setVolume: (level: number) => void;
    toggleMute: () => void;
    startSlideshow: () => void;
    stopSlideshow: (userInitiated?: boolean) => void;
    setPlaybackSpeed: (speed: number) => void;
    // Generic/Arbitrary Audio Actions
    playArbitraryText: (text: string) => void;
    setGenericPlaybackStatus: (status: ReportState['genericPlaybackStatus']) => void;
    setGenericAudioUrl: (url: string | null) => void;
    stopArbitraryText: () => void;
}


// ... (createInitialReportState ommitted for brevity)
const createInitialReportState = (): ReportState => ({
    reportName: null,
    _hasHydrated: false,
    reportData: null,
    imageManifest: null,
    allPages: [],
    currentPageIndex: 0,
    currentImageIndex: 0,
    // C28: Set minimalist defaults
    isTreeNavOpen: false,
    expandedSections: {},
    isChatPanelOpen: false,
    chatPanelWidth: 450,
    imagePanelHeight: 400,
    isFullscreen: false, // C45
    fullscreenMedia: null, // C54
    reportChatHistory: [],
    reportChatInput: '',
    suggestedPrompts: WHITEPAPER_DEFAULT_SUGGESTIONS, // C42: Default to whitepaper, will be overridden on load
    suggestionsStatus: 'idle', // C43
    lastSuggestionRequest: null, // C49
    isPromptVisible: false,
    isTldrVisible: true,
    isContentVisible: true,
    isLoading: true,
    // Main Report Audio State
    playbackStatus: 'idle',
    autoplayEnabled: false,
    currentAudioUrl: null,
    currentAudioPageIndex: null,
    currentTime: 0,
    duration: 0,
    volume: 1,
    isMuted: false,
    slideshowTimer: null,
    nextPageTimer: null,
    playbackSpeed: 1,
    // Generic/Arbitrary Audio State
    genericPlaybackStatus: 'idle',
    genericAudioUrl: null,
    genericAudioText: null,
});

const getFallbackSuggestions = (reportName: string | null) => {
    if (!reportName) return SHOWCASE_DEFAULT_SUGGESTIONS;
    if (reportName.startsWith('v2v_')) return ACADEMY_DEFAULT_SUGGESTIONS;
    if (reportName === 'whitepaper') return WHITEPAPER_DEFAULT_SUGGESTIONS;
    if (reportName === 'anguilla') return ANGUILLA_DEFAULT_SUGGESTIONS;
    return SHOWCASE_DEFAULT_SUGGESTIONS;
};


const _fetchSuggestions = async (
    suggestionType: 'page' | 'conversation',
    context: string,
    reportName: string
): Promise<string[] | null> => {
    const MAX_RETRIES = 3;
    for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
        try {
            const knowledgeBase = getKnowledgeBase(reportName);

            const response = await fetch('/api/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    task: 'generate_suggestions',
                    suggestionType,
                    context,
                    reportName, // C89: Pass reportName to backend for persona-specific prompts
                    knowledgeBase, // C91: Pass knowledgeBase to backend for RAG lookup
                }),
            });

            if (response.status >= 500) {
                console.warn(`[reportStore] Suggestion fetch attempt ${attempt} failed with status ${response.status}. Retrying...`);
                if (attempt === MAX_RETRIES) throw new Error(`Failed after ${MAX_RETRIES} attempts. Last status: ${response.status}`);
                await new Promise(res => setTimeout(res, 1000 * attempt));
                continue;
            }

            if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`API error: ${response.status} ${errorText}`);
            }

            const suggestions = await response.json();
            if (Array.isArray(suggestions) && suggestions.length > 0) {
                return suggestions;
            } else {
                throw new Error('Invalid suggestions format');
            }
        } catch (error) {
            console.error(`[reportStore] Error on suggestion fetch attempt ${attempt}:`, error);
            if (attempt === MAX_RETRIES) return null;
        }
    }
    return null;
};


export const useReportStore = createWithEqualityFn<ReportState & ReportActions>()(
    persist(
        (set, get) => ({
            ...createInitialReportState(),
            setHasHydrated: (hydrated) => set({ _hasHydrated: hydrated }),

            // ... (fetchPageSuggestions, fetchConversationSuggestions, regenerateSuggestions ommitted for brevity)
            fetchPageSuggestions: async (page: ReportPage) => {
                const { suggestionsStatus, reportName } = get(); // C90: Get reportName from store
                if (suggestionsStatus === 'loading' || !page || !reportName) return;

                const context = `Page Title: ${page.pageTitle || 'N/A'}\nTL;DR: ${page.tldr || 'N/A'}\nContent: ${page.content || 'N/A'}`;
                const payload = { reportName, context };
                set({ suggestionsStatus: 'loading', lastSuggestionRequest: { type: 'page', payload } });

                const suggestions = await _fetchSuggestions('page', context, reportName);
                
                // The guard clause now works as intended for report transitions
                if (get().reportName !== reportName) {
                    console.log(`[reportStore] Stale page suggestions for "${reportName}" ignored.`);
                    return;
                }

                if (suggestions) {
                    set({ suggestedPrompts: suggestions, suggestionsStatus: 'idle' });
                } else {
                    set({ suggestedPrompts: getFallbackSuggestions(reportName), suggestionsStatus: 'error' });
                }
            },

            fetchConversationSuggestions: async (history: ChatMessage[]) => {
                const { suggestionsStatus, reportName } = get(); // C90: Get reportName from store
                if (suggestionsStatus === 'loading' || history.length === 0 || !reportName) return;
                
                // Take the last 2 messages (user + assistant)
                const relevantHistory = history.slice(-2);
                const context = relevantHistory.map(m => `${m.author}: ${m.message}`).join('\n\n');
                const payload = { reportName, context };
                set({ suggestionsStatus: 'loading', lastSuggestionRequest: { type: 'conversation', payload } });

                const suggestions = await _fetchSuggestions('conversation', context, reportName);

                if (get().reportName !== reportName) {
                    console.log(`[reportStore] Stale conversation suggestions for "${reportName}" ignored.`);
                    return;
                }

                if (suggestions) {
                    set({ suggestedPrompts: suggestions, suggestionsStatus: 'idle' });
                } else {
                    set({ suggestedPrompts: getFallbackSuggestions(reportName), suggestionsStatus: 'error' });
                }
            },

            regenerateSuggestions: async () => {
                const { lastSuggestionRequest } = get();
                if (!lastSuggestionRequest || get().suggestionsStatus === 'loading') return;

                const { type, payload } = lastSuggestionRequest;
                set({ suggestionsStatus: 'loading' });

                const suggestions = await _fetchSuggestions(type, payload.context, payload.reportName);

                if (get().reportName !== payload.reportName) {
                    console.log(`[reportStore] Stale regenerated suggestions for "${payload.reportName}" ignored.`);
                    return;
                }
                
                if (suggestions) {
                    set({ suggestedPrompts: suggestions, suggestionsStatus: 'idle' });
                } else {
                    set({ suggestedPrompts: getFallbackSuggestions(payload.reportName), suggestionsStatus: 'error' });
                }
            },

            loadReport: async (contentData: ReportContentData, imageManifest: ImageManifestData) => {
                if (!contentData || !imageManifest) {
                    console.error("loadReport called with undefined data.");
                    set({ isLoading: false });
                    return;
                }
                const reportNameFromData = contentData.reportId;
                
                set(createInitialReportState());

                const defaultSuggestions = getFallbackSuggestions(reportNameFromData);

                set({ 
                    reportName: reportNameFromData, // C90: Use the ID from the data file as the source of truth
                    _hasHydrated: true, 
                    isLoading: true,
                    suggestedPrompts: defaultSuggestions,
                });

                try {
                    const reconstructedPages: ReportPage[] = [];
                    contentData.sections.forEach(section => {
                        const processPages = (pages: RawReportPage[]) => {
                            (pages || []).forEach(rawPage => {
                                const imagePrompts: ReportImagePrompt[] = (rawPage.imageGroupIds || []).map(groupId => {
                                    const groupMeta = imageManifest.imageGroups[groupId];
                                    if (!groupMeta) {
                                        console.warn(`Image group metadata not found for groupId: ${groupId}`);
                                        return null;
                                    }

                                    const images: ReportImage[] = [];
                                    const imageBasePath = imageManifest.basePath;
                                    
                                    if (groupMeta.imageCount === 1 && !groupMeta.baseFileName.endsWith('-')) {
                                        const fileName = `${groupMeta.baseFileName}${groupMeta.fileExtension}`;
                                        const url = `${imageBasePath}${groupMeta.path}${fileName}`;
                                        images.push({
                                            imageId: `${rawPage.pageId}-${groupId}-1`,
                                            url,
                                            prompt: groupMeta.prompt,
                                            alt: groupMeta.alt,
                                        });
                                    } else {
                                        for (let i = 1; i <= groupMeta.imageCount; i++) {
                                            const fileName = `${groupMeta.baseFileName}${i}${groupMeta.fileExtension}`;
                                            const url = `${imageBasePath}${groupMeta.path}${fileName}`;
                                            images.push({
                                                imageId: `${rawPage.pageId}-${groupId}-${i}`,
                                                url,
                                                prompt: groupMeta.prompt,
                                                alt: groupMeta.alt,
                                            });
                                        }
                                    }
                                    
                                    return {
                                        promptId: groupId,
                                        promptText: groupMeta.prompt,
                                        images,
                                    };
                                }).filter((p): p is ReportImagePrompt => p !== null);

                                reconstructedPages.push({
                                    pageId: rawPage.pageId,
                                    pageTitle: rawPage.pageTitle,
                                    tldr: rawPage.tldr,
                                    content: rawPage.content,
                                    imagePrompts,
                                });
                            });
                        };
                        
                        if (section.pages) processPages(section.pages);
                        if (section.subSections) section.subSections.forEach(sub => processPages(sub.pages));
                    });
                    
                    set({
                        reportData: contentData,
                        imageManifest: imageManifest,
                        allPages: reconstructedPages,
                        isLoading: false,
                    });
                    get().setActiveExpansionPath(get().currentPageIndex);
                } catch (error) {
                    console.error(`Failed to process report data for ${reportNameFromData}.`, error);
                    set({ isLoading: false });
                }
            },
            
            // ... (rest of actions ommitted for brevity)
            startSlideshow: () => {
                const { stopSlideshow, allPages, currentPageIndex, duration, nextPage, autoplayEnabled, playbackSpeed } = get();
                stopSlideshow(false); // Stop any existing timers

                const currentPage = allPages[currentPageIndex];
                if (!currentPage || !autoplayEnabled) return;

                const actualDuration = duration / playbackSpeed;
                const actualDurationMs = actualDuration * 1000;

                if (actualDurationMs <= 0 || !isFinite(actualDurationMs)) return;

                const nextPageTimer = setTimeout(() => {
                    if (get().autoplayEnabled) {
                        nextPage();
                    }
                }, actualDurationMs + 500);
                set({ nextPageTimer });

                const images = currentPage.imagePrompts?.[0]?.images;
                if (!images || images.length <= 1) return;

                const timePerImage = actualDurationMs / images.length;
                
                const slideshowTimer = setInterval(() => {
                    if (!get().autoplayEnabled) {
                        clearInterval(slideshowTimer);
                        return;
                    }
                    set(state => {
                        const nextImageIndex = state.currentImageIndex + 1;
                        if (nextImageIndex < images.length) {
                            return { currentImageIndex: nextImageIndex };
                        } else {
                            clearInterval(slideshowTimer);
                            return { slideshowTimer: null };
                        }
                    });
                }, timePerImage);

                set({ slideshowTimer });
            },
            
            stopSlideshow: (userInitiated = false) => {
                const { slideshowTimer, nextPageTimer } = get();
                if (slideshowTimer) clearInterval(slideshowTimer);
                if (nextPageTimer) clearTimeout(nextPageTimer);
                if (userInitiated) {
                    set({ slideshowTimer: null, nextPageTimer: null, autoplayEnabled: false });
                } else {
                    set({ slideshowTimer: null, nextPageTimer: null });
                }
            },

            playArbitraryText: async (text: string) => {
                const { genericPlaybackStatus, genericAudioText, stopArbitraryText } = get();

                if (genericPlaybackStatus === 'playing' && genericAudioText === text) {
                    stopArbitraryText(); 
                    return;
                }
                
                stopArbitraryText();

                // C95: Replace "VS Code" with "V S Code" for better TTS pronunciation
                const modifiedText = text.replace(/VS Code/g, 'V S Code');
                set({ genericPlaybackStatus: 'generating', genericAudioText: text }); // Store original text for state comparison

                try {
                    const response = await fetch('/api/tts', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ text: modifiedText }), // Send modified text to API
                    });

                    if (!response.ok) throw new Error(`TTS server failed with status: ${response.status}`);

                    const audioBlob = await response.blob();
                    const newUrl = URL.createObjectURL(audioBlob);
                    set({ genericAudioUrl: newUrl, genericPlaybackStatus: 'playing' });
                } catch (error) {
                    console.error('[reportStore] Failed to play arbitrary text:', error);
                    set({ genericPlaybackStatus: 'error' });
                }
            },
            stopArbitraryText: () => {
                const { genericAudioUrl } = get();
                if (genericAudioUrl) URL.revokeObjectURL(genericAudioUrl);
                set({ genericPlaybackStatus: 'idle', genericAudioUrl: null, genericAudioText: null });
            },
            setGenericPlaybackStatus: (status) => set({ genericPlaybackStatus: status }),
            setGenericAudioUrl: (url) => set({ genericAudioUrl: url }),

            nextPage: () => {
                get().stopSlideshow(false);
                set(state => {
                    const newIndex = (state.currentPageIndex + 1) % state.allPages.length;
                    if (newIndex === 0 && state.currentPageIndex === state.allPages.length - 1 && state.autoplayEnabled) {
                        return { currentPageIndex: newIndex, currentImageIndex: 0, autoplayEnabled: false, playbackStatus: 'idle' };
                    }
                    return { currentPageIndex: newIndex, currentImageIndex: 0, playbackStatus: 'idle' };
                });
            },
            prevPage: () => {
                get().stopSlideshow(true);
                set(state => ({
                    currentPageIndex: (state.currentPageIndex - 1 + state.allPages.length) % state.allPages.length,
                    currentImageIndex: 0,
                    playbackStatus: 'idle',
                }));
            },
            nextPageInFullscreen: () => {
                const { currentPageIndex, allPages } = get();
                const newIndex = Math.min(allPages.length - 1, currentPageIndex + 1);
                if (newIndex === currentPageIndex) return;

                const newPage = allPages[newIndex];
                const newImage = newPage?.imagePrompts?.[0]?.images?.[0];
                if (!newPage || !newImage) return;
                
                set({
                    currentPageIndex: newIndex,
                    currentImageIndex: 0,
                    fullscreenMedia: {
                        src: newImage.url,
                        description: newImage.prompt,
                        content: newPage.content,
                    }
                });
            },
            prevPageInFullscreen: () => {
                const { currentPageIndex, allPages } = get();
                const newIndex = Math.max(0, currentPageIndex - 1);
                if (newIndex === currentPageIndex) return;

                const newPage = allPages[newIndex];
                const newImage = newPage?.imagePrompts?.[0]?.images?.[0];
                if (!newPage || !newImage) return;

                set({
                    currentPageIndex: newIndex,
                    currentImageIndex: 0,
                    fullscreenMedia: {
                        src: newImage.url,
                        description: newImage.prompt,
                        content: newPage.content,
                    }
                });
            },
            goToPageByIndex: (pageIndex) => {
                get().stopSlideshow(true);
                if (pageIndex >= 0 && pageIndex < get().allPages.length) {
                    set({ currentPageIndex: pageIndex, currentImageIndex: 0, playbackStatus: 'idle' });
                }
            },
            nextImage: () => {
                get().stopSlideshow(true);
                set(state => {
                    const currentPage = state.allPages[state.currentPageIndex];
                    const totalImages = currentPage?.imagePrompts?.[0]?.images.length ?? 0;
                    if (totalImages <= 1) return state;
                    return { currentImageIndex: (state.currentImageIndex + 1) % totalImages };
                });
            },
            prevImage: () => {
                get().stopSlideshow(true);
                set(state => {
                    const currentPage = state.allPages[state.currentPageIndex];
                    const totalImages = currentPage?.imagePrompts?.[0]?.images.length ?? 0;
                    if (totalImages <= 1) return state;
                    return { currentImageIndex: (state.currentImageIndex - 1 + totalImages) % totalImages };
                });
            },
            handleKeyDown: (event: KeyboardEvent) => {
                const target = event.target as HTMLElement;
                if (target && (target.tagName === 'INPUT' || target.tagName === 'TEXTAREA' || target.tagName === 'SELECT')) return;
                
                if (event.key.startsWith('Arrow')) event.preventDefault();
                switch (event.key) {
                    case 'ArrowUp': get().prevPage(); break;
                    case 'ArrowDown': get().nextPage(); break;
                    case 'ArrowLeft': get().prevImage(); break;
                    case 'ArrowRight': get().nextImage(); break;
                }
            },
            toggleTreeNav: () => set(state => ({ isTreeNavOpen: !state.isTreeNavOpen })),
            toggleSectionExpansion: (sectionId) => set(state => ({ expandedSections: { ...state.expandedSections, [sectionId]: !state.expandedSections[sectionId] } })),
            setActiveExpansionPath: (pageIndex) => {
                const { reportData } = get();
                if (!reportData) return;
                let pageCounter = 0;
                let activeSectionId: string | null = null;
                let activeSubSectionId: string | null = null;
                for (const section of reportData.sections) {
                    const processPages = (pages: RawReportPage[], currentSubSectionId?: string) => {
                        for (let i = 0; i < (pages || []).length; i++) {
                            if (pageCounter === pageIndex) {
                                activeSectionId = section.sectionId;
                                if (currentSubSectionId) activeSubSectionId = currentSubSectionId;
                                return true;
                            }
                            pageCounter++;
                        }
                        return false;
                    };
                    if (section.pages && processPages(section.pages)) break;
                    if (section.subSections) {
                        for (const sub of section.subSections) {
                            if (processPages(sub.pages, sub.subSectionId)) break;
                        }
                    }
                    if (activeSectionId) break;
                }
                if (activeSectionId) {
                    set(state => ({ expandedSections: { ...state.expandedSections, [activeSectionId!]: true, ...(activeSubSectionId && { [activeSubSectionId]: true }), } }));
                }
            },
            toggleChatPanel: () => set(state => ({ isChatPanelOpen: !state.isChatPanelOpen })),
            setChatPanelWidth: (width) => set({ chatPanelWidth: Math.max(300, width) }),
            setImagePanelHeight: (height) => set({ imagePanelHeight: Math.max(200, height) }),
            setIsFullscreen: (isFullscreen) => set({ isFullscreen }),
            toggleFullscreen: (element) => {
                if (!document.fullscreenElement) {
                    element?.requestFullscreen().catch(err => {
                      console.error(`Error attempting to enable full-screen mode: ${err.message} (${err.name})`);
                    });
                  } else {
                    document.exitFullscreen();
                  }
            },
            openFullscreenMedia: (media) => set({ fullscreenMedia: media }),
            closeFullscreenMedia: () => set({ fullscreenMedia: null }),
            setReportChatInput: (input) => set({ reportChatInput: input }),
            setSuggestedPrompts: (prompts) => set({ suggestedPrompts: prompts }),
            addReportChatMessage: (message) => set(state => ({ reportChatHistory: [...state.reportChatHistory, message].slice(-50), })),
            updateReportChatMessage: (id, chunk) => set(state => ({ reportChatHistory: state.reportChatHistory.map(msg => msg.id === id ? { ...msg, message: msg.message + chunk, status: 'streaming' } : msg) })),
            setReportChatMessage: (id, message) => set(state => ({ reportChatHistory: state.reportChatHistory.map(msg => msg.id === id ? { ...msg, message } : msg) })),
            updateReportChatStatus: (id, status) => set(state => ({ reportChatHistory: state.reportChatHistory.map(msg => msg.id === id ? { ...msg, status } : msg) })),
            clearReportChatHistory: (currentPageTitle) => {
                const { fetchPageSuggestions, allPages, currentPageIndex } = get();
                const initialMessage: ChatMessage = { author: 'Ascentia', flag: '🤖', message: `Ask me anything about "${currentPageTitle}".`, channel: 'system', };
                set({
                    reportChatHistory: [initialMessage],
                    reportChatInput: '',
                });
                const currentPage = allPages[currentPageIndex];
                if (currentPage) {
                    fetchPageSuggestions(currentPage);
                }
            },
            togglePromptVisibility: () => set(state => ({ isPromptVisible: !state.isPromptVisible })),
            toggleTldrVisibility: () => set(state => ({ isTldrVisible: !state.isTldrVisible })),
            toggleContentVisibility: () => set(state => ({ isContentVisible: !state.isContentVisible })),
            setPlaybackStatus: (status) => set({ playbackStatus: status }),
            setAutoplay: (enabled) => { 
                get().stopSlideshow(!enabled);
                set({ autoplayEnabled: enabled }); 
                if (enabled) {
                    set({ currentImageIndex: 0 });
                }
            },
            setCurrentAudio: (url, pageIndex) => set(state => {
                if (state.currentAudioPageIndex === pageIndex && state.currentAudioUrl === url) {
                    return state;
                }
                return {
                    currentAudioUrl: url,
                    currentAudioPageIndex: pageIndex,
                    playbackStatus: url ? 'buffering' : 'idle',
                    currentTime: 0,
                    duration: 0,
                };
            }),
            setAudioTime: (time) => set({ currentTime: time }),
            setAudioDuration: (duration) => set({ duration: duration }),
            setVolume: (level) => set({ volume: level }),
            toggleMute: () => set(state => ({ isMuted: !state.isMuted })),
            setPlaybackSpeed: (speed) => set({ playbackSpeed: speed }),
        }),
        {
            name: 'aiascent-dev-report-storage',
            storage: createJSONStorage(() => localStorage),
            onRehydrateStorage: () => (state) => {
                if (state) state.setHasHydrated(true);
            },
            partialize: (state) => ({
                isTreeNavOpen: state.isTreeNavOpen,
                expandedSections: state.expandedSections,
                isChatPanelOpen: state.isChatPanelOpen,
                chatPanelWidth: state.chatPanelWidth,
                imagePanelHeight: state.imagePanelHeight,
                isPromptVisible: state.isPromptVisible,
                isTldrVisible: state.isTldrVisible,
                isContentVisible: state.isContentVisible,
                autoplayEnabled: state.autoplayEnabled,
                volume: state.volume,
                isMuted: state.isMuted,
                playbackSpeed: state.playbackSpeed,
            }),
        }
    )
);

export const useReportState = <T>(selector: (state: ReportState & ReportActions) => T) => {
    return useReportStore(selector, shallow);
};
</file_artifact>

<file path="public/assets/images/anguilla-presentation/ask/ask-pilot.webp">
<metadata>
{
  "name": "ask-pilot.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/ask",
  "fileType": "WEBP",
  "sizeInBytes": 113676
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/capital/capital-cloud.webp">
<metadata>
{
  "name": "capital-cloud.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/capital",
  "fileType": "WEBP",
  "sizeInBytes": 145376
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/capital/capital-fund.webp">
<metadata>
{
  "name": "capital-fund.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/capital",
  "fileType": "WEBP",
  "sizeInBytes": 221730
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/citizenry/citizenry-heritage.webp">
<metadata>
{
  "name": "citizenry-heritage.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/citizenry",
  "fileType": "WEBP",
  "sizeInBytes": 188650
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/citizenry/citizenry-v2v.webp">
<metadata>
{
  "name": "citizenry-v2v.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/citizenry",
  "fileType": "WEBP",
  "sizeInBytes": 217392
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/resilience/resilience-twin.webp">
<metadata>
{
  "name": "resilience-twin.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/resilience",
  "fileType": "WEBP",
  "sizeInBytes": 143510
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/state/state-embassy.webp">
<metadata>
{
  "name": "state-embassy.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/state",
  "fileType": "WEBP",
  "sizeInBytes": 173982
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/state/state-frictionless.webp">
<metadata>
{
  "name": "state-frictionless.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/state",
  "fileType": "WEBP",
  "sizeInBytes": 52304
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/vision/vision-crisis.webp">
<metadata>
{
  "name": "vision-crisis.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/vision",
  "fileType": "WEBP",
  "sizeInBytes": 294178
}
</metadata>
</file_artifact>

<file path="public/assets/images/anguilla-presentation/vision/vision-intro.webp">
<metadata>
{
  "name": "vision-intro.webp",
  "directory": "c:/Projects/aiascent-dev/public/assets/images/anguilla-presentation/vision",
  "fileType": "WEBP",
  "sizeInBytes": 205658
}
</metadata>
</file_artifact>

<file path="scripts/scaffold_anguilla_images.mjs">
// scripts/scaffold_anguilla_images.mjs
import fs from 'fs/promises';
import path from 'path';

const MANIFEST_PATH = path.resolve(process.cwd(), 'public/data/anguilla_manifest.json');
const BASE_OUTPUT_DIR = path.resolve(process.cwd(), 'public/assets/images/anguilla-presentation');

async function scaffoldImages() {
    console.log('🚀 Starting Anguilla Image Scaffolding...');

    try {
        // 1. Read Manifest
        const manifestContent = await fs.readFile(MANIFEST_PATH, 'utf-8');
        const manifest = JSON.parse(manifestContent);

        console.log(`   Found ${Object.keys(manifest.imageGroups).length} image groups.`);

        // 2. Iterate and Create
        for (const [groupId, group] of Object.entries(manifest.imageGroups)) {
            // Construct full directory path: public/assets/images/anguilla-presentation/{group.path}
            // Note: group.path usually includes the trailing slash, e.g., "01-cover/"
            const dirPath = path.join(BASE_OUTPUT_DIR, group.path);

            // Create Directory
            await fs.mkdir(dirPath, { recursive: true });
            console.log(`   📁 Created: ${dirPath}`);

            // Create prompt.md
            const promptFilePath = path.join(dirPath, 'prompt.md');
            const promptContent = `# Image Prompt for ${groupId}\n\n${group.prompt}`;
            
            // Only write if it doesn't exist to avoid overwriting manual edits (though for scaffolding, overwriting is usually fine)
            await fs.writeFile(promptFilePath, promptContent, 'utf-8');
            console.log(`      📝 Wrote prompt.md`);
        }

        console.log('\n✅ Scaffolding Complete. You can now generate images into these directories.');

    } catch (error) {
        console.error('❌ Error scaffolding images:', error);
    }
}

scaffoldImages();
</file_artifact>

<file path="src/Artifacts/A208 - Anguilla Project - Image System Prompt.md">
# Artifact A208: Anguilla Project - Image System Prompt
# Date Created: C4
# Author: AI Model & Curator
# Updated on: C5 (Added Text Rendering Protocols and Cultural Allegories)

- **Key/Value for A0:**
- **Description:** The master system prompt for generating imagery for the Anguilla Project. It defines the "Solarpunk Caribbean" aesthetic, emphasizing resilience, culture, and sovereignty, with specific protocols for rendering text and allegories.
- **Tags:** anguilla, design, images, prompt engineering, system prompt, aesthetic, solarpunk

## 1. Overview

This document provides the master system prompt to be used when generating visual assets for the **Anguilla Project: The First AI-Native Nation**.

The goal is to create a visual identity that is **aspirational, resilient, and culturally grounded**. We must avoid generic "sci-fi" tropes (chrome, neon, flying cars) and instead depict a grounded, achievable future where technology enhances, rather than replaces, the island's natural beauty and heritage.

## 2. The Master System Prompt

**System Prompt:**

You are a visionary architectural concept artist and futurist specializing in "Solarpunk" and "Climate Resilience" aesthetics. Your task is to visualize the future of Anguilla as the world's first AI-Native Nation.

**Your Core Aesthetic: "Solarpunk Caribbean"**

1.  **The Vibe:** Optimistic, resilient, lush, and high-tech. It is a future where nature and technology exist in perfect harmony.
2.  **Materials:** Use local materials (white limestone, coral stone, wood) blended with advanced sustainable tech (transparent solar glass, white aerogel insulation, vertical gardens).
3.  **Color Palette:**
    *   **Primary:** Turquoise/Teal (The Caribbean Sea).
    *   **Base:** Bright White (Limestone/Architecture) & Verdant Green (Lush Flora).
    *   **Accent:** Golden Hour Sunlight (Hope/Opportunity).
4.  **Lighting:** Always bright, natural, and sun-drenched. Avoid dark, dystopian, or "Cyberpunk" neon lighting. Shadows should be sharp and blue, typical of the tropical sun.
5.  **Cultural Respect:**
    *   **People:** Depict diverse Caribbean individuals (Afro-Caribbean descent) as the active agents, leaders, and innovators—not just passive beneficiaries. They are the "Citizen Architects."
    *   **Context:** Include subtle nods to Anguillian culture (e.g., boat racing sloops in the background, salt ponds, traditional architectural motifs).

**Text Rendering Protocols (CRITICAL):**

*   **Handling Text:** If the user prompt includes text enclosed in double quotes (e.g., "THE AI CAPITAL"), you **MUST** render this text visibly and legibly in the image.
*   **Typography:** Use a modern, clean, **bold sans-serif font** (similar to Futura or Roboto).
*   **Integration:** Text should not look like a watermark. It must be **diegetic** (part of the world).
    *   *Holographic:* Floating in 3D space with a soft cyan glow.
    *   *Architectural:* Carved into a white stone wall or mounted as metal lettering on a building.
    *   *Digital:* Displayed on a high-tech glass interface within the scene.
*   **Spelling:** Ensure absolute spelling accuracy for the quoted text.

**Cultural Allegories & Symbols:**

*   **The Three Dolphins:** (From the national flag) Representing **Endurance, Unity, and Strength**. Use this motif in cloud formations, architectural layouts (three curved structures), or data visualizations.
*   **The Racing Boat:** Anguilla's national sport. Use the sleek, white sails of racing sloops as a metaphor for **Speed, Agility, and Heritage**. They should be visible in the background or stylized in the architecture.
*   **Salt:** (Historical industry) Use crystalline, white geometric shapes to represent **Stored Value and Memory**.
*   **The Roots:** Digital networks visualized as organic mangrove root systems connecting the island, implying **Stability and Resilience**.

**Technical Constraints:**
*   **Aspect Ratio:** 16:9 (Cinematic Widescreen).
*   **Style:** Photorealistic Concept Art. High detail, 8k resolution.
*   **Composition:** Wide shots to show scale and context, or medium shots to show human emotion and interaction.

When generating images, always ensure the technology looks like it *belongs* on the island, not like a spaceship landed on it.
</file_artifact>

<file path="public/data/anguilla_imagemanifest.json">
{
  "manifestId": "anguilla-images",
  "basePath": "/assets/images/anguilla-presentation/",
  "imageGroups": {
    "vision-intro": {
      "path": "vision/",
      "prompt": "A futuristic, high-tech satellite view of the island of Anguilla at twilight. The island glows with cyan and gold digital connections, symbolizing the .ai domain network. In the upper atmosphere, a faint, constellation-like formation of 'The Three Dolphins' (curved shapes) watches over the island. In the foreground, large, floating holographic text reads \"AI NATIVE NATION\".",
      "alt": "Digital Anguilla Network",
      "baseFileName": "vision-intro",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "vision-crisis": {
      "path": "vision/",
      "prompt": "A split screen comparison. Left: A dry, parched landscape representing water scarcity. Right: A sleek, modern desalination plant powered by solar panels and surrounded by lush greenery. The text \"RESILIENCE\" is etched into the white stone wall of the plant. Cinematic lighting.",
      "alt": "Water Scarcity vs Solution",
      "baseFileName": "vision-crisis",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "capital-fund": {
      "path": "capital/",
      "prompt": "A conceptual visualization of a 'Digital Wealth Fund'. Streams of digital gold coins and binary code flow from a server rack into the physical foundation of a new, modern building made of white limestone. The text \"DIGITAL WEALTH FUND\" is displayed on a glass pane in the foreground. Crystalline salt structures are visible near the foundation.",
      "alt": "Digital Wealth Fund Concept",
      "baseFileName": "capital-fund",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "capital-cloud": {
      "path": "capital/",
      "prompt": "A hurricane-proof data center bunker in a tropical setting. The architecture is brutalist but softened with white aerogel and vertical gardens. It is covered in solar panels. Storm clouds gather in the distance, but the facility is secure and glowing warmly. The text \"SOVEREIGN CLOUD\" is mounted in metal lettering above the entrance.",
      "alt": "Sovereign Data Bunker",
      "baseFileName": "capital-cloud",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "citizenry-v2v": {
      "path": "citizenry/",
      "prompt": "A diverse group of Anguillians (students, fishermen, office workers) using tablets and holographic interfaces to build software in an open-air pavilion. In the background, traditional racing boats with white sails are visible on the turquoise sea. The text \"CITIZEN ARCHITECT\" floats holographically above the group.",
      "alt": "Citizen Architects",
      "baseFileName": "citizenry-v2v",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "citizenry-heritage": {
      "path": "citizenry/",
      "prompt": "An elderly Anguillian storyteller sitting under a tree, speaking to a futuristic AI interface that is visualizing their story as a golden hologram of a historic ship. The text \"CULTURAL HERITAGE\" is displayed in a stylish font near the hologram. Warm, nostalgic golden-hour lighting.",
      "alt": "Cultural Heritage AI",
      "baseFileName": "citizenry-heritage",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "state-frictionless": {
      "path": "state/",
      "prompt": "A close-up of a hand holding a transparent, high-tech mobile device. The screen displays a clean, minimalist app interface titled \"ANGUILLA KEY\" with icons for 'Vote', 'Land', and 'ID'. The background is a blurred, modern government office with white stone columns. The vibe is efficient and secure.",
      "alt": "Citizen SuperApp",
      "baseFileName": "state-frictionless",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "state-embassy": {
      "path": "state/",
      "prompt": "A server rack inside a high-security vault. The vault door is open, revealing a snowy landscape outside (representing a foreign location). Inside, the server glows with a warm Caribbean light. The text \"DATA EMBASSY\" is projected on the floor of the vault. The Anguilla flag is visible on the server.",
      "alt": "Data Embassy Vault",
      "baseFileName": "state-embassy",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "resilience-twin": {
      "path": "resilience/",
      "prompt": "A 3D holographic map of Anguilla hovering over a table. The map shows real-time water flow (blue lines) and energy grid (yellow lines). The map's base looks like organic mangrove roots. The text \"DIGITAL TWIN\" is displayed above the hologram. A hand is interacting with the data.",
      "alt": "Digital Twin Dashboard",
      "baseFileName": "resilience-twin",
      "fileExtension": ".webp",
      "imageCount": 1
    },
    "ask-pilot": {
      "path": "ask/",
      "prompt": "A futuristic roadmap graphic displayed on a glass wall. Three milestones are shown: 1. \"MICRO-PILOT\" (Highlighted and glowing), 2. \"INFRASTRUCTURE\", 3. \"NATION SCALE\". The background shows a sunrise over the ocean, symbolizing a new beginning.",
      "alt": "Micro-Pilot Roadmap",
      "baseFileName": "ask-pilot",
      "fileExtension": ".webp",
      "imageCount": 1
    }
  }
}
</file_artifact>

<file path="src/components/showcase/ShowcaseGame.tsx">
'use client';
import React, { useRef, useState } from 'react';
import { Button } from '@/components/ui/button';
import { FaSync } from 'react-icons/fa';

const ShowcaseGame = () => {
  const iframeRef = useRef<HTMLIFrameElement>(null);
  const [isGameLoading, setIsGameLoading] = useState(true);

  const handleRefresh = () => {
    if (iframeRef.current) {
      setIsGameLoading(true);
      // Resetting the src attribute is a safe way to force an iframe to reload its content
      // without running into cross-origin security issues.
      iframeRef.current.src = iframeRef.current.src;
    }
  };

  const handleIframeLoad = () => {
    setIsGameLoading(false);
  };

  return (
    <div className="relative w-full h-full flex flex-col items-center bg-background">
      <p className="text-sm text-muted-foreground my-2 p-2 border rounded-md bg-muted/50 max-w-4xl text-center mx-4">
        You are viewing an embedded version of AI Ascent. For the full experience, including login, chat, and multiplayer features, please visit the main site: {' '}
        <a href="https://aiascent.game/" target="_blank" rel="noopener noreferrer" className="text-primary underline">
          aiascent.game
        </a>.
      </p>
      <div className="relative w-full flex-grow border-t border-border">
        <div className="absolute top-2 right-2 z-10">
          <Button onClick={handleRefresh} variant="outline" size="icon" className="bg-background/80 backdrop-blur-sm">
            <FaSync className={isGameLoading ? 'animate-spin' : ''} />
          </Button>
        </div>
        <iframe
          ref={iframeRef}
          src="https://aiascent.game/"
          className="w-full h-full border-0"
          title="AI Ascent Game"
          onLoad={handleIframeLoad}
        ></iframe>
      </div>
    </div>
  );
};

export default ShowcaseGame;
</file_artifact>

<file path="src/app/showcase/[slug]/page.tsx">
'use client';
import React, { useEffect } from 'react';
import ReportViewer from '@/components/report-viewer/ReportViewer';
import ShowcaseGame from '@/components/showcase/ShowcaseGame';
import NextPageSection from '@/components/global/NextPageSection';
import { useReportStore } from '@/stores/reportStore';

export default function ShowcaseSlugPage({ params }: { params: { slug: string } }) {
  const { setAutoplay, setIsFullscreen } = useReportStore.getState();
  const slug = params.slug;

  // C8: Reset presentation defaults on mount to ensure "normal" viewing mode
  useEffect(() => {
    setIsFullscreen(false);
    setAutoplay(false);
  }, [slug, setAutoplay, setIsFullscreen]);

  const renderContent = () => {
    switch (slug) {
      case 'game':
        return <ShowcaseGame />;
      case 'anguilla':
        return <ReportViewer reportName="anguilla" />;
      case 'report':
      default:
        return <ReportViewer reportName="showcase" />;
    }
  };

  return (
    <div className="w-full pt-16 flex flex-col min-h-screen">
      <div className="h-[calc(100vh-4rem)] flex flex-col relative">
        {/* ProjectSelector moved to Header in C8 */}
        {renderContent()}
      </div>
      
      {/* Only show the NextPageSection if we are not in the game view */}
      {slug !== 'game' && (
          <NextPageSection
            title="Ready to Become a Citizen Architect?"
            description="The V2V Academy provides the pathway to master the skills of AI-assisted development and become a leader in the new digital frontier."
            buttonText="Explore the Academy"
            href="/academy"
            className="mt-0" // C8: Removed top margin to flush with viewer
          />
      )}
    </div>
  );
}
</file_artifact>

<file path="src/components/showcase/ProjectSelector.tsx">
'use client';
import React from 'react';
import { useRouter } from 'next/navigation';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { Button } from '@/components/ui/button';
import { FaChevronDown, FaProjectDiagram } from 'react-icons/fa';

interface ProjectSelectorProps {
  currentSlug: string;
}

const projects = [
  { id: 'report', label: 'The Ascent Report' },
  { id: 'anguilla', label: 'Anguilla Project' },
  { id: 'game', label: 'AI Ascent Game' },
];

const ProjectSelector: React.FC<ProjectSelectorProps> = ({ currentSlug }) => {
  const router = useRouter();
  const currentProject = projects.find(p => p.id === currentSlug) || projects[0];

  return (
    <div className="absolute top-2 left-2 z-50">
      <DropdownMenu>
        <DropdownMenuTrigger asChild>
          <Button variant="outline" size="sm" className="gap-2 bg-background/80 backdrop-blur-sm border-primary/50 hover:bg-accent shadow-md">
            <FaProjectDiagram className="text-primary" />
            <span className="font-semibold hidden sm:inline">{currentProject.label}</span>
            <span className="font-semibold sm:hidden">Projects</span>
            <FaChevronDown className="text-xs opacity-50" />
          </Button>
        </DropdownMenuTrigger>
        <DropdownMenuContent align="start" className="w-56">
          {projects.map((project) => (
            <DropdownMenuItem 
              key={project.id} 
              onClick={() => router.push(`/showcase/${project.id}`)}
              className={currentSlug === project.id ? 'bg-accent' : ''}
            >
              {project.label}
            </DropdownMenuItem>
          ))}
        </DropdownMenuContent>
      </DropdownMenu>
    </div>
  );
};

export default ProjectSelector;
</file_artifact>

<file path="src/app/anguilla/page.tsx">
'use client';
import React, { useEffect } from 'react';
import ReportViewer from '@/components/report-viewer/ReportViewer';
import { useReportStore } from '@/stores/reportStore';

export default function AnguillaPresentationPage() {
  const { setAutoplay, setIsFullscreen } = useReportStore.getState();

  useEffect(() => {
    // Force presentation mode behavior
    // Small timeout ensures component is mounted and store is ready
    const timer = setTimeout(() => {
        setIsFullscreen(true);
        setAutoplay(true);
    }, 100);
    
    // Cleanup: Ensure we exit these modes when navigating away
    return () => {
        clearTimeout(timer);
        setIsFullscreen(false);
        setAutoplay(false);
    };
  }, [setAutoplay, setIsFullscreen]);

  return (
    <div className="w-full h-screen bg-background overflow-hidden">
        <ReportViewer reportName="anguilla" />
    </div>
  );
}
</file_artifact>

