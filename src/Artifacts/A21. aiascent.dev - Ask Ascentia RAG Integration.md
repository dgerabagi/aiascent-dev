# Artifact A21: aiascent.dev - Ask Ascentia RAG Integration

# Date Created: C15

# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** A guide explaining the implementation of the Retrieval-Augmented Generation (RAG) system for the "Ask @Ascentia" chat feature, including instructions for file placement and environment configuration.
- **Tags:** documentation, rag, chat, ascentia, embeddings, faiss, langchain, architecture

## 1. Overview & Goal

The "Ask @Ascentia" chat feature is intended to act as an expert on "The Ascent Report." To achieve this, a simple proxy to a Large Language Model (LLM) is insufficient. The goal is to implement a Retrieval-Augmented Generation (RAG) system that allows Ascentia to ground its responses in the actual content of the report.

This document outlines the architecture of the RAG system and provides the necessary setup instructions for the curator.

## 2. RAG Architecture

The RAG system is implemented within the `/api/chat/route.ts` Next.js API route. It transforms the route from a simple proxy into an intelligent context-aware endpoint.

The workflow is as follows:
1.  **Receive Query:** The API receives a user's question and the `pageContext` (text from the current page the user is viewing).
2.  **Vectorize Query:** The backend sends the user's question to an embedding model endpoint to convert it into a vector representation.
3.  **Load Knowledge Base:** The backend loads a pre-computed FAISS vector index (`report_faiss.index`) and a corresponding text chunk map (`report_chunks.json`).
4.  **Similarity Search:** It performs a similarity search on the user's query vector against the FAISS index to find the most relevant text chunks from the entire report.
5.  **Construct Final Prompt:** It constructs a comprehensive prompt for the LLM, including:
    *   A system prompt defining Ascentia's persona and instructions.
    *   The relevant text chunks retrieved from the knowledge base.
    *   The `pageContext` sent from the client.
    *   The user's original question.
6.  **Proxy to LLM:** The final, context-rich prompt is streamed to the vLLM completion endpoint.
7.  **Stream Response:** The LLM's response is streamed back to the client.

## 3. Curator Setup Instructions

To enable this functionality, you must provide the knowledge base files and configure the necessary environment variables.

### 3.1. Embedding File Placement

The RAG system requires two files that represent the vectorized knowledge base of the report.

1.  **Create Directory:** In your project, create the following directory: `public/data/embeddings/`.
2.  **Place Files:** Copy your `report_faiss.index` and `report_chunks.json` files into this new directory. The chat API is hardcoded to load the knowledge base from this location.

### 3.2. Environment Variable Configuration

The backend needs to know the URL of the embedding model endpoint.

1.  **Edit `.env` file:** Open your `.env` or `.env.local` file.
2.  **Add `EMBEDDING_API_URL`:** Add a new variable that points to your embedding model's API endpoint. For a standard vLLM setup, this is often the same server as your completions endpoint, but with a different path.

**Example `.env` configuration:**
```
# URL for the Text-to-Speech server
TTS_SERVER_URL=http://192.168.1.85:8880/v1/audio/speech

# URL for the vLLM completions endpoint
REMOTE_LLM_URL=http://192.168.1.85:1234

# URL for the vLLM embeddings endpoint
EMBEDDING_API_URL=http://192.168.1.85:1234/v1/embeddings