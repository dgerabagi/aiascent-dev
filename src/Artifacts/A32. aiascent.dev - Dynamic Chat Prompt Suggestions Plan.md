# Artifact A32: aiascent.dev - Dynamic Chat Prompt Suggestions Plan

# Date Created: C35
# Author: AI Model & Curator

- **Key/Value for A0:**
- **Description:** Outlines the technical implementation for generating, parsing, and displaying dynamic, context-aware follow-up questions ("chips") in the Ask @Ascentia chat interface.
- **Tags:** plan, chat, ui, ux, llm, prompt engineering, ascentia

## 1. Overview and Goal

To improve user engagement and guide the conversation within the "Ask @Ascentia" feature, we will implement dynamic prompt suggestions. These will appear as clickable "chips" below the chat history.

*   **Default State:** When the chat is empty or cleared, default prompts will be shown to help the user get started.
*   **Dynamic State:** After every response from Ascentia, the LLM will generate 2-4 relevant follow-up questions based on the conversation context.
*   **Interaction:** Clicking a chip automatically submits that question as a user message. Hovering over a chip shows the full text of the prompt in a tooltip.

## 2. Technical Implementation

### 2.1. Backend: Prompt Engineering (`/api/chat/route.ts`)

The system prompts for Ascentia (defined in `A27` and implemented in the API route) must be updated. We will instruct the LLM to output suggestions at the very end of its response in a structured, machine-parseable format.

To ensure reliable parsing from a streaming response, we will use distinct delimiters.

**Updated Instruction to LLM:**
> "Finally, after your main response, generate 2-4 short, relevant follow-up questions the user might want to ask next based on this conversation. Output them strictly as a JSON array of strings wrapped in specific delimiters like this: `:::suggestions:::["Question 1?", "Question 2?"]:::end_suggestions:::`."

### 2.2. State Management (`src/stores/reportStore.ts`)

The `ReportState` needs to store the current set of suggested prompts.

*   **New State:** `suggestedPrompts: string[]`
*   **Default Value:** `['How does DCE work?', 'How do I install DCE?']`
*   **New Action:** `setSuggestedPrompts(prompts: string[])`
*   **Reset Logic:** When chat history is cleared via `clearReportChatHistory`, the `suggestedPrompts` should be reset to the default value.

### 2.3. Frontend: Parsing and UI (`ReportChatPanel.tsx`)

The frontend component handles parsing the stream and rendering the UI.

1.  **Parsing:** As the response streams in, or once it completes, the frontend must detect the `:::suggestions:::` block.
    *   Extract the JSON array string.
    *   Parse it into a string array.
    *   Call `setSuggestedPrompts` to update the store.
    *   **Crucially**, remove the entire `:::suggestions:::...:::end_suggestions:::` block from the message text before rendering it to the user, so they never see the raw data structure.

2.  **UI Rendering:**
    *   Create a new container below the chat history but above the input textarea.
    *   Map through the `suggestedPrompts` array.
    *   Render each prompt as a compact button or badge. CSS text-overflow with ellipsis should be used to keep them small.
    *   Use a standard HTML `title` attribute or a UI Tooltip component to show the full prompt text on hover.

3.  **Interaction:**
    *   Add an `onClick` handler to each chip that calls the existing `handleSend` logic with the chip's text payload.

## 3. User Experience Flow

1.  User opens chat. Sees default chips: "How does DCE work?", "How do I install DCE?".
2.  User clicks "How does DCE work?".
3.  The question is added to chat history.
4.  Ascentia streams a response explaining DCE.
5.  Hidden at the end of the stream is: `:::suggestions:::["What is context curation?", "Tell me about the workflow."]::end_suggestions:::`.
6.  Frontend parses this, hides it from the chat bubble, and updates the chips.
7.  User now sees new chips: "What is context curation?", "Tell me about the workflow.".