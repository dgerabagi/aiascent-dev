# Artifact A32: aiascent.dev - Dynamic Chat Prompt Suggestions Plan

# Date Created: C35
# Author: AI Model & Curator

# Updated on: C45 (Add multi-report context isolation and page-by-page generation strategy)

- **Key/Value for A0:**
- **Description:** Outlines the technical implementation for generating, parsing, and displaying dynamic, context-aware follow-up questions ("chips") in the Ask @Ascentia chat interface.
- **Tags:** plan, chat, ui, ux, llm, prompt engineering, ascentia

## 1. Overview and Goal

To improve user engagement and guide the conversation within the "Ask @Ascentia" feature, we will implement dynamic prompt suggestions. These will appear as clickable "chips" below the chat history.

*   **Page-Specific Generation:** When a user navigates to a new page within a report, a request is made to the backend to generate suggestions based on that specific page's content. This ensures suggestions are relevant from the very first interaction.
*   **Dynamic Generation:** After every response from Ascentia, the LLM will also generate 2-4 relevant follow-up questions based on the conversation context.
*   **Interaction:** Clicking a chip automatically submits that question as a user message.

## 2. Technical Implementation

### 2.1. Backend: Dual-Mode API (`/api/chat/route.ts`)

The chat API route supports two modes for handling suggestions:

1.  **`task: 'generate_suggestions'`:** A specialized, non-streaming mode for pre-generating suggestions.
    *   **Trigger:** Called by the frontend when a new report page is loaded.
    *   **Input:** Receives only the `pageContext`.
    *   **Prompt:** Uses a dedicated system prompt that instructs the LLM to *only* generate a JSON array of questions based on the provided text.
    *   **Output:** Returns a clean JSON array `["Question 1?", "Question 2?"]`.
    *   **Robustness:** The backend parsing logic must be resilient to minor LLM formatting errors, extracting the JSON array even if it's embedded in other text.

2.  **Standard Chat Mode:**
    *   **Trigger:** Called for a normal user chat query.
    *   **Prompt Engineering:** The main system prompts are updated to instruct the LLM to append suggestions to the end of its response in a structured, machine-parseable format, using distinct delimiters.
    *   **Updated Instruction:** "Finally, after your main response, generate 2-4 short, relevant follow-up questions... Output them strictly as a JSON array of strings wrapped in specific delimiters: `:::suggestions:::[\"Question 1?\", \"Question 2?\"]:::end_suggestions:::`."

### 2.2. State Management and Context Isolation (`src/stores/reportStore.ts`)

The `ReportState` is the source of truth for suggestions and their status.

*   **State:**
    *   `suggestedPrompts: string[]`: Stores the current list of suggestions.
    *   `suggestionsStatus: 'idle' | 'loading' | 'error'`: Tracks the status of the on-demand suggestion fetching.
    *   `reportName: string | null`: Tracks the currently active report (`'whitepaper'` or `'showcase'`). This is critical for context isolation.
*   **Actions:**
    *   `loadReport(reportName)`: **CRITICAL:** This action must completely reset the entire report state, including `suggestedPrompts` and `suggestionsStatus`, to their initial defaults *before* fetching new data. This prevents state from one report leaking to another.
    *   `fetchAndSetSuggestions(page, reportName)`:
        *   Sets `suggestionsStatus` to `'loading'`.
        *   Calls the backend API with `task: 'generate_suggestions'`.
        *   **Race Condition Prevention:** Before updating the state with the fetched suggestions, it must check if the `reportName` passed to it still matches the *current* `reportName` in the store. If they don't match (i.e., the user has already navigated to a different report), the action must abort and not update the state.
        *   On failure, it sets `suggestionsStatus` to `'error'` and populates `suggestedPrompts` with the correct default questions for the current `reportName`.

### 2.3. Frontend: UI and Logic (`ReportViewer.tsx`, `ReportChatPanel.tsx`)

1.  **Triggering Suggestions (`ReportViewer.tsx`):**
    *   A `useEffect` hook listens for changes to `currentPageIndex`.
    *   When the page changes, it calls the `fetchAndSetSuggestions` action, providing the new page's content and the current `reportName`.

2.  **UI Rendering (`ReportChatPanel.tsx`):**
    *   A new container below the chat history renders the suggestions.
    *   It observes `suggestionsStatus`:
        *   If `'loading'`, it displays a "Generating suggestions..." message or spinner.
        *   If `'idle'` or `'error'`, it maps through the `suggestedPrompts` array and renders each as a `Badge` component.
    *   **Styling:** The `Badge` components should use word-wrapping and have a maximum width to handle longer questions gracefully.

3.  **Parsing In-Chat Suggestions (`ReportChatPanel.tsx`):**
    *   After a normal chat response is fully streamed, the `sendMessage` function must perform a robust search for the `:::suggestions:::` block.
    *   It extracts and parses the JSON content, calls `setSuggestedPrompts`, and then strips the entire block from the message before saving the final, clean content to the chat history.